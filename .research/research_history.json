{
  "research_topic": "Automated prompt optimization through iterative refinement and performance feedback",
  "queries": [
    "automatic prompt tuning",
    "iterative prompt refinement",
    "feedback-driven prompting"
  ],
  "research_study_list": [
    {
      "title": "DePT: Decoupled Prompt Tuning",
      "full_text": "Robust Causal Bandits for Linear Models Zirui Yan Arpan Mukherjee Burak Varıcı Ali Tajer * Abstract Sequential design of experiments for optimizing a reward function in causal systems can be effectively modeled by the sequential design of interventions in causal bandits (CBs). In the existing literature on CBs, a critical assumption is that the causal models remain constant over time. However, this assumption does not necessarily hold in complex systems, which constantly undergo temporal model fluctuations. This paper addresses the robustness of CBs to such model fluctuations. The focus is on causal systems with linear structural equation models (SEMs). The SEMs and the time-varying pre- and post-interventional statistical models are all unknown. Cumulative regret is adopted as the design criteria, based on which the objective is to design a sequence of interventions that incur the smallest cumulative regret with respect to an oracle aware of the entire causal model and its fluctuations. First, it is established that the existing approaches fail to maintain regret sub-linearity with even a few instances of model deviation. Specifically, when the number of instances with model deviation is as few asT 1 2L , where T is the time horizon and L is the length of longest causal path in the graph, the existing algorithms will have linear regret in T. For instance, when T = 105 and L = 3, model deviations in 6 out of 105 instances result in a linear regret. Next, a robust CB algorithm is designed, and its regret is analyzed, where upper and information-theoretic lower bounds on the regret are established. Specifically, in a graph with N nodes and maximum degree d, under a general measure of model deviation C, the cumulative regret is upper bounded by ˜O(dL−1 2 ( √ NT + NC )) and lower bounded by Ω(d L 2 −2 max{ √ T , d2C}). Comparing these bounds establishes that the proposed algorithm achieves nearly optimal ˜O( √ T) regret when C is o( √ T) and maintains sub-linear regret for a broader range of C. 1 Motivation and Overview Causal bandits provide a rich framework to formalize and analyze the sequential experimental design in causal networks. Such design problems appear in applications that involve a network of interacting components that can causally influence one another. Examples include design of experiments in robotics [1], gene expression networks [2], drug discovery [3], and recommendation systems [ 4]. In causal systems, interventions are experimental mechanisms that facilitate uncovering the cause-effect relationships in causal networks and distinguishing them from the conventional association measures [5]. Sequential design of interventions has the key advantage of bringing data-adaptivity in designing the interventions, resulting in an overall reduced *The authors are with the Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180. 1 arXiv:2310.19794v2  [stat.ML]  4 Mar 2024experiment cost and a faster process for forming inferential decisions. For instance, reprogramming the cell via gene perturbation experiments needs a careful design of sequential interventions [6] such that the outcome of one experiment guides the design of the subsequent ones. Causal bandits (CBs) provide a theoretically principled way of sequentially designing interventions to identify one that maximizes a utility for the causal network. Specifically, the canonical model for this utility is a function of the observations obtained from the DAG. The utility is chosen as the average value of a leaf node, which we denote by thereward node. This is a model that is widely used in the causal bandit literature. Accordingly, each intervention mechanism is modeled by an arm, the value of the reward node under an intervention is called the reward, and the sequential selection of the interventions is abstracted as arm selection decisions. The extent of information available about the causal model critically influences the design of CB algorithms. Broadly, there are two central pieces of information: the causal structure (topology) and the data’s pre- and post-intervention statistical models. Depending on the availability of each of these two pieces, there are four possible CB settings. Designing CB algorithms and their improvement over standard bandit algorithms was first demonstrated in [7, 8] for the settings where both the structure and interventional distributions are fully specified. Subsequently, the studies transitioned to more realistic settings and explored how to accommodate unknown structure and distribution information [ 7, 9, 10], (approximately) known distributions but unknown structure [11, 12], and known structure and unknown distributions [13, 14, 15]. Motivation. We investigate CBs from a new perspective. The existing studies all focus on having a fixed causal model over time , which applies to both directions in which the models are known or unknown. In reality, however, large complex causal systems undergo model fluctuations caused by a wide range of reasons such as non-stationarity in the system or heterogeneous data [ 16, 17], measurement errors [ 18], selection bias [19], and missing data [20]. Temporal model fluctuations can change the causal structure or statistical models. For instance, in drug discovery, there are multiple observable variables or representation nodes [21], and the model fluctuations due to measurement errors can occur in both the observable variables or their weights to the representation nodes. However, the algorithm for time-invariant settings can be highly susceptible to model fluctuations. For instance, the CB algorithm in [ 15] enjoys a nearly optimal growth in the horizon T, i.e., O( √ T). Nevertheless, it will lose the optimal rate with even minuscule instances of model deviation. More specifically, as we will show in Section 5.3, the regret becomes linear in T if the system experiences model deviations in T 1 2L instances, where L is the longest causal path in the graph. For even small values of L, T 1 2L will be an extremely small fraction of the instances. For instance, when T = 105 and L = 3, model deviations in 6 out of 105 instances result in a linear regret. Objectives. We pursue four objectives. (1) Under relevant measures of model fluctuations, we design a robust CB algorithm to model deviations over time. (2) We characterize almost matching upper and lower bounds on the regret as a function of model deviation level, time horizon, graph parameters, and the size of the cardinality of the intervention space. (3) We analytically assess the robustness of the relevant existing algorithms, establishing their lack of robustness to model fluctuations. (4) We consider a general intervention setting in which each subset of nodes in the graph is intervenable, and each intervention induces a distinct reward distribution, resulting in an intervention space that grows exponentially with the size of the graph. 2As the final objective, we show that our algorithm circumvents the exponential growth of the achievable regret with the cardinality of the intervention space and breaks it down to linear growth. Our focus is on the causal graphs that are specified by a linear structural equation model (SEM). We assume that the structure (topology) is fixed but the statistical models undergo temporal fluctuations. Contribution & key observations. We design a novel weighting methodology for linear regression that takes advantage of the weighted exploration bounce. This approach enables us to accurately accommodate the impact of model deviations in our regret analysis. Based on that, we characterize novel time-uniform confidence ellipsoid models for robust linear regression, which may be of broader interest in robust linear bandits. Furthermore, we propose a robust CB algorithm and analyze the compounding effect under the novel confidence ellipsoids, which offers insights into the behavior of our regret bound. When considering a known budget of C that captures the level of model deviations over time, the achievable regret is ˜O(dL−1 2 ( √ NT + NC )), where N and d are the number of nodes and maximum degree in the graph, respectively, and L is the length of the largest causal path. Compared to the established lower bound Ω(d L 2 −2 max{ √ T , d2C}), we observe that both bounds scale polynomially in d and exponentially in L. Furthermore, both bounds scale with √ T when the model faces C = o( √ T) and linear in C when C = Ω( √ T). This indicates that our algorithm enjoys nearly optimal regret when C = o( √ T), and it maintains a sub-linear regret when the aggregate model deviation is sub-linear, which is the best possible regret order that any algorithm can achieve. The cost incurred to maintain such robustness is that the regret grows linearly with the deviation amount. Related literature. The earlier studies on CB algorithms assume that both the graph structure and the interventional distributions are known (fully or partially) [22, 23]. More recent studies dispense with one or both of the assumptions. Despite their discrepancies in model and objectives, the common theme in all these studies is that they assumed a fixed causal model. Among the related work that does not make either assumption, [ 9] incorporates causal learning algorithms to CBs but does not improve upon regret of non-causal bandit algorithms; [ 7] focuses on atomic interventions; [ 11] achieves a regret bound that scales with the cardinality of the intervention space; and [14] focuses on binary random variables. In a different direction, there exist studies that assume that the graph structure is known while the distributions are unknown. The relevant literature includes [ 13, 14], which focuses on binary random variables. More recently, [15] focuses on linear systems and generalizes the results to the soft intervention settings, continuous random variables, and arbitrarily large intervention spaces. In parallel, [24] uses soft interventions and generalizes to non-linear models but limits to the Gaussian process SEMs in reproducing kernel Hilbert space (RKHS) and intervention space on controllable action variables. Finally, we note that even though we focus on linear SEMs, we observe that our reward is a non-linear function of the unknown parameters. Hence, our CB model fundamentally differs from linear bandits. This is the case even in the CB settings with a fixed model [15]. Nevertheless, we briefly comment on the literature on linear bandits with model misspecification or contamination. These studies assume fixed (permanent) deviation, whereas, in our setting, the deviations can vary over time [25, 26, 27, 28]. Furthermore, in linear bandits with contamination, the contamination is imposed on the observed rewards [29, 30, 31, 32, 33, 34], whereas we are focusing on model deviation. 3Notations. For N ∈ Z+, we define the set [N] ≜ {1, ··· , N}. The Euclidean norm of a vector X ∈ RN is denoted by ∥X∥. For a subset S ⊆[N], we define XS ≜ X ⊙ 1(S), where ⊙ denotes the Hadamard product and the vector 1(S) ∈ {0, 1}N has 1s at the indices corresponding to S. We denote the i-th column of matrix A ∈ Rm×n by [A]i, and the entry at i-th row and j-th column by [A]i,j. The spectral norm of a matrix is denoted as ∥A∥. We further define the A-norm for positive semidefinite matrix A as ∥X∥A = √ X⊤AX. 2 Causal Bandit Model Causal graphical model. Consider a directed acyclic graph (DAG) denoted by G(V, E), where V = [N] denotes the set of nodes, and E denotes the set of edges, where the ordered tuple (i, j) ∈ E, indicates that there is a directed edge from i to j. Each node i ∈ [N] is associated with a random variable Xi. Accordingly, we define the vector X ≜ [X1, ··· , XN ]⊤. We consider a linear SEM, according to which X = B⊤X + ϵ , (1) where B ∈ RN×N is a strictly upper triangular edge weight matrix, and ϵ ≜ (ϵ1, ··· , ϵN )⊤ denotes the exogenous noise variables, with a known mean ν ≜ E[ϵ]. The noise vector ϵ is 1-sub-Gaussian, and its Euclidean norm is upper bounded by ∥ϵ∥ ≤mϵ. The graph’s structure is assumed to be known, while the weight matrix B associated with the graph is unknown. For any node i ∈ [N], we denote the set of parents of i by pa(i). We denote the maximum in-degree of the graph by d ≜ maxi{|pa(i)|} and the length of the longest directed path in the graph by L. Intervention model. We consider soft interventions on the graph nodes. A soft intervention on node i ∈ [N] alters the conditional distribution of Xi given its parents Xpa(i), i.e., P(Xi|Xpa(i)). An intervention can be applied to a subset of nodes simultaneously. If node i ∈ Vis intervened, the impact of the intervention is a change in the weights of the edges incident on node i. These weights are embedded in [B]i, i.e., the i-th column of B. We denote the post-intervention weight values by [B∗]i ̸= [B]i. Accordingly, corresponding to the interventional weights, we define the interventional weight matrix B∗, which is composed of the columns {[B∗]i : i ∈ [N]}. Note that soft interventions subsume commonly used stochastic hard interventions in which a hard intervention on node i sets [B∗]i = 0. Since we allow any arbitrary combination of nodes to be selected for concurrent intervention, there exist 2N interventional actions to choose from. We define A ≜ 2V as the set of all possible interventions, i.e., all possible subsets of [N]. For any intervention a ∈ A, we define the post-intervention weight matrix Ba such that columns corresponding to the non-intervened nodes retain their observational values from B, and the columns corresponding to the intervened nodes change to their new interventional values from B∗. The columns of Ba are specified by [Ba]i ≜ [B]i · 1 {i /∈ a} + [B∗]i · 1 {i ∈ a} , (2) where 1 denotes the indicator function. The interventions change the probability models of X. We define Pa as the probability measure of X under intervention a ∈ A. For any given B and B∗ we assume that 4∥[Ba]i∥ ≤mB. Without loss of generality, we assume mB = 1. Due to the boundedness of noise ϵ and column of Ba matrices, there exists m ∈ R+ such that ∥X∥ ≤m. Causal bandit model. Our objective is the sequential design of interventions. The set of possible interventions can be modeled as a multi-armed bandit setting with 2N arms, one arm corresponding to each possible intervention. Following the canonical CB model [ 8, 22], we designate node N (i.e., the node without a descendant) as the reward node. Accordingly, XN specifies the reward value. We denote the expected reward collected under intervention a ∈ Aby µa ≜ Ea[XN ] , (3) where Ea denotes expectation under Pa. We denote the intervention that yields the highest average reward by a∗ ≜ arg maxa∈A µa; denote the sequence of interventions by {a(t) ∈ A: t ∈ N}; and denote the data generated at time t and under intervention a(t) by X(t) = [X1(t), ··· , XN (t)]⊤. The learner’s goal is to minimize the average cumulative regret over the time horizonT with respect to the reward accumulated by an oracle aware of the systems model, interventional distributions, and model fluctuations. We define the expected accumulated regret as E[R(T)] ≜ T µa∗ − TX t=1 E[XN (t)] . (4) 3 Temporal Model Fluctuations Due to the size and complexity of the graphical models that represent complex systems, assuming that the observational and interventional models B and B∗ remain unchanged over time is a strong assumption. These models can undergo temporal variations due to various reasons, such as model misspecifications, stochastic behavior of the system, and adversarial influences. To account for such variations, we refer to Ba(t) as the nominal model of the graph at time t and denote the actual time-varying unknown model by Da(t). Accordingly, we define the deviation of the actual model from the nominal model by ∆a(t) ≜ Da(t) − Ba(t) . (5) To quantify the impact of model deviations on the regret R(T), we specify two measures that capture the extent of deviations. The first measure captures the maximum number of times each node deviates from the nominal model. The second measure provides a budget for the maximum deviation in the linear model that model deviations can inflict over time. Clearly, if the model of node i undergoes deviation at time t under intervention a, we have \r\r\u0002 ∆a(t) \u0003 i \r\r ̸= 0. Measure 1: Deviation Frequency (DF). This measure accounts for how frequently each node’s model can deviate from its nominal model, and over a horizon T it is defined as CDF ≜ max i∈[N] TX t=1 max a(t)∈A 1 n\r\r\u0002 ∆a(t) \u0003 i \r\r\r ̸= 0 \t . (6) 5This model is adopted from misspecified bandit literature [ 26]. To avoid unbounded deviations, we assume that the deviation inflicted on each node at any given time is bounded by a constant mc ∈ R+, i.e., max i∈[N] max t∈[T] max a(t)∈A \r\r\u0002 ∆a(t) \u0003 i \r\r ≤ mc . (7) Measure 2: Aggregate Deviation (AD). This measure quantifies the aggregate deviation over time. Specifi- cally, we define the maximum aggregate deviation as CAD ≜ max i∈[N] TX t=1 max a(t)∈A ∥[∆(t)]i∥ . (8) This measure of deviation is also standard in stochastic bandits [34], where the deviation budget is defined as the maximum deviation in the reward that the adversary can inflict over time. We will observe that CDF and CAD impact the regret results similarly. Hence, to unify the results and present them in a way that applies to both measures, we use C to represent the level of model deviation. For measure 1, we define C as the product of a constant factor mc and CDF, while for measure 2, we define C as CAD. We assume that the model deviation budgets specified by C are known to the learner, allowing the CB algorithm to adapt to the varying levels of model deviation. 4 Robust-LCB Algorithm In this section, we present the details of our algorithm and provide the performance guarantee (regret analysis) in Section 5. We also provide theoretical comparisons to the existing algorithms designed for CBs with fixed models, establishing their lack of robustness against model variations. Algorithm overview. Identifying the best intervention hinges on determining which of the distributions {Pa : a ∈ A}maximizes the expected reward. Nevertheless, these2N distributions are unknown. Therefore, a direct approach entails estimating these probability distributions, the complexity of which grows exponentially with N. To circumvent this, we leverage the fact that specifying these distributions has redundancies since all depend on the observational and interventional matrices B and B∗. These matrices can be fully specified by 2Nd scalars, where d is the maximum degree of the causal graph. Hence, at its core, our proposed approach aims to estimate these two matrices. We design an algorithm that has two intertwined key objectives. One pertains to therobust estimation of matrices B and B∗ when the observations are generated by the non-nominal models. For this purpose, we design a weighted ordinary least squares (W-OLS) estimator. The structure of the estimator and the associated confidence ellipsoids for the estimates are designed to circumvent model deviations effectively. The second objective is designing a decision rule for the sequential selection of the interventions over time. This sequential selection, naturally, is modeled as a multi-armed bandit problem. Therefore, we design an upper confidence bound (UCB)–based algorithm for the sequential selection of the interventions over time. Next, we present the details of the Robust Linear Causal Bandit (Robust-LCB). The steps involved in this algorithm are summarized in Algorithm 1. 6Algorithm 1 Robust-LCB 1: Input: Horizon T, causal graph G, action set A, mean noise vector ν, deviation budget C 2: Initialization: Initialize B(0) = B∗(0) = 0N×N and Vi(0) = V∗ i (0) = IN , ∀i ∈ [N]. 3: for t = 1, 2, . . . , Tdo 4: Compute UCBa(t) according to (21) for a ∈ A. 5: Pull a(t) = arg maxa∈A UCBa(t) and observe X(t)=( X1(t), . . . , XN (t))⊤. 6: for i ∈ {1, . . . , N} do 7: Set wi(t) as (19), update [B(t)]i according to (9) and update [B∗(t)]i according to (10). 8: end for 9: end for Countering model deviations. Our approach to circumventing model deviations is to identify and filter out the samples generated by the non-nominal models. We refer to these samples as outlier samples. This facilitates forming estimates for B and B∗ based on the samples generated by the nominal models. Since the model deviations may happen on multiple nodes simultaneously, the Robust-LCB is designed to identify the nodes undergoing deviation over time and discard the outlier samples generated by these nodes. Such filtration is implemented via assigning time-varying and data-adaptive weights to different nodes such that the weight assigned to node i ∈ [N] at time t ∈ N balance two factors: the probability of node i ∈ [N] undergoing deviation at t and the contribution of that sample to the estimator. These weights, subsequently, control how the samples from different nodes contribute to estimating B and B∗. Robust estimation. We design the weighted ordinary least squares (OLS) estimators for B and B∗, which at time t ∈ N are denoted by B(t) and B∗(t), respectively. To estimate the observational weights[B]i, we use the samples from instances at which node i is not intervened. Conversely, to estimate the interventional weights [B∗]i, we use the samples from the instances at which node i is intervened. By defining {wi(t) ∈ R+ : i ∈ [N]} as the set of weights assigned to the nodes at time t ∈ N, i-th columns of these estimates are specified as follows. [B(t)]i ≜ [Vi(t)]−1 X s∈[t]:i/∈a(s) wi(s)Xpa(i)(s)(Xi(s) − νi) , (9) [B∗(t)]i ≜ [V∗ i (t)]−1 X s∈[t]:i∈a(s) wi(s)Xpa(i)(s)(Xi(s) − νi) , (10) where we have defined the weighted Gram matrices as Vi(t) ≜ X s∈[t]:i/∈a(s) wi(s)Xpa(i)(s)X⊤ pa(i)(s) + IN , (11) V∗ i (t) ≜ X s∈[t]:i∈a(s) wi(s)Xpa(i)(s)X⊤ pa(i)(s) + IN . (12) Furthermore, we define the matrices associated with the squared weights as eVi(t) ≜ X s∈[t]:i/∈a(s) w2 i (s)Xpa(i)(s)X⊤ pa(i)(s) + IN , (13) 7eV∗ i (t) ≜ X s∈[t]:i∈a(s) w2 i (s)Xpa(i)(s)X⊤ pa(i)(s) + IN . (14) Similarly to (2), we denote the relevant and Gram matrices for node i under intervention a ∈ Aby eVi,a(t) ≜ 1 {i ∈ a}eV∗ i (t) + 1 {i /∈ a}eVi(t) . (15) Confidence ellipsoids. After performing estimation in each round, we construct the confidence ellipsoids for the OLS estimators {Ci(t) : i ∈ [N]} for the observational weights and {C∗ i (t) : i ∈ [N]} for the interventional weights Ci(t) ≜ n θ ∈ B1 : ∥θ − [B(t − 1)]i∥Vi(t−1)[ eVi(t−1)]−1Vi(t−1) ≤ βt o , (16) C∗ i (t) ≜ n θ ∈ B1 : ∥θ − [B∗(t − 1)]i∥V∗ i (t−1)[ eV∗ i (t−1)]−1V∗ i (t−1) ≤ βt o , (17) where B1 is the unit ball in RN and {βt ∈ R+, t∈ N} is a sequence of non-decreasing confidence radii that control the size of the confidence ellipsoids, which we will specify. Accordingly, we define the relevant confidence ellipsoid for node i under intervention a ∈ Aas Ci,a(t) ≜ 1 {i ∈ a} C∗ i (t) + 1 {i /∈ a} Ci(t) . (18) Weight designs. Designing the weights {wi(t) : i ∈ [N]} at time t is instrumental in effectively winnowing out the outlier samples. We select the weights that bring the confidence radius βt down to nearly constant wi(t) ≜ min \u001a 1 C , 1 C \r\rXpa(i)(t) \r\r [ eVi,a(t)(t)]−1 \u001b , (19) where the weights are inversely proportional to the norm ∥Xpa(i)(t)∥[ eVi,a(t)(t)]−1 and deviation budget C , and they are truncated at 1/C, which ensures that the weights are not arbitrarily large. We refer the term ∥Xpa(i)(t)∥[ eVi,a(t)(t)]−1 as weighted exploration bonus. A higher exploration bonus means lower confidence in the sample. Setting the weights as the inverse of the exploration bonus avoids potentially significant regret caused by both the stochastic noise and model deviations. We scale the weights proportional to 1/C to use smaller weights when the model deviation level is higher. Intervention selection. We adopt a UCB-based rule for sequentially selecting the interventions. Specifically, at each time t, our algorithm selects the intervention that maximizes a UCB, defined as the maximum value of expected reward when the edge weights are in the confidence ellipsoids {Ci,a(t), i∈ [N]}, under that intervention. Recall the expected reward µa for a ∈ Adefined in (3) is a function of the edge weights Ba, which can be decomposed according to the following lemma. Lemma 1. [15, Lemma 1] Consider a linear SEM G(V, E) with intervention a ∈ 2V, whose weight matrix is denoted by Ba ∈ RN×N . Furthermore, consider the function f(A) ≜ PL ℓ=0[Aℓ]N for A ∈ RN×N , where we denote Aℓ as the ℓ-th power of matrix A. We have µa = ⟨f(Ba), ν⟩ , (20) where ν = (ν1, ··· , νN ), and νi = E[ϵi] denotes the mean of the noise vector corresponding to nodei ∈ [N]. 8Thus, for any intervention a ∈ A, the UCB is naturally defined as UCBa(t) ≜ max {∀i∈[N]:[Θ]i∈Ci,a(t)} ⟨f(Θ), ν⟩ . (21) Based on the UCB in (21), at time t, our algorithm selects the intervention that maximizes the UCB, a(t) = arg max a∈A UCBa(t) . (22) 5 Regret Analysis In this section, we present the performance guarantees for the proposed Robust-LCB algorithm. We first provide the upper bound on the average cumulative regret in Section 5.1. We also establish a minimax lower bound in Section 5.2 that shows the tightness of our upper bound. By comparing our regret with that of LinSEM-UCB in Section 5.3, we evaluate the robustness of our algorithm. 5.1 Regret Upper Bound In order to derive the upper bound, we begin by providing a concentration bound for the W-OLS estimator. Notably, we investigate a vector norm that differs from existing work in robust bandits. This norm was first investigated in [35] under the non-stationary setting, and our investigation builds on this to provide novel insights into the robust behavior of the W-OLS. Lemma 2 (Estimator concentration). Under a deviation budget C, with a probability at least 1 − 2δ, for any node i ∈ [N] and t ≥ 0, we have ∥[B(t)]i − Bi∥Vi(t)[ eVi(t)]−1Vi(t) ≤ βt(δ) , (23) and ∥[B∗(t)]i − B∗ i ∥V∗ i (t)[ eV∗ i (t)]−1V∗ i (t) ≤ βt(δ) , (24) where we have defined βt(δ) ≜ p 2 log (1/δ) + d log (1 +m2t/dC2) + 1 +m . Proof. We will provide the proof corresponding to the observational weights [B(t)]i, while the proof for the interventional weights [B∗(t)]i follows similarly. For any node i ∈ [N], we decompose the error in estimation ∥[B(t)]i − Bi∥Vi(t)[ eVi(t)]−1Vi(t) for t ≥ 0 as follows. ∥[B(t)]i − [B]i∥Vi(t)[ eVi(t)]−1Vi(t) (25) = \r\r\r\r[Vi(t)]−1 X s∈[t],i/∈a(t) wi(s)Xpa(i)(s) h X⊤ pa(i)(s)[D(t)]i + ϵi(s) − νi i − [B]i \r\r\r\r Vi(t)[ eVi(t)]−1Vi(t) (26) ≤ \r\r\r\r[bB(t)]i − [B]i \r\r\r\r Vi(t)[ eVi(t)]−1Vi(t) | {z } I1: Stochastic and regularization error + \r\r\r\r\r\r X s∈[t],i/∈a(t) Xpa(i)(s)X⊤ pa(i)(s)[∆(s)]i \r\r\r\r\r\r [ eVi(t)]−1 | {z } I2: Fluctuation error . (27) 9where bB(t) refers to the auxiliary estimators which correspond to the ridge regression estimator but with the removal of deviation’s impact on the output, i.e., [bB(t)]i = [Vi(t)]−1 X s∈[t],i/∈a(t) wi(s)Xpa(i)(s) h X⊤ pa(i)(s)[B]i + ϵi(s) − νi i . (28) The stochastic and regularization errors can be bounded by the following lemma. Lemma 3. For all node i ∈ [N], with probability at least 1 − δ, ∀t ≥ 0, we have I1 = \r\r\r[bB(t)]i − [B]i \r\r\r Vi(t)[ eVi(t)]−1Vi(t) (29) ≤ 1 + vuut2 log(1/δ) + d log   1 + m2 P s∈[t−1],i/∈a(t) w2 i (s) d ! . (30) Proof. Note that the weights wi(t) are predictable, i.e., Fi(t − 1) measurable, if the σ-algebra is defined as Fi(t) = σ(Xpa(i)(1), ϵi(1), Xpa(i)(2), ϵi(2), ··· , Xpa(i)(t), ϵi(t), Xpa(i)(t + 1))similarly to the one used in [36]. This modification of the filtration allows weights to depend on the current value of Xpa(i)(t). Then the lemma results from [35, Theorem 1] with µt = λt = 1. Since wi(s) ≤ 1 C for all s ∈ [t] and i ∈ [N], we can further upper bound the stochastic and regularization error as follows. I1 ≤ 1 + s 2 log(1/δ) + d log \u0012 1 + m2t dC2 \u0013 . (31) Now we need to bound the fluctuation error I2, which is bounded as follows. I2 = \r\r\r\r\r\r [ eVi(t)]−1/2 X s∈[t],i/∈a(t) wi(s)Xpa(i)(s)X⊤ pa(i)(s)[∆B(s)]i \r\r\r\r\r\r (32) ≤ X s∈[t],i/∈a(t) wi(s) \r\r\r[ eVi(t)]−1/2Xpa(i)(s)X⊤ pa(i)(s)[∆B(s)]i \r\r\r (33) = X s∈[t],i/∈a(t) wi(s) \r\r\r[ eVi(t)]−1/2Xpa(i)(s) \r\r\r \f\f\fX⊤ pa(i)(s)[∆B(s)]i \f\f\f (34) ≤ m X s∈[t],i/∈a(t) wi(s) ∥∆B(s)]i∥ \r\rXpa(i)(s) \r\r [ eVi(t)]−1 (35) ≤ m X s∈[t],i/∈a(t) wi(s) ∥∆B(s)]i∥ \r\rXpa(i)(s) \r\r [ eVi(s)]−1 (36) ≤ m , (37) where (33) follows from the triangle inequality, (35) follows from the fact that \r\rXpa(i)(s) \r\r ≤ m, (36) holds since we have ∥x∥[ eVi(t)]−1 ≤ ∥x∥[ eVi(s)]−1 for any s ∈ [t] and x ∈ RN , and (37) is obtained using the 10definition of the sequence of weights {wi(s) : s ∈ [t]}. Finally, substituting the results of Lemma 3 and (37), with probability at least 1 − δ, for all t ≥ 0, we have ∥[B(t)]i − Bi∥Vi(t)[ eVi(t)]−1Vi(t) ≤ 1 + m + p 2 log (1/δ) + d log (1 +m2t/dC2) . (38) Similarly, for the estimators for interventional weights, with probability at least 1 − δ, for all t ≥ 0, we have ∥[B∗(t)]i − B∗ i ∥V∗ i (t)[ eV∗ i (t)]−1V∗ i (t) ≤ 1 + m + p 2 log (1/δ) + d log (1 +m2t/dC2) . (39) Combining the results in (38) and (39) we complete the proof. The previous lemma offers high probability error bounds for estimators. Due to the causal structure, these errors accumulate and propagate along the causal path, leading to the reward node N. Consequently, we analyze the compounding impacts of estimation errors and model deviations. This analysis involves examining the eigenvalues of the weighted Gram matrices Vi,a(t)(t) and eVi,a(t)(t). We introduce the subsequent lemma to show a bound on the accumulated estimation errors on the reward node with proof provided in Section B. Lemma 4. For any given intervention a ∈ Amatrices A ∈ RN×N and Mi ∈ RN×N for all i ∈ [N] , define ∆A ≜ A − Ba , and ∆(ℓ) A ≜ Aℓ − Bℓ a . (40) If A shares the same support with Ba, Mi ⪰ I and [Mi]j = [M⊤ i ]j = ei if [B]j,i = 0, and if the following bound holds ∥[∆A]i∥Mi ≤ β , (41) then for all ℓ ∈ [L], we have \r\r\r \u0002 ∆(ℓ) A \u0003 N \r\r\r < d ℓ−1 2 (β + 1)ℓ max i∈[N] λ −1 2 min \u0000 Mi \u0001 . (42) Next, building on the estimation error bounds established in Lemma 2 and the compounding error bounds established in Lemma 4, we derive a unified regret bound that applies to both measures of model deviation. It is noteworthy that the analysis is distinctly different from that in the time-invariant setting since we are facing model fluctuations, for which we have designed novel weights for the W-OLS estimator. Theorem 5 (Regret upper bound). Under a deviation budget C, by setting δ = 1 2NT and βt(δ) according to Lemma 2, the average cumulative regret ofRobust-LCB is upper bounded by E[R(T)] ≤ 2m + ˜O \u0010 dL−1 2 ( √ NT + NC ) \u0011 . (43) Proof sketch. Characterizing the regret bound involves decomposing the regret into two parts, depending on whether the concentration inequality in Lemma 2 holds. If the concentration inequality does not hold, the regret is upper bound by a constant term. Otherwise, the estimation errors are upper bounded in Lemma 2 with their compounding effects on the reward node N upper bounded by Lemma 4. Next, the behavior of the eigenvalues of weighted Gram matrices Vi,a(t)(t) and eVi,a(t) is investigated to reach the final result. See Appendix C for the detailed proof. 11The regret bound derived in Theorem 5 can be decomposed into two parts. The first term recovers the order of the optimal rate achieved in the time-invariant setting. The second term captures the impact of model deviation on the regret bound, that is, the cost of handling unknown model fluctuations. Next, we present a lower bound that confirms the tightness and optimality of our upper bound. Remark 6. Robust-LCB works when replacing the deviation budget C with an upper bound ¯C. All the analyses and performance guarantees remains valid when C is substituted with its upper bound ¯C. 5.2 Regret Lower Bound For our analysis of the lower bound, we first show the tightness of the second term of the upper bound that captures the model deviation level C. Building on this insight, we then combine this with the existing lower bound in the time-invariant setting to show the tightness of our regret bound. Theorem 7 (Regret lower bound). For any degreed and graph length L, for any algorithm with knowledge of C, there exists a bandit instance such that the expected regret is at least E[R(T)] ≥ Ω(d L 2 C) . (44) Proof. We construct two instances of causal bandits and demonstrate that under specific deviations, no algorithm can distinguish between them and the initial stage. We consider two linear SEM causal bandit instances sharing the same hierarchical graph G as shown in Figure 1. Let us examine the parameterization <latexit sha1_base64=\"GW1F+kcSoF5bXPxQExkRnOAthGk=\">AAAB6nicbVDLSgNBEOyNr7gajXr0MhgCnsKuoOYYEMRjRPOAZAmzk9lkyMzsMjMrhCWf4MWDIl7FD/ETvPk3Th4HTSxoKKq66e4KE8608bxvJ7e2vrG5ld92d3YLe/vFg8OmjlNFaIPEPFbtEGvKmaQNwwyn7URRLEJOW+Hoauq3HqjSLJb3ZpzQQOCBZBEj2Fjprt3ze8WSV/FmQKvEX5BSrfCZlq/dj3qv+NXtxyQVVBrCsdYd30tMkGFlGOF04nZTTRNMRnhAO5ZKLKgOstmpE1S2Sh9FsbIlDZqpvycyLLQei9B2CmyGetmbiv95ndRE1SBjMkkNlWS+KEo5MjGa/o36TFFi+NgSTBSztyIyxAoTY9NxbQj+8surpHlW8S8q57c2jSrMkYdjOIFT8OESanADdWgAgQE8wjO8ONx5cl6dt3lrzlnMHMEfOO8/YJeQGw==</latexit> X 1 <latexit sha1_base64=\"kOFFBxnNewrD+ftLKDL6WvsOKas=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gNqDkGBPEY0TwgWcLsZDYZMju7zEMISz7BiwdFvIof4id482+cPA6aWNBQVHXT3RWmnCnted/O2vrG5tZ2bsfd3cvvHxQOj5oqMZLQBkl4ItshVpQzQRuaaU7bqaQ4DjlthaOrqd96oFKxRNzrcUqDGA8EixjB2kp37V6lVyh6ZW8GtEr8BSnW8p+mdO1+1HuFr24/ISamQhOOler4XqqDDEvNCKcTt2sUTTEZ4QHtWCpwTFWQzU6doJJV+ihKpC2h0Uz9PZHhWKlxHNrOGOuhWvam4n9ex+ioGmRMpEZTQeaLIsORTtD0b9RnkhLNx5ZgIpm9FZEhlphom45rQ/CXX14lzUrZvyif39o0qjBHDk7gFM7Ah0uowQ3UoQEEBvAIz/DicOfJeXXe5q1rzmLmGP7Aef8BYhuQHA==</latexit> X 2 <latexit sha1_base64=\"/tB1w14cT2j4In5Ng7kfekbmB+M=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9hV1BwDgniMaB6QLGF2MpsMmZ1d5iGEJZ/gxYMiXsUP8RO8+TdOHgdNLGgoqrrp7gpTzpT2vG9nZXVtfWMzt+Vu7+R39wr7Bw2VGElonSQ8ka0QK8qZoHXNNKetVFIch5w2w+HVxG8+UKlYIu71KKVBjPuCRYxgbaW7VvesWyh6ZW8KtEz8OSlW85+mdO1+1LqFr04vISamQhOOlWr7XqqDDEvNCKdjt2MUTTEZ4j5tWypwTFWQTU8do5JVeihKpC2h0VT9PZHhWKlRHNrOGOuBWvQm4n9e2+ioEmRMpEZTQWaLIsORTtDkb9RjkhLNR5ZgIpm9FZEBlphom45rQ/AXX14mjdOyf1E+v7VpVGCGHBzBMZyAD5dQhRuoQR0I9OERnuHF4c6T8+q8zVpXnPnMIfyB8/4DY5+QHQ==</latexit> X 3 <latexit sha1_base64=\"gRv4qt+P1PD72rm9NZtvJC3k1vE=\">AAAB6nicbVDJSgNBEK1xjaPRqEcvjSHgKcyIS44BQTxGNAskQ+jp9CRNenqGXoQw5BO8eFDEq/ghfoI3/8bOctDEBwWP96qoqhemnCnted/Oyura+sZmbsvd3snv7hX2DxoqMZLQOkl4IlshVpQzQeuaaU5bqaQ4DjlthsOrid98oFKxRNzrUUqDGPcFixjB2kp3re5Zt1D0yt4UaJn4c1Ks5j9N6dr9qHULX51eQkxMhSYcK9X2vVQHGZaaEU7HbscommIyxH3atlTgmKogm546RiWr9FCUSFtCo6n6eyLDsVKjOLSdMdYDtehNxP+8ttFRJciYSI2mgswWRYYjnaDJ36jHJCWajyzBRDJ7KyIDLDHRNh3XhuAvvrxMGqdl/6J8fmvTqMAMOTiCYzgBHy6hCjdQgzoQ6MMjPMOLw50n59V5m7WuOPOZQ/gD5/0HZSOQHg==</latexit> X 4 <latexit sha1_base64=\"jWK7IdiCEguN9jBkyv69pJ3sWZY=\">AAAB6nicbVDLSgNBEOyNr7gajXr0MhgCnsKuoOYYEMRjRPOAZAmzk9lkyOzsMg8hLPkELx4U8Sp+iJ/gzb9x8jhoYkFDUdVNd1eYcqa05307ubX1jc2t/La7s1vY2y8eHDZVYiShDZLwRLZDrChngjY005y2U0lxHHLaCkdXU7/1QKViibjX45QGMR4IFjGCtZXu2r3zXrHkVbwZ0CrxF6RUK3ya8rX7Ue8Vv7r9hJiYCk04Vqrje6kOMiw1I5xO3K5RNMVkhAe0Y6nAMVVBNjt1gspW6aMokbaERjP190SGY6XGcWg7Y6yHatmbiv95HaOjapAxkRpNBZkvigxHOkHTv1GfSUo0H1uCiWT2VkSGWGKibTquDcFffnmVNM8q/kXl/NamUYU58nAMJ3AKPlxCDW6gDg0gMIBHeIYXhztPzqvzNm/NOYuZI/gD5/0HZqeQHw==</latexit> X 5 <latexit sha1_base64=\"aJMiRkZZ4Z2JKfpwrmjZ7zw0tJU=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gVjDkGBPEY0TwgWcLsZDYZMju7zEMISz7BiwdFvIof4id482+cPA6aWNBQVHXT3RWmnCnted/O2vrG5tZ2bsfd3cvvHxQOj5oqMZLQBkl4ItshVpQzQRuaaU7bqaQ4DjlthaOrqd96oFKxRNzrcUqDGA8EixjB2kp37V6lVyh6ZW8GtEr8BSnW8p+mdO1+1HuFr24/ISamQhOOler4XqqDDEvNCKcTt2sUTTEZ4QHtWCpwTFWQzU6doJJV+ihKpC2h0Uz9PZHhWKlxHNrOGOuhWvam4n9ex+ioGmRMpEZTQeaLIsORTtD0b9RnkhLNx5ZgIpm9FZEhlphom45rQ/CXX14lzfOyXylf3No0qjBHDk7gFM7Ah0uowQ3UoQEEBvAIz/DicOfJeXXe5q1rzmLmGP7Aef8BaCuQIA==</latexit> X 6 <latexit sha1_base64=\"64A1tlHKmb/VSfXlbg6KIfbqbpI=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gVNDkGBPEY0TwgWcLsZDYZMju7zEMISz7BiwdFvIof4id482+cPA6aWNBQVHXT3RWmnCnted/O2vrG5tZ2bsfd3cvvHxQOj5oqMZLQBkl4ItshVpQzQRuaaU7bqaQ4DjlthaOrqd96oFKxRNzrcUqDGA8EixjB2kp37V6lVyh6ZW8GtEr8BSnW8p+mdO1+1HuFr24/ISamQhOOler4XqqDDEvNCKcTt2sUTTEZ4QHtWCpwTFWQzU6doJJV+ihKpC2h0Uz9PZHhWKlxHNrOGOuhWvam4n9ex+ioGmRMpEZTQeaLIsORTtD0b9RnkhLNx5ZgIpm9FZEhlphom45rQ/CXX14lzfOyf1m+uLVpVGGOHJzAKZyBDxWowQ3UoQEEBvAIz/DicOfJeXXe5q1rzmLmGP7Aef8Baa+QIQ==</latexit> X 7 <latexit sha1_base64=\"f4rVjlCwu3XfgwQzn2uCV400ST4=\">AAAB6nicbVDLSgNBEOyJr7gajXr0MhgCnsKuoOYYEMRjRPOAZAmzk9lkyOzsMjMrhCWf4MWDIl7FD/ETvPk3Th4HTSxoKKq66e4KEsG1cd1vlFtb39jcym87O7uFvf3iwWFTx6mirEFjEat2QDQTXLKG4UawdqIYiQLBWsHoauq3HpjSPJb3ZpwwPyIDyUNOibHSHel5vWLJrbgz4FXiLUipVvhMy9fOR71X/Or2Y5pGTBoqiNYdz02MnxFlOBVs4nRTzRJCR2TAOpZKEjHtZ7NTJ7hslT4OY2VLGjxTf09kJNJ6HAW2MyJmqJe9qfif10lNWPUzLpPUMEnni8JUYBPj6d+4zxWjRowtIVRxeyumQ6IINTYdx4bgLb+8SppnFe+icn5r06jCHHk4hhM4BQ8uoQY3UIcGUBjAIzzDCxLoCb2it3lrDi1mjuAP0PsPbk2QJA==</latexit> a 1 <latexit sha1_base64=\"IHd8T5pe0RHQg52et+PXLM+RsRA=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gNqDkGBPEY0TwgWcLsZDYZMju7zEMISz7BiwdFvIof4id482+cPA6aWNBQVHXT3RWmnCnted/O2vrG5tZ2bsfd3cvvHxQOj5oqMZLQBkl4ItshVpQzQRuaaU7bqaQ4DjlthaOrqd96oFKxRNzrcUqDGA8EixjB2kp3uFfpFYpe2ZsBrRJ/QYq1/KcpXbsf9V7hq9tPiImp0IRjpTq+l+ogw1IzwunE7RpFU0xGeEA7lgocUxVks1MnqGSVPooSaUtoNFN/T2Q4Vmoch7Yzxnqolr2p+J/XMTqqBhkTqdFUkPmiyHCkEzT9G/WZpETzsSWYSGZvRWSIJSbapuPaEPzll1dJs1L2L8rntzaNKsyRgxM4hTPw4RJqcAN1aACBATzCM7w43HlyXp23eeuas5g5hj9w3n8Ab9GQJQ==</latexit> a 2 <latexit sha1_base64=\"bpCUv4G5qKTy7DuCK+mbMgo4qPQ=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9hV1BwDgniMaB6QLGF2MpsMmZldZmaFsOQTvHhQxKv4IX6CN//GyeOgiQUNRVU33V1hwpk2nvftrKyurW9s5rbc7Z387l5h/6Ch41QRWicxj1UrxJpyJmndMMNpK1EUi5DTZji8mvjNB6o0i+W9GSU0ELgvWcQINla6w92zbqHolb0p0DLx56RYzX+mpWv3o9YtfHV6MUkFlYZwrHXb9xITZFgZRjgdu51U0wSTIe7TtqUSC6qDbHrqGJWs0kNRrGxJg6bq74kMC61HIrSdApuBXvQm4n9eOzVRJciYTFJDJZktilKOTIwmf6MeU5QYPrIEE8XsrYgMsMLE2HRcG4K/+PIyaZyW/Yvy+a1NowIz5OAIjuEEfLiEKtxADepAoA+P8AwvDneenFfnbda64sxnDuEPnPcfcVWQJg==</latexit> a 3 <latexit sha1_base64=\"saH4Qt8mbQmQ6BiST7ArygSCzko=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gVHzkGBPEY0TwgWcLsZDYZMjO7zMwKYcknePGgiFfxQ/wEb/6Nk8dBEwsaiqpuurvChDNtPO/bWVldW9/YzG252zv53b3C/kFDx6kitE5iHqtWiDXlTNK6YYbTVqIoFiGnzXB4NfGbD1RpFst7M0poIHBfsogRbKx0h7tn3ULRK3tToGXiz0mxmv9MS9fuR61b+Or0YpIKKg3hWOu27yUmyLAyjHA6djuppgkmQ9ynbUslFlQH2fTUMSpZpYeiWNmSBk3V3xMZFlqPRGg7BTYDvehNxP+8dmqiSpAxmaSGSjJbFKUcmRhN/kY9pigxfGQJJorZWxEZYIWJsem4NgR/8eVl0jgt+xfl81ubRgVmyMERHMMJ+HAJVbiBGtSBQB8e4RleHO48Oa/O26x1xZnPHMIfOO8/ctmQJw==</latexit> a 4 <latexit sha1_base64=\"dVacZFpZA35IpIATo1oe6BkHHEA=\">AAAB6nicbVDLSgNBEOyNr7gajXr0MhgCnsKuoOYYEMRjRPOAZAmzk9lkyOzsMg8hLPkELx4U8Sp+iJ/gzb9x8jhoYkFDUdVNd1eYcqa05307ubX1jc2t/La7s1vY2y8eHDZVYiShDZLwRLZDrChngjY005y2U0lxHHLaCkdXU7/1QKViibjX45QGMR4IFjGCtZXucO+8Vyx5FW8GtEr8BSnVCp+mfO1+1HvFr24/ISamQhOOler4XqqDDEvNCKcTt2sUTTEZ4QHtWCpwTFWQzU6doLJV+ihKpC2h0Uz9PZHhWKlxHNrOGOuhWvam4n9ex+ioGmRMpEZTQeaLIsORTtD0b9RnkhLNx5ZgIpm9FZEhlphom45rQ/CXX14lzbOKf1E5v7VpVGGOPBzDCZyCD5dQgxuoQwMIDOARnuHF4c6T8+q8zVtzzmLmCP7Aef8BdF2QKA==</latexit> a 5 <latexit sha1_base64=\"uNb+wWXvGoOQbw9pHfIscHxlmgk=\">AAAB6nicbVDLSgNBEOyNr7gajXr0MhgCnsKuYMwxIIjHiCYRkiXMTmaTIbOzyzyEsOQTvHhQxKv4IX6CN//GyeOgiQUNRVU33V1hypnSnvft5NbWNza38tvuzm5hb794cNhSiZGENknCE3kfYkU5E7Spmeb0PpUUxyGn7XB0OfXbD1Qqlog7PU5pEOOBYBEjWFvpFveqvWLJq3gzoFXiL0ipXvg05Sv3o9ErfnX7CTExFZpwrFTH91IdZFhqRjiduF2jaIrJCA9ox1KBY6qCbHbqBJWt0kdRIm0JjWbq74kMx0qN49B2xlgP1bI3Ff/zOkZHtSBjIjWaCjJfFBmOdIKmf6M+k5RoPrYEE8nsrYgMscRE23RcG4K//PIqaZ1V/Grl/MamUYM58nAMJ3AKPlxAHa6hAU0gMIBHeIYXhztPzqvzNm/NOYuZI/gD5/0HdeGQKQ==</latexit> a 6 <latexit sha1_base64=\"ye6tWzTWlm0DTXAFu5sg3clq1iU=\">AAAB6nicbVDLSgNBEOyNr7gajXr0MhgCnsKuoMkxIIjHiCYRkiXMTmaTIbOzyzyEsOQTvHhQxKv4IX6CN//GyeOgiQUNRVU33V1hypnSnvft5NbWNza38tvuzm5hb794cNhSiZGENknCE3kfYkU5E7Spmeb0PpUUxyGn7XB0OfXbD1Qqlog7PU5pEOOBYBEjWFvpFveqvWLJq3gzoFXiL0ipXvg05Sv3o9ErfnX7CTExFZpwrFTH91IdZFhqRjiduF2jaIrJCA9ox1KBY6qCbHbqBJWt0kdRIm0JjWbq74kMx0qN49B2xlgP1bI3Ff/zOkZHtSBjIjWaCjJfFBmOdIKmf6M+k5RoPrYEE8nsrYgMscRE23RcG4K//PIqaZ1V/IvK+Y1NowZz5OEYTuAUfKhCHa6hAU0gMIBHeIYXhztPzqvzNm/NOYuZI/gD5/0Hd2WQKg==</latexit> a 7 Figure 1: Example of the hierarchical graph with graph degree d = 3 and length L = 2. of the two bandit instances, referred to as I1 = {B, B∗, ϵ} and I2 = {¯B, ¯B∗, ϵ}. For the existing edges in graph (i, j) ∈ Efor i < jand i, j∈ [N], we define • If j < N: [B]i,j = [ ¯B]i,j = [B∗]i,j = [ ¯B∗]i,j = r 1 d . (45) • If j = N: [B∗]i,j = −[ ¯B∗]i,j = r 1 d , and [B]i,j = [ ¯B]i,j = 0 . (46) 12For the noises, we define ϵi ∼ ( N(1, 1) for i ∈ {1, ··· , d} N(0, 1) for i /∈ {1, ··· , d} . (47) Thus, the only difference between the two bandit instances lies in the intervention weights assigned to the reward node. In the first bandit instance, the optimal action is when the reward node isintervened. In contrast, in the second bandit instance, the best action is associated with the reward node being not intervened. The regret incurred from choosing the sub-optimal action on the reward node is dL/2. Next, consider the scenario where deviations only occur on the reward node during the initial C rounds. Furthermore, consider the scenario in which, at each time t ∈ {1, ··· , C}, the weights are set to 0, and the learner only observes a random noise as the reward. In this case, the learner has no information about the [B]N and can only make random guesses. After C rounds, either ENi(C) or EN∗ i (C) is no less then C 2 . Consequently, there must exist a bandit instance at which the algorithm plays the sub-optimal arm at least C 2 times. We conclude that it must incur 1 2d L 2 C regret with probability at least 1/2. Ignoring the constant, by using Markov’s inequality, we have E[R(T)] ≥ 1 2d L 2 C × P \u0012 R(T) ≥ 1 2d L 2 C \u0013 = Ω(d L 2 C) . (48) Theorem 8. [15, Theorem 6] For any degreed and graph length L and any algorithm, there exists a causal bandit instance such that the expected regret is at least E[R(T)] ≥ Ω(d L 2 −2√ T) . (49) Combining the results in Theorem 7 and Theorem 8, we can conclude a minimax lower bound in the following corollary. Corollary 9. For any degreed and graph length L and any algorithm, there exists a causal bandit instance such that the expected regret is at least E[R(T)] ≥ Ω(d L 2 −2 max{ √ T , d2C}) . (50) This corollary shows both the lower bounds depends on √ T, and that the regret bound of our proposed algorithm is tight in terms of T and C. Besides these, however, the achievable and lower bounds have a gap due to mismatching dependence on the number of nodes N and the exact order of the exponential scaling of dimension d with graph length L. The dependence on N arises in the techniques used in Theorem 5 and we conjecture that the dependency of the upper bound on N can be diminishing as T grows. We provide some insights into tightening dependence on N. Let Li denote the length of the longest causal path that ends at node i ∈ [N]. If we can first bound the cumulative estimation error for the expected value of node i with Li = 1, then we can use induction to bound that for increasing Li and derive an regret upper bound independent of N. The mismatch in the exact order d with L exists in all the relevant literature, even in simpler settings. For instance, consider linear bandits with time-invariant models with dimension d, which 13can be considered a special case of our linear causal bandit model by setting L = 1 and no model variations. For the widely used optimism in the face of uncertainty linear bandit algorithm (OFUL) in [36], the lower and upper bounds behave according to ˜O( √ dT) and ˜O(d √ T), respectively. This gap in terms of √ d and d matches exactly our gap. 5.3 Comparison with the Time-invariant Setting To highlight the robustness, we compare the analytical results with those of the time-invariant setting [15]. While our results in terms of the deviation budgets are general, for illustration purposes, we consider deviation budgets that scale sub-linearly with respect to T by setting C = Tα for α ∈ (0, 1). Confidence ellipsoids. In the time-invariant setting, the confidence ellipsoid radius is set to β′ T ≜ 1 + p 2 log (2NT ) + d log (1 +mT2/d) . (51) Compared with this, our choice of confidence radius is similar to the time-invariant setting. The main contrast is the inclusion of an additional term m, which accounts for the cost of robustness. The difference in logarithmic terms arises from using different weights and norms in our algorithm. Regret bounds. In the time-invariant setting, the regret bound scales as [15] E[R(T)] ≤ 2m + ˜O(β′L T d L 2 √ NT ) . (52) If we directly apply the algorithm designed for the time-invariant setting to the model fluctuation setting with proper adjustments, we observed it would exhibit model deviation robustness but only for substantially small deviation levels. Specifically, to make the time-invariant algorithm robust, we need to adjust the term β′ T so that it scales linearly with C (see Appendix D for details). This, in turn, induces a term CL in the regret bound. Consequently, to preserve a sub-linear regret growth T, α need to fall in the interval (0, 1 2L), which is a highly restrictive regime of model deviations, especially for graphs with long directed paths. For instance, under the DF measure, for T = 105 and a graph with L = 3, a node cannot be compromised more than 6 samples to maintain sub-linear regret. This deviation level can be well below the noise level of the model, establishing that the algorithms designed for the time-invariant setting lack robustness. On the other hand, our Robust-LCB algorithm achieves a regret scaling of Tmax{1 2 ,α}, which preserves the optimal rate ˜O( √ T) under the regime α ≤ 1 2, which is independent of other parameters. Furthermore, a non-linear growth of regret in T is achieved for deviation with α ∈ (1 2, 1), which is a significant improvement over the regime (0, 1 2L). In the context of the earlier example with T = 105, this indicates that the algorithm remains ˜O( √ T) regret when the model faces C ≤ 316 outlier samples and achieves sub-linear regret later on. The cost incurred for integrating robustness is reflected in the additional low-order terms we have in our regret bounds. We also remark that by settingC = 1, our regret bound recovers the order of the time-invariant setting. 14<latexit sha1_base64=\"GW1F+kcSoF5bXPxQExkRnOAthGk=\">AAAB6nicbVDLSgNBEOyNr7gajXr0MhgCnsKuoOYYEMRjRPOAZAmzk9lkyMzsMjMrhCWf4MWDIl7FD/ETvPk3Th4HTSxoKKq66e4KE8608bxvJ7e2vrG5ld92d3YLe/vFg8OmjlNFaIPEPFbtEGvKmaQNwwyn7URRLEJOW+Hoauq3HqjSLJb3ZpzQQOCBZBEj2Fjprt3ze8WSV/FmQKvEX5BSrfCZlq/dj3qv+NXtxyQVVBrCsdYd30tMkGFlGOF04nZTTRNMRnhAO5ZKLKgOstmpE1S2Sh9FsbIlDZqpvycyLLQei9B2CmyGetmbiv95ndRE1SBjMkkNlWS+KEo5MjGa/o36TFFi+NgSTBSztyIyxAoTY9NxbQj+8surpHlW8S8q57c2jSrMkYdjOIFT8OESanADdWgAgQE8wjO8ONx5cl6dt3lrzlnMHMEfOO8/YJeQGw==</latexit> X 1 <latexit sha1_base64=\"kOFFBxnNewrD+ftLKDL6WvsOKas=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gNqDkGBPEY0TwgWcLsZDYZMju7zEMISz7BiwdFvIof4id482+cPA6aWNBQVHXT3RWmnCnted/O2vrG5tZ2bsfd3cvvHxQOj5oqMZLQBkl4ItshVpQzQRuaaU7bqaQ4DjlthaOrqd96oFKxRNzrcUqDGA8EixjB2kp37V6lVyh6ZW8GtEr8BSnW8p+mdO1+1HuFr24/ISamQhOOler4XqqDDEvNCKcTt2sUTTEZ4QHtWCpwTFWQzU6doJJV+ihKpC2h0Uz9PZHhWKlxHNrOGOuhWvam4n9ex+ioGmRMpEZTQeaLIsORTtD0b9RnkhLNx5ZgIpm9FZEhlphom45rQ/CXX14lzUrZvyif39o0qjBHDk7gFM7Ah0uowQ3UoQEEBvAIz/DicOfJeXXe5q1rzmLmGP7Aef8BYhuQHA==</latexit> X 2 <latexit sha1_base64=\"/tB1w14cT2j4In5Ng7kfekbmB+M=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9hV1BwDgniMaB6QLGF2MpsMmZ1d5iGEJZ/gxYMiXsUP8RO8+TdOHgdNLGgoqrrp7gpTzpT2vG9nZXVtfWMzt+Vu7+R39wr7Bw2VGElonSQ8ka0QK8qZoHXNNKetVFIch5w2w+HVxG8+UKlYIu71KKVBjPuCRYxgbaW7VvesWyh6ZW8KtEz8OSlW85+mdO1+1LqFr04vISamQhOOlWr7XqqDDEvNCKdjt2MUTTEZ4j5tWypwTFWQTU8do5JVeihKpC2h0VT9PZHhWKlRHNrOGOuBWvQm4n9e2+ioEmRMpEZTQWaLIsORTtDkb9RjkhLNR5ZgIpm9FZEBlphom45rQ/AXX14mjdOyf1E+v7VpVGCGHBzBMZyAD5dQhRuoQR0I9OERnuHF4c6T8+q8zVpXnPnMIfyB8/4DY5+QHQ==</latexit> X 3 <latexit sha1_base64=\"gRv4qt+P1PD72rm9NZtvJC3k1vE=\">AAAB6nicbVDJSgNBEK1xjaPRqEcvjSHgKcyIS44BQTxGNAskQ+jp9CRNenqGXoQw5BO8eFDEq/ghfoI3/8bOctDEBwWP96qoqhemnCnted/Oyura+sZmbsvd3snv7hX2DxoqMZLQOkl4IlshVpQzQeuaaU5bqaQ4DjlthsOrid98oFKxRNzrUUqDGPcFixjB2kp3re5Zt1D0yt4UaJn4c1Ks5j9N6dr9qHULX51eQkxMhSYcK9X2vVQHGZaaEU7HbscommIyxH3atlTgmKogm546RiWr9FCUSFtCo6n6eyLDsVKjOLSdMdYDtehNxP+8ttFRJciYSI2mgswWRYYjnaDJ36jHJCWajyzBRDJ7KyIDLDHRNh3XhuAvvrxMGqdl/6J8fmvTqMAMOTiCYzgBHy6hCjdQgzoQ6MMjPMOLw50n59V5m7WuOPOZQ/gD5/0HZSOQHg==</latexit> X 4 <latexit sha1_base64=\"f4rVjlCwu3XfgwQzn2uCV400ST4=\">AAAB6nicbVDLSgNBEOyJr7gajXr0MhgCnsKuoOYYEMRjRPOAZAmzk9lkyOzsMjMrhCWf4MWDIl7FD/ETvPk3Th4HTSxoKKq66e4KEsG1cd1vlFtb39jcym87O7uFvf3iwWFTx6mirEFjEat2QDQTXLKG4UawdqIYiQLBWsHoauq3HpjSPJb3ZpwwPyIDyUNOibHSHel5vWLJrbgz4FXiLUipVvhMy9fOR71X/Or2Y5pGTBoqiNYdz02MnxFlOBVs4nRTzRJCR2TAOpZKEjHtZ7NTJ7hslT4OY2VLGjxTf09kJNJ6HAW2MyJmqJe9qfif10lNWPUzLpPUMEnni8JUYBPj6d+4zxWjRowtIVRxeyumQ6IINTYdx4bgLb+8SppnFe+icn5r06jCHHk4hhM4BQ8uoQY3UIcGUBjAIzzDCxLoCb2it3lrDi1mjuAP0PsPbk2QJA==</latexit> a 1 <latexit sha1_base64=\"bpCUv4G5qKTy7DuCK+mbMgo4qPQ=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9hV1BwDgniMaB6QLGF2MpsMmZldZmaFsOQTvHhQxKv4IX6CN//GyeOgiQUNRVU33V1hwpk2nvftrKyurW9s5rbc7Z387l5h/6Ch41QRWicxj1UrxJpyJmndMMNpK1EUi5DTZji8mvjNB6o0i+W9GSU0ELgvWcQINla6w92zbqHolb0p0DLx56RYzX+mpWv3o9YtfHV6MUkFlYZwrHXb9xITZFgZRjgdu51U0wSTIe7TtqUSC6qDbHrqGJWs0kNRrGxJg6bq74kMC61HIrSdApuBXvQm4n9eOzVRJciYTFJDJZktilKOTIwmf6MeU5QYPrIEE8XsrYgMsMLE2HRcG4K/+PIyaZyW/Yvy+a1NowIz5OAIjuEEfLiEKtxADepAoA+P8AwvDneenFfnbda64sxnDuEPnPcfcVWQJg==</latexit> a 3 <latexit sha1_base64=\"saH4Qt8mbQmQ6BiST7ArygSCzko=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gVHzkGBPEY0TwgWcLsZDYZMjO7zMwKYcknePGgiFfxQ/wEb/6Nk8dBEwsaiqpuurvChDNtPO/bWVldW9/YzG252zv53b3C/kFDx6kitE5iHqtWiDXlTNK6YYbTVqIoFiGnzXB4NfGbD1RpFst7M0poIHBfsogRbKx0h7tn3ULRK3tToGXiz0mxmv9MS9fuR61b+Or0YpIKKg3hWOu27yUmyLAyjHA6djuppgkmQ9ynbUslFlQH2fTUMSpZpYeiWNmSBk3V3xMZFlqPRGg7BTYDvehNxP+8dmqiSpAxmaSGSjJbFKUcmRhN/kY9pigxfGQJJorZWxEZYIWJsem4NgR/8eVl0jgt+xfl81ubRgVmyMERHMMJ+HAJVbiBGtSBQB8e4RleHO48Oa/O26x1xZnPHMIfOO8/ctmQJw==</latexit> a 4 <latexit sha1_base64=\"IHd8T5pe0RHQg52et+PXLM+RsRA=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gNqDkGBPEY0TwgWcLsZDYZMju7zEMISz7BiwdFvIof4id482+cPA6aWNBQVHXT3RWmnCnted/O2vrG5tZ2bsfd3cvvHxQOj5oqMZLQBkl4ItshVpQzQRuaaU7bqaQ4DjlthaOrqd96oFKxRNzrcUqDGA8EixjB2kp3uFfpFYpe2ZsBrRJ/QYq1/KcpXbsf9V7hq9tPiImp0IRjpTq+l+ogw1IzwunE7RpFU0xGeEA7lgocUxVks1MnqGSVPooSaUtoNFN/T2Q4Vmoch7Yzxnqolr2p+J/XMTqqBhkTqdFUkPmiyHCkEzT9G/WZpETzsSWYSGZvRWSIJSbapuPaEPzll1dJs1L2L8rntzaNKsyRgxM4hTPw4RJqcAN1aACBATzCM7w43HlyXp23eeuas5g5hj9w3n8Ab9GQJQ==</latexit> a 2 Figure 2: Chain graph with N = 4. <latexit sha1_base64=\"GW1F+kcSoF5bXPxQExkRnOAthGk=\">AAAB6nicbVDLSgNBEOyNr7gajXr0MhgCnsKuoOYYEMRjRPOAZAmzk9lkyMzsMjMrhCWf4MWDIl7FD/ETvPk3Th4HTSxoKKq66e4KE8608bxvJ7e2vrG5ld92d3YLe/vFg8OmjlNFaIPEPFbtEGvKmaQNwwyn7URRLEJOW+Hoauq3HqjSLJb3ZpzQQOCBZBEj2Fjprt3ze8WSV/FmQKvEX5BSrfCZlq/dj3qv+NXtxyQVVBrCsdYd30tMkGFlGOF04nZTTRNMRnhAO5ZKLKgOstmpE1S2Sh9FsbIlDZqpvycyLLQei9B2CmyGetmbiv95ndRE1SBjMkkNlWS+KEo5MjGa/o36TFFi+NgSTBSztyIyxAoTY9NxbQj+8surpHlW8S8q57c2jSrMkYdjOIFT8OESanADdWgAgQE8wjO8ONx5cl6dt3lrzlnMHMEfOO8/YJeQGw==</latexit> X 1 <latexit sha1_base64=\"kOFFBxnNewrD+ftLKDL6WvsOKas=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gNqDkGBPEY0TwgWcLsZDYZMju7zEMISz7BiwdFvIof4id482+cPA6aWNBQVHXT3RWmnCnted/O2vrG5tZ2bsfd3cvvHxQOj5oqMZLQBkl4ItshVpQzQRuaaU7bqaQ4DjlthaOrqd96oFKxRNzrcUqDGA8EixjB2kp37V6lVyh6ZW8GtEr8BSnW8p+mdO1+1HuFr24/ISamQhOOler4XqqDDEvNCKcTt2sUTTEZ4QHtWCpwTFWQzU6doJJV+ihKpC2h0Uz9PZHhWKlxHNrOGOuhWvam4n9ex+ioGmRMpEZTQeaLIsORTtD0b9RnkhLNx5ZgIpm9FZEhlphom45rQ/CXX14lzUrZvyif39o0qjBHDk7gFM7Ah0uowQ3UoQEEBvAIz/DicOfJeXXe5q1rzmLmGP7Aef8BYhuQHA==</latexit> X 2 <latexit sha1_base64=\"/tB1w14cT2j4In5Ng7kfekbmB+M=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9hV1BwDgniMaB6QLGF2MpsMmZ1d5iGEJZ/gxYMiXsUP8RO8+TdOHgdNLGgoqrrp7gpTzpT2vG9nZXVtfWMzt+Vu7+R39wr7Bw2VGElonSQ8ka0QK8qZoHXNNKetVFIch5w2w+HVxG8+UKlYIu71KKVBjPuCRYxgbaW7VvesWyh6ZW8KtEz8OSlW85+mdO1+1LqFr04vISamQhOOlWr7XqqDDEvNCKdjt2MUTTEZ4j5tWypwTFWQTU8do5JVeihKpC2h0VT9PZHhWKlRHNrOGOuBWvQm4n9e2+ioEmRMpEZTQWaLIsORTtDkb9RjkhLNR5ZgIpm9FZEBlphom45rQ/AXX14mjdOyf1E+v7VpVGCGHBzBMZyAD5dQhRuoQR0I9OERnuHF4c6T8+q8zVpXnPnMIfyB8/4DY5+QHQ==</latexit> X 3 <latexit sha1_base64=\"f4rVjlCwu3XfgwQzn2uCV400ST4=\">AAAB6nicbVDLSgNBEOyJr7gajXr0MhgCnsKuoOYYEMRjRPOAZAmzk9lkyOzsMjMrhCWf4MWDIl7FD/ETvPk3Th4HTSxoKKq66e4KEsG1cd1vlFtb39jcym87O7uFvf3iwWFTx6mirEFjEat2QDQTXLKG4UawdqIYiQLBWsHoauq3HpjSPJb3ZpwwPyIDyUNOibHSHel5vWLJrbgz4FXiLUipVvhMy9fOR71X/Or2Y5pGTBoqiNYdz02MnxFlOBVs4nRTzRJCR2TAOpZKEjHtZ7NTJ7hslT4OY2VLGjxTf09kJNJ6HAW2MyJmqJe9qfif10lNWPUzLpPUMEnni8JUYBPj6d+4zxWjRowtIVRxeyumQ6IINTYdx4bgLb+8SppnFe+icn5r06jCHHk4hhM4BQ8uoQY3UIcGUBjAIzzDCxLoCb2it3lrDi1mjuAP0PsPbk2QJA==</latexit> a 1 <latexit sha1_base64=\"bpCUv4G5qKTy7DuCK+mbMgo4qPQ=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9hV1BwDgniMaB6QLGF2MpsMmZldZmaFsOQTvHhQxKv4IX6CN//GyeOgiQUNRVU33V1hwpk2nvftrKyurW9s5rbc7Z387l5h/6Ch41QRWicxj1UrxJpyJmndMMNpK1EUi5DTZji8mvjNB6o0i+W9GSU0ELgvWcQINla6w92zbqHolb0p0DLx56RYzX+mpWv3o9YtfHV6MUkFlYZwrHXb9xITZFgZRjgdu51U0wSTIe7TtqUSC6qDbHrqGJWs0kNRrGxJg6bq74kMC61HIrSdApuBXvQm4n9eOzVRJciYTFJDJZktilKOTIwmf6MeU5QYPrIEE8XsrYgMsMLE2HRcG4K/+PIyaZyW/Yvy+a1NowIz5OAIjuEEfLiEKtxADepAoA+P8AwvDneenFfnbda64sxnDuEPnPcfcVWQJg==</latexit> a 3 <latexit sha1_base64=\"saH4Qt8mbQmQ6BiST7ArygSCzko=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gVHzkGBPEY0TwgWcLsZDYZMjO7zMwKYcknePGgiFfxQ/wEb/6Nk8dBEwsaiqpuurvChDNtPO/bWVldW9/YzG252zv53b3C/kFDx6kitE5iHqtWiDXlTNK6YYbTVqIoFiGnzXB4NfGbD1RpFst7M0poIHBfsogRbKx0h7tn3ULRK3tToGXiz0mxmv9MS9fuR61b+Or0YpIKKg3hWOu27yUmyLAyjHA6djuppgkmQ9ynbUslFlQH2fTUMSpZpYeiWNmSBk3V3xMZFlqPRGg7BTYDvehNxP+8dmqiSpAxmaSGSjJbFKUcmRhN/kY9pigxfGQJJorZWxEZYIWJsem4NgR/8eVl0jgt+xfl81ubRgVmyMERHMMJ+HAJVbiBGtSBQB8e4RleHO48Oa/O26x1xZnPHMIfOO8/ctmQJw==</latexit> a 4 <latexit sha1_base64=\"IHd8T5pe0RHQg52et+PXLM+RsRA=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gNqDkGBPEY0TwgWcLsZDYZMju7zEMISz7BiwdFvIof4id482+cPA6aWNBQVHXT3RWmnCnted/O2vrG5tZ2bsfd3cvvHxQOj5oqMZLQBkl4ItshVpQzQRuaaU7bqaQ4DjlthaOrqd96oFKxRNzrcUqDGA8EixjB2kp3uFfpFYpe2ZsBrRJ/QYq1/KcpXbsf9V7hq9tPiImp0IRjpTq+l+ogw1IzwunE7RpFU0xGeEA7lgocUxVks1MnqGSVPooSaUtoNFN/T2Q4Vmoch7Yzxnqolr2p+J/XMTqqBhkTqdFUkPmiyHCkEzT9G/WZpETzsSWYSGZvRWSIJSbapuPaEPzll1dJs1L2L8rntzaNKsyRgxM4hTPw4RJqcAN1aACBATzCM7w43HlyXp23eeuas5g5hj9w3n8Ab9GQJQ==</latexit> a 2 <latexit sha1_base64=\"gRv4qt+P1PD72rm9NZtvJC3k1vE=\">AAAB6nicbVDJSgNBEK1xjaPRqEcvjSHgKcyIS44BQTxGNAskQ+jp9CRNenqGXoQw5BO8eFDEq/ghfoI3/8bOctDEBwWP96qoqhemnCnted/Oyura+sZmbsvd3snv7hX2DxoqMZLQOkl4IlshVpQzQeuaaU5bqaQ4DjlthsOrid98oFKxRNzrUUqDGPcFixjB2kp3re5Zt1D0yt4UaJn4c1Ks5j9N6dr9qHULX51eQkxMhSYcK9X2vVQHGZaaEU7HbscommIyxH3atlTgmKogm546RiWr9FCUSFtCo6n6eyLDsVKjOLSdMdYDtehNxP+8ttFRJciYSI2mgswWRYYjnaDJ36jHJCWajyzBRDJ7KyIDLDHRNh3XhuAvvrxMGqdl/6J8fmvTqMAMOTiCYzgBHy6hCjdQgzoQ6MMjPMOLw50n59V5m7WuOPOZQ/gD5/0HZSOQHg==</latexit> X 4 <latexit sha1_base64=\"jWK7IdiCEguN9jBkyv69pJ3sWZY=\">AAAB6nicbVDLSgNBEOyNr7gajXr0MhgCnsKuoOYYEMRjRPOAZAmzk9lkyOzsMg8hLPkELx4U8Sp+iJ/gzb9x8jhoYkFDUdVNd1eYcqa05307ubX1jc2t/La7s1vY2y8eHDZVYiShDZLwRLZDrChngjY005y2U0lxHHLaCkdXU7/1QKViibjX45QGMR4IFjGCtZXu2r3zXrHkVbwZ0CrxF6RUK3ya8rX7Ue8Vv7r9hJiYCk04Vqrje6kOMiw1I5xO3K5RNMVkhAe0Y6nAMVVBNjt1gspW6aMokbaERjP190SGY6XGcWg7Y6yHatmbiv95HaOjapAxkRpNBZkvigxHOkHTv1GfSUo0H1uCiWT2VkSGWGKibTquDcFffnmVNM8q/kXl/NamUYU58nAMJ3AKPlxCDW6gDg0gMIBHeIYXhztPzqvzNm/NOYuZI/gD5/0HZqeQHw==</latexit> X 5 <latexit sha1_base64=\"dVacZFpZA35IpIATo1oe6BkHHEA=\">AAAB6nicbVDLSgNBEOyNr7gajXr0MhgCnsKuoOYYEMRjRPOAZAmzk9lkyOzsMg8hLPkELx4U8Sp+iJ/gzb9x8jhoYkFDUdVNd1eYcqa05307ubX1jc2t/La7s1vY2y8eHDZVYiShDZLwRLZDrChngjY005y2U0lxHHLaCkdXU7/1QKViibjX45QGMR4IFjGCtZXucO+8Vyx5FW8GtEr8BSnVCp+mfO1+1HvFr24/ISamQhOOler4XqqDDEvNCKcTt2sUTTEZ4QHtWCpwTFWQzU6doLJV+ihKpC2h0Uz9PZHhWKlxHNrOGOuhWvam4n9ex+ioGmRMpEZTQeaLIsORTtD0b9RnkhLNx5ZgIpm9FZEhlphom45rQ/CXX14lzbOKf1E5v7VpVGGOPBzDCZyCD5dQgxuoQwMIDOARnuHF4c6T8+q8zVtzzmLmCP7Aef8BdF2QKA==</latexit> a 5 Figure 3: Confounded parallel graph N = 5. <latexit sha1_base64=\"GW1F+kcSoF5bXPxQExkRnOAthGk=\">AAAB6nicbVDLSgNBEOyNr7gajXr0MhgCnsKuoOYYEMRjRPOAZAmzk9lkyMzsMjMrhCWf4MWDIl7FD/ETvPk3Th4HTSxoKKq66e4KE8608bxvJ7e2vrG5ld92d3YLe/vFg8OmjlNFaIPEPFbtEGvKmaQNwwyn7URRLEJOW+Hoauq3HqjSLJb3ZpzQQOCBZBEj2Fjprt3ze8WSV/FmQKvEX5BSrfCZlq/dj3qv+NXtxyQVVBrCsdYd30tMkGFlGOF04nZTTRNMRnhAO5ZKLKgOstmpE1S2Sh9FsbIlDZqpvycyLLQei9B2CmyGetmbiv95ndRE1SBjMkkNlWS+KEo5MjGa/o36TFFi+NgSTBSztyIyxAoTY9NxbQj+8surpHlW8S8q57c2jSrMkYdjOIFT8OESanADdWgAgQE8wjO8ONx5cl6dt3lrzlnMHMEfOO8/YJeQGw==</latexit> X 1 <latexit sha1_base64=\"kOFFBxnNewrD+ftLKDL6WvsOKas=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gNqDkGBPEY0TwgWcLsZDYZMju7zEMISz7BiwdFvIof4id482+cPA6aWNBQVHXT3RWmnCnted/O2vrG5tZ2bsfd3cvvHxQOj5oqMZLQBkl4ItshVpQzQRuaaU7bqaQ4DjlthaOrqd96oFKxRNzrcUqDGA8EixjB2kp37V6lVyh6ZW8GtEr8BSnW8p+mdO1+1HuFr24/ISamQhOOler4XqqDDEvNCKcTt2sUTTEZ4QHtWCpwTFWQzU6doJJV+ihKpC2h0Uz9PZHhWKlxHNrOGOuhWvam4n9ex+ioGmRMpEZTQeaLIsORTtD0b9RnkhLNx5ZgIpm9FZEhlphom45rQ/CXX14lzUrZvyif39o0qjBHDk7gFM7Ah0uowQ3UoQEEBvAIz/DicOfJeXXe5q1rzmLmGP7Aef8BYhuQHA==</latexit> X 2 <latexit sha1_base64=\"/tB1w14cT2j4In5Ng7kfekbmB+M=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9hV1BwDgniMaB6QLGF2MpsMmZ1d5iGEJZ/gxYMiXsUP8RO8+TdOHgdNLGgoqrrp7gpTzpT2vG9nZXVtfWMzt+Vu7+R39wr7Bw2VGElonSQ8ka0QK8qZoHXNNKetVFIch5w2w+HVxG8+UKlYIu71KKVBjPuCRYxgbaW7VvesWyh6ZW8KtEz8OSlW85+mdO1+1LqFr04vISamQhOOlWr7XqqDDEvNCKdjt2MUTTEZ4j5tWypwTFWQTU8do5JVeihKpC2h0VT9PZHhWKlRHNrOGOuBWvQm4n9e2+ioEmRMpEZTQWaLIsORTtDkb9RjkhLNR5ZgIpm9FZEBlphom45rQ/AXX14mjdOyf1E+v7VpVGCGHBzBMZyAD5dQhRuoQR0I9OERnuHF4c6T8+q8zVpXnPnMIfyB8/4DY5+QHQ==</latexit> X 3 <latexit sha1_base64=\"gRv4qt+P1PD72rm9NZtvJC3k1vE=\">AAAB6nicbVDJSgNBEK1xjaPRqEcvjSHgKcyIS44BQTxGNAskQ+jp9CRNenqGXoQw5BO8eFDEq/ghfoI3/8bOctDEBwWP96qoqhemnCnted/Oyura+sZmbsvd3snv7hX2DxoqMZLQOkl4IlshVpQzQeuaaU5bqaQ4DjlthsOrid98oFKxRNzrUUqDGPcFixjB2kp3re5Zt1D0yt4UaJn4c1Ks5j9N6dr9qHULX51eQkxMhSYcK9X2vVQHGZaaEU7HbscommIyxH3atlTgmKogm546RiWr9FCUSFtCo6n6eyLDsVKjOLSdMdYDtehNxP+8ttFRJciYSI2mgswWRYYjnaDJ36jHJCWajyzBRDJ7KyIDLDHRNh3XhuAvvrxMGqdl/6J8fmvTqMAMOTiCYzgBHy6hCjdQgzoQ6MMjPMOLw50n59V5m7WuOPOZQ/gD5/0HZSOQHg==</latexit> X 4 <latexit sha1_base64=\"jWK7IdiCEguN9jBkyv69pJ3sWZY=\">AAAB6nicbVDLSgNBEOyNr7gajXr0MhgCnsKuoOYYEMRjRPOAZAmzk9lkyOzsMg8hLPkELx4U8Sp+iJ/gzb9x8jhoYkFDUdVNd1eYcqa05307ubX1jc2t/La7s1vY2y8eHDZVYiShDZLwRLZDrChngjY005y2U0lxHHLaCkdXU7/1QKViibjX45QGMR4IFjGCtZXu2r3zXrHkVbwZ0CrxF6RUK3ya8rX7Ue8Vv7r9hJiYCk04Vqrje6kOMiw1I5xO3K5RNMVkhAe0Y6nAMVVBNjt1gspW6aMokbaERjP190SGY6XGcWg7Y6yHatmbiv95HaOjapAxkRpNBZkvigxHOkHTv1GfSUo0H1uCiWT2VkSGWGKibTquDcFffnmVNM8q/kXl/NamUYU58nAMJ3AKPlxCDW6gDg0gMIBHeIYXhztPzqvzNm/NOYuZI/gD5/0HZqeQHw==</latexit> X 5 <latexit sha1_base64=\"aJMiRkZZ4Z2JKfpwrmjZ7zw0tJU=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gVjDkGBPEY0TwgWcLsZDYZMju7zEMISz7BiwdFvIof4id482+cPA6aWNBQVHXT3RWmnCnted/O2vrG5tZ2bsfd3cvvHxQOj5oqMZLQBkl4ItshVpQzQRuaaU7bqaQ4DjlthaOrqd96oFKxRNzrcUqDGA8EixjB2kp37V6lVyh6ZW8GtEr8BSnW8p+mdO1+1HuFr24/ISamQhOOler4XqqDDEvNCKcTt2sUTTEZ4QHtWCpwTFWQzU6doJJV+ihKpC2h0Uz9PZHhWKlxHNrOGOuhWvam4n9ex+ioGmRMpEZTQeaLIsORTtD0b9RnkhLNx5ZgIpm9FZEhlphom45rQ/CXX14lzfOyXylf3No0qjBHDk7gFM7Ah0uowQ3UoQEEBvAIz/DicOfJeXXe5q1rzmLmGP7Aef8BaCuQIA==</latexit> X 6 <latexit sha1_base64=\"64A1tlHKmb/VSfXlbg6KIfbqbpI=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gVNDkGBPEY0TwgWcLsZDYZMju7zEMISz7BiwdFvIof4id482+cPA6aWNBQVHXT3RWmnCnted/O2vrG5tZ2bsfd3cvvHxQOj5oqMZLQBkl4ItshVpQzQRuaaU7bqaQ4DjlthaOrqd96oFKxRNzrcUqDGA8EixjB2kp37V6lVyh6ZW8GtEr8BSnW8p+mdO1+1HuFr24/ISamQhOOler4XqqDDEvNCKcTt2sUTTEZ4QHtWCpwTFWQzU6doJJV+ihKpC2h0Uz9PZHhWKlxHNrOGOuhWvam4n9ex+ioGmRMpEZTQeaLIsORTtD0b9RnkhLNx5ZgIpm9FZEhlphom45rQ/CXX14lzfOyf1m+uLVpVGGOHJzAKZyBDxWowQ3UoQEEBvAIz/DicOfJeXXe5q1rzmLmGP7Aef8Baa+QIQ==</latexit> X 7 <latexit sha1_base64=\"f4rVjlCwu3XfgwQzn2uCV400ST4=\">AAAB6nicbVDLSgNBEOyJr7gajXr0MhgCnsKuoOYYEMRjRPOAZAmzk9lkyOzsMjMrhCWf4MWDIl7FD/ETvPk3Th4HTSxoKKq66e4KEsG1cd1vlFtb39jcym87O7uFvf3iwWFTx6mirEFjEat2QDQTXLKG4UawdqIYiQLBWsHoauq3HpjSPJb3ZpwwPyIDyUNOibHSHel5vWLJrbgz4FXiLUipVvhMy9fOR71X/Or2Y5pGTBoqiNYdz02MnxFlOBVs4nRTzRJCR2TAOpZKEjHtZ7NTJ7hslT4OY2VLGjxTf09kJNJ6HAW2MyJmqJe9qfif10lNWPUzLpPUMEnni8JUYBPj6d+4zxWjRowtIVRxeyumQ6IINTYdx4bgLb+8SppnFe+icn5r06jCHHk4hhM4BQ8uoQY3UIcGUBjAIzzDCxLoCb2it3lrDi1mjuAP0PsPbk2QJA==</latexit> a 1 <latexit sha1_base64=\"IHd8T5pe0RHQg52et+PXLM+RsRA=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gNqDkGBPEY0TwgWcLsZDYZMju7zEMISz7BiwdFvIof4id482+cPA6aWNBQVHXT3RWmnCnted/O2vrG5tZ2bsfd3cvvHxQOj5oqMZLQBkl4ItshVpQzQRuaaU7bqaQ4DjlthaOrqd96oFKxRNzrcUqDGA8EixjB2kp3uFfpFYpe2ZsBrRJ/QYq1/KcpXbsf9V7hq9tPiImp0IRjpTq+l+ogw1IzwunE7RpFU0xGeEA7lgocUxVks1MnqGSVPooSaUtoNFN/T2Q4Vmoch7Yzxnqolr2p+J/XMTqqBhkTqdFUkPmiyHCkEzT9G/WZpETzsSWYSGZvRWSIJSbapuPaEPzll1dJs1L2L8rntzaNKsyRgxM4hTPw4RJqcAN1aACBATzCM7w43HlyXp23eeuas5g5hj9w3n8Ab9GQJQ==</latexit> a 2 <latexit sha1_base64=\"bpCUv4G5qKTy7DuCK+mbMgo4qPQ=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9hV1BwDgniMaB6QLGF2MpsMmZldZmaFsOQTvHhQxKv4IX6CN//GyeOgiQUNRVU33V1hwpk2nvftrKyurW9s5rbc7Z387l5h/6Ch41QRWicxj1UrxJpyJmndMMNpK1EUi5DTZji8mvjNB6o0i+W9GSU0ELgvWcQINla6w92zbqHolb0p0DLx56RYzX+mpWv3o9YtfHV6MUkFlYZwrHXb9xITZFgZRjgdu51U0wSTIe7TtqUSC6qDbHrqGJWs0kNRrGxJg6bq74kMC61HIrSdApuBXvQm4n9eOzVRJciYTFJDJZktilKOTIwmf6MeU5QYPrIEE8XsrYgMsMLE2HRcG4K/+PIyaZyW/Yvy+a1NowIz5OAIjuEEfLiEKtxADepAoA+P8AwvDneenFfnbda64sxnDuEPnPcfcVWQJg==</latexit> a 3 <latexit sha1_base64=\"saH4Qt8mbQmQ6BiST7ArygSCzko=\">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gVHzkGBPEY0TwgWcLsZDYZMjO7zMwKYcknePGgiFfxQ/wEb/6Nk8dBEwsaiqpuurvChDNtPO/bWVldW9/YzG252zv53b3C/kFDx6kitE5iHqtWiDXlTNK6YYbTVqIoFiGnzXB4NfGbD1RpFst7M0poIHBfsogRbKx0h7tn3ULRK3tToGXiz0mxmv9MS9fuR61b+Or0YpIKKg3hWOu27yUmyLAyjHA6djuppgkmQ9ynbUslFlQH2fTUMSpZpYeiWNmSBk3V3xMZFlqPRGg7BTYDvehNxP+8dmqiSpAxmaSGSjJbFKUcmRhN/kY9pigxfGQJJorZWxEZYIWJsem4NgR/8eVl0jgt+xfl81ubRgVmyMERHMMJ+HAJVbiBGtSBQB8e4RleHO48Oa/O26x1xZnPHMIfOO8/ctmQJw==</latexit> a 4 <latexit sha1_base64=\"dVacZFpZA35IpIATo1oe6BkHHEA=\">AAAB6nicbVDLSgNBEOyNr7gajXr0MhgCnsKuoOYYEMRjRPOAZAmzk9lkyOzsMg8hLPkELx4U8Sp+iJ/gzb9x8jhoYkFDUdVNd1eYcqa05307ubX1jc2t/La7s1vY2y8eHDZVYiShDZLwRLZDrChngjY005y2U0lxHHLaCkdXU7/1QKViibjX45QGMR4IFjGCtZXucO+8Vyx5FW8GtEr8BSnVCp+mfO1+1HvFr24/ISamQhOOler4XqqDDEvNCKcTt2sUTTEZ4QHtWCpwTFWQzU6doLJV+ihKpC2h0Uz9PZHhWKlxHNrOGOuhWvam4n9ex+ioGmRMpEZTQeaLIsORTtD0b9RnkhLNx5ZgIpm9FZEhlphom45rQ/CXX14lzbOKf1E5v7VpVGGOPBzDCZyCD5dQgxuoQwMIDOARnuHF4c6T8+q8zVtzzmLmCP7Aef8BdF2QKA==</latexit> a 5 <latexit sha1_base64=\"uNb+wWXvGoOQbw9pHfIscHxlmgk=\">AAAB6nicbVDLSgNBEOyNr7gajXr0MhgCnsKuYMwxIIjHiCYRkiXMTmaTIbOzyzyEsOQTvHhQxKv4IX6CN//GyeOgiQUNRVU33V1hypnSnvft5NbWNza38tvuzm5hb794cNhSiZGENknCE3kfYkU5E7Spmeb0PpUUxyGn7XB0OfXbD1Qqlog7PU5pEOOBYBEjWFvpFveqvWLJq3gzoFXiL0ipXvg05Sv3o9ErfnX7CTExFZpwrFTH91IdZFhqRjiduF2jaIrJCA9ox1KBY6qCbHbqBJWt0kdRIm0JjWbq74kMx0qN49B2xlgP1bI3Ff/zOkZHtSBjIjWaCjJfFBmOdIKmf6M+k5RoPrYEE8nsrYgMscRE23RcG4K//PIqaZ1V/Grl/MamUYM58nAMJ3AKPlxAHa6hAU0gMIBHeIYXhztPzqvzNm/NOYuZI/gD5/0HdeGQKQ==</latexit> a 6 <latexit sha1_base64=\"ye6tWzTWlm0DTXAFu5sg3clq1iU=\">AAAB6nicbVDLSgNBEOyNr7gajXr0MhgCnsKuoMkxIIjHiCYRkiXMTmaTIbOzyzyEsOQTvHhQxKv4IX6CN//GyeOgiQUNRVU33V1hypnSnvft5NbWNza38tvuzm5hb794cNhSiZGENknCE3kfYkU5E7Spmeb0PpUUxyGn7XB0OfXbD1Qqlog7PU5pEOOBYBEjWFvpFveqvWLJq3gzoFXiL0ipXvg05Sv3o9ErfnX7CTExFZpwrFTH91IdZFhqRjiduF2jaIrJCA9ox1KBY6qCbHbqBJWt0kdRIm0JjWbq74kMx0qN49B2xlgP1bI3Ff/zOkZHtSBjIjWaCjJfFBmOdIKmf6M+k5RoPrYEE8nsrYgMscRE23RcG4K//PIqaZ1V/IvK+Y1NowZz5OEYTuAUfKhCHa6hAU0gMIBHeIYXhztPzqvzNm/NOYuZI/gD5/0Hd2WQKg==</latexit> a 7 Figure 4: Hierarchical graph with d = 3 and L = 2. 6 Empirical Evaluations In this section, we assess the robustness of the Robust-LCB algorithm and its scaling behavior with the graph parameters. To the best of our knowledge, there is no baseline CB algorithm that can be used as a natural baseline for performance comparisons. Furthermore, soft interventions on continuous variables of a CB model are implemented by only LinSEM-UCB of [15]. Therefore, to assess the robustness, we compare our Robust-LCB algorithm with LinSEM-UCB and the standard non-causal UCB algorithm. Parameter setting: We consider three types of graph: chain graph (Figure 2), confounded parallel graph (Figure 3) and hierarchical graph (Figure 4). The noise terms are uniformly sampled from [0, 2]. We consider the general setting that all nodes that have parents can suffer from model deviations. The norm of both observational weights ∥[B]i∥ and the interventional weights ∥[B∗]i∥ are set to 0.5 and 1 respectively. • Chain graph. The chain graph in Figure 2 is a fundamental element of causal graphs. The observational weights [B]i−1,i and interventional weights [B∗]i−1,i for node i ∈ [N] are set to 0.5 and 1, respectively. • Hierarchical graph. For the experiments reported in Figure 5, we use the graph structure with L = 2 layers, and each layer has the same number of nodes d ∈ {1, 2, 3, 4, 5}. For the experiments illustrated in Figures 6c and 7c, we set the number of nodes in Layer 2 to 3 and Layer 1 to 9. We set the observational and interventional weights for node i ∈ [N] to 0.5/ p |pa(i)| and 1/ p |pa(i)|, respectively. • Confounded parallel graph. The confounded parallel graph, as shown in Figure 3, is a mixture of the parallel graph and confounded graph in [8], where node 1 is the parent of all other nodes, and the reward node N is the child of all other nodes. We set the observational and the interventional weights for nodes i ∈ {2, . . . , N− 1} to 0.5 and 1, respectively. For the reward node N, its parents’ observational and interventional weights are set to 0.5/ √ N − 1 and 1/ √ N − 1, respectively. We let the deviations on the model occur at earlier rounds to simulate the worst-case scenario for a given deviation level C. When a deviation occurs on node i ∈ [N], the weights are deliberately altered to change the optimal action, thereby challenging the algorithm’s performance. The simulations are repeated 100 times, and the average cumulative regret is reported. 151 2 3 4 5 graph degree d 102 103 104 105 Cumulative Regret 1013 1015 1017 1019 Robust-LCB Lower bound Upper bound Figure 5: Cumulative regret of Robust-LCB with different graph degree d and L = 2, N = Ld + 1. Scaling behavior with degree d. Figure 5 illustrates the variations of the cumulative regret E[R(T)] versus the graph degree d when T = 40000. We compare the regret of the Robust-LCB algorithm (blue curve with its scale on the left axis), the lower bound characterized in Section 5.2 (black curve with its scale on the left axis), and the upper bound specified in (43) (red curve with its scale on the right axis). All three curves suggest a polynomial behavior in d, which conforms to our theoretical results. 0 10000 20000 30000 40000 Number of Iterations T 0 20000 40000 60000 80000Cumulative Regret Robust-LCB LinSEM-UCB UCB (a) Chain graph. 0 10000 20000 30000 40000 Number of Iterations T 0 20000 40000 60000Cumulative Regret Robust-LCB LinSEM-UCB UCB (b) Confounded parallel graph. 0 10000 20000 30000 40000 Number of Iterations T 0 50000 100000 150000Cumulative Regret Robust-LCB LinSEM-UCB UCB (c) Hierarchical graph Figure 6: Cumulative regret when C = √ T for different algorithms. Comparison of the bounds. Figure 6 compares the cumulative regret of Robust-LCB with that of LinSEM- UCB and UCB under a model deviation level of C = T0.5. It demonstrates that only Robust-LCB achieves sub-linear regret, whereas the other two algorithms incur linear regret with respect to the horizon T. Further- more, it is noteworthy that the LinSEM-UCB always exhibits nearly the worst possible regret as the design of the deviation also showcases the worst case for LinSEM-UCB. In contrast, the regret of UCB tends to be the worst possible outcome as the graph’s complexity increases even when the deviation is not designed for it. Nevertheless, these findings imply that the estimators of these algorithms become ineffective when faced with such deviations, resulting in the selection of a sub-optimal (possibly the worst) arm. Robustness against C. Figure 7 plots the cumulative regret at T = 40000 when the model deviation budget C changes from 2 to 2000. We observe that Robust-LCB and UCB perform slightly better than our algorithm when there is almost no deviation. This can be viewed as the compromise needed to guarantee the robustness 1610 100 1000 deviation level C 0 20000 40000 60000 80000Cumulative Regret Robust-LCB LinSEM-UCB UCB (a) Chain graph. 10 100 1000 deviation level C 0 20000 40000 60000Cumulative Regret Robust-LCB LinSEM-UCB UCB (b) Confounded parallel graph. 10 100 1000 deviation level C 0 50000 100000 150000Cumulative Regret Robust-LCB LinSEM-UCB UCB (c) Hierarchical graph. Figure 7: Cumulative regret at T = 40000 for different deviation levels C. of our algorithm. However, both Robust-LCB and UCB begin to fail with even a minor model deviation, as small as C = 15 for all three scenarios. Furthermore, they tend to reach nearly the worst possible regret when the deviation level rises high enough (e.g., 200 for the hierarchical graph). In the chain graph scenario, the UCB algorithm exhibits moderate regret when C ≤ 300. However, as demonstrated in Figure 6a, it mistakenly treats a non-optimal arm as the optimal one, resulting in linear regret, which happens when C ≥ 10. In comparison, Robust-LCB outperforms when the deviation is more than negligible, as its regret scales sub-linear with the deviation level C. 7 Concluding Remarks In this paper, we have studied the sequential design of interventions over graphical causal models where both the observational and interventional models are unknown and undergo temporal variations. We have considered the general soft intervention model and have designed an algorithm for identifying the intervention mechanism that optimizes a utility function over the graph while exhibiting robustness against model variations. This objective has been naturally cast and analyzed as a causal bandit problem. We have focused on causal structures described by linear structural equation models. We have analyzed the proposed algorithm from a cumulative regret perspective, where we have characterized the dependence of the regret on the graph parameters. The main observations are the following. (i) The proposed algorithm maintains sub-linear or nearly optimal regret under a wide range of relevant model deviation measures. This contrasts sharply with the existing algorithms designed for fixed models, which lose their sub-linear regret rate with a minimum level of model variations. (ii) While the cardinality of the intervention space grows exponentially with the graph size, our regret maintains a linear growth in the graph size. (iii) The regret bound depends on the graph structure only through its parameters (maximum degree and the length of the longest causal path). Finally, we have established an information-theoretic lower bound to demonstrate the tightness of our upper bound. We conclude by providing some potential future directions. The first direction is tightening the gap between the graph-dependent parameters in the upper and lower bounds. Secondly, an alternative model to consider is the setting is the case of non-stationary bandits, in which the temporal variations can be potentially permanent. In such settings, the relevant metric to analyze is the dynamic regret specifically designed for non-stationary bandits. This metric becomes relevant when assuming the absence of a nominal model and the 17permanent effects of deviations. Lastly, we expect that our insights in this paper, including the effectiveness of weighted exploration bonus, can be extended into more general causal bandit settings, as well as other structured bandit settings involving model deviation. Examples include extending it to causal bandits with general link functions and combinatorial bandits. A Additional Notations First, we provide notations that are useful in our analyses. Since we are dealing with matrices, we denote the singular values of a matrix A ∈ RM×N , where M ≥ N, by σ1(A) ≥ σ2(A) ≥ ··· ≥σN (A) . (53) In the proof, we often work with zero-padded vectors and corresponding matrices. As a result, the matrices that contain these vectors have non-trivial null space leading to zero singular values. In such cases, we use the effective smallest singular value that is non-zero. We denote the effective largest and smallest eigenvalues that correspond to effective dimensions of a positive semidefinite matrix A with rank k by σmax (A) ≜ σ1(A) , and σmin (A) ≜ σk(A) . (54) For a square matrix U = AA⊤ ∈ RN×N , we denote the effective largest and smallest eigenvalues by1 λmax (U) ≜ λmax \u0010 AA⊤ \u0011 = σ2 max(A) , and λmin (U) ≜ λmin \u0010 AA⊤ \u0011 = σ2 min(A) . (55) Then we construct data matrices that are highly related to Gram matrices. At time t ∈ N and for any node i ∈ [N], the data matrices Ui(t) ∈ Rt×N and U∗ i (t) ∈ Rt×N consist of the weighted observational and interventional data, respectively. Specifically, for anys ∈ [t] and i ∈ [N], we define \u0002 U⊤ i (t) \u0003 s ≜ 1 {i /∈ a(s)} p wi(s)X⊤ pa(i)(s) , and \u0002 U∗ i ⊤(t) \u0003 s ≜ 1 {i ∈ a(s)} p wi(s)X⊤ pa(i)(s) . (56) Similarly to (2), we denote the relevant data matrices for node i ∈ [N] under intervention a ∈ Aby Ui,a(t) ≜ 1 {i ∈ a}U∗ i (t) + 1 {i /∈ a}Ui(t) , (57) Vi,a(t) ≜ 1 {i ∈ a}V∗ i (t) + 1 {i /∈ a}Vi(t) . (58) Combining (56) and (58), we have Vi,a(t) = U⊤ i,a(t)Ui,a(t) + IN . (59) Similarly we define the data matrices that are related to eVi,a(t) as \u0002eU⊤ i (t) \u0003 s ≜ 1 {i /∈ a(s)}wi(s)X⊤ pa(i)(s) , and \u0002eU∗ i ⊤(t) \u0003 s ≜ 1 {i ∈ a(s)}wi(s)X⊤ pa(i)(s) . (60) 1For matrix V = U + I, we denote the effective smallest eigenvalues by λmin (V) ≜ σ2 min(A) + 1. 18The relevant data matrices for node i ∈ [N] under intervention a ∈ Aare eUi,a(t) ≜ 1 {i ∈ A}U∗ i (t) + 1 {i /∈ A}Ui(t) , and eVi,a(t) = eU⊤ i,a(t) eUi,a(t) + IN . (61) Define N∗ i (t) as the number of times that node i ∈ [N] is intervened, and Ni(t) as its complement, i.e., N∗ i (t) ≜ tX s=1 1 {i ∈ a(s)} , and Ni(t) ≜ t − N∗ i (t) . (62) Accordingly, for any i ∈ [N] and t ∈ N, define Ni,a(t) ≜ 1 {i ∈ a}N∗ i (t) + 1 {i /∈ a}Ni(t) , (63) Furthermore, we define the error and its l-th power of the estimator of B matrices as ∆a(t) ≜ Ba(t) − Ba, ∀i ∈ [N] , and ∆(ℓ) a (t) ≜ Bℓ a(t) − Bℓ a . (64) B Proof of Lemma 4 Proof. When it is clear from context, we use the shorthand terms B for Ba in this proof. According to the given definitions ∆(ℓ) A = [∆A + B]ℓ − Bℓ, each term in the binomial expansion of ∆(ℓ) A can be represented as a product involving factors of ∆A and B. For any ℓ ∈ [L] and k ∈ [ℓ] ∪ {0}, there exist \u0000ℓ k \u0001 terms that consist of the ∆A factor appearing k times and the B factor appearing(ℓ − k) times. We denote the set of these product terms by Hℓ,k ≜ {H : H has ∆A factor k times and B factor ℓ − k times} . (65) Therefore, sets Hℓ,1, . . . ,Hℓ,ℓ contain all valid products composed of B and ∆A. Hence, we can write the expansion of ∆(ℓ) A as ∆(ℓ) A = ℓX k=1 X H∈Hℓ,k H . (66) To bound the norm of∆(ℓ) A , we first bound the norm of each element in the summation. We show by induction that for any ℓ ∈ [L], k ∈ [ℓ] ∪ {0}, and H ∈ Hℓ,k, ∥[H]i∥ ≤d ℓ−1 2 βkλk , ∀i ∈ [N] , where λ ≜ max i∈[N] λ−1/2 min \u0000 Mi \u0001 . (67) Consider ℓ = 1. For k = 0, we have H1,0 = {B} and ∥[B]i∥ ≤1. For k = 1, H1,1 = {∆A}, and ∥[∆A]i∥ ≤ ∥[∆A]i ⊙ 1(pa(i))∥Mi λ−1/2 min \u0000 Mi \u0001 = ∥[∆A]i∥Mi λ−1/2 min \u0000 Mi \u0001 ≤ βλ . (68) Therefore, (67) holds for ℓ = 1. Now assume that (67) holds for every 1, . . . , ℓ−1, for some ℓ ≥ 2. Consider a product term H ∈ Hℓ,k, for some k ∈ [ℓ] ∪ {0}. The first factor of H is either Ba or ∆A, and we analyze the induction step for each of these possibilities separately. 19Case 1. If H starts with B, represent it by H = B ¯H, where ¯H ∈ Hℓ−1,k and k ∈ [ℓ − 1] ∪ {0}. Using the induction assumption for the elements of set Hℓ−1,k we obtain ∥[H]i∥2 = \r\r(B ¯H)i \r\r2 = X u,v∈pa(i) [B]u,i[B]v,i ¯H⊤ v ¯Hu (69) ≤ d X u∈pa(i) ([B]u,i)2 \r\r ¯Hu \r\r2 ≤ dℓ−1β2kλ2 X u∈pa(i) ([B]u,i)2 = dℓ−1β2kλ2k∥[B]i∥2 | {z } ≤1 (70) ≤ dℓ−1β2kλ2k , (71) where in (70) we use Cauchy–Schwarz inequality and the inductive hypothesis (67). Case 2.. If H starts with ∆A represent it by H = ∆A ¯H, where ¯H ∈ Hℓ−1,k−1 and k ∈ [ℓ]. Similarly to the first case, we have ∥[H]i∥2 = \r\r[∆A ¯H]i \r\r2 = X u,v∈pa(i) [∆A]u,i[∆A]v,i[ ¯H]⊤ v [ ¯H]u (72) ≤ d X u∈pa(i) ([∆A]u,i)2 \r\r[ ¯H]u \r\r2 ≤ dℓ−1β2k−2λ2k−2 X u∈pa(i) ([∆A]u,i)2 (73) = dℓ−1β2k−2λ2k−2 ∥[∆A]i∥2 ≤ dℓ−1β2k−2λ2k−2β2λ−1 min \u0000 Mi \u0001 | {z } ≤λ2 (74) ≤ dℓ−1β2kλ2k . (75) where (73) we use Cauchy-Schwarz inequality, and in (74) we use the fact that [∆A]i = [∆A]i ⊙ 1(pa(i)) and ∥[∆A]i∥ ≤ ∥[∆A]i∥Mi λ−1/2 min \u0000 Mi \u0001 . Taking the square-roots of both sides in (71) and (75) yields ∥[H]i∥ ≤d ℓ−1 2 βkλk , (76) which is the desired inequality for all k ∈ [ℓ] ∪ {0}. This completes the proof of (67) by induction. The final result follows by applying (76) to each term in the sum (66). \r\r\r \u0002 ∆(ℓ) A \u0003 i \r\r\r = \r\r\r\r\r ℓX k=1 X H∈Hℓ,k [H]i \r\r\r\r\r (77) ≤ ℓX k=1 X H∈Hℓ,k ∥[H]i∥ ≤d ℓ−1 2 ℓX k=1 |Hℓ,k|βkλk = d ℓ−1 2 ℓX k=1 \u0012ℓ k \u0013 βkλk (78) < d ℓ−1 2 (β + 1)ℓλ = d ℓ−1 2 (β + 1)ℓ max i∈[N] λ−1/2 min \u0000 Mi \u0001 , (79) where (79) is due to the binomial expansion of (β + 1)ℓ and Mi ⪰ I, ∀i ∈ [N] such that λ ≤ 1. C Proof of Theorem 5 The proof mechanism follows the same line of arguments as [15, Theorem 5] but with major distinctions. Firstly, we provide a new lemma on the bound on the power of estimation error ∆(ℓ) a (t) ≜ Bℓ a(t − 1) − Bℓ a, 20Furthermore, the effect of the weighted ridge regression, distinct confidence ellipsoids, and the definition of error events, namely eEi,n(t) and eE∗ i,n(t) are investigated. Finally, we bound a new function of eigenvalues of weighted Gram matrices Vi,a(t)(t) and eVi,a(t). To begin with, we first state the lemma which upper bounds the estimation error. Based on the above estimation error, we start the proof by decomposing the regret in (4) as E[R(T)] = E \" TX t=1 \u0000 µa∗ − µa(t) \u0001 # (80) = E \" TX t=1  f(Ba∗) − f(Ba(t)), ϵ(t) \u000b # (81) = E \" TX t=1  f(Ba∗) − f(Ba(t)), ν \u000b # , (82) where the second equation is a result of [15, Lemma 1], which is stated as Lemma 1. The last equation is due to the inner product being a linear function and E[ϵ] = ν. Now define the error events Ei and E∗ i for i ∈ [N] for each estimator Ei ≜ \u001a ∀t ∈ [T] : ∥[B(t − 1)]i − [B]i∥Vi(t)[ eVi(t)]−1Vi(t) ≤ βt \u001b , (83) and E∗ i ≜ \u001a ∀t ∈ [T] : ∥[B∗(t − 1)]i − [B∗]i∥V∗ i (t)[ eV∗ i (t)]−1V∗ i (t) ≤ βt \u001b , (84) where the βt is chosen as βt ≜ p 2 log (2NT ) + d log (1 +m2t/dC2) + 1 +m , (85) Let E∩ denote the event that all of the events {Ei, E∗ i : i ∈ [N]} occur simultaneously, i.e., E∩ ≜ \u0010 N\\ i=1 Ei \u0011\\\u0010 N\\ i=1 E∗ i \u0011 . (86) Then by Lemma 2, we have P(Ec ∩) ≤ NX i=1 P(Ec i ) + NX i=1 P(E∗ i c) ≤ NX i=1 \u0012 1 2NT + 1 2NT \u0013 = 1 T . (87) Then we can bound the regret as follows. R(T) ≤ E \" 1 {Ec ∩} TX t=1  f(Ba∗) − f(Ba(t)), ν \u000b # + E \" 1 {E∩} TX t=1 r(t) # (88) ≤ 2mTP(Ec ∩) + E \" 1 {E∩} TX t=1 r(t) # (89) ≤ 2m + E \" 1 {E∩} TX t=1  f(Ba∗) − f(Ba(t)), ν \u000b # . (90) 21According to the arm selection rules, we have UCBa∗(t) ≤ UCBa(t)(t). Let eBa be the matrix that attains UCBa(t), i.e., eBa = arg max [Θ]i∈Ci,a(t) ⟨f(Θ), ν⟩ . (91) Then under the event E∩, we have  f(Ba∗) − f(Ba(t)), ν \u000b ≤ UCBa∗(t) −  f(Ba(t)), ν \u000b (92) ≤ UCBa(t)(t) −  f(Ba(t)), ν \u000b (93) = ⟨f(eBa(t)) − f(Ba(t)), ν⟩ . (94) Subsequently, we use (94) to derive an upper bound for the expectation term in (90), thereby eliminating the dependence on a∗. E \" 1 {E∩} TX t=1  f(Ba∗) − f(Ba(t)), ν \u000b # ≤ E \" 1 {E∩} TX t=1 D f(eBa(t)) − f(Ba(t)), ν E# (95) ≤ ∥ν∥E \" 1 {E∩} TX t=1 \r\r\rf(eBa(t)) − f(Ba(t)) \r\r\r # (96) ≤ ∥ν∥E \" 1 {E∩} TX t=1 \r\r\rf(eBa(t)) − f(Ba(t)(t)) \r\r\r # + ∥ν∥E \" 1 {E∩} TX t=1 \r\r\rf(eBa(t)(t)) − f(Ba(t)) \r\r\r # . (97) Note that (96) follows from the Cauchy-Schwarz inequality while (97) is due to the triangle inequality. Next, we examine the norm term within the expectation expression in (97). By applying the definition of f, we obtain \r\r\rf(eBa(t)) − f(Ba(t)(t)) \r\r\r ≤ LX ℓ=1 \r\r\r \u0002eBℓ a(t) \u0003 N − \u0002 Bℓ a(t)(t) \u0003 N \r\r\r , (98) \r\r\rf(eBa(t)(t)) − f(Ba(t)) \r\r\r ≤ LX ℓ=1 \r\r\r \u0002 Bℓ a(t)(t) \u0003 N − \u0002 Bℓ a(t) \u0003 N \r\r\r . (99) By the definition of Gram matrices, we have eVi,a(t)(t) ⪯ Vi,a(t)(t) , Vi,a(t)(t)[ eVi,a(t)(t)]−1Vi,a(t)(t) ⪰ Vi,a(t)(t) ⪰ IN . (100) We consider the following tuple of matrices \u0010 eBa(t), Ba(t)(t), Vi,a(t)(t)[ eVi,a(t)(t)]−1Vi,a(t)(t), i∈ [N] \u0011 , (101) and \u0010 Ba(t)(t), Ba(t), Vi,a(t)(t)[ eVi,a(t)(t)]−1Vi,a(t)(t), i∈ [N] \u0011 . (102) 22These tuple of matrices satisfy the condition of Lemma 4. We further define the maximum confidence radius βT ≜ p 2 log (2NT ) + d log (1 +m2T/dC 2) + 1 +m and λ(t) ≜ max i∈[N] r λmax \u0010 eVi,a(t)(t) \u0011 λmin \u0000 Vi,a(t)(t) \u0001 , λ T ≜ E \" TX t=1 λ(t) # . (103) By using Lemma 4 to upper bound the tow terms in (99), we have E \" 1 {E∩} TX t=1  f(Ba∗) − f(Ba(t)), ν \u000b # (104) < 2 LX ℓ=1 d ℓ−1 2 \u0000 βt + 1 \u0001ℓE \" 1 {E∩} TX t=1 max i∈[N] λ−1/2 min \u0000 Vi,a(t)(t)[ eVi,a(t)(t)]−1Vi,a(t)(t) \u0001 # (105) ≤ 2 LX ℓ=1 d ℓ−1 2 \u0000 βT + 1 \u0001ℓE \" 1 {E∩} TX t=1 λ(t) # (106) ≤ 2λT LX ℓ=1 d ℓ−1 2 \u0000 βT + 1 \u0001ℓ (107) ≤ 4λT \u0000 βT + 1 \u0001Ld L−1 2 , (108) where in (108), we use the fact that PL i=1 qL ≤ 2qL for q ≥ 2 and √ d(βT + 1) > 2. Bounding E hPT i=1 λ(t) i What remains is to bound the term E hPT i=1 λ(t) i , where λ(t) is a function involving the eigenvalues of both Gram matrices Vi,a(t)(t) and eVi,a(t)(t). To proceed, we define the second-moment matrices and its effective largest and smallest eigenvalues as Σi,a(t) ≜ EX∼Pa h Xpa(i)(t)X⊤ pa(i)(t) i , (109) κmin ≜ min i∈[N],a∈A,t∈[T] σmin (Σi,a(t)) , (110) κmax ≜ max i∈[N],a∈A,t∈[T] σmax (Σi,a(t)) , (111) where κmin > 0 is guaranteed since there is no deterministic relation between nodes and their patients. These variables are inherent to the system and remain unknown to the learner. Given our focus on the weighted OLS estimator, we also introduce singular values related to auxiliary variables X′ pa(i)(t) ≜ p wi(t)Xpa(i)(t) and eXpa(i)(t) ≜ wi(t)Xpa(i)(t). Accordingly, we define the second weighted moment matrices as follows. Σ′ i,a,wi(t) ≜ EX∼Pa h wi(t)Xpa(i)(t)X⊤ pa(i)(t) i , (112) eΣi,a,wi(t) ≜ EX∼Pa h w2 i (t)Xpa(i)(t)X⊤ pa(i)(t) i . (113) To bound the singular value of the weighted second moment, we first need uniform bounds for the weights. We find a bound for the norm of ∥Xpa(i)(t)∥[ eVi,a(t)]−1 across all a ∈ A. This yields the following result. ∥Xpa(i)(t)∥[ eVi,a(t)(t)]−1 ≤ 1 λ1/2 min( eVi,a(t)(t)) ∥Xpa(i)(t)∥ ≤m . (114) 23Then, the weights can be bounded by 1 Cm ≤ wi(t) = min ( 1 C , 1 C∥Xpa(i)(t)∥[ eVi,a(t)(t)]−1 ) ≤ 1 C . (115) Subsequently, we can bound the minimum and maximum singular values of matricesΣ′ i,a,wi(t) and eΣi,a,wi(t). κ′ min = 1 Cm κmin ≤ σmin \u0000 Σ′ i,a,wi(t) \u0001 ≤ σmax \u0000 Σ′ i,a,wi(t) \u0001 ≤ 1 C κmax = κ′ max , (116) ˜κmin = 1 C2m2 κmin ≤ σmin \u0010 eΣi,a,wi(t) \u0011 ≤ σmax \u0010 eΣi,a,wi(t) \u0011 ≤ 1 C2 κmax = ˜κmax . (117) Moreover, we have \r\r\rX′ pa(i)(t) \r\r\r ≤ m′ = 1√ C m and \r\r\reXpa(i)(t) \r\r\r ≤ ˜m = 1 C m. In order to proceed, we need upper and lower bounds for the maximum and minimum singular values ofUi,a(t)(t). However, these bounds depend on the number of non-zero rows of Ui,a(t)(t) matrices, which equals to values of the random variable Ni,a(t)(t). Firstly, we define the weighted constants γn ≜ max \b αm2√n, α2m2\t , (118) γ′ n ≜ max \b αm′2√n, α2m′2\t , (119) ˜γn ≜ max \b α ˜m2√n, α2 ˜m2\t , ∀n ∈ [T] . (120) Then for every i ∈ [N], t∈ [T], and n ∈ [t], we define the error events corresponding to the maximum and minimum singular values of Ui(t) and eUi(t) as Ei,n(t) ≜ ( Ni(t) = n and \u001a σmin (Ui(t)) ≤ q max \b 0, nκ′ min − γ′n \t or σmax (Ui(t)) ≥ p nκ′max + γ′n o) , (121) E∗ i,n(t) ≜ ( N∗ i (t) = n and \u001a σmin (U∗ i (t)) ≤ q max \b 0, nκ′ min − γ′n \t or σmax (U∗ i (t)) ≥ p nκ′max + γ′n o) , (122) eEi,n(t) ≜ ( Ni(t) = n and n σmin \u0010 eUi(t) \u0011 ≤ p max {0, n˜κmin − ˜γn} or σmax \u0010 eUi(t) \u0011 ≥ p n˜κmax + ˜γn o) , (123) eE∗ i,n(t) ≜ ( N∗ i (t) = n and n σmin \u0010 eU∗ i (t) \u0011 ≤ p max {0, n˜κmin − ˜γn} or σmax \u0010 eU∗ i (t) \u0011 ≥ p n˜κmax + ˜γn o) . (124) 24In other words, the event Ei,n(t) indicates the situation in which either σmin (Ui(t)) or σmax (Ui(t)) violates the established lower and upper bounds. Likewise, E∗ i,n(t), eEi,n(t) and eE∗ i,n(t) are associated with the singular values of U∗ i (t), eUi(t) and eU∗ i (t). The next result shows that these events occur with a low probability. Lemma 10. The probability of the error events Ei,n(t), E∗ i,n(t), eEi,n(t) and eE∗ i,n(t) defined in (121) to (124) are upper bounded as P(Ei,n(t)) ≤ d exp \u0012 −3α2 16 \u0013 , P(E∗ i,n(t)) ≤ d exp \u0012 −3α2 16 \u0013 , (125) and P(eEi,n(t)) ≤ d exp \u0012 −3α2 16 \u0013 , P(eE∗ i,n(t)) ≤ d exp \u0012 −3α2 16 \u0013 . (126) Proof. This is a direct result from [15, Lemma 3], where we use κ′ min, κ′ max and γ′ for events Ei,n(t) and E∗ i,n(t) and using ˜κmin, ˜κmax and ˜γ for events eEi,n(t) and eE∗ i,n(t). The only difference is that one need to use the following lemma, which is an immediate result of [15, Lemma 6]. Lemma 11. Consider matrices U and A that satisfy \r\r\rU⊤U − A \r\r\r ≤ γ . (127) Then we have, σmax (U) ≤ p σmax (A) + γ , and σmin (U) ≥ p max{0, σmin (A) − γ} . (128) Now that we have bounds on the probability of error events, we define the union error eventE∪ as E∪ ≜ {∃ (i, t, n) : i ∈ [N], t∈ [T], n∈ [t], Ei,n(t) or E∗ i,n(t) or eEi,n(t) or eE∗ i,n(t)} . (129) By taking a union bound and using Lemma 10 we have P(E∪) ≤ NX i=1 TX t=1 tX n=1 \u0010 P(Ei,n(t)) + P(E∗ i,n(t)) + P(eEi,n(t)) + P(eE∗ i,n(t)) \u0011 (130) ≤ 2NT (T + 1)d exp \u0012 −3α2 16 \u0013 . (131) Bounding E h 1 {E∪}PT t=1 λ(t) i Since λmin \u0000 Vi,a(t)(t) \u0001 ≥ 1, we have the following unconditional upper bound. λ(t) = max i∈[N] r λmax \u0010 eVi,a(t)(t) \u0011 λmin \u0000 Vi,a(t)(t) \u0001 ≤ max i∈[N] r λmax \u0010 eVi,a(t)(t) \u0011 (132) = max i∈[N] vuutλmax  tX s=1 1 {i ∈ a(s)}w2 i (s)Xpa(i)(s)X⊤ pa(i)(s) + IN ! (133) 25≤ max i∈[N] vuut1 + 1 C2 tX s=1 1 {i ∈ a(s)}λmax \u0010 Xpa(i)(s)X⊤ pa(i)(s) \u0011 (134) ≤ max i∈[N] vuut1 + 1 C2 tX s=1 λmax \u0010 Xpa(i)(s)X⊤ pa(i)(s) \u0011 (135) = max i∈[N] vuut1 + 1 C2 tX s=1 \r\rXpa(i)(s) \r\r2 (136) ≤ r m2 C2 t + 1 , (137) where (137) follows from the fact that ∥X∥ ≤m. Hence, we have E \" 1 {E∪} TX t=1 λ(t) # (132) ≤ E \" 1 {E∪} TX t=1 r λmax \u0010 eVN,a(t)(t) \u0011# (138) (137) ≤ E \" 1 {E∪} TX t=1 r m2 C2 t + 1 # (139) = E[1 {E∪}] TX t=1 r m2 C2 t + 1 (140) = P(E∪) TX t=1 r m2 C2 t + 1 | {z } A1 . (141) Furthermore, A1 is bounded as TX t=1 r m2 C2 t + 1 ≤ m C √ T + 1 + T−1X t=1 (m C √ t + 1) (142) ≤ m C √ T + T + Z T t=1 m C √ tdt (143) = m C √ T + T + 2m 3C (T3/2 − 1) . (144) By setting α = q 16 3 log(2dNT 5/2(T + 1)), we obtain E \" 1 {E∪} TX t=1 p m2t + 1 # (141) ≤ P(E∪) TX t=1 p m2t + 1 (145) (131) ≤ NT (T + 1)d exp(log(dNT 5/2(T + 1)))| {z } =T−3/2 TX t=1 p m2t + 1 (146) (144) ≤ T−3/2 \u0012m C √ T + T + 2m 3C (T3/2 − 1) \u0013 (147) 26< m CT + 2m 3C + 1 . (148) Bounding E h 1 {Ec ∪}PT t=1 λ(t) i Considering the event Ec ∪, which encompasses all the events {Ec i,n(t), E∗ i,n c(t), eEc i,n(t), eE∗ i,n c(t) : i ∈ [N], t∈ [T], n∈ [t]} being hold. Therefore, we can use the following bounds on the singular values σmin \u0000 Ui,a(t)(t) \u0001 ≥ q max \b 0, Ni,a(t)(t)κ′ min − γ′n \t , (149) σmax \u0010 eUi,a(t)(t) \u0011 ≤ q Ni,a(t)(t)˜κmax + ˜γn . (150) Thus, the targeted sum can be upper-bounded by E \" 1 {Ec ∪} TX t=1 λ(t) # = E  1 {Ec ∪} TX t=1 max i∈[N] r λmax \u0010 eVi,a(t)(t) \u0011 λmin \u0000 Vi,a(t)(t) \u0001   (151) = E  1 {Ec ∪} TX t=1 max i∈[N] q σ2max \u0000eUi,a(t)(t) \u0001 + 1 σ2 min \u0000 Ui,a(t)(t) \u0001 + 1   (152) ≤ E TX t=1 max i∈[N]   q Ni,a(t)(t)˜κmax + ˜γn + 1 max \b 0, Ni,a(t)(t)κ′ min − γ′n \t + 1   . (153) It is worth noting that the term in the summation has a critical point, and we bound the two regions separately. To initiate this process, we define the function h(x) as h(x) ≜ √x˜κmax + ˜γn + 1 max \b 0, xκ′ min − γ′n \t + 1 , x > 0 . (154) In order to analyze the behavior of the function h, we introduce τ ≜ α2m6 κ2 min as the critical point. Note that when x ≤ τ, we have xκ′ min < γn. In this case, h(x) is equal to h(x) = p x˜κmax + ˜γn + 1 , (155) which is an increasing function over the region. To upper bound the h function when x > τ, we define the g function when x > τas follows. g(x) ≜ p xκmax + αm2√x xκmin/m − αm2√x + C xκmin/m − αm2√x , x ≥ τ . (156) Then when x > τ, we establish the following relation. h(x) = √x˜κmax + ˜γn + 1 xκ′ min − γ′n + 1 = p xκmax + γn + C2 xκmin/m − γn + C < √xκmax + γn xκmin/m − γn = g(x) , (157) where the equality holds due to the definitions in (118)-(120) and the inequality holds due to the fact that√ x+a2 y+a < √x y + a y when a >0. Moreover, when x > τ, we have γn = αm2√n. Based on this, we can use 27the g function to upper bound h function when x > τ. However, since g tends to infinity at τ, we begin bounding from a larger constant, specifically 4τ. However, for the summation when bounding x ≤ 4τ, we need to consider the following monotonicity. Lemma 12. h(x) and g(x) are both decreasing functions when x > τ, where τ is defined as α2m6 κ2 min . Proof. See Appendix E. Now we are ready to bound the last term E \" 1 {Ec ∪} TX t=1 λ(t) # ≤ E TX t=1 max i∈[N] h(Ni,a(t)(t)) . (158) We define the set of time indices at which the chosen actions are under-explored as Kh ≜ \b t ∈ [T] | ∃i ∈ [N] : Ni,a(t)(t) ≤ 4τ \t . (159) It can be readily verified that |Kh| ≤8Nτ . Furthermore, when x ∈ Kh, we have h(x) ≤ h(τ) ≤ 1 C q κmaxτ + αm2√τ + 1 , x≤ τ . (160) Then we can upper bound the summation when Kh occurs as follows. E \u0014 TX t=1 1 {t ∈ Kh} max i∈[N] h(Ni,a(t)(t)) \u0015 ≤ 2 NX i=1 4τX n=1 h(n) ≤ 8Nτ \u0012 1 C q κmaxτ + αm2√τ + 1 \u0013 . (161) Now we only need to bound the remaining part when t ̸∈ Kh E TX t=1 1 {t ∈ Kc h} max i∈[N] h(Ni,a(t)(t)) . (162) Note that when t ∈ Kc h, we have Ni,a(t)(t) > τfor all i ∈ [N], and we have max i∈[N] h(Ni,a(t)(t)) ≤ max i∈[N] g(Ni,a(t)(t)) . (163) Furthermore, note that there might be multiple nodes that achieve maxi∈[N] g(Ni,a(t)(t)). Without loss of generality, we select the solution with minimum index as arg max (or arg min). We denote the node that achieves the maximum value of the function g as it. it ≜ arg max i∈[N] g(Ni,a(t)(t)) = arg min i∈[N] Ni,a(t)(t) , (164) where the last equality is a result of the fact that g is a decreasing function when x ≥ τ. Note that it does not capture whether i belongs to a(t) or not. To address this challenge, we define the sets of time indices where it = i for each of these two cases as follows. Si ≜ {t ∈ [T] : t ̸∈ Kh, it = i, i /∈ a(t)} , ∀i ∈ [N] , (165) 28and S∗ i ≜ {t ∈ [T] : t ̸∈ Kh, it = i, i∈ a(t)} , ∀i ∈ [N] . (166) Denote the elements of Si by Si,1, . . . , Si,|Si|. Until time Si,n, the event {it = i, i /∈ a(t)} occurs exactly n times outside Kh set. Similarly {it = i, i∈ a(t)} occurs n times outside Kh set until time S∗ i,n. Then Ni(Si,n) = Si,nX t=1 1 {i /∈ a(t)} ≥ Si,nX t=1 1 {it = i, i /∈ a(t)} = n + 4τ , (167) and N∗ i (S∗ i,n) = S∗ i,nX t=1 1 {i ∈ a(t)} ≥ S∗ i,nX t=1 1 {it = i, i∈ a(t)} = n + 4τ . (168) Using the above results and noting that g is a decreasing function, we obtain TX t=1 1 {t ∈ Kc h} max i∈[N] g(Nit,a(t)(t)) = TX t=1 1 {t ∈ Kc h}g(Nit,a(t)(t)) (169) = NX i=1 X t:t∈Si g(Ni(t)) + NX i=1 X t:t∈S∗ i g(N∗ i (t)) (170) = NX i=1 |Si|X n=1 g(Ni(Si,n))| {z } (167) ≤ g(n+4τ) + NX i=1 |S∗ i |X n=1 g(N∗ i (S∗ i,n)) | {z } (168) ≤ g(n+4τ) (171) ≤ NX i=1 |Si|+4τX n=4τ+1 g(n) + NX i=1 |S∗ i |+4τX n=4τ+1 g(n) . (172) We bound the discrete sums through integrals and define Gτ (y) = Z y x=4τ g(x)dx , y ≥ 4τ . (173) Since g(x) is a positive, non-increasing function, for any k ∈ N, k≥ 4τ + 1 we have kX n=4τ+1 g(n) ≤ Z k x=4τ g(x)dx = Gτ (k) . (174) Then, the summation is upper bounded by TX t=1 1 {t ∈ Kc h} max i∈[N] g(Nit,a(t)(t)) ≤ NX i=1 |Si|+4τX n=4τ+1 g(n) + NX i=1 |S∗ i |+4τX n=4τ+1 g(n) (175) (174) ≤ NX i=1 Gτ (|Si| + 4τ) + NX i=1 Gτ (|S∗ i | + 4τ) . (176) Since g(x) is positive and decreasing, and G(y) is defined as an integral of the g function with a positive first derivative and negative second derivative, it can be deduced thatG is a concave function. NX i=1 Gτ (|Si| + 4τ) + NX i=1 Gτ (|S∗ i | + 4τ) ≤ 2N × Gτ   1 2N NX i=1 |Si| + 1 2N NX i=1 |S∗ i | + 4τ ! (177) 29≤ 2N × Gτ \u0012 T 2N + 4τ \u0013 . (178) Next, we proceed to establish an upper bound for the function G. Lemma 13. The G function can be upper bounded as Gτ \u0012 T 2N + 4τ \u0013 ≤ 2 √mκmax κmin  r T 2N + √τ log  r T 2N + √τ !! + 4 κmin 4 r T 2N + 2 s αm5 κ3 min log   q 1 τ 4 q T 2N + 4√ 4 + 1 q 1 τ 4 q T 2N + 4√ 4 − 1   + 2m log \u0012 κmin m q T 2N + αm2 \u0013 κmin C . (179) Proof. See Appendix F Combining the results in (148), (172), (178) and (179), let E1 denote the accumulation of terms that exhibit at most logarithmic growth rates with respect to T and C. E1 = 4N √mκmax κmin √τ log  r T 2N + √τ ! + 4N s αm5 κ3 min log   q 1 τ 4 q T 2N + 4√ 4 + 1 q 1 τ 4 q T 2N + 4√ 4 − 1   (180) + 8Nτ \u0012 1 C q κmaxτ + αm2√τ + 1 \u0013 + m CT + 2m 3C + 1 . (181) Therefore, the final result for the bound is E \" TX t=1 λ(t) # ≤ 4√mκmax κmin √ NT + 8 κmin 4 r N3T 2 (182) + 4Nm κmin log   κmin m r T 2N + αm2 ! C + E1 . (183) Plugging (183) into (108), we have E[R(T)] ≤ 4(βT + 1)Ld L−1 2 E \" TX t=1 λ(t) # = ˜O(dL−1 2 √ NT + dL−1 2 NC ) . (184) 30D Discussion on LinSEM-UCB Lemma 14. By setting the robust confidence radius β′ t(δ) = 1 + Cm2 + p 2 log(δ) + d log(1 + m2t/d) . (185) Then with probability at least 1 − 2δ, for any node i ∈ [N] and t ∈ N, the OLS estimator satisfies [B]i ∈ Ci(t) , and [B∗]i ∈ C∗ i (t) . (186) Proof. We will provide the proof corresponding to the observational weights |[B(t)]i, while the proof for the interventional weights [B∗(t)]i follows similarly. For any node i ∈ [N], let us decompose the error in estimation ∥[B(t)]i − Bi∥Vi(t) as follows. ∥[B(t)]i − Bi∥Vi(t) (187) = \r\r\r\r[Vi(t)]−1 X s∈[t−1],i/∈a(t) Xpa(i)(s) \u0002 X⊤ pa(i)(s) ([B]i + [∆(s)]i) + ϵi(s) − νi \u0003 − [B]i \r\r\r\r Vi(t) (188) ≤ \r\r\r\r X s∈[t−1],i/∈a(t) Xpa(i)(s)(ϵi(s) − νi) \r\r\r\r [Vi(t)]−1 | {z } I1: Stochastic error (189) + \r\r\r\r X s∈[t−1],i/∈a(t) Xpa(i)(s)X⊤ pa(i)(s)[∆(s)]i \r\r\r\r [Vi(t)]−1 | {z } I2: Fluctuation error + ∥[B]i∥[Vi(t)]−1 | {z } I3: Regularization error . (190) Similarly to [ 37, Theorem 20.5], with probability at least 1 − δ the sum of regularization error and the stochastic error I1 + I3 can be bounded by 1 + p 2 log(δ) + d log(1 + m2T/d). However, the fluctuation error I2 arises due to model deviation, and it needs to be upper-bounded to obtain the confidence sequence for the OLS estimator. Similar to the existing literature on robust linear bandits [32, 38], one can bound this term by triangle inequality and Cauchy-Schwarz inequality, as follows. I2 ≤ X s∈[t−1],i/∈a(t) \r\r\rVi(t)−1/2Xpa(i)(s)X⊤ pa(i)(s)[∆B(s)]i \r\r\r ≤ m X s∈[t−1],i/∈a(t) ∥Xpa(i)(s)∥[Vi(t)]−1 ∥[∆B(s)]i∥ ≤ Cm2 . (191) Thus we conclude that with probability 1 − δ ∥[B(t)]i − Bi∥Vi(t) ≤ β′ t(δ) . (192) Then we have the following theorem for the regret of LinSEM-UCB under model deviation. 31Theorem 15. Under a deviation budget C, by setting δ = 1 2NT and β′ t(δ) according to (185), the average cumulative regret of LinSEM-UCBis upper bounded by E[R(T)] ≤ 2m + ˜O(β′L T d L 2 √ NT ) , (193) where β′ T = 1 + Cm2 + p 2 log(2NT ) + d log(1 + m2t/d) Proof. This theorem directly follows [15, Theorem 2] by changing the confidence radius. E Proof of Lemma 12 We first prove that h(x) is a decreasing function. We start with the derivation of h(x) and show that it is negative in the domain of x ≥ τ. To simplify the notations, let c1 = κmax, c2 = αm2, c3 = κmin. The first order derivative of h(x) has the following closed-form. d dxh(x) = (2c2 − 4c3 √x)C2 + (c2 + 2c1 √x)C − 2c1c3x3/2 + c2 2 √x − 3c2c3x 4√x p c1x + c2 √x + C(c3x − c2 √x + C)2 . (194) Considering when x > τ, the denominator of the derivative is positive, it is adequate to show the numerator is negative. We notice that the numerator is a quadratic function of C, and we have the following relation 2c2 − 4c3 √x ≤ −2c2 < 0 , c 2 + 2c1 √x >0 . (195) Thus, it is sufficient to show − (c2 + 2c1 √x)2 4(2c2 − 3√c3) − 2c1c3x3/2 + c2 2 √x − 3c2c3x <0 (196) ⇐ (c2 + 2c1 √x)2 + 4(−2c1c3x3/2 + c2 2 √x − 3c2c3x)(3√c3 − 2c2) < 0 (197) ⇐ (c2 + 2c1 √x)2 + 8(−2c1c3x3/2 − 2c2 2 √x)c2 < 0 (198) ⇐ (c2 2 + 4c1c2 √x − 16c3 2 √x) + (4c2 1x − 16c1c3x3/2) < 0 (199) ⇐ (2αm4 − 15α3m6)√x + κmax(4m2 − 16αm3)x <0 , (200) where (198) is due to 2c2 − 4c3 √x ≥ −2c2 > 0 and c2 2 √x − 3c2c3x ≤ −2c2 2 √x <0. The (200) is true since α >2 and the assumption m ≥ 1. From the above analysis, we can conclude that h(x) is a decreasing function for x > τ. Then we prove the g(x) function is decreasing when x > τ. The derivative of g(x) can be calculated as d dxg(x) = −2κmaxκminx/m + α2m4 − 3α2m2κmin √x/m 4x p κmax + αm2√x(κminx/m − αm2√x)2 − κmin/m − αm2 2√x (κminx/m − αm2√x)2 < 0 . (201) We conclude that h(x) and g(x) are both decreasing when x > τ. 32F Proof of Lemma 13 We start from the following results for integrals. Lemma 16 (Useful integral). For a, b >0, the following integrals hold. Z 1√ax − abdx = 2 √x a + 2b a log(|√x − b|) + c . (202) Z 4√x ax − b√xdx = 4 4√x a − 2 √ b a3/2 log  \f\f\f\f\f 1 + pa b 4√x 1 − pa b 4√x \f\f\f\f\f ! + c , (203) Z 1 ax − b√xdx = 2 log(|a√x − b|) a + c . (204) Then the G function can be upper bounded as Gτ \u0012 T 2N + 4τ \u0013 = Z T 2N +4τ x=4τ g(x)dx (205) = Z T 2N +4τ x=4τ p xκmax + αm2√x xκmin/m − αm2√xdx + Z T 2N +4τ x=4τ C xκmin/m − αm2√xdx (206) ≤ Z T 2N +4τ x=4τ rmκmax κmin 1p xκmin/m − αm2√x dx + Z T 2N +4τ x=4τ r αm2(1 + mκmax κmin ) x1/4 xκmin/m − αm2√xdx + Z T 2N +4τ x=4τ C xκmin/m − αm2√xdx (207) ≤ Z T 2N +4τ x=4τ s m2κmax κmin 1√xκmin − √τκmin dx + Z T 2N +4τ x=4τ r αm2(1 + mκmax κmin ) x1/4 xκmin/m − αm2√xdx + Z T 2N +4τ x=4τ C xκmin/m − αm2√xdx (208) < 2 √mκmax κmin  r T 2N + 4τ + √τ log  r T 2N + 4τ − √τ ! − 2√τ ! + 4 4 q T 2N + 4τ κmin + 2 s αm5 κ3 min log   q κmin αm3 4 q T 2N + 4τ + 1 q κmin αm3 4 q T 2N + 4τ − 1   − 4 4√ 4τ κmin + 2m log \u0012 κmin m q T 2N + 4τ − αm2 \u0013 κmin C (209) ≤ 2 √mκmax κmin  r T 2N + √τ log  r T 2N + √τ !! 33+ 4 κmin 4 r T 2N + 2 s αm5 κ3 min log   q 1 τ 4 q T 2N + 4√ 4 + 1 q 1 τ 4 q T 2N + 4√ 4 − 1   + 2m log \u0012 κmin m q T 2N + αm2 \u0013 κmin C . (210) where (208) is due to the inequality √x − y ≥ √x − y√x when x ≥ y, and we use Lemma 16 in (209). References [1] Ossama Ahmed, Frederik Träuble, Anirudh Goyal, Alexander Neitz, Manuel Wuthrich, Yoshua Bengio, Bernhard Schölkopf, and Stefan Bauer, “Causalworld: A robotic manipulation benchmark for causal structure and transfer learning,” in Proc. International Conference on Learning Representations, virtual, May 2021. [2] Md. Bahadur Badsha and Audrey Qiuyan Fu, “Learning causal biological networks with the principle of mendelian randomization,” Frontiers in Genetics, vol. 10, May 2019. [3] Siqi Liu, Kay Choong See, Kee Yuan Ngiam, Leo Anthony Celi, Xingzhi Sun, and Mengling Feng, “Reinforcement learning for clinical decision support in critical care: Comprehensive review,” Journal of Medical Internet Research, vol. 22, no. 7, July 2020. [4] Yan Zhao, Mitchell Goodman, Sameer Kanase, Shenghe Xu, Yannick Kimmel, Brent Payne, Saad Khan, and Patricia Grao, “Mitigating targeting bias in content recommendation with causal bandits,” in Proc. ACM Conference on Recommender Systems Workshop on Multi-Objective Recommender Systems, Seattle, W A, September 2022. [5] Judea Pearl, Causality, Cambridge University Press, Cambridge, UK, 2009. [6] Jiaqi Zhang, Chandler Squires, and Caroline Uhler, “Matching a desired causal state via shift interven- tions,” in Proc. Advances in Neural Information Processing Systems, virtual, December 2021. [7] Yangyi Lu, Amirhossein Meisami, and Ambuj Tewari, “Causal bandits with unknown graph structure,” in Proc. Advances in Neural Information Processing Systems, virtual, December 2021. [8] Finnian Lattimore, Tor Lattimore, and Mark D Reid, “Causal bandits: Learning good interventions via causal inference,” in Proc. Advances in Neural Information Processing Systems, Barcelona, Spain, December 2016. [9] Arnoud De Kroon, Joris Mooij, and Danielle Belgrave, “Causal bandits without prior knowledge using separating sets,” in Proc. Conference on Causal Learning and Reasoning, Eureka, CA, April 2022. [10] Shi Feng, Nuoya Xiong, and Wei Chen, “Combinatorial causal bandits without graph skeleton,” arXiv:2301.13392. 34[11] Blair Bilodeau, Linbo Wang, and Daniel M Roy, “Adaptively exploiting d-separators with causal bandits,” in Proc. Advances in Neural Information Processing Systems, New Orleans, LA, December 2022. [12] Mikhail Konobeev, Jalal Etesami, and Negar Kiyavash, “Causal bandits without graph learning,” arXiv:2301.11401. [13] Aurghya Maiti, Vineet Nair, and Gaurav Sinha, “A causal bandit approach to learning good atomic interventions in presence of unobserved confounders,” in Proc. Conference on Uncertainty in Artificial Intelligence, Eindhoven, Netherlands, August 2022. [14] Shi Feng and Wei Chen, “Combinatorial causal bandits,” in Proc. the AAAI Conference on Artificial Intelligence, Washington, D.C., 2023. [15] Burak Varici, Karthikeyan Shanmugam, Prasanna Sattigeri, and Ali Tajer, “Causal bandits for linear structural equation models,” The Journal of Machine Learning Research, vol. 24, no. 297, pp. 1–59, 2023. [16] Biwei Huang, Kun Zhang, Jiji Zhang, Ruben Sanchez-Romero, Clark Glymour, and Bernhard Schölkopf, “Behind distribution shift: Mining driving forces of changes and causal arrows,” in Proc. IEEE International Conference on Data Mining, New Orleans, LA, November 2017. [17] Kun Zhang, Biwei Huang, Jiji Zhang, Clark Glymour, and Bernhard Schölkopf, “Causal discovery from nonstationary/heterogeneous data: Skeleton estimation and orientation determination,” in Proc. International Joint Conferences on Artificial Intelligence, Melbourne, Australia, August 2017. [18] Kun Zhang, Mingming Gong, Joseph Ramsey, Kayhan Batmanghelich, Peter Spirtes, and Clark Glymour, “Causal discovery in the presence of measurement error: Identifiability conditions,” arXiv:1706.03768, 2017. [19] Kun Zhang, Jiji Zhang, Biwei Huang, Bernhard Schölkopf, and Clark Glymour, “On the identifiability and estimation of functional causal models in the presence of outcome-dependent selection.,” in Proc. Conference on Uncertainty in Artificial Intelligence, New York City, NY , June 2016. [20] Ruibo Tu, Cheng Zhang, Paul Ackermann, Karthika Mohan, Hedvig Kjellström, and Kun Zhang, “Causal discovery in the presence of missing data,” in Proc. International Conference on Artificial Intelligence and Statistics, Okinawa, Japan, April 2019. [21] Junying Li, Deng Cai, and Xiaofei He, “Learning graph-level representation for drug discovery,” arXiv:1709.03741, 2017. [22] Yangyi Lu, Amirhossein Meisami, Ambuj Tewari, and William Yan, “Regret analysis of bandit problems with causal background knowledge,” in Proc. Conference on Uncertainty in Artificial Intelligence , virtual, August 2020. 35[23] Vineet Nair, Vishakha Patil, and Gaurav Sinha, “Budgeted and non-budgeted causal bandits,” in Proc. International Conference on Artificial Intelligence and Statistics, virtual, April 2021. [24] Scott Sussex, Anastasiia Makarova, and Andreas Krause, “Model-based causal bayesian optimization,” in Proc. International Conference on Learning Representations, Kigali, Rwanda, May 2023. [25] Avishek Ghosh, Sayak Ray Chowdhury, and Aditya Gopalan, “Misspecified linear bandits,” in Proc. Conference on Artificial Intelligence, San Francisco, CA, February 2017. [26] Tor Lattimore, Csaba Szepesvari, and Gellert Weisz, “Learning with good feature representations in bandits and in RL with a generative model,” in Proc. International Conference on Machine Learning, virtual, July 2020. [27] Dylan J Foster, Claudio Gentile, Mehryar Mohri, and Julian Zimmert, “Adapting to misspecification in contextual bandits,” in Proc. Advances in Neural Information Processing Systems, virtual, December 2020. [28] Sanath Kumar Krishnamurthy, Vitor Hadad, and Susan Athey, “Adapting to misspecification in contextual bandits with offline regression oracles,” in Proc. International Conference on Machine Learning, Stockholm, Sweden, July 2021. [29] Yingkai Li, Edmund Y Lou, and Liren Shan, “Stochastic linear optimization with adversarial corruption,” arXiv:1909.02109, 2019. [30] Ilija Bogunovic, Arpan Losalka, Andreas Krause, and Jonathan Scarlett, “Stochastic linear bandits robust to adversarial attacks,” in Proc. International Conference on Artificial Intelligence and Statistics, virtual, April 2021. [31] Chung-Wei Lee, Haipeng Luo, Chen-Yu Wei, Mengxiao Zhang, and Xiaojin Zhang, “Achieving near instance-optimality and minimax-optimality in stochastic and adversarial linear bandits simultaneously,” in Proc. International Conference on Machine Learning, virtual, July 2021. [32] Heyang Zhao, Dongruo Zhou, and Quanquan Gu, “Linear contextual bandits with adversarial corrup- tions,” arXiv:2110.12615, 2021. [33] Chen-Yu Wei, Christoph Dann, and Julian Zimmert, “A model selection approach for corruption robust reinforcement learning,” in Proc. International Conference on Algorithmic Learning Theory, Paris, France, March 2022. [34] Jiafan He, Dongruo Zhou, Tong Zhang, and Quanquan Gu, “Nearly optimal algorithms for linear contextual bandits with adversarial corruptions,” in Proc. Advances in Neural Information Processing Systems, New Orleans, LA, December 2022. 36[35] Yoan Russac, Claire Vernade, and Olivier Cappé, “Weighted linear bandits for non-stationary environ- ments,” in Proc. Advances in Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 2019. [36] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári, “Improved algorithms for linear stochastic bandits,” in Proc. Advances in Neural Information Processing Systems, Granada, Spain, December 2011. [37] Tor Lattimore and Csaba Szepesvári, Bandit Algorithms, Cambridge University Press, Cambridge, UK, 2020. [38] Qin Ding, Cho-Jui Hsieh, and James Sharpnack, “Robust stochastic linear contextual bandits under adversarial attacks,” in Proc. International Conference on Artificial Intelligence and Statistics, virtual, March 2022. 37",
      "references": [
        "Causalworld: A robotic manipulation benchmark for causal structure and transfer learning",
        "Learning causal biological networks with the principle of mendelian randomization",
        "Reinforcement learning for clinical decision support in critical care: Comprehensive review",
        "Mitigating targeting bias in content recommendation with causal bandits",
        "Causality",
        "Matching a desired causal state via shift interventions",
        "Causal bandits with unknown graph structure",
        "Causal bandits: Learning good interventions via causal inference",
        "Causal bandits without prior knowledge using separating sets",
        "Combinatorial causal bandits without graph skeleton",
        "Adaptively exploiting d-separators with causal bandits",
        "Causal bandits without graph learning",
        "A causal bandit approach to learning good atomic interventions in presence of unobserved confounders",
        "Combinatorial causal bandits",
        "Causal bandits for linear structural equation models",
        "Behind distribution shift: Mining driving forces of changes and causal arrows",
        "Causal discovery from nonstationary/heterogeneous data: Skeleton estimation and orientation determination",
        "Causal discovery in the presence of measurement error: Identifiability conditions",
        "On the identifiability and estimation of functional causal models in the presence of outcome-dependent selection.",
        "Causal discovery in the presence of missing data",
        "Learning graph-level representation for drug discovery",
        "Regret analysis of bandit problems with causal background knowledge",
        "Budgeted and non-budgeted causal bandits",
        "Model-based causal bayesian optimization",
        "Misspecified linear bandits",
        "Learning with good feature representations in bandits and in RL with a generative model",
        "Adapting to misspecification in contextual bandits",
        "Adapting to misspecification in contextual bandits with offline regression oracles",
        "Stochastic linear optimization with adversarial corruption",
        "Stochastic linear bandits robust to adversarial attacks",
        "Achieving near instance-optimality and minimax-optimality in stochastic and adversarial linear bandits simultaneously",
        "Linear contextual bandits with adversarial corruptions",
        "A model selection approach for corruption robust reinforcement learning",
        "Nearly optimal algorithms for linear contextual bandits with adversarial corruptions",
        "Weighted linear bandits for non-stationary environments",
        "Improved algorithms for linear stochastic bandits",
        "Bandit Algorithms",
        "Robust stochastic linear contextual bandits under adversarial attacks"
      ],
      "meta_data": {
        "arxiv_id": "2310.19794v2",
        "authors": [
          "Zirui Yan",
          "Arpan Mukherjee",
          "Burak Varıcı",
          "Ali Tajer"
        ],
        "published_date": "2023-10-30T17:58:01Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses lack of robustness of causal bandit (CB) algorithms when the underlying causal/statistical model drifts over time. In linear SEM causal graphs with known topology but unknown (and time-varying) edge weights under both observational and soft-interventional regimes, shows existing fixed-model CB methods can suffer linear regret with very few deviations (≈T^{1/(2L)} instances, L=longest path). Proposes Robust-LCB, a robust causal bandit algorithm with regret upper bound \\tilde{O}(d^{(L-1)/2}(\\sqrt{NT}+NC)) under a deviation budget C (covering both deviation frequency and aggregate deviation), and proves information-theoretic lower bounds \\Omega(d^{L/2-2} \\max\\{\\sqrt{T}, d^2 C\\}) and \\Omega(d^{L/2}C) for the deviation term, establishing near-optimal \\tilde{O}(\\sqrt{T}) scaling when C=o(\\sqrt{T}) and sublinear regret over a broad range of deviations. Also demonstrates the algorithm avoids exponential dependence on the intervention space size (2^N) by estimating shared SEM parameters (B,B*) instead of arm means directly.",
        "methodology": "Model: linear structural equation model X = B^\\top X + \\epsilon on a known DAG; soft interventions change columns of B to B* for intervened nodes; reward is expected value of a leaf node (node N), expressible as \\mu_a = \\langle f(B_a), \\nu\\rangle with f(A)=\\sum_{\\ell=0}^L [A^\\ell]_N. Temporal deviations: actual model D_a(t)=B_a(t)+\\Delta_a(t) with budgets C_DF (deviation frequency) or C_AD (aggregate deviation), unified as C. Algorithm (Robust-LCB): (i) robust parameter estimation of observational/interventional edge weights via weighted OLS (W-OLS) per node/column using only rounds where that node is (not) intervened; (ii) time-uniform confidence ellipsoids in a nonstandard norm V \\tilde{V}^{-1} V for W-OLS under bounded deviations; (iii) UCB action selection where UCB_a(t)=\\max_{\\Theta: [\\Theta]_i\\in C_{i,a}(t)} \\langle f(\\Theta),\\nu\\rangle. Key design: adaptive weights w_i(t)=min{1/C, 1/(C \\|X_{pa(i)}(t)\\|_{\\tilde{V}_{i,a(t)}(t)^{-1}})} that downweight likely-outlier samples via an inverse “weighted exploration bonus,” enabling deviation terms to enter regret linearly (NC) rather than catastrophically (C^L). Analysis uses decomposition of estimation error into stochastic/regularization and fluctuation parts, self-normalized concentration for predictable weights, and a compounding-error lemma bounding how column-wise estimation errors propagate along length-L causal paths with degree d.",
        "experimental_setup": "Synthetic simulations (no real datasets). Graph families: (1) Chain graph (N=4) with observational edge weights 0.5 and interventional weights 1 along the chain; (2) Confounded parallel graph (e.g., N=5): node 1 parent of others; reward node child of nodes 2..N-1, with weights scaled (0.5 and 1, with reward-node incoming weights normalized by \\sqrt{N-1}); (3) Hierarchical graphs with L=2 layers and varying maximum in-degree d, with weights set to 0.5/\\sqrt{|pa(i)|} (observational) and 1/\\sqrt{|pa(i)|} (interventional). Noise terms sampled uniformly from [0,2]. Deviations injected in early rounds (worst-case) on nodes with parents; when deviation occurs, edge weights are altered adversarially to change the optimal intervention. Compared algorithms: Robust-LCB vs LinSEM-UCB (fixed-model causal bandit baseline for linear SEMs) vs standard non-causal UCB over interventions. Metrics: average cumulative regret over horizon (typically T=40,000) averaged over 100 Monte Carlo runs. Additional scaling experiment: vary degree d (with N=Ld+1) and compare empirical regret to theoretical upper and lower bounds; robustness experiments vary deviation budget C (e.g., C=\\sqrt{T} or C from 2 to 2000).",
        "limitations": "Assumes known and fixed causal graph topology (DAG) and known exogenous-noise mean vector \\nu; only edge weights (observational/interventional) are unknown. Focuses on linear SEMs with bounded/sub-Gaussian noise and bounded variables/weights; reward is a nonlinear function of parameters, but analysis leverages linearity of SEM. Requires learner to know deviation budget C (or an upper bound) to set weights/confidence radii. Regret bounds scale exponentially in longest path length L and polynomially in degree d; upper bound has additional \\sqrt{N} dependence not matched by the lower bound (gap in N and in d–L exponent), and the UCB computation involves maximizing \\langle f(\\Theta),\\nu\\rangle over confidence sets (may be computationally heavy for large N, though parameter estimation avoids 2^N arm enumeration). Deviation model captures bounded magnitude/frequency/aggregate but not fully nonstationary “permanent drift” settings (dynamic regret not analyzed). Empirics are limited to synthetic graphs and specific deviation injection strategies.",
        "future_research_directions": "Tighten theory: reduce/eliminate the extra \\sqrt{N} dependence in the regret upper bound (e.g., via node-wise induction using path lengths L_i) and close the remaining gap in d/L scaling between upper and lower bounds. Extend to nonstationary settings with permanent drift (no nominal model) and analyze dynamic regret. Broaden model class beyond linear SEMs to nonlinear link functions or RKHS/GP SEMs with time-varying parameters; adapt the weighted-exploration-bonus/robust confidence ellipsoid ideas to these settings. Explore robustness when the causal structure itself changes over time (topology drift) or is unknown and must be learned online under deviations. Investigate computationally efficient approximations for the UCB optimization over confidence sets for large graphs/intervention sets, and extensions to other structured bandits such as combinatorial bandits and more general soft-intervention constraints.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Compressed Video Prompt Tuning",
      "full_text": "Anisotropic behaviour of S-wave and P-wave states of heavy quarkonia at finite  magnetic field  Manohar Lala, Siddhartha Solankia, Rishabh Sharmaa, and Vineet Kumar Agotiyaa1  a Department of Physics, Central University of Jharkhand Ranchi, India, 835 222  We studied the effect of momentum space anisotropy on heavy quarkonium states using an extended  magnetized effective fugacity quasiparticle model (EQPM). Both the real and imagin ary part of the  potential has been modified through the dielectric function by including the anisotropic parameter ξ. The  real part of the medium modified potential becomes more attractive in the presence of the anisotropy and  constant magnetic field. The binding energy of the 1S, 2S, and 1P quarkonium states including anisotropy  effects for both the oblate and the isotropic case were studied. We find that the binding energy of QQ¯  states becomes stronger in the presence of anisotropy. However, the magnetic field is found to reduce the  binding energy. The thermal width of the charmonium and bottomonium 1 S states have been studied at  constant magnetic field eB = 0.3 GeV 2 for isotropic and prolate cases. The effect of magnetic field on the  mass spectra of the 1P state for the oblate case was also examined. The dissociation temperature for the  1S, 2S, and 1P charmonium and bottomonium have been determined to be higher for the oblate case with  respect to the isotropic  case.  KEYWORDS: Momentum anisotropy, Dissociation  Temperature, Extended Quasi Particle Model, Magnetic  Field, Quark-gluon Plasma, Thermal width, Heavy Ion  Collision.  PACS numbers:25.75.-q;24.85.+p;12.38.Mh ;  12.38.Gc, 05.70.Ce, 25.75.+r, 52.25.Kn   I. INTRODUCTION  Quarkonium states are sensitive to several important  features of the Quark Gluon Plasma (QGP), including  Landau damping and energy loss as mentioned in ref. [1,  2, 3]. Based on the experimental observations,  quarkonium suppression, among the other signatur es, is  regarded as the clear probe of the QGP [4, 5, 6, 7, 8]. In the  five decades since the discovery of the J/ψ in 1974 [9, 10],  quarkonium dissociation due to color screening in the  deconfined medium suggested by Matsui and Satz [11] has  become a pioneering research area in the particle physics.  Several studies such as [12, 13, 14, 15, 16, 17] discuss the  important refinements essential to the study of  quarkonium in the thermal Hot Quantum  Chromodynamics (QCD) medium. The physics of the heavy  quarkonia, a  strongly interacting matter, in the last two  decades has great interest in the presence of the magnetic  field upto the scale square mass of the pion   or even  larger details of which can be found in ref. [18]. Such kind  of the studies are very relevant to the highly dense  astrophysical compact bodies like the magnetars [19] and  also useful for the study of the cosmological aspects [20].  Besides of these facts, the main motivation to study the    1 agotiya81@gmail.com  effect of magnetic field in the heavy quarkonia was  triggered by the fact that the order of the magnitude of this  field e.g. eB = m2π at Relativistic Heavy Ion Collider (RHIC)  and  at Large Hadron Collider(LHC) during the  lead-lead collision could be produced in the laboratory  when the two heavy ions, traveling nearly equal to the  speed of light, colliding with each other at zero impact  parameter in the colliding region [21, 22, 23, 24, 25]. It is  believed that such a large magnitude of the magnetic field  produces at very early stages of the universe shortly after  the b ig bang. However, it is not certain how long these  generated magnetic field and to what extent survives in  the thermalisation process of the QGP formation. Various  theoretical models-based studies as well as the Lattice  Quantum Chromodynamics (LQCD) predic tion, discuss  several phenomena effecting the properties of the  quarkonia in the presence of the background magnetic  field. However, the effects related to the magnetic field are  of particular interest because of the fact that the heavy  quarkonia are very sensitive to the earlier condition of the  universe. Studies such as [26, 27, 28, 29, 30, 31, 32, 33]  briefly explained the quarkonium spectra and the  production rates in the magnetic field regime. Since QCD  with non -zero magnetic field eB does not have a sign  problem, one can obtain the QCD phase diagram in the T −  eB plane using Monte Carlo calculations of the LQCD as the  first ab-initio principle. Since, we know that the magnetic  field generally causes the momentum anisotropy which is  responsible for the instability in the Yang -Mills fields.  Hence, this momentum anisotropy has played a vital role  in the evolution of the QCD medium. When the charged  particle placed in the strong magnetic field, the energy  associated with circular motion of the charged parti cle,  because of Lorentz force, is discretized (quantized). These  quantized energy levels, due to the magnetic field effect,  are known as Landau levels. However, in the presence of  2  the strong magnetic field qf eB >> T 2 only lowest Landau  levels are populated (l = 0). This indicates the importance  of the lowest Landau level dynamics.  In the present case, we are working under strong magnetic  field qf eB >> T 2 using the effective fugacity quasi -particle  model to study the anisotropic behaviour of the heavy  quarkonia. One of the remarkable reasons to include the  momentum anisotropy is that the QGP produced in the  non-central collision does not possess isotropy. Moreover,  momentum anisotropy present at each and every stage of  the heavy ion collisions. This fact trig gered most to study  the anisotropic effect on the quarkonium properties in  magnetic field regime. Several authors [34, 35, 36, 37, 38,  39, 40, 41] studied various observables of the quark gluon  plasma by considering the momentum anisotropy.  Following the r ef. [42, 43, 44], the anisotropy has been  introduced at the levels of distribution function. The gluon  self-energy used to obtain gluon propagator and in turn to  determine the dielectric permittivity in the presence of the  anisotropy. The present manuscr ipt is organized in the  following manner:  Quasi-particle Debye screening mass in the presence of  magnetic field has been briefly discussed in section II (A).  The quark-antiquark potential in the anisotropic medium  is described in section II (B). The effect  of momentum  space anisotropy on the binding energy, dissociation  temperature, thermal width and the mass spectra of the  quarkoinum states in the presence of magnetic field has  been briefly studied in section III. We discuss the results  of the present work in section IV. Finally, we conclude our  work in section V.   II. MODEL SETUP  A. Effective Quasi-Particle Model Extension in magnetic field  and Debye Screening Mass  In the quasi-particle description, the system of the  interacting particles is supposed to be non -interacting or  weakly interacting by means of the effective fugacity [45]  or with the effective mass [46, 47]. Nambu -Jona-Laisino  (NJL) and P olykov Nambu -Jona-Laisino (PNJL) quasi - particles models [48], self -consistent quasi -particles  model [49, 50, 51] etc. include the effective masses. Here,  we considered the effective fugacity quasi -particle model  (EQPM), in the presence of magnetic field, which  interprets the QCD EoS as non-interacting quasi-partons  with effective fugacity parameter zg for gluons and zq for  quarks encoding all the interacting effects taking place in  the medium. The distribution function for quasi -gluons  and the quasi-quarks/quasi-antiquarks [52] are given as:   𝑓𝑔/𝑞 = 𝑓𝑔/𝑞𝑒−𝛽𝐸𝑝 1∓𝑓𝑔/𝑞𝑒−𝛽𝐸𝑝. (1)  It is noted that Ep = |p⃗ | = p for the gluons whereas     𝐸𝑝 = √𝑝2 + 𝑚2 + 2𝑛|𝑞𝑓𝑒𝐵|  for quarks/antiquarks. Where ‘ l’ denotes the Landau  levels. In high energy physics, ¯ h = c = KB = 1. Therefore, β  = T1 . One can look the physical significance of the effective  fugacity in the following dispersion relation:                           ωg = T2∂T ln(zg) + p. (2)     ωq = T2∂T ln(zg) + qPz2 + m2 + 2l|qfeB|. (3)  The first term in the above equations Eq. 2 and Eq. 3  represents the collective excitation of the quasi -gluons  and quasi-quarks(quasi-partons). Thus, it is inferred that  the effective fugacity zg and zq describes the hot QCD  medium effects. The effective fuga city like in the other  quasi-particle model modify the Tµν energy tensor as  discussed in [53]. Now, the extended version of the  Effective fugacity quasi-particles model in the presence of  magnetic field requires the modification of the dispersion  relation as defined in the above Eq. 2 and Eq. 3 by the  relativistic discretized Landau Levels. Thus, in view of this,  the quark/antiquark distribution function can be obtained  as below:   𝑓𝑞 𝑜 = 𝑧𝑞𝑒 −𝛽√𝑝𝑧2+𝑚2+2𝑛|𝑞𝑓𝑒𝐵| 1+𝑧𝑔𝑒 −𝛽√𝑝𝑧2+𝑚2+2n|𝑞𝑓𝑒𝐵| . (4)  The effect of the magnetic field B = Bzˆ is taken along the  z-axis. Since the plasma contains both the charged and the  quasi-neutral particles, hence it shows collective  behaviour. The Debye mass is an important quantity to  describe the screening of the color forces in the Hot QCD  medium. The Debye screening mass can be defined as the  ability of the plasma to shield out the electric potential  applied to it. In the studies [54, 55, 56, 57], detailed  definition of the Debye mass can be found. To determine  the Debye mass, in terms of the magnetic fiel d, we start  from the gluon-self energy as below:   𝑚𝐷 2 = Π00(𝜔 = 𝑝,|p⃗ | ⟶ 0 (5)  According to the [58], gluon self-energy modified as:  Π00(𝜔 = 𝑝,|𝑝⃖| ⟶ 0) = 𝑔2|𝑒𝐵| 2𝜋2𝑇 ∫ ∞ 0 𝑑𝑝𝑧𝑓𝑞 0(1 − 𝑓𝑞 0)       (6)  Thus, Debye mass for quarks using the distribution  function defined by Eq. 4 is given below as:   𝑚𝐷 2 = 4𝛼 𝜋𝑇 |𝑒𝐵|∫ ∞ 0 𝑑𝑝𝑧𝑓𝑞 0(1 − 𝑓𝑞 0). (7) 3  Since the magnetic field has no effect on the gluons,  therefore, the gluonic contribution to the Debye mass will  remain unchanged. In other words, the d istribution  function for the gluons with or without magnetic field will  remain intact. For perturbative QCD, in the presence of  magnetic field, the Debye screening mass can be derived  with the application of kinetic theory approach. Both  these approaches provide the similar results for the Debye  mass in the presence of the magnetic field. So, the Debye  mass for the nf = 3 and Nc = 3 will:  𝑚𝐷 2 = 4𝛼 ( 6𝑇2 𝜋 𝑃𝑜𝑙𝑦𝐿𝑜𝑔[2,𝑧𝑔] + 3𝑒𝐵 𝜋 𝑧𝑞 1+𝑧𝑞 )                (8)  The Debye mass for the ideal EoS [ zq,g = 1] representing  non-interacting quarks and gluons becomes:   𝑚𝐷 2 = 4𝜋𝛼 (𝑇2 + 3𝑒𝐵 2𝜋2)                      (9)  α is two loop coupling constant depending upon the  temperature [59] and is given below:   𝛼(𝑇) = 6𝜋 (33−2𝑛𝑓)𝑙𝑛( 𝑇 𝜆𝑇 ) {1 − (3(153−19𝑛𝑓)) (33−2𝑛𝑓)2 𝑙𝑛(2𝑙𝑛( 𝑇 𝜆𝑇 )) 𝑙𝑛( 𝑇 𝜆𝑇 ) }     .  where nf denotes the number of flavor which is 3 in our  case and λT is the QCD renormalisation scale.  B.  Quark-antiquark potential in the anisotropic medium  The solution of the Scrodinger equation (SE), although  a century has passed, is still an important tool for both  the physicists and chemists. The SE played a vital role to  obtained not only the energy spe ctrum of the di -atomic  and poly-atomic molecule but also the spectrum of the  heavy quarkonium system. The solution of the SE for the  different potentials, as found in [60, 61, 62, 63], has been  obtained using generalized Boopp’s shift method and  standard p eturbation theory. In the present work,  medium modified potential [64] has been used to  investigate the properties of the heavy quarkonia. The  Cornell potential having both Coulombic as well as the  string part [64] is given by:             V(𝑟) = − 𝛼 𝑟 + 𝜎𝑟                                                  (10)  To modified this s tatic potential (Eq.10), we use the  Fourier transformation. In the above equation, α and σ  are the coupling constant and the string tension  respectively. Here we take the two-loop coupling  constant depending upon on the temperature. The value  of the string  constant has been taken σ = 0 .184 GeV 2.  Whereas ‘r’ is the effective radius of the respective  quarkonium states. The reason behind the modification  of the potential is that the string tension doesn’t vanish  at or near the transition temperature Tc and transition is  just “crossover” from hadronic to quark -gluon plasma  (QGP). Since the heavy ions collisions are non -central,  the spatial anisoptropy generates at the very early  stages. As the QGP expands or evolves with time,  different pressure gradient arises which are responsible  for mapping the spatial anisotropy to the momentum  anisoptropy. In the present formalism, anisotropy has  been introduced at the particle’s distribution level.  Following the studies [43, 44, 65], the isotropic function  has been employ ed to determine the anisotropic  distribution function given below:   𝑓(𝐩) → 𝑓𝜉(𝐩) = 𝐶𝜉 𝑓(√𝑝2 + 𝜉(𝑝 ⋅ 𝑛̂)2) (11)  where f(p), denotes the effective fugacity quasi -particle  isotropic distribution function [66, 67, 68], nˆ is unit  vector representing the direction of momentum  anisotropy. The parameter ξ represents the anisotropy of  the medium. For isotropic case ξ = 0, for oblate form and  prolate form the parameter ξ in the nˆ direction lies in the  ranges ξ > 0 and −1 < ξ < 0 respectively. Since the effects of  different equations of state (EoS) enter through the Debye  mass (mD). So, to make Debye mass intact from the effects  of anisotropy, present in the medium, we use the following  normalisation constant Cξ. With this normalisation  constant Cξ, the Debye mass will remain same for both the  isotropic and anisotropic cases [65]. Therefore, the  normalization constant, Cξ, is written as:  𝐶𝜉 = { √|𝜉| 𝑡𝑎𝑛−1√|𝜉|           𝑖𝑓   − 1 ≤ 𝜉 < 0 √𝜉 𝑡𝑎𝑛−1√𝜉          𝑖𝑓      𝜉 ≥ 0                         (12)      Eq. 12 can be simplified as:   ,            (13)  For small anisotropic effect, ξ can be written as:  if − 1 ≤ ξ < 0   if ξ ≥ 0                                                                                   (14)  or simply in the small ξ limit, we have:   .                                            (15)  4  Following the assumption given by [14, 69, 70], potential  of the dissipative anisotropic medium has been modified  in the Fourier transform by dividing it with medium  dielectric permittivity,    . (16)  Taking the inverse of Fourier transform defined above, the  in medium/corrected potential reads off:   . (17)  V(¯ k) is the Fourier transform of V( r) defined by Eq. 10  and given as      . (18)  magnetic field  Thus, to obtained the modified form of the potential it is  necessary to calculate the dielectric permittivity   and this can be done by two methods: (I) Using the gluon  self-energy in finite temperature QCD [71, 72] and (II)  Using Semi -classical transport theory (many particles  kinetic theory up to one loop order) [44, 73, 74]. By using  the above-mentioned methods, one can obtained the  gluon self-energy Πµν which in turns provide static gluon  propagator as given below:       ∆µν = k2gµν − kµkν + Πµν(ω,k). (19)  Now, from the temporal component of the gluon  propagators [75], the dielectric tensor in the Fourier space  can be written as below:   . (20)  where ∆ 00 represents the static limit of the “00”  component of the gluon propagators in the Coulomb  gauge. Also Eq. 20, according to linear response theory,  provides the relation between the dielectric permittivity  and the ∆ 00. The real and the imaginary part of the   dielectric tensor obtained from the real part of the retard  propagator and imaginary part of symmetric propagators  respectively [76], in the static limit, are given below by  Eqs. 21 and 22:      𝜖−1(𝑘) = 𝜋 𝑇 𝑚𝐷 2 ( 𝑘2 𝑘(𝑘2+𝑚𝐷 2 )2 − 𝜉𝑘2 ( −1 3𝑘(𝑘2+𝑚𝐷 2 )2 + 3sin2𝜃𝑛 4𝑘(𝑘2+𝑚𝐷 2 )2 − 2𝑚𝐷 2 (3sin2(𝜃𝑛)−1) 3𝑘(𝑘2+𝑚𝐷 2 )3 ))                                      (22)  where  cos(θn) = cos(θr)cos(θpr)+sin(θr)sin(θpr)cos(φpr).  (23)    FIG. 1. Variation of real potential with distance ‘r’(fm) for isotropic and the oblate case at fixed value of temperature and  0.12   0.15   0.21  0.18  r (fm)  -1.65  -1.5  -1.35  -1.2  -1.05  -0.9  -0.75  ξ  =0   (if   θ  =0  o  )  ξ  =0.3 (  if   θ  =0  o  )  ξ  =0   (if   θ  =90  o  )  ξ  =0.3 (  if   θ  =90  o  )  T=300 MeV and eB=0.3 GeV  2 5  θn is the angle between the particle momentum p, and  anisotropy direction nˆ. θr is angle between r, and n. The  azimuthal φpr and the polar angle θpr lies between p and r.  The term mD denotes the quasiparticle Debye mass which  depends on the temperature and magnetic field and is  briefly described in sec.III(A).  As the limit, T → 0 real part of the potential goes to unity  and when ξ = 0, the imaginary part becomes zero. With  these limits,  the modified form of the potential simply  reduces to the Cornell potential. Now, by substituting the  real part of the dielectric tensor ) defined by Eqs. 21  in Eq. 17, the real part of the interquark potential can be  written as below:    After separating t he coulombic ( α) and string ( σ) term  from the above equation, the real part of the potential will  look like:  R𝑒[𝑉(𝑟,𝜉,𝑇,𝑒𝐵)] = (1 + 𝜉 3)( 𝜎 𝑚𝐷 − 𝛼 ( 1 𝑠 + 1 2)𝑚𝐷) + 𝜉 𝑠 16 ( 7 3 − cos(2𝜃𝑟))                                                          (25)  Similarly, by putting the imaginary part of the dielectric  tensor defined by Eqs. 22 in Eq. 17, the imaginary part of  complex potential will become:  Im[𝑉(𝑟,𝜉,𝑇,𝑒𝐵)] = 𝜋𝑇𝑚𝐷 2 ∫ 𝑑3𝐤 (2𝜋)3/2 (𝑒𝑖𝐤⋅𝐫 − 1)(−√ 2 𝜋 𝛼 𝑘2 − 4𝜎 √2𝜋𝑘4)( 𝑘 (𝑘2+𝑚𝐷 2 )2 − 𝜉 ( −𝑘 3(𝑘2+𝑚𝐷 2 )2 + 3𝑘sin2𝜃𝑛 4(𝑘2+𝑚𝐷 2 )2 − 2𝑚𝐷 2 𝑘(3sin2(𝜃𝑛)−1) 3(𝑘2+𝑚𝐷 2 )3 ))                           (26)    Again, separating the coulombic (α) and the string (σ) part  of above equation 26, the i maginary potential can be  rewritten as:  Im[𝑉(𝑟,𝜃𝑟,𝑇,𝑒𝐵)] = −(1 + 𝜉 3)𝑇 ( 𝛼𝑠2 3 + 𝜎𝑠4 30𝑚𝐷 2 )log( 1 𝑠) + 𝜉𝑇 log( 1 𝑠){( 𝛼𝑠2 10 + 𝜎𝑠4 140𝑚𝐷 2 ) − cos2𝜃𝑟( 𝛼𝑠2 10 + 𝜎𝑠4 70𝑚𝐷 2 )}                                        (27)                                             III. PROPERTIES OF HEAVY QUARKONIA  A. Binding energy of the different quarkonium states  Real binding energies of the heavy quarkonium can be  obtained by solving the Schr¨odinger equation with the  first order perturbation in anisotropy parameter, ξ, as  done in [75, 77, 78]. With this, the real binding energy (EB)  becomes:  Re[𝐸𝐵(𝑇,𝜉,𝑒𝐵)] = ( 𝑚𝑄𝜎2 𝑚𝐷 4 𝑛2 + 𝛼𝑚𝐷 + 𝜉 3 ( 𝑚𝑄𝜎2 𝑚𝐷 4 𝑛2 + 𝛼𝑚𝐷 + 2𝑚𝑄𝜎2 𝑚𝐷 4 𝑛2 ))                                                                                  (28)  This is valid only for the ground and first excited states   of the charmonium and bottomonium i.e. J/ψ, Υ, Ψ’ , and Υ’  . But in order to find the binding energies of 1 P states of  the charmonium and bottomonium, the correction term  (potential and the kinetic  energy) must be  added to the  binding energy of the Ψ’, and Υ’ . These correction term  have been obtained by using the variational treatment  method [79, 80, 81, 82, 83], in which the total energy  consists of the kinetic energy correction and most  importantly the correction added to the spin  dependent  potential which makes the ψ and χc (both are first excited  state) degenerate and hence obeys the Pauli’s exclusion  principle. The correction energy term, as found in the [81,  82, 84], is given below               TABLE I. Lower bound of dissociation temperature for isotropic  case at Tc = 197MeV  Temperatures are in the unit of Tc  For isotropic case (ξ = 0)  States eB = 0.3GeV 2 eB = 0.5GeV 2 eB = 0.7GeV 2  J/ψ 1.5482 1.3578 1.0406  Υ 2.0304 1.8908 1.6751  Ψ’ 1.0532 0.7614 0.3553  Υ’ 1.4340 1.2309 0.8756  χc 1.2944 1.0406 0.5203  χb 1.6116 1.4213 1.091    TABLE II. Lower bound of dissociation temperature for oblate  case at Tc = 197MeV  Temperatures are in the unit of Tc  For oblate case (ξ = 0.3)  States eB = 0.3GeV 2 eB = 0.5GeV 2 eB = 0.7GeV 2  J/ψ 1.6497 1.4847 1.1928  Υ 2.1700 2.0431 1.8401  Ψ’ 1.1421 0.8756 0.4568  Υ’ 1.5355 1.3451 1.0279  χc 1.3452 1.1040 0.6598  χb 1.6878 1.5101 1.2055  6              𝐸𝜒𝑐,𝜒𝑏 𝐶𝑜𝑟𝑟 = 𝑚𝑄𝜎2 6𝑚𝐷 8                                                   (29)     Therefore, to evaluate the binding energy of the 1 P states  of heavy quarkonia, we add up this correction energy term  to the binding energy of the   as defined in above  equation. Hence, we have:   𝐸(𝜒𝑐,𝜒𝑏) = 𝐸(Ψ′,Υ′) + 𝐸(𝜒𝑐,𝜒𝑏) 𝑐𝑜𝑟𝑟  (30)  This implies  𝐸(𝜒𝑐,𝜒𝑏) = ( 𝑚𝑄𝜎2 𝑚𝐷 4 𝑛2 + 𝛼𝑚𝐷 + 𝜉 3 ( 𝑚𝑄𝜎2 𝑚𝐷 4 𝑛2 + 𝛼𝑚𝐷 + 2𝑚𝑄𝜎2 𝑚𝐷 4 𝑛2 )) − 𝑚𝑄𝜎2 6𝑚𝐷 8                                                                                     (31)                                                                                                    In the present work, the masses of 1P state as charmonia  (mχc = 1 .865GeV and bottomonium ( mχb =5.18GeV ) have  been taken and details of which can found in [85] and  references therein.      B. Dissociation of quarkonium states in in the presence of  anisotropy and strong magnetic field  Once we obtained the binding energies of the quarkonium  states, it is customary to study the  dissociation pattern of  the quarkonium states when their binding energ ies  becomes zero. But in the ref. [86], the authors have argued  that it is not essential to have zero binding energy to  dissociate the states but when the binding energy is less  than the temperature (EB ≤ T), a state is weakly bound and  hence, it is destroy ed by the thermal fluctuations. The  authors in [75, 86, 87], propose another condition for the  dissociation of the quarkonium in [88] and reference  therein, using thermal effects can be states and is 2 EB ≤  Γ(T) or Γ(T) ≥ 2EB, where Γ(T) is the thermal width of the  respective state. Thus, there are two major criteria used to  determine the dissociation temperature (TD). The upper  bound and the lower bound of the dissociation  temperature, as one can found  obtained by the following  conditions:    𝐸(𝐽/𝜓,Υ,𝜓′,Υ′) = ( 𝑚𝑄𝜎2 𝑚𝐷 4 𝑛2 + 𝛼𝑚𝐷  + 𝜉 3 ( 𝑚𝑄𝜎2 𝑚𝐷 4 𝑛2 + 𝛼𝑚𝐷 + 2𝑚𝑄𝜎2 𝑚𝐷 4 𝑛2 ))    = { 𝑇𝐷   → 𝑈𝑝𝑝𝑒𝑟  𝑏𝑜𝑢𝑛𝑑  𝑜𝑓  𝑡ℎ𝑒  𝑞𝑢𝑎𝑟𝑘𝑜𝑛𝑖𝑢𝑚  𝑠𝑡𝑎𝑡𝑒𝑠 3𝑇𝐷 →   𝐿𝑜𝑤𝑒𝑟  𝑏𝑜𝑢𝑛𝑑  𝑜𝑓𝑡ℎ𝑒    𝑞𝑢𝑎𝑟𝑘𝑜𝑛𝑖𝑢𝑚  𝑠𝑡𝑎𝑡𝑒𝑠                                                                                            (32)     Eq. 32 is applicable for the 1 S and 2S states of the heavy  quarkonia. The dissociation temperature for the 1P states  of charmonium and bottomoniu m has been cal culated  using the following relation:    𝐸(𝜒𝑐,𝜒𝑏) = ( 𝑚𝑄𝜎2 𝑚𝐷 4 𝑛2 + 𝛼𝑚𝐷 + 𝜉 3 ( 𝑚𝑄𝜎2 𝑚𝐷 4 𝑛2 + 𝛼𝑚𝐷 + 2𝑚𝑄𝜎2 𝑚𝐷 4 𝑛2 )) + 𝑚𝑄𝜎2 6𝑚𝐷 8     = { 𝑇𝐷 → 𝑈𝑝𝑝𝑒𝑟  𝑏𝑜𝑢𝑛𝑑  𝑜𝑓  𝑡ℎ𝑒  𝑞𝑢𝑎𝑟𝑘𝑜𝑛𝑖𝑢𝑚  𝑠𝑡𝑎𝑡𝑒𝑠 3𝑇𝐷 →  𝐿𝑜𝑤𝑒𝑟  𝑏𝑜𝑢𝑛𝑑  𝑜𝑓𝑡ℎ𝑒    𝑞𝑢𝑎𝑟𝑘𝑜𝑛𝑖𝑢𝑚  𝑠𝑡𝑎𝑡𝑒𝑠                                                                                          (33)     C. Thermal width of 1S state of charmonium and  bottomonium  Since the imaginary part of the potential give rise to the  thermal width which in turn also used to calculate the  dissociation point of the quarkonium states. The thermal  width of the quarkonium states can be obtained by using  following ansatz:                   Γ(𝑇) = −∫ 𝑑3𝑟 |Ψ(𝑟)|2Im 𝑉(𝑟)         (34)  where, Ψ(r) is the Coulombic wave function. The  Coulombic wave function for ground state (1 S,  corresponding to n = 1 (J/ψ and Υ)) given as   Ψ1S(𝑟) = 1 √𝜋𝑎03 𝑒 −𝑟 𝑎0 (35)  where, a0=2/(mQα) denotes Bohr radius of the quarkonia  system. Now from Eq. 34, we have  Γ1S(𝑇) = ( 𝜉 3 + 1)∫ 𝑑3𝑟|Ψ1S(𝑟)|2 {𝛼𝑇𝑠2log( 1 𝑠)( 1 3 − 𝜉 3−cos2𝜃𝑟 20 ) + 2𝜎𝑇 𝑚𝐷 2 𝑠4log( 1 𝑠) 1 20 ( 1 3 − 𝜉 2−cos2𝜃𝑟 14 )}            (36)  Solving the above equation, we get the thermal width for  1S-state as below:  Γ1S(𝑇) = 𝑇 ( 𝜉 3 − 2) 𝑚𝐷 2 𝛼𝑚𝑄 2 {( 1 6 (−25 + 12𝛾𝐸 + 12log(2) − 12log(𝑎0) − 12log(𝑚𝑄)) + 3𝜎 10𝛼3𝑚𝑄 2 ) − (49 + 20𝛾𝐸 − 12log(𝑎0)) + 20log( 2 𝑚𝑄 )}                                        (37)  It is important to note that in ref. [75] while considering  up to leading logarithmic order of imaginary potential, the  authors too have taken the width up to the leading  logarithmic. Thus, the dissociation width for 1 S-state  would be of the form:  Γ1S(𝑇) = 𝑇( 4 𝛼𝑚𝑄 2 + 12𝜎 𝛼4𝑚𝑄 4 )(1 − 𝜉 6)𝑚𝐷 2 log( 𝛼𝑚𝑄 2𝑚𝐷 )             (38)    7   panel) at different values of the magnetic field.         FIG. 2. Shows the variation of binding energy of the J/ψ with temperature for the isotropic case(left panel) and oblate case(right panel)  at different values of the magnetic field.    FIG. 3 Shows the variation of binding energy of the Υ with temperature for the isotropic case(left panel) and oblate case(right  8  FIG. 4. Shows the variation of binding energy of the ψ’ with temperature for the isotropic case(left panel) and oblate case(right panel)  at different values of the magnetic field.    FIG. 5. Shows the variation of binding energy of the Υ’ with temperature for the isotropic case(left panel) and oblate case(right panel)  at different values of the magnetic field.    9            FIG. 6. Shows the variation of binding energy of the χc with temperature for the isotropic case(left panel) and oblate case(right panel)  at different values of the magnetic field.  10      FIG. 7. Shows the variation of binding energy of the χb with temperature for the isotropic case (left panel) and oblate case (right panel)  at different values of the magnetic field.          11  TABLE III. Upper bound of dissociation temperature for isotropic  case at Tc = 197MeV  Temperatures are in the unit of Tc  For isotropic case (ξ = 0)  States eB = 0.3GeV 2 eB = 0.5GeV 2 eB = 0.7GeV 2  J/ψ 2.0304 1.8908 1.6757  Υ 2.6522 2.5507 2.3857  Ψ’ 1.4213 1.2182 0.9010  Υ’ 1.8908 1.7385 1.5101  χc 1.6243 1.4467 1.1299  χb 2.0431 1.9035 1.6751    TABLE IV. Upper bound of dissociation temperature for oblate  case at Tc = 197MeV  Temperatures are in the unit of Tc  For oblate case (ξ = 0.3)  States eB = 0.3GeV 2 eB = 0.5GeV 2 eB = 0.7GeV 2  J/ψ 2.1827 2.0558 1.8654  Υ 2.8299 2.7411 2.5888  Ψ’ 1.5355 1.3578 1.0659  Υ’ 2.030 1.8908 1.6878  χc 1.7131 1.5482 1.2690  χb 2.1573 1.9035 1.6751  :    D. Mass Spectra of the different quarkonium states in  anisotropic hot QCD medium at finite magnetic  field  The mass spectra of the different quarkonium states in  the presence of magnetic field along with the effect of  anisotropic parameter ξ can be obtained by the relation:   M = 2mQ + EB. (39)  But in the current work we have calculated the mass  spectra of only 1 P state of the heavy quarkonia by using  the following relation:            𝑀 = 2𝑚𝑄 + 𝐸(Ψ′,Υ′) + 𝐸(𝜒𝑐,𝜒𝑏) 𝑐𝑜𝑟𝑟                           (40)  Hence using Eq. 28 and 31 in Eq. 40, we have:  2𝑚𝑄 + ( 𝑚𝑄𝜎2 𝑚𝐷 4 𝑛2 + 𝛼𝑚𝐷 + 𝜉 3 ( 𝑚𝑄𝜎2 𝑚𝐷 4 𝑛2 + 𝛼𝑚𝐷 + 2𝑚𝑄𝜎2 𝑚𝐷 4 𝑛2 )) + 𝑚𝑄𝜎2 6𝑚𝐷 8                                                                                                 (41)  Where 𝑚𝑄  (𝑚𝑐,𝑏)  is the mass of the heavy quarkonia,  𝐸(ψ′,Υ′) is the binding energy of the ψ′,Υ′ and 𝐸(𝜒𝑐,𝜒𝑏) 𝑐𝑜𝑟𝑟  is the  energy correction/ mass gap correction obtained using  variational treatment method.  of the heavy quarko nia, E (ψ’, Υ’) is the binding energy of  the ψ’, Υ’ and 𝐸𝜒𝑐,𝜒𝑏 𝐶𝑜𝑟𝑟   is the ene rgy correction/ mass gap  correction obtained using variational treatment method.                  IV .  RESUL TS AND DISCUSSION  Heavy quarkonia properties have been investigated, by  means of in medium modification to the Cornell potential  (perturbative as well as non -perturbative), using  extended quasi -particle approach in the presence of  strong magnetic field limit qfeB >> T 2. Sin ce at the early  stages of the Ultra -Relativistic Heavy ion Collisions  (URHIC), the anisotropy arises in the beam direction as  the system expands. At ξ = 0, the string term makes the  real potential more attractive compared to the case when  the potential is modified using Coulombic part only. This  means that the respective quarkonium states becomes  more bound with both the Coulombic and string part in  comparison to the case when Coulombic part is modified  alone. Here in the present work we have consider the  weak anisotropy for the oblate case, ξ = 0.3 and isotropic  case, ξ = 0 with the fixed value of the critical temperature  Tc = 197MeV . The variation of the real potential with the  distance (r in fm) has been shown in Figure 1 at eB = 0.3  GeV 2 and temperature T = 300MeV for oblate and isotropic  case. For the isotropic case, ξ = 0, we have same variation  for both parallel and perpendicular case. This is because  of the fact that the system is expanding longitudinally. On  the other hand, for the oblate case ξ = 0.3 the real potential  have lower value for the parallel case( θ = 0 o) in  comparison to the perpendicular case (θ = 90o). Figures 2,  3, 4, 5, 6 and 7 shows the variation of  the binding energy  of J/ψ, Υ, ψ’ , Υ ’ , χc and χb with the temperature at finite  values of the magnetic field for both the isotropic case ξ =  0 (left panel) and oblate case ξ = 0 .3 ( right panel)  respectively. From all these figures, it has been deduced  that the binding energy of all the  quarkonium sta tes  1S(J/ψ, Υ), 2 S (ψ’ , Υ’ ) and 1 P(χc, χb) decreases with the  temperature. Also, as we goes from lower to higher  magnetic field values, the binding energy also has lower  values as can be seen from the Figures 2, 3, 4, 5, 6 and 7.  However, it is noticed that binding energy of all the above  mentioned states have higher values for the oblate case in  competition to the isotropic case. In other words, the  anisotropy seems to an additional handle to decipher the  properties of the quarkonium states.  In anisotropic medium, the binding energy of the QQ¯ pair  get stronger with increase in the anisotropy. This is due to 12  the fact that the Debye screening mass in anisotropic  medium is always much lower compared to the isotropic  one. Hence, quarkonium states are strongly bound.  Figure  8 shows the variation of the thermal width of  charmonium(ΓJ/ψ) ( left panel) and bottomonium ( ΓΥ)  (right panel) at eB = 0 .3 GeV 2 for the isotropic case and  oblate case. It has been noticed from the Figure 8, that  there is an increase in the the rmal width with the  temperature for both cases. Although thermal width has  lower value for the oblate case in competition to the  isotropic case. It is also noticed that the thermal width of  the Upsilon (ΓΥ) is much smaller than the J/ψ (ΓJ/ψ). This is  due to the fact the bottomonium states are smaller in size  and larger in masses than the charmonium states and  hence get dissociated at higher temperatures. Mass  spectra of the χc and χb has been shown in Figure 9. There  is decreasing pattern of the mass spect ra of χc and χb with  the temperature at ξ = 0.3 at finite magnetic fields. Mass  spectra of these states at ξ = 0.3 (oblate case) is found to  be very closed to the particle data group 2018 [89] . The  dissociation temperatures for the 1S(J/ψ,Υ), 2S (ψ’ , Υ ’ )  and 1P(χc, χb) has been given in the Table I, II, III, and IV.  Lower bound of dissociation temperatures for different  states have been shown in Table I and III for the isotropic  case. Whereas the Table II and IV shows the different  values of dissociation temperatures for the oblate case ξ =  0.3. In general, the dissociation temperatures decrease  with the magnetic field. It is pertinent to mention here that  for the oblate case dissociation temperatures have found  to be higher in comparison to the isot ropic case as seen  from the tables.                V. CONCLUSIONS  The dissociation process of the heavy quarkonium  states 1S(J/ψ, Υ), 2S (ψ’ , Υ’ ) and 1P(χc, χb) in the isotropic  medium at finite magnetic field using extended quasi - particle model has been investigated. The real part of  potential becomes more attractive in anistropic hot QCD  medium compared to isotropic case at constant magnetic  field. The bindi ng energy of the different quarkonium  states decreases with the temperature as well as with  magnetic field for both the isotropic and oblate case.  However, the binding energy has higher values for the  oblate case ξ = 0.3 compared to isotropic case (ξ = 0). From  the Figures 2, 3, 4, 5, 6 and 7 it is deduced that the binding  energies of the 2 S states ( ψ’ ,Υ’ ) are smaller than the  binding energies of 1S(J/ψ, Υ) and 1P(χc, χb) states for both  the isotropic and oblate case in presence of magnetic field.  The dissociation temperatures reduces as we increase the  magnetic field for both the isotropic and oblate case. It is  also found that the dissociation temperature for all the 1S,  2S and 1P states have higher values for the oblate case 0 <  ξ < 1 compared to the case when ξ = 0. It is also noted here  that the dissociation temperature of 1 S and 1 P states of  charmonium and bottomonium are higher than th e 2S  states. This is because of the fact that the ψ’ and Υ’ states  are highly unstable or loosely  bound states. The  hierarchical order of dissociation temperatures ( TD) for  the different quarkonium states is TD’s (1S) > TD’s (1P) >  TD’s (2 S). Thermal width is found to increases with the  temperature at constant magnetic field eB = 0.3GeV 2. The  thermal width also increases with ξ indicating quicker  dissociation of the states. In future, this work will be  extended to calculate the survival probability or the  nuclear modification factor of different quarkonium states  with respect to transverse momentum, c entrality, and  rapidity which is the key point to quantify the various  properties of the medium produced during Heavy Ion  Collisions (HIC) at LHC and RHIC.                           VI.                     DAT A AVAILABILITY  The data used to support the findings of this study are  available from the corresponding author upon request.   VII. CONFLICTS OF INTEREST  The authors declare that they have no conflicts of  interest regarding the publication of this paper.   VIII. ACKNOWLEDGEMENT  VKA acknowledge the Science and Engineering research  Board (SERB) Project No. EEQ/2018/000181 New Delhi  for providing the financial support. We record our sincere  gratitude to the people of India for their generous support  for the research in basic sciences       13  References  [1] L. D. Landau and E. M. Lifshitz, “Electrodynamics of  continuous media,” Butterworth-Heinemann, 2nd Edition,  vol. 8, 1984.  [2] M. C. Chu and T . Matsui, “Dynamic Debye screening for a  heavy quark -antiquark pair traversing a quark -gluon  plasma,” Physical Review D, vol. 39, Iss. 1-7, pp. 1892, 1989.  [3] Y. Koike, “Energy loss of high energy partons traversing a  quark-gluon plasma,” AIP Conference Proceedings, 243, pp.  916, 1992.  [4] L. McLerran, “The physics of the quark-gluon plasma,”  Review of Moderm Physics, vol. 58, Iss. 4, pp. 1021, 1986.  [5] PHENIX Collaboration, “Formation of dense partonic matter  in relativistic nucleus –nucleus collisions  at RHIC:  Experimental evaluation by the PHENIX Collaboration,”  Nuclear Physics A, vol. 757, Iss. 1-2, pp. 184-283, 2005.  [6] PHOBOS Collaboration, “The PHOBOS perspective on  discoveries at RHIC,” Nuclear Physics A, vol. 757, Iss. 1-2, pp.  28-101, 2005.  [7] BRAHMS Collaboration, “Quark Gluon Plasma an Color  Glass Condensate at RHIC? The perspective from the  BRAHMS experiment,” Nuclear Physics A , vol. 757, Iss. 1 -2,  pp. 1-27, 2005.  [8] STAR Collaboration, “Experimental and theoretical  challenges in the search for the qu ark–gluon plasma: The  STAR Collaboration’s critical assessment of the evidence  from RHIC collisions,” Nuclear Physics A , vol. 757, Iss. 1 -2,  pp. 102-183, 2005.  [9] J. J. Aubert, P . J. Biggs, J. Burger, M. Chen et al., “Experimental  Observation of a Heavy parti cle J,” Physics Review Letters ,  vol. 33, no. 23, pp. 1404-1406, 1974.    FIG. 8. Variation of thermal width of J/ψ and Υ with the temperature in the left and the right panel respectively at ξ = 0.3 and eB =  0.3GeV 2.    FIG. 9. Dependency of mass spectra of the χc and χb with temperature in the left and the right panel respectively at different magnetic  field at  ξ = 0.3      14  [10] J. E. Augstin, A. M. Bayoarski, M. Breidenbach, F.Bulos, et al.,  “Discovery of Narrow resonance in e+e− Annihilation,”  Physics Review Letters, vol. 33, no. 23, pp. 14061408, 1974.  [11] T . Matsui and H. Satz, “ J/ψ suppression by quark -gluon  plasma formation,” Physics Letters B, vol. 178, Iss. 4, pp. 416- 422, 1986.  [12] A. Mocsy, P . Petreczky, “Heavy quarkonia survival in  potential model,” Europian Physical Journal C, vol. 43, pp. 77- 80, 2005.  [13] A. Mocsy, P . Petreczky, “Quarkonia correlators above  deconfinement,” Physical Review D, vol. 73, Iss. 7, pp074007,  2007.  [14] V. K. Ago tiya, V. Chandra, and B. K. Patra, “Dissociation of  quarkonium in a hot QCD medium,” Physical Review C , vol.  80, Iss. 2 pp. 025210, 2009.  [15] B. K. Patra, V. K. Agotiya, and V. Chandra, “Charmonium  suppression in the presence of dissipative forces in a  strongly-coupled quark –gluon plasma,” Europian Physical  Journal C, vol. 67, pp. 465, 2010.  [16] S. Datta, F. Karsch, P . Petreczky, and InesWetzorke,  “Behavior of charmonium systems after deconfinement,”  Physical Review D, vol. 69, Iss. 9, pp. 094507, 2004.  [17] N. Brambill a, J. Ghiglieri, P . Petreczky, and Antonio Vairo,  “Static quark -antiquark pairs at finite temperature,”  Physical Review D, vol. 78, Iss. 1, pp. 014017, 2008.  [18] D. Kharzeev, K. Landsteiner, A. Schmitt and H. U. Yee,  “Strongly Interacting Matter in Magnetic Fi elds,” Lectucre  Notes Physics, vol. 871, pp. 1-11, 2013.  [19] R. C. Duncan and C. Thompson, “Formation of Very Strongly  Magnetized Neutron Stars: Implications for Gamma -Ray  Bursts,” Astrophysical Journal 392, L9L13, 1992.  [20] D. Grasso and H. R. Rubinstein, “Magnetic Fields in the Early  Universe,” Physics Report , vol. 34 8, pp. 163 -266, 2001  [astro-ph/0009061].  [21] V. Skokov, A. Y. Illarionov and V. Toneev, “Estimate of the  magnetic filed in heavy ion cillisions,” International Journal  of Modern Physics A , vol. 24, No.31, pp. 5925, 2009  [arXiv:0907.1396 [nucl-th]].  [22] V. Voronyuk, V. D. Toneev, W. Cassing, E. L. Bratkovskaya, V. P .  Konchakovski and S. A. Voloshin, “Electromagnetic field  evolution in relativistic heavy -ion collisions,” Physical  Review C, vol. 83, Iss. 5, pp. 054911, 2011 [arXiv:1103.4239  [nucl-th]].  [23] A. Bzdak and V.  Skokov, “Event-by-event fluctuations of  magnetic and electric fields in heavy ion collisions,” Physics  Letters B , vol. 710, Iss. 1, pp. 171 -174,  2012[arXiv:1111.1949 [hep-ph]].  [24] W. T . Deng and X. G. Huang, “Event-by-event generation of  electromagnetic fiel ds in heavy -ion collisions,” Physical  Review C, vol. 85, Iss. 4, pp. 044907, 2011  [arXiv:1201.5108 [nucl-th]].  [25] K. Tuchin, “Production in Strong Electromagnetic Fields in  Relativistic Heavy-Ion Collisions,” Advances in High Energy  Physics, vol. 2013, Articl e ID 490495,  2013[arXiv:1301.0099].  [26] C. S. Machado, F. S. Navarra, E. G. de Oliveira, J. Noronha and  M. Strickland, “Heavy quarkonium production in a strong  magnetic field,” Physical Review D, vol. 88, Iss. 3, pp. 034009,  2013[arXiv:1305.3308 [hep-ph]].  [27] C. S. Machado, S. I. Finazzo, R. D. Matheus and J. Noronha,  “Modification of the B meson mass in a magnetic field from  QCD sum rules,” Physical Review D, vol. 89, Iss. 7, pp. 074027,  2014[arXiv:1307.1797 [hep-ph]].  [28] J. Alford and M. Strickland, “Charmonia and bottomonia in a  magnetic field,” Physical Review D , vol. 88, Iss. 10, pp.  105017, 2013[arXiv:1309.3003 [hep-ph]].  [29] D. Dudal and T . G. Mertens, “Melting of charmonium in a  magnetic field from an effective AdS/QCD model,” Physical  Review D, vol. 91, Iss. 8, pp. 086002, 2015[arXiv:1410.3297  [hep-th]].  [30] S. Cho, K. Hattori, S. H. Lee, K. Morita and S. Ozaki, “ QCD Sum  Rules for Magnetically Induced Mixing between ηc and J/ψ,”  Physics Review Letters , vol. 113, Iss. 17, pp. 172301,  2014[arXiv:1406.4586 [hep-ph]].  [31] S. Cho, K. Hattori, S. H. Lee, K. Morita and S. Ozaki,  “Charmonium spectroscopy in strong magnetic fields by  QCD sum rules: S -wave ground states,” Physical Review D ,  vol. 91, Iss. 4, pp. 045025, 2015 [arXiv:1411.7675 [hepph]].  [32] P . Filip,“Heavy Flavor Mesons in Strong Magnetic Fields,”PoS  CPOD, 2013, 035.2013; arXiv:1504.07008 [hep-ph]  [33] X. Guo, S. Shi, N. Xu, Z. Xu and P . Zhuang, “ Magnetic Field  Effect on Charmonium Production in High Energy Nuclear  Collisions,”Physical Letters B , vol. 751, pp. 215219, 2015 ;  arXiv:1502.04407 [hep-ph].  [34] P . Romatschke, “Momentum broadening in an anisotropic  plasma,” Physical Review C, vol.75, Iss.  1, pp. 014901, 2007.  [35] P . Romatschke and M. Strickland, “Collisional energy loss of  a heavy quark in an anisotropic quark -gluon plasma,”  Physical Review D, vol. 71, Iss. 12, pp. 125008, 2005.  [36] B. Schenke and M. Strickland, “Photon production from an  anisotropic quark-gluon plasma,” Physical Review D, vol. 76,  Iss. 2, pp. 025023, 2007.  [37] M. Martinez and M. Strickland, “Measuring QuarkGluon - Plasma Thermalization Time with Dileptons,” Physics  Review Letters, vol. 100, Iss. 10, pp. 102301, 2008.  [38] M. Martinez and M. Strickland, “Pre -equilibrium dilepton  production from an anisotropic quark -gluon plasma,”  Physical Review C, vol. 78, Iss. 3, pp. 034917, 2008.  [39] A. Dumitru, Y. Nara, B. Schenke and M. Strickland, “et  broadening in unstable non -Abelian pla smas,” Physical  Review C, vol. 78, Iss. 2, pp. 024909, 2008.  [40] R. Baier and Y. Mehtar-Tani, “Jet quenching and broadening:  The transport coefficient ˆq in an anisotropic plasma,”  Physical Review C, vol. 78, pp. 064906, 2008.  [41] A. Dumitru, Y. Guo and M. Strickl and, “The heavyquark  potential in an anisotropic plasma,” Physics Letters B , vol.  662, Iss. 1, pp. 37-62, 2008.  [42] Y. Burnier, M. Laine, M. Vepsa´la´inen, “Quarkonium  dissociation in the presence of a small momentum space  anisotropy,” Physics Letters B, vol. 678, Iss. 1, pp. 86, 2009.  [43] P . Romatschke and M. Strickland, “Collective modes of an  anisotropic quark-gluon plasma,” Physical Review D, vol. 68,  Iss. 3, pp. 036004, 2003.  [44] M. Y. Jamal, S. Mitra and V. Chandra, “Collective excitations  of hot QCD medium in a quasiparticle description,” Physical  Review D, vol. 95, Iss. 1, pp. 094022, 2017. 15  [45] V. Chandra and V. Ravishanker , “Quasi-particle model for  lattice QCD: quark -gluon plasma in heavy ion  collisions,”European Physics Journal C , vol. 64, pp. 63 -72,  2009.  [46] V. Gol oviznin and H. satz, “Extreme States of Matter in  Strong Interaction Physics: An Introduction,”Z.Physics.C, 57,  671, 1994.  [47] A. Peshier, B. Kampfer, O. P . Pavlenko and G. Soff, “Massive  quasiparticle model of the SU(3) gluon plasma,” Physical  Review D, vol. 54, Iss. 3, pp. 2399, 1996.  [48] A. Dumitru, R. D. Pisarski, “Degrees of freedom and the  deconfining phase transition,” Physics Letters B, vol. 525, pp.  95, 2002.  [49] V. M. Bannur, “Self-consistent quasiparticle model for quark- gluon plasma,”Physical Review C, vol. 75, Iss. 4, pp. 044905,  2006.  [50] V. M. Bannur, “Comments on quasiparticle models of quark– gluon plasma,” Physics Letters B, vol. 647, Iss. 4, 271, 2007.  [51] V. M. Bannur, “Quasi -particle model for QGP with nonzero  densities,” Journal of High Energy Physics, 0709, 046, 2007.  [52] M. Kurian and V. Chandra, “Bulk viscosity of a hot QCD  medium in a strong magnetic field within the relaxationtime  approximation” Physical Review D , vol. 97, Iss. 11, pp.  116008, 2018.  [53] V. Chandra, “Transport properties of anisotropically  expanding quark -gluon plasma within a quasiparticle  model,” Physical Review D, vol.86, isss. 11, pp. 114008, 2012.  [54] A. Rebhan, “Non -Abelian Debye mass at next -to-leading  order ,” Physical Review D, vol. 48, Iss. 9, pp. 3967, 1993.  [55] E. Shuryak, Sov. Physics, “Theory of hadron plasma ” Journal  of Experimental and Theoretical Physics , vol. 47, pp. 212,  1978.  [56] S. Mrowczynski, “Topics in the transport theory of the quark  gluon plasma,” Physics of Particles and Nuclei , vol. 30, pp.  419, 1999.  [57] K. Yagi, T . Hatsuda and Y. Miake, “Quark-gluon plasma: From  big bang to little bang,” Particles and Nuclei Physics  Cosmology, vol. 23, pp. 1-446, 2005.  [58] J. JanKowski, D. Blaschke and O. Kaczmarek, “Debye masses  at the QCD transition in the PNJL model,”  arXiv:1508.05322[hep-ph] 2015.  [59] M. Laine and Y. Schroder, “Two-loop QCD gauge coupling at  high temperatures,” Journal of High Energy Physics , 067,  2005.  [60] A. Maireche, “Heavy Light meson in the symmetries of  extended non -relativistic quark model,” Yanbu Journal of  Engineering and Science, vol. 17, 1441H, 2019.  [61] A. Maireche, “The impact of the deformed space -phase  parameters into HAs and HLM systems with the i mproved  Hulthen plus Hellman potential in the presence of  temperature-dependent confined Coulomb potential within  the framework of DSE,” Revista Mexicana de Fisica , vol. 68,  0570502,1-21, 2022.  [62] A. Maireche, “ Analytical Expressions to Energy Eigenvalues  of the Hydrogenic Atoms and the Heavy Light Mesons in the  Framework of 3D-NCPS Symmetries Using the Generalized  Bopp’s Shift Method,” Bulgarian Journal of Physics , vol. 49,  pp. 239-265, 2022.  [63] A. Maireche, “ A new Nonrelativistic Investigation for  Interactions i n One -electron Atoms with Modified  Vibrational-Rotational Analysis of Supersingular plus  Quadratic Potential: Extended Quantum Mechanics,” Journal  of nano and electronics physics , vol. 8, no. 4(2), pp. 04076,  2016.  [64] E. Eichten, K. Gottfried, T . Kinoshita, K. D. Lane and T . M. Yan,  “Charmonium: Comparison with experiment,” Physical  Review D, vol. 21, Iss. 1, pp. 203, 1980.  [65] M. E. Carrington, K. Deja and S. Mrowczynski, “Plasmons in  anisotropic quark-gluon plasm,” Physical Review C , vol. 90,  Iss. 3, pp. 034913, 2014.  [66] V. Chandra, R. Kumar and V. Ravishankar, “Hot QCD  equations of state and relativistic heavy ion  collisions,”Physical Review C , vol. 76, Iss. 5, pp. 054909,  2007; [Erratum: Phys. Rev. C, vol. 76, pp. 069904, 2007.  [67] V. Chandra, A. Ranjan and V. Ravisha nkar, “On the chromo- electric permittivity and Debye screening in hot  QCD,”European Physics Journal A, vol. 40, pp. 109-117, 2009.  [68] V. Chandra and V. Ravishankar, “Quasiparticle description of  (2+1)-flavor lattice QCD equation of state,” Physical Review  D, vol. 84, Iss. 7, pp. 074013, 2011.  [69] S. Solanki, M. Lal, R. Sharma and V. K. Agotiya, “Study of  quarkonium properties using SUSQYM with baryonic  chemical potential,” Internatinal journal of modern physics A,  vol. 37, No. 31n32, pp. 2250196, 2022.  [70] V. K. Agoti ya, V. Chandra, M. Y. Jamal and I. Nilima,  “Dissociation of heavy quarkonia in Hot QCD medium in a  Quasiparticle model,” Physical Review D , vol. 94, Iss. 9, pp.  094006, 2016.  [71] M. H. Thoma, “ New Developments and Applications of  Thermal Field Theory,” arXiv:hep-ph/0010164.  [72] E. Braaten and R. D. Pisarski, “Soft amplitudes in hot gauge  theories: A general analysis,” Nuclear Physics B, vol. 337, Iss.  3, pp. 569-634, 1990.  [73] J. P . Blaizot and E. Iancu, “Soft Collective Excitations in Hot  Gauge Theories,” Nuclear Physics B, vol. 417, pp. 608 -673,  1994.  [74] P . F. Kelly, Q. Liu, C. Lucchesi and C. Manuel, “Classical  transport theory and hard thermal loops in the quarkgluon  plasma,” Physical Review D , vol. 50, Iss. 6, pp. 4209, 1994.  [75] L. Thakur, U. Kakade and B. K. Patra, “D issociation of  Quarkonium in a Complex Potential,” Physical Review D, vol.  89, pp. 094020, 2014.  [76] M. Y. Jamal, I. Nilima, V. Chandra, and V. K. Agotiya,  “Dissociation of heavy quarkonia in an anisotropic hot QCD  medium in a quasiparticle model,” Physical Review D, vol. 97,  Iss. 9, pp. 094033, 2018.  [77] M. Margotta, K. McCarty, C. McGahan, M. Strickland and D. Y.  Elorriaga, “Quarkonium states in a complexvalued  potential,” Physical Review D , vol. 83, Iss. 10, pp. 105019,  2011.  [78] M. Strickland and D. Bazow, “Thermal  bottomonium  suppression at RHIC and LHC,” Nuclear Physics A , vol. 879,  pp. 25-28, 2012.  [79] G. Hardekopf and J. Sucher, G. Hardekopf and J. Sucher,  “Relativistic Wave Equations in Momentum Space,” Physical  Review A, vol.30, no.2, pp.703-711, 1984. 16  [80] J. W. Darew ych and M. Horbatsch, “The Relativistic Two - Body Coulomb System,” Journal of Physics B: Atomic,  Molecular and Optical Physics, vol. 22, no.7, pp. 973, 1989.  [81] J. W. Darewych and M. Horbatsch, “ A variational Treatment  of the Relativistic Two -Fermion Bound -State System in  Quantum Electrodynamics,” Journal of Physics B:Atomic,  Molecular and Optical Physics, vol. 23, no. 3, pp.337, 1990.  [82] J. W. Darewych, M. Horbatsch and R. Kounik, “Variational  Basis-State Expansion Calculation of the Mass Gap in Scalar  Field Theory,” Physical Review D, vol. 39, no.2, pp. 499 -502,  1989.  [83] L. Kluberg and H. Satz, L. Kluberg and H. Satz, “Color  deconfinement an d charmonium production in nuclear  collisions,” arXiv: 0901.3831, 2009.  [84] A. Jakovac, P . Petreczky, K. Petrov and A. Velytsky,  “Quarkonium correlators and spectral functions at zero and  finite temperature,” Physical Review D , vol.75, no.1, pp.  014506, 2007.  [85] I. Nilima and V. K. Agotiya, “Bottomonium Suppression in  Nucleus-Nucleus Collisions Using Effective Fugacity Quasi - Particle Model,” Advances in High Energy Physics, vol. 2018,  Article ID 8965413, 2018.  [86] A. Mocsy and P . Petreczky, “Color Screening Melts  Quarkonium,” Physical Review Letters , vol. 99, no. 21, pp.  211602, 2007.  [87] D. Kharzeev, L. D McLerran and H. Satz, “Non-  Perturbative Quarkonium Dissociation in Hadronic Matter,”  Physics Letters B, vol. 356, no. 2-3, pp. 349-353, 1995.  [88] M. Lal, S. Solanki, R. Sha rma and V. K. Agotiya, “Melting of  quarkonia in strong magnetic field,” Indian Journal of Pure  and Applied Physics, vol. 60, no. 6, pp. 475-481, 2022.  [89] M. Tanabashi, K. Hagiwara, K. Hikasa et.al, “Review of  Particle Physics:Particle Data Gropup (PDG),” Physical  Review D, vol. 98, pp. 030001, 2018.",
      "references": [
        "Electrodynamics of continuous media,",
        "Dynamic Debye screening for a heavy quark -antiquark pair traversing a quark -gluon plasma,",
        "Energy loss of high energy partons traversing a quark-gluon plasma,",
        "The physics of the quark-gluon plasma,",
        "Formation of dense partonic matter in relativistic nucleus –nucleus collisions  at RHIC: Experimental evaluation by the PHENIX Collaboration,",
        "The PHOBOS perspective on  discoveries at RHIC,",
        "Quark Gluon Plasma an Color  Glass Condensate at RHIC? The perspective from the  BRAHMS experiment,",
        "Experimental and theoretical  challenges in the search for the qu ark–gluon plasma: The  STAR Collaboration’s critical assessment of the evidence  from RHIC collisions,",
        "Experimental  Observation of a Heavy parti cle J,",
        "Discovery of Narrow resonance in e+e− Annihilation,",
        " J/ψ suppression by quark -gluon  plasma formation,",
        "Heavy quarkonia survival in  potential model,",
        "Quarkonia correlators above  deconfinement,",
        "Dissociation of  quarkonium in a hot QCD medium,",
        "Charmonium  suppression in the presence of dissipative forces in a  strongly-coupled quark –gluon plasma,",
        "Behavior of charmonium systems after deconfinement,",
        "Static quark -antiquark pairs at finite temperature,",
        "Strongly Interacting Matter in Magnetic Fi elds,",
        "Formation of Very Strongly  Magnetized Neutron Stars: Implications for Gamma -Ray  Bursts,",
        "Magnetic Fields in the Early  Universe,",
        "Estimate of the  magnetic filed in heavy ion cillisions,",
        "Electromagnetic field  evolution in relativistic heavy -ion collisions,",
        "Event-by-event fluctuations of  magnetic and electric fields in heavy ion collisions,",
        "Event-by-event generation of  electromagnetic fiel ds in heavy -ion collisions,",
        "Production in Strong Electromagnetic Fields in  Relativistic Heavy-Ion Collisions,",
        "Heavy quarkonium production in a strong  magnetic field,",
        "Modification of the B meson mass in a magnetic field from  QCD sum rules,",
        "Charmonia and bottomonia in a  magnetic field,",
        "Melting of charmonium in a  magnetic field from an effective AdS/QCD model,",
        " QCD Sum  Rules for Magnetically Induced Mixing between ηc and J/ψ,",
        "Charmonium spectroscopy in strong magnetic fields by  QCD sum rules: S -wave ground states,",
        "Heavy Flavor Mesons in Strong Magnetic Fields,",
        " Magnetic Field  Effect on Charmonium Production in High Energy Nuclear  Collisions,",
        "Momentum broadening in an anisotropic  plasma,",
        "Collisional energy loss of  a heavy quark in an anisotropic quark -gluon plasma,",
        "Photon production from an  anisotropic quark-gluon plasma,",
        "Measuring QuarkGluon - Plasma Thermalization Time with Dileptons,",
        "Pre -equilibrium dilepton  production from an anisotropic quark -gluon plasma,",
        "et  broadening in unstable non -Abelian pla smas,",
        "Jet quenching and broadening:  The transport coefficient ˆq in an anisotropic plasma,",
        "The heavyquark  potential in an anisotropic plasma,",
        "Quarkonium  dissociation in the presence of a small momentum space  anisotropy,",
        "Collective modes of an  anisotropic quark-gluon plasma,",
        "Collective excitations  of hot QCD medium in a quasiparticle description,",
        "Quasi-particle model for  lattice QCD: quark -gluon plasma in heavy ion  collisions,",
        "Extreme States of Matter in  Strong Interaction Physics: An Introduction,",
        "Massive  quasiparticle model of the SU(3) gluon plasma,",
        "Degrees of freedom and the  deconfining phase transition,",
        "Self-consistent quasiparticle model for quark- gluon plasma,",
        "Comments on quasiparticle models of quark– gluon plasma,",
        "Quasi -particle model for QGP with nonzero  densities,",
        "Bulk viscosity of a hot QCD  medium in a strong magnetic field within the relaxationtime  approximation,",
        "Transport properties of anisotropically  expanding quark -gluon plasma within a quasiparticle  model,",
        "Non -Abelian Debye mass at next -to-leading  order ,",
        "Theory of hadron plasma ,",
        "Topics in the transport theory of the quark  gluon plasma,",
        "Quark-gluon plasma: From  big bang to little bang,",
        "Debye masses  at the QCD transition in the PNJL model,",
        "Two-loop QCD gauge coupling at  high temperatures,",
        "Heavy Light meson in the symmetries of  extended non -relativistic quark model,",
        "The impact of the deformed space -phase  parameters into HAs and HLM systems with the i mproved  Hulthen plus Hellman potential in the presence of  temperature-dependent confined Coulomb potential within  the framework of DSE,",
        " Analytical Expressions to Energy Eigenvalues  of the Hydrogenic Atoms and the Heavy Light Mesons in the  Framework of 3D-NCPS Symmetries Using the Generalized  Bopp’s Shift Method,",
        " A new Nonrelativistic Investigation for  Interactions i n One -electron Atoms with Modified  Vibrational-Rotational Analysis of Supersingular plus  Quadratic Potential: Extended Quantum Mechanics,",
        "Charmonium: Comparison with experiment,",
        "Plasmons in  anisotropic quark-gluon plasm,",
        "Hot QCD  equations of state and relativistic heavy ion  collisions,",
        "On the chromo-  electric permittivity and Debye screening in hot  QCD,",
        "Quasiparticle description of  (2+1)-flavor lattice QCD equation of state,",
        "Study of  quarkonium properties using SUSQYM with baryonic  chemical potential,",
        "Dissociation of heavy quarkonia in Hot QCD medium in a  Quasiparticle model,",
        " New Developments and Applications of  Thermal Field Theory,",
        "Soft amplitudes in hot gauge  theories: A general analysis,",
        "Soft Collective Excitations in Hot  Gauge Theories,",
        "Classical  transport theory and hard thermal loops in the quarkgluon  plasma,",
        "D issociation of  Quarkonium in a Complex Potential,",
        "Dissociation of heavy quarkonia in an anisotropic hot QCD  medium in a quasiparticle model,",
        "Quarkonium states in a complexvalued  potential,",
        "Thermal  bottomonium  suppression at RHIC and LHC,",
        "Relativistic  Wave Equations in Momentum Space,",
        "The Relativistic Two - Body Coulomb System,",
        " A variational Treatment  of the Relativistic Two -Fermion Bound -State System in  Quantum Electrodynamics,",
        "Variational  Basis-State Expansion Calculation of the Mass Gap in Scalar  Field Theory,",
        "Color  deconfinement an d charmonium production in nuclear  collisions,",
        "Quarkonium correlators and spectral functions at zero and  finite temperature,",
        "Bottomonium Suppression in  Nucleus-Nucleus Collisions Using Effective Fugacity Quasi - Particle Model,",
        "Color Screening Melts  Quarkonium,",
        "Non-  Perturbative Quarkonium Dissociation in Hadronic Matter,",
        "Melting of  quarkonia in strong magnetic field,",
        "Review of  Particle Physics:Particle Data Gropup (PDG),"
      ],
      "meta_data": {
        "arxiv_id": "2306.00908v1",
        "authors": [
          "Manohar Lal",
          "Siddhartha Solanki",
          "Rishabh Sharma",
          "Vineet Kumar Agotiya"
        ],
        "published_date": "2023-06-01T17:06:17Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Investigates how momentum-space anisotropy (parameter ξ) and an external strong magnetic field modify the in-medium complex heavy-quark potential and, consequently, the binding energies, thermal widths, dissociation temperatures, and (for 1P) mass spectra of charmonium and bottomonium in a QGP. Key findings: (i) anisotropy (oblate ξ>0) makes the real part of the potential more attractive and increases binding energies and dissociation temperatures relative to isotropy; (ii) increasing eB reduces binding energies and lowers dissociation temperatures; (iii) thermal widths rise with temperature and are smaller for bottomonium than charmonium; (iv) predicted dissociation hierarchy TD(1S)>TD(1P)>TD(2S) persists, with oblate medium shifting TD upward.",
        "methodology": "Uses an extended magnetized Effective Fugacity Quasiparticle Model (EQPM) to encode QCD EoS via effective fugacities z_g, z_q and to compute a magnetic-field-dependent Debye mass m_D (quark sector modified by Landau quantization in strong-field/LLL regime). Introduces momentum anisotropy by deforming the distribution function f(p)→f_ξ(p) with a normalization C_ξ chosen to keep m_D unchanged between isotropic/anisotropic cases. Constructs a complex in-medium Cornell potential by dividing its Fourier transform by the dielectric function derived from anisotropic HTL-like gluon propagators; obtains analytic small-ξ expressions for ReV and ImV. Computes binding energies via Schrödinger equation with first-order perturbation in ξ; includes a variationally-motivated correction term for 1P states. Computes thermal width from ImV using Coulombic 1S wavefunctions and leading-log approximation; determines dissociation temperatures using criteria E_B≈T (upper bound) and E_B≈3T or alternatively Γ≥2E_B (lower bound discussions).",
        "experimental_setup": "No experimental data; a model-based numerical/analytic study. Inputs/parameters: Cornell potential with σ=0.184 GeV^2; two-loop running coupling α(T) with n_f=3, N_c=3 and renormalization scale λ_T (not fixed explicitly in excerpt). Strong-field assumption q_f eB >> T^2 and magnetic field along z; evaluates at constant fields eB=0.3, 0.5, 0.7 GeV^2. Considers isotropic ξ=0 and weak oblate anisotropy ξ=0.3 (also discusses prolate range but main results emphasize oblate; thermal width plotted for isotropic vs prolate/oblate depending on section). Uses Tc=197 MeV to report TD in units of Tc. Studies angular dependence of potential (θ_r=0°,90°). Reports TD tables for 1S (J/ψ, Υ), 2S (ψ′, Υ′), and 1P (χ_c, χ_b) states and compares χ-state mass spectra trends qualitatively to PDG 2018 values.",
        "limitations": "Relies on potential-model framework and static, homogeneous medium; uses small-anisotropy expansion (first order in ξ) and primarily one benchmark ξ=0.3, limiting applicability to strongly anisotropic early-time QGP. Assumes strong magnetic field/LLL dominance and treats eB as constant in time, neglecting realistic rapid decay and spatial inhomogeneity in heavy-ion collisions. Magnetic field affects screening only through m_D (gluons taken unaffected), and spin/mixing effects induced by B on quarkonium levels are largely not modeled beyond a phenomenological 1P correction. Imaginary part/thermal width uses Coulombic wavefunctions and leading-log approximations, which may miss non-Coulombic and nonperturbative contributions. Model parameters (effective fugacities, λ_T) and EoS dependence introduce uncertainty; limited direct validation against lattice QCD spectral functions or experimental R_AA.",
        "future_research_directions": "Couple the anisotropic+magnetized complex potential to realistic space-time evolution (anisotropic hydrodynamics) with time-dependent electromagnetic fields to predict observables such as survival probabilities and nuclear modification factors R_AA vs p_T, centrality, and rapidity. Extend beyond weak-anisotropy expansion and explore broader ξ, including dynamical anisotropy and different anisotropy directions relative to B. Incorporate higher Landau levels and relax q_f eB>>T^2 to cover moderate-field regimes relevant at later times. Improve spectroscopy by including explicit spin-dependent interactions, Zeeman splitting, and magnetically induced state mixing (e.g., η_c–J/ψ) within the same framework. Benchmark against lattice-QCD results for Debye mass/potential and spectral functions at finite eB, and quantify parameter/systematic uncertainties of EQPM inputs.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Visual Fourier Prompt Tuning",
      "full_text": "On the Potential and Limitations of Few-Shot In-Context Learning to Generate Metamorphic Specifications for Tax Preparation Software Dananjay Srinivas∗1 Rohan Das∗1 Saeid Tizpaz-Niari2 Ashutosh Trivedi1 Maria Leonor Pacheco1 1 University of Colorado Boulder 2University of Texas El Paso 1{first.last}@colorado.edu 2saeid@utep.edu Abstract Due to the ever-increasing complexity of in- come tax laws in the United States, the num- ber of US taxpayers filing their taxes using tax preparation software (henceforth, tax soft- ware) continues to increase. According to the U.S. Internal Revenue Service (IRS), in FY22, nearly 50% of taxpayers filed their individual income taxes using tax software. Given the legal consequences of incorrectly filing taxes for the taxpayer, ensuring the correctness of tax software is of paramount importance. Meta- morphic testing has emerged as a leading solu- tion to test and debug legal-critical tax software due to the absence of correctness requirements and trustworthy datasets. The key idea behind metamorphic testing is to express the proper- ties of a system in terms of the relationship between one input and its slightly metamor- phosed twinned input. Extracting metamor- phic properties from IRS tax publications is a tedious and time-consuming process. As a response, this paper formulates the task of gen- erating metamorphic specifications as atransla- tion task between properties extracted from tax documents—expressed in natural language—to a contrastive first-order logic form. We per- form a systematic analysis on the potential and limitations of in-context learning with Large Language Models (LLMs) for this task, and outline a research agenda towards automating the generation of metamorphic specifications for tax preparation software. 1 Introduction Recent surveys estimate that between 40 to 50% of taxpayers in the United States use tax prepara- tion software to file their taxes (Farrington, 2023). Given the pervasive use of these systems and the penalties associated with filing taxes incor- rectly (IRS, 2023), making sure that tax preparation software is free of bugs is of paramount impor- tance. However, there are considerable challenges *Equal Contribution that make the application of standard software test- ing approaches infeasible in this domain. Among these, the absence of correctness requirements and the lack of publicly available benchmarks (due to obvious privacy concerns) are primary obstacles to automatic testing and debugging of such systems. To address these challenges, Tizpaz-Niari et al. (2023) introduced a testing framework for U.S. tax preparation software guided by metamorphic rela- tions. The authors define metamorphic relations as relations between two similar individuals who differ in key characteristics that put them in differ- ent tax income buckets. This way, they are able to evaluate the outcome of an individual taxpayer in comparison with individuals who are deemed similar to them. With the help of tax preparation experts, they were able to manually derive a com- prehensive set of critical correctness properties of tax preparation outcomes expressed in first-order logic, and develop a search strategy to automat- ically generate test cases for an open-source tax preparation software. While their approach was successful, deriving formal metamorphic specifica- tions is a time-consuming process that requires sig- nificant domain expertise. Moreover, any changes to the tax code would require continuous efforts to make sure that specifications remain up to date. In this paper, we set out to explore whether re- cent advances in Large Language Models (LLMs) (Brown et al., 2020; Touvron et al., 2023a; Taori et al., 2023; Wang et al., 2023) can help reduce the manual effort required to derive metamorphic specifications for this domain. While supervised learning without sizeable training datasets is infea- sible, recent findings show that LLMs can perform surprisingly well in few-shot scenarios for a wide variety of language tasks, including traditional NLP tasks like translation and question-answering (Brown et al., 2020), vision-language tasks (Mona- jatipoor et al., 2023), and socio-linguistic tasks like morality framing (Roy et al., 2022). arXiv:2311.11979v1  [cs.SE]  20 Nov 2023To perform our study, we first define the extrac- tion of metamorphic specifications for tax prepa- ration software as a language to first-order logic translation task. To do this, we curate a dataset of 33 high-quality natural language properties de- rived from tax documents* and their correspond- ing formal logic representation Tizpaz-Niari et al. (2023). As curating these high-quality examples is expensive, we formulate the problem as a few- shot learning task and experiment with various in- context learning strategies to obtain a formal rep- resentation for each property. In-context learning is a task-adaptation technique that does not up- date the parameters of the LLM, but rather primes the model response by providing a sequence of demonstrations (Brown et al., 2020). Recent work has shown that in-context learning can lead to bet- ter out-of-domain performance than few-shot fine- tuning (Awadalla et al., 2022; Si et al., 2023). We perform a comprehensive analysis of our re- sults and show that using few-shot in-context learn- ing, the model makes between 1-2 mistakes per property on average when generating predicates, variables and operators. While this is encouraging for a model that has barely seen any in-domain, tasks-specific data, there is a lot of room for im- provement before these specifications are usable for automated testing. This work represents a first step towards the automated generation of metamorphic specifications for tax preparation software. In Sec. 5 we outline a research agenda for this purpose. 2 Related Work Converting natural language utterances to logic forms has a long history in the Natural Language Processing community. Semantic parsing is a long-standing NLP task that looks at this prob- lem (Kamath and Das, 2019). Most of the work in this space has been either tailored to going from natural language to linguistically-motivated meaning representations (Palmer et al., 2010; Ba- narescu et al., 2013), or to executable programs like SQL queries (Sun et al., 2018) and robotic com- mands (Dukes, 2014). The solutions employed for these domains are not readily applicable to our case. On the one hand, datasets for linguis- tically motivated meaning representations usually deal with simple general utterances, and their struc- *These documents include Form 1040 (U.S. Individual Income Tax Return), Publication 596 (Earned Income Tax Credit), Schedule 8812 (Qualifying Children and Other De- pendents), and Form 8863 (Education Credits). tured forms are very distinct from the structured forms needed for automated software verification. On the other hand, executable domains like SQL have the advantage that there are large-scale, read- ily available repositories of in-domain data, which is not the case for low-resource, closed domains like tax preparation software. More recently, there has been increased inter- est in exploring NLP techniques to derive formal specifications from technical documentation. For example, Pacheco et al. (2022) look at the task of extracting finite-state machines from network protocol RFCs, Zhang et al. (2020) extract LTL correctness specifications from prose policies for IoT apps, and Chen et al. (2019) look at sentences in a developer guide to extend the finite-state ma- chines for a small set of payment services. In all of these cases, authors are either able to leverage off-the-shelf parsers or employ supervised methods over a curated in-domain dataset. 3 Task Description Metamorphic testing is a well-studied strategy in the systems community (Chen et al., 2020). It was developed to circumvent the absence of oracles (i.e., black box modules that decide whether the output of the system is correct for a given input). The main idea behind metamorphic testing is that correctness can be established by contrasting mul- tiple input-output behaviors (Segura et al., 2016). The running example is search-engine validation; while it might be impossible to know the expected items that should be returned for any search query q, if we have a more restrictive query q′, we know that the number of retrieved items for q should be greater than the number of retrieved items for q′. This allows for developing strategies to test whether systems satisfy the metamorphic relations. To formulate our task, we build on the definition of metamorphic specifications for tax-preparation software proposed by Tizpaz-Niari et al. (2023). We have a fixed set P of metamorphic properties written in natural language, expressing aspects of the U.S. Individual Income Tax Returns that relate to disability, credits, and deductions. For each prop- erty p ∈ P, we want to synthesize a specification s in first-order logic that captures the metamorphic property. Fig. 1 shows an example of the input and expected output. In Appendix A, we detail the syn- tax and semantics of metamorphic specifications used for the tax preparation software, as well as theInput: Property Output: Specification An individual with the married filing jointly (MFJ) status with a disabled spouse must receive similar or higher tax benefits compared to a similar individual but without the disabled spouse. ∀x(x.sts= MFJ) =⇒ ∀y((x ≡s_blindy) ∧(x.s_blind∧ ¬y.s_blind)) =⇒ F(x) ≥ F(y) Table 1: An example of input and expected output. Here, the quantification is over the domain of taxpaying individuals, the relation ≡pred relates equal individuals except for predicate pred, and the function F characterizes the tax return for an individual. full list of formal specifications for metamorphic relations used for demonstration and testing. 4 Experimental Evaluation In this section, we evaluate the performance of LLMs on the natural language to metamorphic specifications translations. We first outline the eval- uation dimensions that we consider, then we de- scribe our experimental setup and results. 4.1 Evaluation Dimensions In order to investigate LLMs’ abilities to convert natural language properties into metamorphic spec- ifications, we evaluate different dimensions to iden- tify how the number of examples that the model gets to observe, the prompting strategies used, the LLM implementation used, as well as the charac- teristics of the task affect model performance. Number of examples for few-shot learning: We look at the effect that the number of demon- strations has on model performance. Since we are working with an extremely reduced dataset, we limit the maximum number of examples that the model sees to two. In-context learning strategies: We seek to study if decomposed prompting (Khot et al., 2023) is effective in translating natural language into for- mal metamorphic specifications. To do this, we compare the performance between an end-to-end (E2E) prompting strategy and different decom- posed strategies. To do this, we break down the task of generating metamorphic relations into two sub-tasks: (1) Generating the relevant logical pred- icates, and (2) Generating the specification in first- order logic. We experiment with two decomposed strategies; the first one (Implicit) allows the model to access the context used to generate the predi- cates by prompting each task subsequently, while the second one (Explicit) decouples the predicate and FOL generation entirely, allowing each mod- ule to completely focus on its sub-task. Example prompts for all strategies are provided in App. B. LLM implementation: We evaluate all three learning strategies on GPT-3.5. However, since we do not have access to the GPT-3.5 weights, we are reliant on APIs provided by OpenAI. In the interest of transparency and reproducibility, we also try out the Explicit strategy on Alpaca (Taori et al., 2023), an open-source instruction-tuned model built over LLaMA 2 (Touvron et al., 2023b). Example difficulty: Different properties can be captured by varying numbers of predicates. In our evaluation, we compare the model performance against the number of predicates required to cap- ture a metamorphic property. We stipulate that the difficulty of capturing a metamorphic expression increases as the number of predicates increases. Error types: We qualitatively evaluate the dif- ferent types of errors that the model makes. We qualify different errors by looking at specific cases where the model generated incorrect predicates, operators, or variables. 4.2 Results Experimental Setup: For GPT-3.5, we prompt via the ChatCompletion API provided by OpenAI. We use the default API settings as outlined in their documentation. For Alpaca, we use the fine-tuned weights from the LoRa-adapted (Hu et al., 2021) Alpaca 7B model, and run inference to generate responses for our prompts. Performance Metrics and Annotation: Stan- dard generation metrics like BLEU, ROGUE, ChrF and BLEURT rely either on word and charac- ter matching (Papineni et al., 2002; Lin, 2004; Popovi´c, 2015), or semantic embedding similar- ity (Sellam et al., 2020). Metrics that rely on lex- ical matching are ill-suited to evaluate the gener- ation of first-order-logic propositions, as the op- erators, predicate names and variables used can vary significantly while maintaining semantic con- sistency (e.g., the operation x ≡S y and the pred- icate EqualExcept (x, y, S) express the same re- lationship). Similarly, embedding-based metrics that have been trained on large textual repositoriesdo not account for valid substitutions in logic that lack similarity in natural language (e.g., F(x) and TaxReturn (x) can be used to denote the same predicate). For this reason, we resort to human evaluation and define a quality score ranging from 1 to 5 based on the following rubric: Rating Explanation 5 The generated predicates or FOL match the ground truth.4 The generated identities have the correct semantic sense, but incorrect format.3 There is 1 mistake in the set of predicates, variables, operators2 There are 2 mistakes in the set of predicates, variables, operators1 The generated identities are completely incorrect. Table 2: Rubric for Performance Quality Using this rubric, the first two authors of this paper annotated the generated texts with their qual- itative judgments. A total of 33 examples were evaluated (see Appendix A), and average results are reported in Tables 3 and 4. Predicate and FOL quality based on prompt- ing style and number of examples: In Table 3, we look at the effect that examples have on dif- ferent styles of prompting. We found that on av- erage, GPT Explicit decomposed prompting per- formed best among all strategies when demonstra- tions were provided. Additionally, we find higher variation in results for GPT E2E, suggesting that decomposed prompting might lead to slightly more consistent results. As expected, demonstrating with a higher number of examples helps the model gen- erate better translations. Nevertheless, the average quality for all strategies sits solidly in the middle of the rubric (about 1-2 errors per example), shed- ding light on the limitations of few-shot in-context learning as-is for this task. Alpaca Explicit pro- duces nearly identical results to GPT Explicit in the zero-shot scenario. However, performance does not improve when Alpaca is provided with two ex- amples instead of just one, unlike GPT Explicit. The Alpaca implementation tested is significantly smaller (7B vs. 175B for GPT 3.5). We hypoth- esize that Alpaca 7B likely needs access to more context to be able to improve its performance for this task. We leave an in-depth exploration of the capabilities of smaller LLMs for future work. Effect of predicate complexity on translation: In this evaluation, we fix the number of examples the model sees to two. We then evaluate how differ- ent numbers of predicates affect the quality of trans- lations. Results can be seen in Tab. 4. For all strate- gies, there is a clear drop in performance for harder examples. This is consistent with our hypothesis that more complex propositions are more difficult to translate. Similarly to the previous analysis, we find that performance on decomposed prompting performs better than E2E prompting. Given the overhead of manual evaluation, we limited this test to GPT-3.5. In Appendix C, we provide generation examples for each score in the annotation rubric. 4.3 Qualitative Analysis of Errors In this section, we qualitatively evaluate different errors after qualifying them on the basis of correct- ness with regard to 1) Predicates; 2) Variables; and 3) Operators. We observe that while the model is somewhat robust in translating natural language into logic, it encounters problems when trying to express identities in metamorphic forms. Errors observed in predicates: The most com- mon error that we observed in the translations was due to a lack of consideration for predicates that are expected in metamorphic templates; such as EqualExcept (x, y, S). where x and y are entities that are similar except through the set of predicates defined in S. Errors observed in variables: We see that mod- els often declare variables or omit variables with- out much structure. Due to the stochastic nature of LLM predictions, it often reuses (or is primed to reuse) the same variables it has declared for dif- ferent predicates; automatically invalidating the metamorphic translation. Errors observed in operators: Models often imitate the same operators that were provided in the prompts. For instance, when we provided a prompt that used ≥; the prompt generated the prediction us- ing the same operator to describe the opposite kind of relationship. Similarly, the model frequently uses commonly occurring pairs of logic notation like ( ∀ and ∃). Summarizing qualitative analysis: Our qual- itative study revealed that while LLMs hold the potential to perform well on metamorphic transla- tion tasks, we also observe simple errors caused due to stochastic dependencies. More training data or post-hoc filtering may benefit this approach from generating erroneous responses. 5 Findings, Limitations and Future Work Our experiments show that while LLMs are able to make some headway in the task of metamor- phic translation, more work needs to be done ton-shot GPT E2E GPT Implicit GPT Explicit Alpaca Explicit Pred. FOL Pred. FOL Pred. FOL Pred. FOL 0 1.77 (0.99) 1.19 (0.49) 1.15 (0.46) 1.03 (0.19) 1.00 (0.00) 1.03 (0.19) 1.00 (0.00) 1.00 (0.00) 1 3.29 (1.38) 2.25 (1.48) 3.46 (1.39) 1.27 (0.60) 4.03 (1.02) 2.77 (1.28) 1.61 (1.09) 1.74 (2.02) 2 3.96 (1.15) 2.85 (1.70) 3.65 (1.09) 1.42 (0.95) 4.27 (0.87) 3.16 (1.23) 1.63 (1.08) 1.71 (1.16) Table 3: Average (S.D.) quality scores for Predicate and FOL generation by in-context learning strategy. Strategy # Predicates < 4 4 ≤ # Predicates < 6 # Predicates ≥ 6 Pred. FOL Pred. FOL Pred. FOL GPT E2E 3.83 (1.60) 3.16 (2.04) 4.07 (0.95) 3.31 (1.54) 3.88 (1.25) 1.88 (1.46) GPT Implicit 3.50 (1.22) 1.83 (1.60) 3.66 (0.88) 1.50 (0.79) 3.75 (1.39) 1.00 (0.00) GPT Explicit 4.20 (1.03) 3.50 (0.97) 4.58 (0.67) 3.58 (1.08) 3.88 (0.83) 2.13 (1.25) Table 4: Average (S.D.) quality scores for Predicate and FOL generation for each in-context learning strategy, grouped by difficulty level in terms of the number of ground truth predicates in the statement. improve performance in order to build safeguards for tax preparation software. Below, we list three key directions for future work. Task-oriented prompt learning: It is clear that few-shot in-context learning is not enough to fully automate the generation of metamorphic specifi- cations for tax preparation software. Recent work has shown that learning prompts that are tailored specifically to the task at hand can help improve in- context learning performance (Chung et al., 2022; Han et al., 2022; Zhang et al., 2023). Going for- ward, we would like to explore strategies to dy- namically adapt prompting strategies to maximize performance and minimize common mistakes. Leveraging additional data sources: While we do not have access to a comprehensive dataset containing metamorphic specifications, there are several external resources that we could indirectly exploit to design weakly-supervised methods and representation learning strategies for the task at hand. For example, we could exploit unlabeled tax-centric documents, out-of-domain language-to- logic datasets, and LLMs trained over code and other structured representations. There is a body of work on task-adaptive pre-training (Gururangan et al., 2020), domain adaptation (Daumé III, 2007), and code-based LLMs for semantic parsing (Shin and Van Durme, 2022) that we could exploit to either complement or replace vanilla in-context learning. Closing the loop: While we explored the task of generating metamorphic specifications by trans- lating from properties in natural language to con- trastive first-order logic forms, this task alone is not sufficient to fully automate the verification of tax preparation software. To close the loop, two tasks remain: (1) Automatically extracting prop- erties from free-form tax documents, and (2) Tak- ing potentially noisy specifications and generating executable tests. While closing the loop is an am- bitious task, we hypothesize that these additional steps could help inform the translation process, po- tentially improving performance. For example, if we could automatically extract more properties - even if noisy- from raw documents, we could easily expand the training examples without additional manual cost. Moreover, connecting the extracted specifications with symbolic, executable modules, could serve as a form of indirect feedback to inform the translation module. 6 Acknowledgements Tizpaz-Niari and Trivedi were partially supported by the NSF under grants CCF-2317206 and CCF- 2317207. This work utilized the Alpine high- performance computing resource at the University of Colorado Boulder. Alpine is jointly funded by the University of Colorado Boulder, the University of Colorado Anschutz, Colorado State University, and the NSF (award 2201538). References Anas Awadalla, Mitchell Wortsman, Gabriel Ilharco, Sewon Min, Ian Magnusson, Hannaneh Hajishirzi, and Ludwig Schmidt. 2022. Exploring the landscape of distributional robustness for question answering models. In Findings of the Association for Computa- tional Linguistics: EMNLP 2022, pages 5971–5987,Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Dis- course, pages 178–186, Sofia, Bulgaria. Association for Computational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma- teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Ad- vances in Neural Information Processing Systems , volume 33, pages 1877–1901. Curran Associates, Inc. T. Y . Chen, S. C. Cheung, and S. M. Yiu. 2020. Meta- morphic testing: A new approach for generating next test cases. Yi Chen, Luyi Xing, Yue Qin, Xiaojing Liao, XiaoFeng Wang, Kai Chen, and Wei Zou. 2019. Devils in the guidance: Predicting logic vulnerabilities in payment syndication services through automated documenta- tion analysis. In 28th USENIX Security Symposium (USENIX Security 19), pages 747–764, Santa Clara, CA. USENIX Association. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Al- bert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh- ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja- cob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. Hal Daumé III. 2007. Frustratingly easy domain adap- tation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256–263, Prague, Czech Republic. Association for Computational Linguistics. Kais Dukes. 2014. SemEval-2014 task 6: Supervised semantic parsing of robotic spatial commands. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 45–53, Dublin, Ireland. Association for Computational Lin- guistics. Robert Farrington. 2023. How much americans pay to file their taxes. The College Investor. Suchin Gururangan, Ana Marasovi ´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don’t stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 8342–8360, Online. Association for Computational Linguistics. Seungju Han, Beomsu Kim, Jin Yong Yoo, Seokjun Seo, Sangbum Kim, Enkhbayar Erdenee, and Buru Chang. 2022. Meet your favorite character: Open- domain chatbot mimicking fictional characters with only a few utterances. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5114–5132, Seattle, United States. Association for Computational Lin- guistics. J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. ArXiv, abs/2106.09685. IRS. 2023. Penalties. Internal Revenue Services Web- site. Aishwarya Kamath and Rajarshi Das. 2019. A survey on semantic parsing. In Automated Knowledge Base Construction (AKBC). Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sab- harwal. 2023. Decomposed prompting: A modular approach for solving complex tasks. In The Eleventh International Conference on Learning Representa- tions. Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics. Masoud Monajatipoor, Liunian Harold Li, Mozhdeh Rouhsedaghat, Lin Yang, and Kai-Wei Chang. 2023. MetaVL: Transferring in-context learning ability from language models to vision-language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 495–508, Toronto, Canada. Association for Computational Linguistics. Maria Leonor Pacheco, Max von Hippel, Ben Wein- traub, Dan Goldwasser, and Cristina Nita-Rotaru. 2022. Automated attack synthesis by extracting fi- nite state machines from protocol specification doc- uments. In 43rd IEEE Symposium on Security and Privacy, SP 2022, San Francisco, CA, USA, May 22-26, 2022, pages 51–68. IEEE. Martha Palmer, Daniel Gildea, and Nianwen Xue. 2010. Semantic Roles, pages 1–19. Springer International Publishing, Cham.Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Compu- tational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Maja Popovi´c. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392–395, Lisbon, Portugal. Association for Computational Linguistics. Shamik Roy, Nishanth Sridhar Nakshatri, and Dan Goldwasser. 2022. Towards few-shot identification of morality frames using in-context learning. In Proceedings of the Fifth Workshop on Natural Lan- guage Processing and Computational Social Science (NLP+CSS), pages 183–196, Abu Dhabi, UAE. As- sociation for Computational Linguistics. Sergio Segura, Gordon Fraser, Ana B. Sanchez, and An- tonio Ruiz-Cortés. 2016. A survey on metamorphic testing. IEEE Transactions on Software Engineering, 42(9):805–824. Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text genera- tion. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881–7892, Online. Association for Computational Linguistics. Richard Shin and Benjamin Van Durme. 2022. Few- shot semantic parsing with language models trained on code. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, pages 5417–5425, Seattle, United States. Association for Computational Linguistics. Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber, and Lijuan Wang. 2023. Prompting GPT-3 to be reli- able. In The Eleventh International Conference on Learning Representations. Yibo Sun, Duyu Tang, Nan Duan, Jianshu Ji, Guihong Cao, Xiaocheng Feng, Bing Qin, Ting Liu, and Ming Zhou. 2018. Semantic parsing with syntax- and table- aware SQL generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 361–372, Melbourne, Australia. Association for Computational Linguistics. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Saeid Tizpaz-Niari, Verya Monjezi, Morgan Wagner, Shiva Darian, Krystia Reed, and Ashutosh Trivedi. 2023. Metamorphic testing and debugging of tax preparation software. In 45th IEEE/ACM Interna- tional Conference on Software Engineering: Soft- ware Engineering in Society, SEIS@ICSE 2023, Mel- bourne, Australia, May 14-20, 2023, pages 138–149. IEEE. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris- tian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hos- seini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V . Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, An- gela Fan, Melanie Kambadur, Sharan Narang, Aure- lien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions. In Proceed- ings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484–13508, Toronto, Canada. Association for Computational Linguistics. Hengyuan Zhang, Dawei Li, Yanran Li, Chenming Shang, Chufan Shi, and Yong Jiang. 2023. Assisting language learners: Automated trans-lingual defini- tion generation via contrastive prompt learning. In Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), pages 260–274, Toronto, Canada. Association for Computational Linguistics. Shiyu Zhang, Juan Zhai, Lei Bu, Mingsong Chen, Linzhang Wang, and Xuandong Li. 2020. Automated generation of ltl specifications for smart home iot us- ing natural language. In 2020 Design, Automation & Test in Europe Conference & Exhibition (DATE), pages 622–625.A Metamorphic Relations. Syntax and Semantics. We review the syntax and semantics of first-order logic for metamor- phic specification in the context of tax prepara- tion software. Let X = {X1, X2, . . . , Xn} is the set of variables corresponding to various fields about an individual in the tax return form and F : D1×D2×···×D n→R≥0 is the federal tax return computed by the software, where Di is the domain of variable Xi. We write D for D1 ×D2 ×···×D n. These variables correspond to intuitive labels such as age (numerical variable), blind (Boolean vari- able), and sts (filing status with values such as MFJ, married filing jointly, and MFS, married fil- ing separately). For an individual x ∈ D, we write x(i) for the value of i-th variable, or x.lab for the value of variable lab. Let L be the set of all labels. For labels L ⊆ Land inputs x ∈ Dand y ∈ D, we say that y is a metamorphose of x with the exceptions of labels L, and we write x ≡L y if ∀ℓ ̸∈ L we have that x.ℓ = y.ℓ. A metamorphic relation is a first-order logic formula with variables in X, constants from domains in D, relation ≡L, comparisons {<, ≤, =, ≥, >} over numeric vari- ables, predicate ¬ (negation) for Boolean valued labels, real-valued function for federal tax return F : D →R, Boolean connectives ∧, ∨, ¬, =⇒ , ⇐⇒ , and quantifiers ∃x.ϕ(x) and ∀x.ϕ(x) with natural interpretations. We assume that the formu- las are given in the prenex normal form, i.e. a block of quantifiers followed by a quantifier free formula. Selected Specifications. Following (Tizpaz-Niari et al., 2023), we consider aspects of Individual In- come Tax Return that relate to disability, credits, and deductions. Specifically, we focus on on fields related to the standard deductions for senior and disable individuals; the Earned Income Tax Credit (EITC), a refundable tax credits for lower income households; the Child Tax Credit (CTC), a non- refundable credit to reduce the taxes owed based on the number of qualifying children under the age of 17; the Educational Tax Credit (ETC) that helps students with the cost of higher education by low- ering their owed taxes or increasing their refund; and the Itemized Deduction (ID), an option for tax- payers with significant tax deductible expenses. We use scenarios and examples described in the policies above to synthesize metamorphic relations. Table 5 shows 33 metamorphic relations in 6 do- mains for the tax year 2020. For properties #9 to #12, we assume MAGI (modified adjusted gross income) is equivalent to AGI. Next, we provide a brief explanation of some of these properties. 1. A senior (over age of 65) must receive sim- ilar or better tax benefits when compared to a person without the seniority who is similar in every other aspect (due to higher standard deductions for seniors). 2. A blind individual must receive similar or bet- ter tax benefits when compared to a person without the disability who is similar in every other aspect (due to higher standard deduc- tions for blind individuals). 3. An individual with the married filing jointly (MFJ ) status with a senior spouse must re- ceive similar or higher tax benefits compared to a similar individual but without the senior spouse. 4. An individual with the married filing jointly (MFJ ) status with a disabled spouse must re- ceive similar or higher tax benefits compared to a similar individual but without the disabled spouse. 5. An individual who files with the head of house- hold (HoH ) status should receive similar or higher tax return benefits compared to a simi- lar individual who files with the single status. 6. An individual who files the tax with the quali- fied widow (QW) status should receive simi- lar or higher tax return benefits compared to a similar individual who files the tax with single status. 7. An individual who files the tax with the quali- fied widow (QW) status should receive simi- lar or higher tax return benefits compared to a similar individual who files with the head of household (HoH ) status. 8. An individual with the married filing sepa- rately (MFS ) status who claims EITC credits should receive the same tax return compared to a similar individual (with the same status) who does not claim EITC credits. 9. An individual with the married filing jointly (MFJ ) status with AGI over 56, 844 who claims EITC credits should receive the same tax return compared to a similar individual(with the same status) who does not claim EITC credits. 10. An individual with the married filing jointly (MFJ ) status with AGI less than or equal 56, 844 who claims EITC credits should re- ceive a higher tax returns compared to a sim- ilar individual who has AGI greater than 56, 844. 11. Among two qualified individuals with EITC, one with higher EITC claims receives higher or equal tax return benefits. 12. An individual who has the investment income less than $3,650, computed as the sum of lines 2a, 2b, 3a, and 7 in Form 1040, should receive a higher tax returns compared to a similar individual with the investment income greater than $3,650. 13. An individual with single status with AGI less than or equal to 15, 820 who is 25 years old or older should receive a higher tax re- turns compared to a similar individual who is younger than 25 years old. 14. An individual with head of household status with AGI less than or equal to 15, 820 who is 25 years old or older should receive a higher tax returns compared to a similar individual who is younger than 25 years old. 15. An individual with head of household status with AGI less than or equal to 41, 757 and with one qualified children who is younger than 25 years old should receive a similar re- turn compared to a similar individual who is 25 years or older (the age test is not relevant when having at least one qualified children). 16. An individual with qualifying widow status with AGI less than or equal to 15, 820 who is 25 years old or older without any quali- fied children should receive a higher tax re- turns compared to a similar individual who is younger than 25 years old. 17. An individual with Qualifying Widow status with AGI less than or equal to 41, 757 and with one qualified children who is younger than 25 years old should receive a similar re- turn compared to a similar individual who is 25 years or older. 18. A married filing jointly status with AGI less than or equal to 21, 710 who is 25 years old or older without any qualified children should receive a higher tax returns compared to a sim- ilar individual who is younger than 25 years old. 19. An individual with Qualifying Widow status with AGI less than or equal to 47, 646 and with one qualified children who is younger than 25 years old should receive a similar re- turn compared to a similar individual who is 25 years or older. 20. An individual who is qualified for EITC credit with no qualified children should receive a similar or lower tax returns compared to a similar individual with one qualified children. 21. An individual who is qualified for EITC credit with one or less qualified children should re- ceive a similar or lower tax returns compared to a similar individual with two qualified chil- dren. 22. An individual who is qualified for EITC credit with two or less qualified children should re- ceive a similar or lower tax returns compared to a similar individual with three qualified children. 23. An individual who is qualified for EITC credit with three or less qualified children should receive a similar tax returns compared to a similar individual with more than three quali- fied children. 24. Among two qualified married filing jointly (MFJ ) individuals, one with higher child tax credits receives higher or equal tax return ben- efits. 25. This 4-property requires a comparison be- tween four “similar” individuals since there is a relation between two variables of inter- ests: AGI and the number of qualified chil- dren/others to claim a CTC. An individual with more qualified dependents must receive higher or similar tax return benefits than an in- dividual with fewer dependents after adjusting for the effects of income levels on the calcula- tions of both the final return and the amounts of CTC claims. Expressing this property re- quires holding the income of two individualsthe same per each qualified number of chil- dren/others. 26. An individual with the married filing sepa- rately (MFS ) status who claims CTC credits should receive similar tax benefits compared to a similar individual with MFS status who does not claim CTC. 27. An individual with the married filing jointly (MFJ ) status with AGI over 180k who claims ETC should receive similar tax ben- efits compared to a similar individual who does not claim ETC. 28. An individual with the married filing jointly (MFJ ) status with AGI below 160k who claims ETC received higher or similar tax re- turn benefits compared to a similar individual who does not claim ETC or claims a lower amount of ETC credit. 29. This 4-property requires a comparison be- tween four “similar” individuals as the rule changes for individuals withAGI below 160k and between 160k and 180k. By holding AGI constant between two individuals with AGI below 160k (varying the ETC claims) and two individuals with AGI between 160k and 180k (varying the ETC claims with the same rate), the property requires that individ- uals with lower income (below 160k) receive higher or similar tax returns. 30. An individual who files with medical/dental expenses (MDE ) below 7.5% of their AGI and itemizes their deductions receives the same return as a similar individual with no MDE claims. 31. An individual who files with a standard deduc- tion who claims itemized deduction (Line 12) should receive similar tax returns compared to a similar individual who does not claim itemized deduction. 32. An individual who files with itemized deduc- tions below the standard deductions receive a lower or similar tax return benefits compared to a similar individual who files with the stan- dard deductions. 33. An individual who files with itemized deduc- tions above the standard deductions receive a higher or similar tax return benefits compared to a similar individual who claims standard deductions.Table 5: Metamorphic properties for five domains in the US tax (2020) policies. F is federal tax return where negative values mean the individual owns payment to the IRS, sts is filing status, s_lab is spouse’s field lab, MFJ : married filing jointly, MFS is married filing separately, S is single filing, HoH is head of household filing, QW is qualifying widow filing, AGI is adjusted gross income, L27 is line 27 of IRS 1040 for Earned Income Tax Credit (EITC), QC is the number of qualified children, OD is the number of other dependents, CTC is child tax credits, L19 is line 19 of IRS 1040 for Child Tax Credit (CTC ), L29 is line 29 of IRS 1040 for Education Tax Credit (ETC ), MDE is medical/dental expenses reported in line 1 of schedule A, iz is to use itemized deductions (ID) vs. standard deductions, and L12 is total itemized deductions (ID) from schedule A. Id Domain Metamorphic Property 1 Disability ∀x, y((x≡agey) ∧ (x.age≥65) ∧ (y.age<65)) =⇒ F(x) ≥ F(y) 2 Disability ∀x, y((x≡blindy) ∧ (x.blind ∧ ¬y.blind)) =⇒ F(x) ≥ F(y) 3 Disability ∀x(x.sts=MFJ ) =⇒ ∀y((x ≡s_age y) ∧ (x.s_age≥65) ∧ (y.s_age<65)) =⇒ F(x) ≥ F(y) 4 Disability ∀x(x.sts = MFJ ) =⇒ ∀y((x ≡s_blind y) ∧ (x.s_blind ∧ ¬y.s_blind)) =⇒ F(x) ≥ F(y) 5 Status ∀x, y((x≡stsy) ∧ (x.sts=HoH) ∧ (y.sts=S)) =⇒ F(x) ≥ F(y) 6 Status ∀x, y((x≡stsy) ∧ (x.sts=QW) ∧ (y.sts=S)) =⇒ F(x) ≥ F(y) 7 Status ∀x, y((x≡stsy) ∧ (x.sts=QW) ∧ (y.sts=HoH)) =⇒ F(x) ≥ F(y) 8 EITC ∀x(x.sts = MFS ) = ⇒ ∀y(x≡L27y ∧ x.L27 > 0.0 ∧ y.L27 = 0.0) = ⇒ F(x)=F(y) 9 EITC ∀x(x.sts=MFJ ) ∧ (x.AGI>56, 844) = ⇒ ∀y(x≡L27y ∧ x.L27>0.0 ∧ y.L27=0.0) =⇒ F(x)=F(y) 10 EITC ∀x((x.sts=MFJ ) ∧ (x.L27>0.0)) =⇒ ∀y(x≡AGIy ∧ x.AGI≤56, 844 ∧ y.AGI>56, 844) =⇒ F(x)≥F(y) 11 EITC ∀x(x.sts=MFJ )∧(x.AGI≤56, 844) =⇒ ∀y((x≡L27y)∧x.L27≥y.L27) =⇒ F(x)≥F(y) 12 EITC ∀x(x.L27>0.0)∀y(x≡{L2a,L2b,3b,7}y ∧x.L2a + x.L2b + x.L3b + x.L7 > 3, 650) ∧ y.L2a + y.L2b + y.L3b + y.L7≤3, 650) =⇒ F(x)≤F(y) 13 EITC ∀x(x.sts=S)∧(x.AGI≤15, 820)∧(x.L27>0.0) =⇒ ∀y((x≡Agey)∧(x.Age<25) ∧ (y.Age ≥ 25)) =⇒ F(x)<F(y) 14 EITC ∀x(x.sts=HoH)∧(x.AGI≤15, 820)∧(x.L27>0.0)∧(x.QC=0) =⇒ ∀y((x≡Agey)∧(x.Age<25) ∧ (y.Age ≥ 25)) =⇒ F(x)<F(y) 15 EITC ∀x(x.sts=HoH)∧(x.AGI≤41, 756)∧(x.L27>0.0)∧(x.QC=1) =⇒ ∀y((x≡Agey)∧(x.Age<25) ∧ (y.Age ≥ 25)) =⇒ F(x)=F(y) 16 EITC ∀x(x.sts=QW)∧(x.AGI≤15, 820)∧(x.L27>0.0)∧(x.QC=0) =⇒ ∀y((x≡Agey)∧(x.Age<25) ∧ (y.Age ≥ 25)) =⇒ F(x)<F(y) 17 EITC ∀x(x.sts=QW)∧(x.AGI≤41, 756)∧(x.L27>0.0)∧(x.QC=1) =⇒ ∀y((x≡Agey)∧(x.Age<25) ∧ (y.Age ≥ 25)) =⇒ F(x)=F(y) 18 EITC ∀x(x.sts=MFJ )∧(x.AGI≤21, 710)∧(x.L27>0.0)∧(x.QC=0) =⇒ ∀y((x≡Agey)∧(x.Age<25) ∧ (y.Age ≥ 25)) =⇒ F(x)<F(y) 19 EITC ∀x(x.sts=MFJ )∧(x.AGI≤47, 646)∧(x.L27>0.0)∧(x.QC=1) =⇒ ∀y((x≡Agey)∧(x.Age<25) ∧ (y.Age ≥ 25)) =⇒ F(x)=F(y) 20 EITC ∀x(x.L27>0.0)∧(x.QC=0) =⇒ ∀y((x≡QCy)∧(y.QC=1) =⇒ F(x)≤F(y)Continuation of Previous Table. Id Domain Metamorphic Property 21 EITC ∀x(x.L27>0.0)∧(x.QC≤1) =⇒ ∀y((x≡QCy)∧(y.QC=2) =⇒ F(x)≤F(y) 22 EITC ∀x(x.L27>0.0)∧(x.QC≤2) =⇒ ∀y((x≡QCy)∧(y.QC=3) =⇒ F(x)≤F(y) 23 EITC ∀x(x.L27>0.0)∧(x.QC=3) =⇒ ∀y((x≡QCy)∧(y.QC>3) =⇒ F(x)=F(y) 24 CTC ∀x(x.sts=MFS )∧(x.AGI≤200k) =⇒ ∀y((x≡L19y)∧(x.L19≥y.L19)) =⇒ F(x)≥F(y)) 25 CTC ∀x, x′(x.sts=x′.sts=MFJ )∧(x.AGI<400k)∧(x′.AGI≥400k)∧⌈x′.AGI−400k⌉1k∗ 0.05<x′.QC ∗ 2k + x.OD ∗ 0.5k =⇒ ∀y, y′(x≡{QC,OD}y)∧(x′≡{QC,OD}y′) ∧ (0≤y.QC=y′.QC≤x.QC=x′.QC ≤ 10) ∧ (0≤y.OD=y′.OD ≤ x.OD=x′.OD ≤ 10) =⇒ (F(x) − F(y)) ≥ (F(x′) − F(y′)) 26 ETC ∀x(x.sts = MFS ) = ⇒ ∀y(x≡L29y ∧ x.L29 > 0.0 ∧ y.L29 = 0.0) = ⇒ F(x)=F(y) 27 ETC ∀x(x.sts=MFJ ) ∧ (x.AGI≥180k) = ⇒ ∀ y(x≡L29y ∧ x.L29>0.0 ∧ y.L29=0.0) =⇒ F(x)=F(y) 28 ETC ∀x(x.sts=MFJ ) ∧ (x.AGI≤160k) = ⇒ ∀ y(x≡L29y ∧ x.L29≥y.L29) =⇒ F(x)≥F(y) 29 ETC ∀x, x′(x.sts=x′.sts=MFJ ) ∧ (x.AGI≤160k) ∧ (160k<x′.AGI<180k) = ⇒ ∀y, y′((x ≡L29 y) ∧ (x′ ≡L29 y′) ∧ (x.L29 = x′.L29 ≥ y.L29 = y′.L29)) =⇒ (F(x) − F(y)) ≥ (F(x′) − F(y′)) 30 ID ∀x, y(x≡MDE y) ∧ (x.MDE ≤x.AGI ∗ 7.5%) ∧ (y.MDE =0.0) =⇒ F(x)=F(y) 31 ID ∀x(¬x.iz) =⇒ ∀y(x≡MDE y ∧ x.MDE> 0.0 ∧ y.MDE =0.0) =⇒ F(x)=F(y) 32 ID ∀x(x.sts=MFJ ) =⇒ ∀y((x≡iz,L12y)∧(x.iz∧¬y.iz)∧(x.L12≤24.8k∧y.L12=0.0)) =⇒ F(x)≤F(y) 33 ID ∀x(x.sts=MFJ ) =⇒ ∀y((x≡iz,L12y)∧(x.iz∧¬y.iz)∧(x.L12>24.8k∧y.L12=0.0)) =⇒ F(x)≥F(y)B Prompt Examples Figure 1: Example of an End2End prompt, that looks at generating predicates and FOL in the same prompt. This is an example for nshot = 1; the example was omitted and another added for 0-shot and 2-shot respectively. Figure 2: Example of a prompt to generate predicates. The prompt is the same for explicit and implicit setups. This is an example for nshot = 1; the example was omitted and another added for 0-shot and 2-shot respectively. Figure 3: Example of a prompt to generate FOL, given that predicates were already generated. This setup is only used in explicit cases (as predicates are provided through context in implicit cases). This is an example for nshot = 1; the example was omitted and another added for 0-shot and 2-shot respectively. C Example Generations for Annotation RubricMetamorphic Statement Generated Predicates Generated First Order Logic Explanation Score An individual with the married filing jointly (MFJ) status with AGI over $56,844 who claims EITC credits should receive the same tax return compared to a similar individual (with the same status) who does not claim EITC credits. MFJ(x): x is an individual with the married filing jointly status. AGI(y, x): y is the adjusted gross income of individual x. ClaimEITC(x): x claims EITC credits. TaxReturn(t, x): t is the tax return of individual x. EqualExcept(x, z, S): where x and z are equal individuals except differing only by the predicates in set S. ∀x∀y(MFJ(x)∧AGI(y,x)>56844∧ ClaimEITC(x)∧ EqualExcept(x,y,ClaimEITC) =⇒TaxReturn(t,x)=TaxReturn(t,y) The model correctly predicts that AGI criteria is applicable to both individuals in question, and only differs on EITC status. 5 An individual with single status with AGI less than or equal to $15,820 who is 25 years old or older should receive a higher tax returns compared to a similar individual who is younger than 25 years old. Single(x): x is an individual with single status. AGI(x, a): x has adjusted gross income of a. Younger(x, y): x is younger than y years old. TaxReturns(r, x): x receives r in tax returns. ∀x∀y(Single(x)∧AGI(x, a)≤15820∧Older(x, 25) ∧Younger(y, 25)∧EqualExcept(x,y,AGI)=⇒ TaxReturns(r, x)>TaxReturns(r, y) All the predicates have been generated correctly. However, the predicate \"EqualExcept\" should only apply to age of the individual and not the AGI. This makes some semantic sense, but is incorrect (not usable). 4 An individual who files the tax with the qualified widow (QW) status should receive similar or higher tax return benefits compared to a similar individual who files the tax with single status. QW(x): x is an individual who files the tax with the qualified widow status. Single(x): x is an individual who files the tax with single status. TaxReturn(t, x): t is the tax return received by individual x. SimilarOrHigher(x, y): individual x receives similar or higher tax return benefits compared to individual y. ∀x∀y(QW(x)∧EqualExcept(x,y,QW)=⇒ SimilarOrHigher(x,y) The system makes a mistake in predicates - it does not use the \"Single\" status to make a judgement on tax return status. 3 An individual with the married filing jointly (MFJ) status with a disabled spouse must receive similar or higher tax benefits compared to a similar individual but without the disabled spouse. MFJ(x): x is an individual with the married filing jointly status. Disabled(y): y is an individual who is disabled. Tax(t, x): t is the tax paid by individual x. SimilarOrHigher(x, z, S): where x and z are similar individuals except for the predicates in set S. ∀x∀y(MFJ(x)∧Disabled(y)∧ SimilarOrHigher(x, y, Disabled)=⇒ Tax(t, x)≤Tax(t, y) Here, the system misses the predicate \"Spouse(x)\". In addition to this, the greater than operator is flipped (or there is a mistake in the \"Tax(t, x)\" predicate as well). 2 An individual with the married filing jointly (MFJ) status with a senior spouse must receive similar or higher tax benefits compared to a similar individual but without the senior spouse. MFJ(x): x is an individual with the married filing jointly status. SeniorSpouse(y): y is a senior spouse of an individual. Tax(t, x): t is the tax paid by individual x. EqualExcept(x, z, S): where x and z are equal individuals except differing only by the predicates in set S. ∀x∀y(MFJ(x)∧SeniorSpouse(y)∧ EqualExcept(x, y, MFJ, SeniorSpouse)=⇒ Tax(t, x)≤Tax(t, y) Here, the SeniorSpouse is incorrectly applied to individual y. Additionally, the predicate EqualExcept takes more than one predicate (and is redundant). Lastly, the≤is flipped (or the \"Tax\" predicate is incorrect). 1 Table 6: In this table, we show examples of our qualitative score rubric. A rating of 5 usually means that the generated logic can be used for metamorphic testing. All these examples were sourced from 2-shot examples in the explicit setup.",
      "references": [
        "Exploring the landscape of distributional robustness for question answering models.",
        "Abstract Meaning Representation for sembanking.",
        "Language models are few-shot learners.",
        "Meta- morphic testing: A new approach for generating next test cases.",
        "Devils in the guidance: Predicting logic vulnerabilities in payment syndication services through automated documenta- tion analysis.",
        "Scaling instruction-finetuned language models.",
        "Frustratingly easy domain adap- tation.",
        "SemEval-2014 task 6: Supervised semantic parsing of robotic spatial commands.",
        "How much americans pay to file their taxes.",
        "Don’t stop pretraining: Adapt language models to domains and tasks.",
        "Meet your favorite character: Open- domain chatbot mimicking fictional characters with only a few utterances.",
        "Lora: Low-rank adaptation of large language models.",
        "Penalties.",
        "A survey on semantic parsing.",
        "Decomposed prompting: A modular approach for solving complex tasks.",
        "ROUGE: A package for auto- matic evaluation of summaries.",
        "MetaVL: Transferring in-context learning ability from language models to vision-language models.",
        "Automated attack synthesis by extracting fi- nite state machines from protocol specification doc- uments.",
        "Semantic Roles, pages 1–19.",
        "Bleu: a method for automatic evalu- ation of machine translation.",
        "chrF: character n-gram F-score for automatic MT evaluation.",
        "Towards few-shot identification of morality frames using in-context learning.",
        "A survey on metamorphic testing.",
        "BLEURT: Learning robust metrics for text genera- tion.",
        "Few- shot semantic parsing with language models trained on code.",
        "Prompting GPT-3 to be reli- able.",
        "Semantic parsing with syntax- and table- aware SQL generation.",
        "Stanford alpaca: An instruction-following llama model.",
        "Metamorphic testing and debugging of tax preparation software.",
        "Llama: Open and efficient foundation language models.",
        "Llama 2: Open foundation and fine-tuned chat models.",
        "Self-instruct: Aligning language models with self-generated instructions.",
        "Assisting language learners: Automated trans-lingual defini- tion generation via contrastive prompt learning.",
        "Automated generation of ltl specifications for smart home iot us- ing natural language."
      ],
      "meta_data": {
        "arxiv_id": "2311.11979v1",
        "doi": "10.18653/v1/2023.nllp-1.23",
        "authors": [
          "Dananjay Srinivas",
          "Rohan Das",
          "Saeid Tizpaz-Niari",
          "Ashutosh Trivedi",
          "Maria Leonor Pacheco"
        ],
        "published_date": "2023-11-20T18:12:28Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Formulates automated derivation of metamorphic specifications for US tax preparation software as a low-resource translation problem from natural-language tax properties to a contrastive first-order logic (FOL) template used in metamorphic testing. Curates a small, high-quality paired dataset (33 NL properties + ground-truth FOL specs from prior expert work) and performs a systematic study of few-shot in-context learning (ICL) with LLMs. Finds that few-shot ICL can produce partially correct specifications but still averages ~1–2 logical mistakes per property (predicates/variables/operators), with performance degrading as predicate complexity increases; decomposed prompting improves consistency vs end-to-end prompting. Provides qualitative error taxonomy and a research agenda to close the automation loop.",
        "methodology": "Treats NL→metamorphic-spec translation as semantic parsing into a specific FOL language featuring: quantified taxpayers, an EqualExcept/≡_L relation capturing similarity except for selected labels, comparisons over numeric fields, Boolean connectives, and a tax-return function F(x). Evaluates prompting strategies for LLMs: (1) end-to-end prompting to output full FOL directly; (2) decomposed prompting that first generates relevant predicates then generates FOL—implemented as Implicit (predicate generation context carried into FOL step) and Explicit (two fully decoupled prompts). Uses few-shot ICL with 0/1/2 demonstrations; compares GPT-3.5 (API) across all strategies and Alpaca-7B (LoRA-adapted) for Explicit prompting. Analyzes impact of example complexity (number of predicates) and categorizes errors in predicates, variable handling, and operator choice/imitation.",
        "experimental_setup": "Dataset: 33 metamorphic properties drawn from IRS forms/publications (e.g., Form 1040, Pub 596 EITC, Schedule 8812 CTC, Form 8863 education credits, Schedule A itemized deductions) paired with expert-authored ground-truth metamorphic FOL specifications (tax year 2020) from Tizpaz-Niari et al. (2023). Models: GPT-3.5 via OpenAI ChatCompletion API with default settings; Alpaca 7B instruction-tuned LLaMA-based model using LoRA weights for local inference. Few-shot regime limited to 0–2 demonstrations due to dataset size. Evaluation: human judgment (two paper authors) instead of BLEU/ROUGE/etc.; 1–5 rubric separately for predicate set quality and FOL quality (5 exact match; 4 semantically right but wrong format; 3/2 = one/two mistakes in predicates/vars/operators; 1 incorrect). Reports averages and standard deviations by prompting strategy and n-shot, and by difficulty buckets based on ground-truth predicate count (<4, 4–5, ≥6). Includes qualitative error analysis with examples.",
        "limitations": "Very small dataset (33) restricts few-shot context and prevents robust generalization claims; n-shot capped at 2 and no fine-tuning explored. Human evaluation by the first two authors introduces subjectivity and lacks inter-annotator agreement statistics. Outputs are often not directly usable for testing because small logical errors (missing EqualExcept template, wrong variable scoping/reuse, operator flips, and operator imitation from demonstrations) invalidate metamorphic relations; performance drops on complex properties with many predicates and on higher-order/4-individual relations. Alpaca-7B shows limited improvement with more demonstrations, suggesting size/capability constraints. Study focuses only on NL property→FOL translation given clean properties; it does not automate property extraction from tax documents nor test-case generation/execution, so end-to-end verification remains open.",
        "future_research_directions": "(1) Task-oriented prompt learning: learn or optimize prompts dynamically for this specific logic language to reduce systematic errors and improve consistency beyond vanilla ICL/decomposed prompting. (2) Leverage additional data sources for weak/transfer supervision: unlabeled tax corpora for task-adaptive pretraining, out-of-domain NL→logic datasets for domain adaptation, and code/structured-representation LLMs for better semantic parsing into formal syntax. (3) Close the loop end-to-end: automatically extract candidate properties from raw tax documents; generate executable metamorphic tests from (possibly noisy) specs; use symbolic execution/constraint solving or test outcomes as feedback to refine translation; expand training examples via noisy property mining plus filtering/verification.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning",
      "full_text": "Vacuum polarization induced by a cosmic string and a brane in AdS spacetime W. Oliveira dos Santos1∗, E. R. Bezerra de Mello1† 1Departamento de F´ ısica, Universidade Federal da Para´ ıba 58.059-970, Caixa Postal 5.008, Jo˜ ao Pessoa, PB, Brazil August 3, 2023 Abstract In this paper we investigate the vacuum polarization effects associated to a charged quantum massive scalar field on a (D+1)-dimensional anti-de Sitter background induced by a magnetic-flux-carrying cosmic string in the braneworld model context. We consider the brane parallel to the anti-de Sitter boundary and the cosmic string orthogonal to them. Moreover, we assume that the field obeys the Robin boundary condition on the brane. Because the brane divides the space into two regions with different properties of the quantum vacuum, we calculate the vacuum expectation value (VEV) of the field squared and the energy-momentum tensor (EMT) in each region. To develop these analyses, we have constructed the positive frequency Wightman function for both regions. The latter is decomposed in a part associated with the anti-de Sitter bulk in the presence of a cosmic string only, and the other part induced by the brane. The vacuum polarization effects associated with the higher-dimensional anti- de Sitter bulk in the presence of cosmic string have been developed in the literature, and here we are mainly interested in the effects induced by the brane. We show that the VEVs of the field squared and the components of the EMT induced by the cosmic string are finite on the brane. Explicitly, we compare these observables with the corresponding ones induced by the brane only, and show that near the brane the contribution induced by the latter is larger than the one induced by the string; however, for points distant from the brane the situation is reversed. Moreover, some asymptotic expressions for the VEV of the field squared and EMT are provided for specific limiting cases of the physical parameters of the model. Also, an application of our results is given for a cosmic string in the Z2-symmetric Randall-Sundrum braneworld model with a single brane. PACS numbers: 98.80.Cq, 11.10.Gh, 11.27.+d 1 Introduction The anti-de Sitter (AdS) spacetime is one of the most interesting spacetimes allowed by the Theory of General Relativity. Due to its maximal symmetry, many problems involving quantum fields propagating can be exactly solvable (see, for example [1]-[7]). This allows to reveal infor- mation on the influence of gravitational field on quantum matter in less symmetric geometries. ∗E-mail: wagner.physics@gmail.com †E-mail: emello@fisica.ufpb.br 1 arXiv:2305.15664v3  [hep-th]  2 Aug 2023In addition, the length scale related to the AdS negative constant curvature, can serve as a regu- larization parameter for infrared divergences in interacting quantum field theories without have to reduce the number of symmetries [8]. Besides, the importance of this theoretical background increased when it was discovered that AdS spacetime generically arises as a ground state in extended supergravity and in string theories. Additional interest in this subject was generated by the appearance of two models where AdS geometry plays a special role. The first model, the AdS/CFT correspondence (for a review see [9]), represents a realization of the holographic prin- ciple and relates string theories or supergravity in the AdS bulk with a conformal field theory living on its boundary. The second model is a realization of a braneworld scenario with large extra dimensions and provides a solution to the hierarchy problem between the gravitational and electroweak mass scales (for reviews on braneworld gravity and cosmology see [10, 11]). According to the Big Bang Theory, at the beginning the Universe was very hot and was in a complete symmetric stage. During its expansion, it cooled and underwent several phase changes, accompanied by spontaneous symmetry breaking resulting in the formation of toplological de- fects [12, 13]. These include domain walls, cosmic strings and monopoles. Among them the cosmic strings are of special interest. Cosmic strings are linear topological defects. The gravitational field produced by an idealized cosmic string may be approximated by a planar angle deficit in the two-dimensional sub-space orthogonal to the string. Although there is no Newtonian potential, the lack of global flatness is responsible for many interesting phenomena as shown many years ago by Linet [14] and Smith [15]. For instance, in Refs.[16, 17] the authors have studied the vacuum polarization effects induced by multiple parallel static straight-line cosmic strings and have shown that two parallel strings mutually attract each other with a Casimir-like force. Moreover, the presence of the string allows effects such as particle-antiparticle pair production by a single photon and bremsstrahlung radiation from charged particles which are not possible in empty Minkowski space due to the conservation of linear momentum [18]. The dimensionless parameter that characterizes the strength of gravitational interactions of strings with matter is its tension, that is given in natural units by Gµ0, being G the Newton’s constant and µ0 its linear mass density, proportional to the square of the symmetry breaking scale energy. In the eighties and early nineties of the last century, cosmic string was considered as possible seeds for large scale structure formation in the Universe. Although recent observational data on the temperature anisotropies of the cosmic microwave background radiation (CMB) have excluded the cosmic strings as the main origin of structures, they are still sources for a number of interesting physical effects such as gamma ray bursts [19], gravitational waves [20] and high energy cosmic rays [21]. Recently, cosmic strings have attracted renewed interest partly because a variant of their formation mechanism is proposed in the framework of brane inflation [22]-[24]. The analysis of the VEV of the bosonic current density, ⟨jµ⟩, and the energy-momentum tensor, ⟨Tµ ν ⟩, induced by a magnetic flux running along the core of an idealized cosmic string in a high-dimensional AdS spacetime, admitting that an extra dimension coordinate is compactified to a circle, were analyzed in [25] and [26], respectively. In both papers it was admitted the presence of an extra magnetic flux enclosed by the compactified dimension. Moreover, the anal- ysis of VEV of fermionic current density and energy-momentum tensor in (1 + 4) −dimensional AdS spacetime in the presence of a cosmic string, considering the compactification of the extra dimension, have been developed in [27] and [28], respectively. Finally considering the presence of a brane parallel to the AdS boundary, the analysis of the effects of the brane on the vacuum fermionic current, ⟨jµ⟩, and the energy-momentum tensor, ⟨Tµ ν ⟩, were investigate in [29] and [30], respectively. The vacuum polarization effects induced by a cosmic string in AdS spacetime were studied in Ref.[31], where the authors have analyzed the VEVs of the field squared and the energy- 2momentum tensor. Here in this paper, we want to continue in this line of investigation and study the VEVs of the field squared and the energy-momentum tensor induced by an idealized cosmic string carrying a magnetic flux running along its core on (1 + D)-dimensional AdS bulk considering the presence of a brane parallel to the AdS boundary. This analysis is developed for both part of the space. Moreover, we admit that the bosonic field obeys the Robin boundary condition (BC) on the brane in both parts of the space. The paper is organized as follows. In the section 2 we present the setup of the problem that we want investigate, and the complete set of normalized positive and negative energy solutions to the Klein-Gordon equation in the presence of a brane parallel to the AdS boundary. In the section 3 we construct the Wightman function for both regions of the space. In the sections 4 and 5, respectively, the VEVs of the field squared and the energy-momentum tensor in the region between the brane and the AdS horizon (R(right)-region) and the AdS boundary and the brane (L(left)-region) are investigated. Various asymptotic limits are considered and numerical results are presented. In the section 6 we apply our analysis to the Randall-Sundrum type model with a single brane and section 7 summarizes the most relevant results obtained. Throughout the paper, we use natural units G = ℏ = c = 1. 2 Model setup In this section we present the model setup, describing the background geometry and the matter field content. We begin by presenting the line element, in cylindrical coordinates, associated with the spacetime geometry we are going to consider, which is a ( D + 1)-dimensional anti-de Sitter (AdS) spacetime containing a cosmic string: ds2 = gµνdxµdxν = e−2y/a \" dt2 − dr2 − r2dϕ2 − DX i=4 (dxi)2 # − dy2 , (1) where r ⩾ 0 and ϕ ∈ [0, 2π/q] define the coordinates on the conical geometry, ( t, z, y, xi) ∈ (−∞, ∞) for i = 4, ..., Dand the parameter a determines the curvature scale of the background spacetime. The latter is related to the cosmological constant, Λ = −D(D − 1)/(2a2), and the Ricci scalar, R = −D(D + 1)/a2. In the case of D = 3, the cosmic string is assumed to be along the y-axis. Moreover, the presence of the cosmic string is codified through the parameter q ≥ 1. Using the Poincar` e coordinate defined byw = aey/a, the line element above can be conformally related to the line element associated with a cosmic string in Minkowski spacetime ds2 = \u0010 a w \u00112 \u0014 dt2 − dr2 − r2dϕ2 − dw2 − DX i=4 (dxi)2 \u0015 . (2) For the new coordinate one has w ∈ [0, ∞). In particular, the values w = 0 and w = ∞ correspond to the AdS boundary and horizon, respectively. As to the matter field content, we will consider a charged massive bosonic field coupled to a gauge field, Aµ. The corresponding field equation that governs the quantum dynamics is given by the Klein-Gordon (KG) equation, (D2 + m2 + ξR)φ(x) = 0 , (3) where the differential operator in the field equation reads D2 = 1p |g| Dµ \u0010p |g|gµνDν \u0011 , Dµ = ∂µ + ieAµ with g = det(gµν) . (4) 3In addition, we also consider the presence of a non-minimal coupling, ξ, between the field and the geometry represented by the Ricci scalar, R. Two specific values for the curvature coupling are of special interest: ξ = 0 and ξ = D−1 4D , that correspond to minimal and conformal coupling, respectively. As to the vector potential, we consider the configuration Aµ = δϕ µAϕ with Aϕ constant, corresponding to a thin magnetic flux along the string’s core. We also consider a codimension one flat boundary, hereafter named brane, located atw = w0 and parallel to the AdS boundary. On the brane we will impose that the field operator obeys the gauge invariant Robin boundary condition, (1 + βnµDµ)φ(x) = 0, w = w0 . (5) The inward pointing vector (with respect to the region under consideration), nµ, is normal to the brane at w = w0. It is defined by nµ = δ(J)δµ 3 a/w, where J = L, δ(L) = −1 in the region 0 ≤ w ≤ w0, L-region, and J = R, δ(R) = 1 in the region w0 ≤ w ≤ ∞, R-region. In addition, the parameter β in (5) is a constant and it encodes the properties of the brane, which in the special cases β = 0 and β = ∞ correspond to the Dirichlet and Neumann boundary conditions, respectively. Moreover, note that the value of this parameter for both the regions divided by the brane could be different in general. In the geometry defined by (2) and in the presence of the vector potential Aµ = δϕ µAϕ, the KG equation (3) becomes \" ∂2 ∂t2 − ∂2 ∂r2 − 1 r ∂ ∂r − 1 r2 \u0012 ∂ ∂ϕ + ieAϕ \u00132 − ∂2 ∂w2 − (1 − D) w ∂ ∂w +M(D, m, ξ) w2 − DX i=4 ∂2 ∂(xi)2 # φ(x) = 0 , (6) where M(D, m, ξ) = a2m2 − ξD(D + 1). According to the symmetry of the problem, the normalized positive energy wave function solutions of (6) reads φσ(x) = Cσw D 2 Zν(pw)Jq|n+α|(λr)e−iEt+iqnϕ+i⃗k·⃗ x∥ , (7) where we have defined the function Zν(pw) = C1Jν(pw) + C2Yν(pw) (8) is a linear combination of the Bessel and Neumann functions [32], with the order given by ν = r D2 4 + a2m2 − ξD(D + 1) . (9) Moreover, E = q λ2 + p2 + ⃗k2, α = eAϕ q = −Φϕ Φ0 , (10) being Φ0 = 2π e , the quantum flux. In (7) ⃗ x∥ represents the coordinates defined in the ( D − 4) extra dimensions, being ⃗k the corresponding momentum, and σ represents the set of quantum 4numbers ( n, λ, p,⃗k), being n = 0 , ±1, ±2, . . ., λ ≥ 0, −∞ < kj < ∞ for j = 4 , ..., D. The quantum number p is determined separately in each region divided by the brane. The coefficient Cσ in (7) is determined from the normalization condition Z dDx p |g|g00φ∗ σ′(x)φσ(x) = 1 2E δσ,σ′ , (11) where the delta symbol on the right-hand side is understood as Dirac delta function for the continuous quantum numbers, λ, p and ⃗k, and Kronecker delta for the discrete one, n. Let us first consider the R-region. By imposing the Robin boundary condition (5) on the flat boundary at w = w0, we get the relation C2/C1 = − ¯Jν(pw0)/ ¯Yν(pw0) for the coefficients in (8). Here and bellow we use the notation ¯F(x) = A0F(x) + B0xF′(x) , (12) with the coefficients A0 = 1 + δ(J) Dβ 2a , B 0 = δ(J) β a . (13) Thus, the mode functions in the R-region that obey the boundary condition (5) can be written presented as, φ(R)σ(x) = C(R)σw D 2 gν(pw0, pw)Jq|n+α|(λr)e−iEt+iqnϕ+i⃗k·⃗ x∥ , (14) where we have introduced the function gν(u, v) = Jν(v) ¯Yν(u) − ¯Jν(u)Yν(v) . (15) Taking into account the continuous spectrum of the quantum number p and the normalization condition (11), we obtain |C(R)σ|2 = (2π)2−Dqpλ 2EaD−1[ ¯J2ν (pw0) + ¯Y 2ν (pw0)] . (16) In the L-region, the region of integration over w in the normalization condition (11) goes over the interval 0 ≤ w ≤ w0. For the solutions with C2 ̸= 0 in (8), the integral over w diverges at the lower limit w = 0 in the range of values ν ≥ 1. Therefore, for this case we should take C2 = 0 according to the normalization condition. On the other hand, in the region 0 ≤ ν <1, the solution (8) with C2 ̸= 0 is normalizable and in order to uniquely define the mode functions an additional boundary condition at the AdS boundary is required [33, 3]. Here, we will choose the Dirichlet condition which gives C2 = 0. Thus, with this choice, the mode function in the L-region are given by φ(L)σ(x) = C(L)σw D 2 Jν(pw)Jq|n+α|(λr)e−iEt+iqnϕ+i⃗k·⃗ x∥ , (17) where, according to the Robin boundary condition (5) on the brane, the eigenvalues of the quantum number p obey the relation: ¯Jν(pw0) = 0 , (18) where the eigenvalues are given by p = pν,i/w0, with pν,i being the positive zeros of the function ¯Jν(x), enumerated by i = 1, 2, .... Note that the roots pν,i do not depend on the location of the brane. From the normalization condition (11), with δp,p′ = δi,i′, and integrating over w in the interval [0, w0], we get |C(L)σ|2 = (2π)2−Dqpν,iλTν(pν,i) w0aD−1 q p2 ν,i + w2 0(λ2 + ⃗k2) , (19) with the function Tν(z) = z[(z2 − ν2)J2 ν (z) + z2(J′ ν(z))2]−1. 53 Wightman Function In this section we present the positive frequency Wightman function,W(x, x′) = ⟨0|ˆφ(x) ˆφ†(x′)|0⟩, where |0⟩ stands for the vacuum state, for both L-region and R-region in a closed form. Here we will assume that the field is prepared in the Poincar´ e vacuum state. To evaluate this function, we use the mode sum formula: W(x, x′) = X σ φσ(x)φ∗ σ(x′) . (20) 3.1 R-region Let us start with the R-region by taking the respective wave function solutions (14) into the above expression. Thus, we have W(x, x′) = q(ww′)D/2 2(2π)D−2aD−1 X σ pλ E gν(pw0, pw)gν(pw0, pw′) ¯J2ν (pw0) + ¯Y 2ν (pw0) Jq|n+α|(λr)Jq|n+α|(λr′) × eiqn∆ϕ+i⃗k·∆⃗ x∥−iE∆t , (21) where gν(u, v) is defined in (15), ∆ t = t −t′, ∆ϕ = ϕ −ϕ′, ∆⃗ x∥ = ⃗ x∥ −⃗ x′ ∥ and with the notation X σ = Z ∞ 0 dλ Z ∞ 0 dp X n Z d⃗k . (22) Now performing a Wick rotation on the time coordinate and using the identity e−E∆τ E = 2√π Z ∞ 0 dse−s2E2−∆τ2/(4s2) , (23) with the energy given by E = q λ2 + p2 + ⃗k2 in the R-region, we can integrate over λ and ⃗k with the help of [34]. The result is, W(x, x′) = qrr′ 2(2π)D/2aD−1 \u0012ww′ rr′ \u0013D/2 Z ∞ 0 dχχ D 2 −2e− r2+r′2+∆⃗ x2 ∥−∆t2 2rr′ χ X n einq∆ϕIq|n+α|(χ) × Z ∞ 0 dppe−rr′ 2χ p2 gν(pw0, pw)gν(pw0, pw′) ¯J2ν (pw0) + ¯Y 2ν (pw0) , (24) where we have introduced a new variable, χ = rr′/(2s2). Note that the Wightman function above presents the contributions coming from the cosmic string and the boundary. However, in this paper we are mainly interested to investigate the vacuum polarization effects associated to the boundary. In this sense, we will split those con- tributions and focus on the boundary induced one. For this end, we proceed in the following way: Wb(x, x′) = W(x, x′) − Wcs(x, x′) , (25) where the term induced by the cosmic string was calculated in [25] and is given by Wcs(x, x′) = qrr′ 2(2π)D/2aD−1 \u0012ww′ rr′ \u0013D/2 Z ∞ 0 dχχ D 2 −2e− r2+r′2+∆⃗ x2 ∥−∆t2 2rr′ χ X n einq∆ϕIq|n+α|(χ) × Z ∞ 0 dppe−rr′ 2χ p2 Jν(pw)Jν(pw′) . (26) 6Thus, replacing (24) and (26) into (25) and using the identity [35] gν(pw0, pw)gν(pw0, pw′) ¯J2ν (pw0) + ¯Y 2ν (pw0) − Jν(pw)Jν(pw′) = −1 2 2X l=1 ¯Jν(pw0) ¯H(l) ν (pw0) H(l) ν (pw)H(l) ν (pw′) , (27) we get Wb(x, x′) = − qrr′ 4(2π)D/2aD−1 \u0012ww′ rr′ \u0013D/2 Z ∞ 0 dχχ D 2 −2e− r2+r′2+∆⃗ x2 ∥−∆t2 2rr′ χ X n einq∆ϕIq|n+α|(χ) × Z ∞ 0 dppe−rr′ 2χ p2 2X l=1 ¯Jν(pw0) ¯H(l) ν (pw0) H(l) ν (pw)H(l) ν (pw′) , (28) where H(l) ν (x), l = 1, 2, are the Hankel functions [32]. The parameter α in Eq.(10) can be written in the form α = n0 + α0, with |α0| < 1 2, (29) being n0 an integer number. This allow us to sum over the quantum number n in (28), using the result obtained in [36], given below, ∞X n=−∞ eiqn∆ϕIq|n+α|(χ) = 1 q X k eχ cos(2πk/q−∆ϕ)eiα(2πk−q∆ϕ) − e−iqn0∆ϕ 2πi X j=±1 jejiπq|α0| Z ∞ 0 dy cosh [qy(1 − |α0|)] − cosh (|α0|qy)e−iq(∆ϕ+jπ) eχ cosh (y)\u0002 cosh (qy) − cos (q(∆ϕ + jπ)) \u0003 , (30) where −q 2 + ∆ϕ Φ0 ≤ k ≤ q 2 + ∆ϕ Φ0 . (31) Substituting the formula above into (28), we can perform the integration overχ using the integral formula [34] Z ∞ 0 xν−1e−β x −γxdx = 2 \u0012β γ \u0013ν/2 Kν(2 p βγ) . (32) The result of these operations is the following: Wb(x, x′) = − (ww′)D/2 2(2π)D/2aD−1 (X k eiα(2πk−q∆ϕ) u D 2 −1 k Z ∞ 0 dppD/2 2X l=1 ¯Jν(pw0) ¯H(l) ν (pw0) × H(l) ν (pw)H(l) ν (pw′)KD 2 −1(puk) − qe−iqn0∆ϕ 2πi X j=±1 jejiπq|α0| × Z ∞ 0 dy cosh [qy(1 − |α0|)] − cosh (|α0|qy)e−iq(∆ϕ+jπ) u D 2 −1 y \u0002 cosh (qy) − cos (q(∆ϕ + jπ)) \u0003 Z ∞ 0 dppD/2 2X l=1 ¯Jν(pw0) ¯H(l) ν (pw0) × H(l) ν (pw)H(l) ν (pw′)KD 2 −1(puy) ) , (33) 7where we have introduced the notation u2 k = r2 + r′2 − 2rr′ cos (2πk/q − ∆ϕ) + ∆⃗ x2 ∥ − ∆t2 u2 y = r2 + r′2 + 2rr′ cosh (y) + ∆⃗ x2 ∥ − ∆t2. (34) As the last step, we rotate the contour integration over p by the angle π/2 (−π/2) for the term l = 1 (l = 2). The result is Wb(x, x′) = − (ww′)D/2 (2π)D/2aD−1 Z ∞ 0 dppD−1 ¯Iν(pw0) ¯Kν(pw0)Kν(pw)Kν(pw′) × (X k eiα(2πk−q∆ϕ)fD 2 −1(puk) − qe−iqn0∆ϕ 2πi X j=±1 jejiπq|α0| × Z ∞ 0 dy cosh [qy(1 − |α0|)] − cosh (|α0|qy)e−iq(∆ϕ+jπ) cosh (qy) − cos (q(∆ϕ + jπ)) fD 2 −1(puy) ) , (35) where we have introduced the notation fµ(x) = Jµ(x) xµ . (36) 3.2 L-region Now we want to calculate the Wightman function in the L-region. Taking the respective wave function solutions (17) into (20), we get W(x, x′) = q(ww′)D/2 (2π)D−2aD−1w2 0 X σ λpν,iq (pν,i/w0)2 + λ2 + ⃗k2 Tν(pν,i)Jν(pν,iw/w0)Jν(pν,iw′/w0) × Jq|n+α|(λr)Jq|n+α|(λr′)einq∆ϕ+i⃗k·∆⃗ x∥−iE∆t , (37) with the notation X σ = Z ∞ 0 dλ ∞X i=1 X n Z d⃗k . (38) Once again making a Wick rotation on the time coordinate and using the identity (23) with the energy in the L-region given by E = q λ2 + ⃗k2 + (pν,i/w0)2, we can integrate over λ and ⃗k, obtaining the following result: W(x, x′) = qrr′ (2π)D/2aD−1w2 0 \u0012ww′ rr′ \u0013D/2 Z ∞ 0 dχχ D 2 −2e− r2+r′2+∆⃗ x2 ∥−∆t2 2rr′ χ X n einq∆ϕIq|n+α|(χ) × ∞X i=1 pν,ie − rr′ 2χw2 0 p2 ν,i Tν(pν,i)Jν(pν,iw/w0)Jν(pν,iw′/w0) , (39) where we have introduced the variable χ = rr′/(2s2). Now, writing the parameter α as in (29) once again and using the formula (30) for the summation over n, we can perform the integration 8over χ using the formula (32). Following these steps, we get the expression W(x, x′) = 2(ww′)D/2 (2π)D/2aD−1w D 2 +1 0 (X k eiα(2πk−q∆ϕ) u D 2 −1 k ∞X i=1 pD/2 ν,i Tν(pν,i)Jν(pν,iw/w0)Jν(pν,iw′/w0) × KD 2 −1(ukpν,i/w0) − qe−iqn0∆ϕ 2πi X j=±1 jejiπq|α0| × Z ∞ 0 dy cosh [qy(1 − |α0|)] − cosh (|α0|qy)e−iq(∆ϕ+jπ) u D 2 −1 y \u0002 cosh (qy) − cos (q(∆ϕ + jπ)) \u0003 × ∞X i=1 pD/2 ν,i Tν(pν,i)Jν(pν,iw/w0)Jν(pν,iw′/w0)KD 2 −1(uypν,i/w0) ) , (40) where the variable uk and uy are defined in (34). In order to develop the summation over i, we will use the generalized Abel-Plana summation formula [37] ∞X i=1 Tν(pν,i)f(pν,i) = 1 2 Z ∞ 0 dzf (z) − 1 2π Z ∞ 0 dz ¯Kν(z) ¯Iν(z) \u0002 e−iνz f(iz) + eiνz f(−iz) \u0003 . (41) For the problem under consideration, the function f(z) is f(z) = zD/2Jν(zw/w0)Jν(zw′/w0)KD 2 −1(2uz/w0) . (42) The first term in (41) will provide the Wightman function induced by the cosmic string in the absence of brane, while the second one is induced by the boundary. For the same argument given in the previous subsection, we concentrate on the brane-induced term. Therefore, after a few intermediate steps, we find Wb(x, x′) = − (ww′)D/2 (2π)D/2aD−1 Z ∞ 0 dvvD/2 ¯Kν(vw0) ¯Iν(vw0) Iν(vw)Iν(vw′) × (X k eiα(2πk−q∆ϕ)fD 2 −1(ukv) − qe−iqn0∆ϕ 2πi X j=±1 jejiπq|α0| × Z ∞ 0 dy cosh [qy(1 − |α0|)] − cosh (|α0|qy)e−iq(∆ϕ+jπ) cosh (qy) − cos (q(∆ϕ + jπ)) fD 2 −1(uyv) ) , (43) where we have made change of variable z = vw0. 4 Field Squared The VEV of the field squared is formally obtained from the Wightman function by taking the coincidence limit, as shown below: ⟨|φ(x)|2⟩ = lim x′→x W(x, x′) , (44) where the notation |φ(x)|2 is understood here as φ(x)φ†(x). As we will see in what follows, in both regions, R and L, the VEV of the field squared can be decomposed in the form: ⟨|φ(x)|2⟩b = ⟨|φ(x)|2⟩(0) b + ⟨|φ(x)|2⟩(q,α0) b , (45) 9where the first term in the right-hand side comes from the term k = 0 in the sum over k in the Wightman functions for both regions, (35) and (43), and it is purely induced by the brane and was analyzed in [38], while the second term is a new contribution induced by the cosmic string and the magnetic flux along its core in the AdS geometry with a flat boundary. In this paper we are mainly interested in the investigation of the latter contribution and henceforth our study is restricted to it. 4.1 R-region Taking the positive frequency Wightman function in the R-region, Eq. (35), into the equation above and subsequently taking the coincidence limit, the second term in (45) reads ⟨|φ(x)|2⟩(q,α0) b = − 2wD (2π)D/2aD−1 Z ∞ 0 dppD−1 ¯Iν(pw0) ¯Kν(pw0)K2 ν (pw) \"[q/2]X′ k=1 cos(2πkα0)fD 2 −1(2rpsk) − q 2π Z ∞ 0 dy h(q, α0, y) cosh (qy) − cos (qπ)fD 2 −1(2rp cosh(y/2)) # , (46) where [q/2] represents the integer part of q/2, and the prime on the sign of the summation over k means that for even values of q, the term k = q/2 should be taken with the coefficient 1 /2. Moreover, henceforth we adopt the notation sk = sin(πk/q) . (47) As to the function h(q, α0, y), it reads h(q, α0, y) = cosh(qy(1 − |α0|)) sin(qπ|α0|) + cosh(qy|α0|) sin(qπ(1 − |α0|)) . (48) Moreover, note that by making a simple change of variable, z = pw0, this VEV depends on the ratio r/w, which is related to the proper distance from the string, and the ratio w/w0, which is related to the proper distance from the brane w/w0 = e(y−y0)/a . (49) This feature is also present in the VEV of the squared field in the L-region and the components of the energy-momentum tensor as we will see below. Let us investigate some special and asymptotic cases for the formula above. For a massless conformal scalar quantum field, we have ν = 1/2, according to (9). Thus, using the correspond- ing modified Bessel functions for this order, we get ⟨|φ(x)|2⟩(q,α0) b = − 2wD−1 (2π)D/2aD−1 Z ∞ 0 dppD−2e−(2w−w0)p (2A0 − B0) sinh(pw0) + 2B0pw0 cosh(pw0) 2A0 − B0(1 + 2pw0) × \"[q/2]X′ k=1 cos(2πkα0)fD 2 −1(2rpsk) − q 2π Z ∞ 0 dy h(q, α0, y) cosh (qy) − cos (qπ) × fD 2 −1(2rp cosh(y/2)) # . (50) Note that this result contrasts with the one for the pure brane-induced term, ⟨|φ|2⟩(0) b which is zero for a conformal massless quantum scalar field [38]. 10For points with the proper distance from the plate much larger compared with the AdS radius, one has w/w0 ≫ 1. Introducing a new variable u = pw and by making use of the formulae for the modified Bessel functions for small values of the argument [32], with the assumption that A0 − νB0 ̸= 0, to the leading order, we get ⟨|φ(x)|2⟩(q,α0) b ≈ − 22−2ν−D/2 πD/2Γ(ν)Γ(ν + 1)aD−1 \u0012A0 + νB0 A0 − νB0 \u0013\u0010w0 w \u00112ν Z ∞ 0 duuD+2ν−1K2 ν (u) × \"[q/2]X′ k=1 cos(2πkα0)fD 2 −1(2rusk/w) − q 2π Z ∞ 0 dy h(q, α0, y) cosh (qy) − cos (qπ) × fD 2 −1(2ru cosh(y/2)/w) # . (51) Finally, we analyze the Minkowskian limit,a → ∞, with fixed coordinate y. In this limit, the geometry under consideration is reduced to the geometry of a cosmic string in the background of a ( D + 1)-dimensional Minkowski spacetime. It can be observed that the coordinate w in the arguments of the modified Bessel functions in this limit can be expressed as w ≈ a + y. Considering that ν ≫ 1, it can be seen that as we approach the Minkowskian limit, the order and the argument of the modified Bessel functions in equation (46) tend to become large. Hence, we can make use of the corresponding uniform asymptotic expansions. Thus, for the leading order, one gets ⟨|φ(x)|2⟩(q,α0),(M) b = − 1 (2π)D/2 Z ∞ m du(u2 − m2) D 2 −1 \"[q/2]X′ k=1 cos(2πkα0) × fD 2 −1(2rsk p u2 − m2) − q 2π Z ∞ 0 dy h(q, α0, y) cosh (qy) − cos (qπ) × fD 2 −1(2r cosh(y/2) p u2 − m2) # 1 + βu 1 − βu e−2u(y−y0) . (52) In Fig. 1 we exhibit the dependence of the field squared, ⟨|φ|2⟩(q,α0) b , (left panel) and the ra- tio ⟨|φ|2⟩(q,α0) b /⟨|φ|2⟩(0) b (right panel) as functions of w/w0 for Dirichlet and Neumann boundary conditions with different values of the deficit angle parameter, q. As we can see from the left panel the VEV of the field squared, induced by the cosmic string only, is finite on the brane and goes to zero for large distances from the brane with ( w0/w)2ν according to the corresponding asymptotic formula (51). The right panel shows that the pure brane-induced contribution is dominant near the brane, while the string-induced term is more relevant for distant points from the brane. Moreover, note that the intensities of this VEV increase with the string parameter, q, and are higher for the Neumann BC. 4.2 L-region Now substituting the Wightman function in the L-region (43) into the formal expression for the field squared (44) and taking the coincidence limit, the second term in the right hand side of 111 2 3 4 5 6 7 8 9 10 w/w0 0.5 0.4 0.3 0.2 0.1 0.0 a2 2 (q, 0) b Dirichlet BC, q = 1.5 Dirichlet BC, q = 2.5 Neumann BC, q = 1.5 Neumann BC, q = 2.5 1 2 3 4 5 6 7 8 9 10 w/w0 4 3 2 1 0 2 (q, 0) b / 2 (0) b Dirichlet BC, q = 1.5 Dirichlet BC, q = 2.5 Neumann BC, q = 1.5 Neumann BC, q = 2.5 Figure 1: The VEV of the field squared ⟨|φ|2⟩(q,α0) b (left panel) and the ratio ⟨|φ|2⟩(q,α0) b /⟨|φ|2⟩(0) b (right panel) are exhibited as functions of w/w0 for different values of q, considering Dirichlet and Neumann boundary conditions. The graphs are plotted for a minimally coupled massless scalar field in D = 3 with fixed parameters r/w0 = 0.5 and α0 = 0.4. (45) reads: ⟨|φ(x)|2⟩(q,α0) b = − 2wD (2π)D/2aD−1 Z ∞ 0 dvvD−1 ¯Kν(vw0) ¯Iν(vw0) I2 ν (vw) \"[q/2]X′ k=0 cos(2πkα0)fD 2 −1(2rvsk) − q 2π Z ∞ 0 dy h(q, α0, y) cosh (qy) − cos (qπ)fD 2 −1(2rv cosh(y/2)) # . (53) Comparing with (46), we see that the brane-induced contribution in the L-region is obtained from the corresponding quantity for the R-region by the replacements I → K, K → I of the modified Bessel functions. Note that an important result which can be observed from the expression above and Eq. (46) is that the VEVs of the field squared in both regions are finite on the brane. The corresponding values can be obtained directly from (46) and (53) by puttingw = w0. This finiteness of the field squared is in clear contrast to the behaviour of the pure brane-induced contribution,⟨|φ(x)|2⟩(0) b , which diverges on the brane as 1 /(w − w0)D−1[38]. As we have proceeded in R-region, let us now analyze some special and limiting cases of the squared field given above. For a massless conformal scalar quantum field in this region, we have ⟨|φ(x)|2⟩(q,α0) b = − 2wD−1 (2π)D/2aD−1 Z ∞ 0 dppD−2e−pw0 [2A0 − B0(1 + 2pw0)] sinh2(pw) (2A0 − B0) sinh(pw0) + 2B0pw0 cosh(pw0) × \"[q/2]X′ k=0 cos(2πkα0)fD 2 −1(2rpsk) − q 2π Z ∞ 0 dy h(q, α0, y) cosh (qy) − cos (qπ) × fD 2 −1(2rp cosh(y/2)) # . (54) We now analyse the asymptotic behaviour for points near the AdS boundary (hyperplane at w = 0), w ≪ w0, with the proper distances from the plane much larger compared with the AdS curvature radius. Thus, introducing the variable u = vw0 in (53) and by using the formulae for 12the modified Bessel function for small values of the argument, to the leading order, we obtain ⟨|φ(x)|2⟩(q,α0) b ≈ − 21−2ν−D/2 πD/2Γ2(ν + 1)aD−1 \u0012 w w0 \u0013D+2ν+2 Z ∞ 0 duuD+2ν−1 ¯Kν(u) ¯Iν(u) \"[q/2]X′ k=0 cos(2πkα0) × fD 2 −1(2rusk/w0) − q 2π Z ∞ 0 dy h(q, α0, y) cosh (qy) − cos (qπ) × fD 2 −1(2ru cosh(y/2)/w0) # . (55) In the Minkowskian limit, we follow the same procedure as in the R-region. The correspond- ing result reads, ⟨|φ(x)|2⟩(q,α0),(M) b = − 1 (2π)D/2 Z ∞ m du(u2 − m2) D 2 −1 \"[q/2]X′ k=0 cos(2πkα0) × fD 2 −1(2rsk p u2 − m2) − q 2π Z ∞ 0 dy h(q, α0, y) cosh (qy) − cos (qπ) × fD 2 −1(2r cosh(y/2) p u2 − m2) # 1 + βu 1 − βu e−2u(y0−y) . (56) It worths to note that the expression above is similar to that of the R-region with y−y0 replaced by y0 − y. This similarity is expected since in the Minkowskian limit the VEV is symmetric to the brane. The left panel in Fig. 2 shows the behaviour of the field squared and right panel the ratio ⟨|φ(x)|2⟩(q,α0) b /⟨|φ(x)|2⟩(0) b as functions of w/w0 for Neumann and Dirichlet boundary conditions and distinct values of q. In both plots we have fixed r/w0 = 0.5 and α0 = 0.4. From the left panel we can see that the field squared in this region goes zero on the AdS boundary, which is in accordance with the asymptotic formula (55), and it is finite on the brane. Moreover, the right panel shows us that the string-induced contribution is dominant near the AdS boundary for the curves with q = 2.5, while the pure brane-induced term is more relevant close to the brane for any value of q. Note that similar to the L-region, this VEV increases with the string parameter, q, but differently from the R-region it is higher for Dirichlet BC. 0.0 0.2 0.4 0.6 0.8 1.0 w/w0 0.04 0.02 0.00 0.02 0.04 0.06 0.08 0.10 0.12 a2 2 (q, 0) b Dirichlet BC, q = 1.5 Dirichlet BC, q = 2.5 Neumann BC, q = 1.5 Neumann BC, q = 2.5 0.0 0.2 0.4 0.6 0.8 1.0 w/w0 1 0 1 2 3 2 (q, 0) b / 2 (0) b Dirichlet BC, q = 1.5 Dirichlet BC, q = 2.5 Neumann BC, q = 1.5 Neumann BC, q = 2.5 Figure 2: The same as in Fig.1 for the R-region. 135 Energy-Momentum Tensor Having obtained the Wightman function and the mean field square, we can proceed to the calculation of the vacuum expectation value of the energy–momentum tensor by making use of the formula developed in [26]: ⟨Tµν⟩ = lim x′→x (DµD† ν′ + D† µ′Dν)W(x, x′) − 2[ξRµν + ξ∇µ∇ν − (ξ − 1/4)gµν∇α∇α]]⟨|φ|2⟩, (57) where Rµν = −Dgµν/a2 is the Ricci tensor for the AdS space-time and Dµ = ∇µ + ieAµ.1 Similarly to the VEV of the field squared, the VEV of the energy–momentum tensor can be decomposed as ⟨Tµν⟩b = ⟨Tµν⟩(0) b + ⟨Tµν⟩(q,α0) b . (58) As already stressed in beginning of the previous section, here the first term in the right-hand side is also purely induced by the brane, already analyzed in [38]. Thus, the analysis below concerns only the second term, which is a new contribution induced by the string and the magnetic flux along its core in the AdS background with a flat boundary. 5.1 R-region Let us start with the R-region, walking through the most important steps of the calculation. The covariant d’Alembertian acting on the squared field in the R-region, Eq. (46), gives □⟨|φ|2⟩b = 4wD+2 (2π)D/2aD+1 Z ∞ 0 dppD+1 ¯Iν(pw0) ¯Kν(pw0) \"[q/2]X′ k=0 cos(2πkα0)S[sk, fD 2 (2rpsk), Kν(pw)] − q 2π Z ∞ 0 dy h(q, α0, y) cosh (qy) − cos (qπ)S[cosh(y/2), fD 2 (2rp cosh(y/2)), Kν(pw)] # , (59) where we have introduced the function S[γ, fµ(x), g(y)] = 2 γ2 \u0002 x2fµ+1(x) − 2fµ(x) \u0003 g2(y) + fµ−1(x) \u0014 (g′(y))2 + D y g′(y)g(y) + \u0012 1 + ν2 y2 \u0013 g2(y) \u0015 , (60) with γ = sk, cosh(y/2). The function fµ(x) is already defined in (36). For the geometry under consideration, only the ∇r∇w and ∇µ∇µ differential operators con- tribute when acting on the VEV of the field squared. The remaining contributions come from the electromagnetic covariant derivatives acting on the Wightman function. As to the azimuthal term, it is more convenient to act the DϕD† ϕ′ operator in (28) for the R-region and (39) for the L-region, and subsequently take the coincidence limit in all coordinates, including the angular one. Following this procedure, we obtain: I(q, α, χ) = ∞X n=−∞ q2(n + α)2Iq|n+α|(χ) , (61) 1As it was explained in [26], the second part on the right-hand side of the energy-momentum tensor comes from two distinct contributions: the one proportional to the non-minimum coupling, ξ, is a consequence of the variation of the Ricci scalar with respect to the metric tensor; and the second one is obtained through some algebraic manipulations and use of the equation of motion associated with the scalar field. 14where χ = rr′/2s2. This sum can be developed by using the differential equation obeyed by the modified Bessel function. Then we get, I(q, α, χ) = \u0012 χ2 d2 dχ2 + χ d dχ − χ2 \u0013 ∞X n=−∞ Iq|n+α|(χ) , (62) where this last sum is given by [39] ∞X n=−∞ Iq|n+α|(χ) = 2 q [q/2]X′ k=0 cos(2πkα0)eχ cos(2πk/q) − 1 π Z ∞ 0 dy e−χ cosh(y)f(q, α0, y) cosh(qy) − cos(qπ) . (63) The brane induced contribution in the VEV of the energy-momentum tensor is calculated by making use of the corresponding parts in the Wightman function and VEV of the field squared. After long but straightforward steps, we get (no summation over µ) ⟨Tµ µ ⟩(q,α0) b = − 2wD+2 (2π) D 2 aD+1 Z ∞ 0 dppD+1 ¯Iν(pw0) ¯Kν(pw0) \"[q/2]X′ k=1 cos(2πkα0)Gµ µ[sk, fD 2 (2rpsk), Kν(pw)] − q 2π Z ∞ 0 dy h(q, α0, y) cosh(qy) − cos(qπ)Gµ µ[cosh(y/2), fD 2 (2rp cosh(y/2)), Kν(pw)] # , (64) with the functions G0 0[γ, fµ(x), g(y)] = −2fµ(x)g2(y) − ξ1 ( 2γ2 \u0002 x2fµ+1(x) − 2fµ(x) \u0003 g2(y) + fµ−1(x) \u0014 (g′(y))2 + D + 4ξ/ξ1 y g′(y)g(y) + \u0012 1 + ν2 y2 \u0013 g2(y) \u0015) , G1 1[γ, fµ(x), g(y)] = 2(4 ξγ2 − 1)fµ(x)g2(y) − ξ1fµ−1(x) \" (g′(y))2 + D + 4ξ/ξ1 y g′(y)g(y) + \u0012 1 + ν2 y2 \u0013 g2(y) # , G2 2[γ, fµ(x), g(y)] = −2(4ξγ2 − 1)[x2fµ+1(x) − fµ(x)]g2(y) − ξ1fµ−1(x) \" (g′(y))2 + D + 4ξ/ξ1 y g′(y)g(y) + \u0012 1 + ν2 y2 \u0013 g2(y) # , G3 3[γ, fµ(x), g(y)] = −2ξ1γ2 \u0002 x2fµ+1(x) − 2fµ(x) \u0003 g2(y) + fµ−1(x) \" −(g′(y))2 + ξ1D y g′(y)g(y) + \u0012 1 + 2m2a2 − ν2 y2 \u0013 g2(y) # ,(65) where ξ1 = 4ξ − 1. For the components µ = 4, ..., D, associated to the extra dimensions, we have (no summation over µ) ⟨Tµ µ ⟩b = ⟨T0 0 ⟩b, as a consequence of the symmetry of the problem. Additionally, we have an off-diagonal component2: G1 3[γ, fµ(x), g(y)] = −γxfµ(x) \u0014Dξ1 + 4ξ y g2(y) + 2ξ1g(y)g′(y) \u0015 . (66) 2Note that although the spacetime line element given by (2) is diagonal, we have the appearance of an off- diagonal component in the scalar field energy-momentum tensor, which is a common feature of similar setups in anti-de Sitter and de Sitter spacetimes with a cosmic string [26, 40]. 15Let us now study some limiting cases for the energy density component, ⟨T0 0 ⟩(q,α0) b . For a massless conformal quantum field, we haveν = 1/2, and using the corresponding modified Bessel functions, the energy density is given by ⟨T0 0 ⟩(q,α0) b = − 4wD+1 (2π) D 2 aD+1D Z ∞ 0 dppDe−(2w−w0)p (2A0 − B0) sinh(pw0) + 2B0pw0 cosh(pw0) 2A0 − B0(1 + 2pw0) × ([q/2]X′ k=1 cos(2πkα0) \" s2 k \u0010 (2rpsk)2fD 2 +1(2rpsk) − 2fD 2 (2rpsk) \u0011 − DfD 2 (2rpsk) + fD 2 −1(2rpsk) # − q 2π Z ∞ 0 dy h(q, α0, y) cosh(qy) − cos(qπ) \" cosh2(y/2) \u0010 (2rp cosh(y/2))2 × fD 2 +1(2rp cosh(y/2)) − 2fD 2 (2rp cosh(y/2)) \u0011 − DfD 2 (2rp cosh(y/2)) + fD 2 −1(2rp cosh(y/2)) #) . (67) Note that in contrast to the vanishing pure brane-induced energy density in the R-region an- alyzed in [38], the corresponding contribution resulting from the presence of the cosmic string and its magnetic flux along its core given by the expression above is non-zero. For distant points from the brane, w/w0 ≫ 1, we introduce a new variable u = pw in (64) for the energy density component and by making use of the formulae for the modified Bessel functions for small values of the argument [32], with the assumption that A0 − νB0 ̸= 0, to the leading order, we get ⟨T0 0 ⟩(q,α0) b ≈ − 22−2ν−D/2 π D 2 Γ(ν)Γ(ν + 1)aD+1 \u0010w0 w \u00112ν Z ∞ 0 dpp2ν+D+1 \u0012A0 + νB0 A0 − νB0 \u0013 × \"[q/2]X′ k=1 cos(2πkα0)G0 0[sk, fD 2 (2rpsk/w), Kν(p)] − q 2π Z ∞ 0 dy h(q, α0, y) cosh(qy) − cos(qπ) × G0 0[cosh(y/2), fD 2 (2rp cosh(y/2)/w), Kν(p)] # . (68) In the Minkowskian limit, a → ∞, the energy density reads, ⟨T0 0 ⟩(q,α0),(M) b = 2 (2π) D 2 Z ∞ m du(u2 − m2) D 2 −1 ([q/2]X′ k=1 cos(2πkα0) \" u2 \u0010 ξ1fD 2 −1(2rsk p u2 − m2) + fD 2 (2rsk p u2 − m2) \u0011 − m2fD 2 (2rsk p u2 − m2) # − q 2π Z ∞ 0 dy h(q, α0, y) cosh(qy) − cos(qπ) \" u2 \u0010 ξ1fD 2 −1(2r cosh(y/2) p u2 − m2) + fD 2 (2r cosh(y/2) p u2 − m2) \u0011 − m2fD 2 (2r cosh(y/2) p u2 − m2) #) × 1 + βu 1 − βu e−2u(y−y0) . (69) In Fig. 3 the energy density induced by the string and its magnetic flux, ⟨T0 0 ⟩(q,α0) b (left panel) and the ratio ⟨T0 0 ⟩(q,α0) b /⟨T0 0 ⟩(0) b (right panel) are displayed as functions of w/w0. Both 16graphs are plotted for a minimally coupled massless scalar field in D = 3 with fixed parameters r/w0 = 0.5 and α0 = 0.4. The curves exhibited correspond to different values of q, considering Dirichlet (β = 0) and Neumann ( β → ∞) boundary conditions. The left panel shows us that the intensity of the energy density increases with the parameter q associated with the string’s angle deficit. Moreover, the energy density goes to zero for large distances from the brane, and according to our asymptotic analysis behaves as ( w0/w)2ν, and it is finite on the brane. We can also observe that in the region under consideration the energy density is positive for Neumann BC and negative for Dirichlet BC and that the intensities of the VEVs are higher for Neumann BC by comparing curves with same q. On the other hand, from the right panel we can read that the contribution induced by the string and its magnetic flux, ⟨T0 0 ⟩(q,α0) b , is negligible compared with the pure brane-induced one, ⟨T0 0 ⟩(0) b , for points close to the brane. On the other hand, for distant points from the brane the former dominates in the total VEV,⟨T0 0 ⟩b. 1 2 3 4 5 6 7 8 9 10 w/w0 0.1 0.0 0.1 0.2 0.3 0.4 0.5 a4 T0 0 (q, 0) b Dirichlet BC, q = 1.5 Dirichlet BC, q = 2.5 Neumann BC, q = 1.5 Neumann BC, q = 2.5 1 2 3 4 5 6 7 8 9 10 w/w0 0 10 20 30 40 50 T0 0 (q, 0) b / T0 0 (0) b Dirichlet BC, q = 1.5 Dirichlet BC, q = 2.5 Neumann BC, q = 1.5 Neumann BC, q = 2.5 Figure 3: The behaviour of the energy density as function ofw/w0 is shown for different values of q and for Dirichlet and Neumann boundary conditions (left panel). The right panel exhibits the ratio between the contribution⟨T0 0 ⟩(q,α0) b and ⟨T0 0 ⟩(0) b also as a function ofw/w0 for distinct values q and Dirichlet and Neumann boundary conditions. The graphs are plotted for a minimally coupled massless scalar field with fixed parameters r/w0 = 0.5 and α0 = 0.4. 5.2 L-region Following the same procedure as in the R-region, the energy-momentum tensor in the L-region reads, ⟨Tµ µ ⟩(q,α0) b = − 2wD+2 (2π) D 2 aD+1 Z ∞ 0 dppD+1 ¯Kν(pw0) ¯Iν(pw0) \"[q/2]X′ k=1 cos(2πkα0)Gµ µ[sk, fD 2 (2rpsk), Iν(pw)] − q 2π Z ∞ 0 dy h(q, α0, y) cosh(qy) − cos(qπ)Gµ µ[cosh(y/2), fD 2 (2rp cosh(y/2)), Iν(pw)] # , (70) with the functions Gµ µ already defined in (65). For the components µ = 4, ..., D, associated to the extra dimensions, in the L-region we also have (no summation overµ) ⟨Tµ µ ⟩b = ⟨T0 0 ⟩b and the off-diagonal component reads the same as in (66). Moreover, note that the energy-momentum tensor in the L-region is obtained from (64) with the replacements I → K and K → I of the modified Bessel functions. 17For a massless conformal quantum field, we have ν = 1 /2, and using the corresponding modified Bessel functions, the energy density is given by ⟨T0 0 ⟩(q,α0) b = − 2wD+1 (2π) D 2 aD+1D Z ∞ 0 dppDe−pw0 2A0 − B0(1 + 2pw0) (2A0 − B0) sinh(pw0) + 2B0pw0 cosh(pw0) × ([q/2]X′ k=1 cos(2πkα0) \"h 2s2 k \u0010 (2rpsk)2fD 2 +1(2rpsk) − 2fD 2 (2rpsk) \u0011 − 2DfD 2 (2rpsk) i × sinh2(pw) + fD 2 −1(2rpsk)[2 cosh2(pw) − 1] # − q 2π Z ∞ 0 dy h(q, α0, y) cosh(qy) − cos(qπ) × \"h 2 cosh2(y/2) \u0010 (2rp cosh(y/2))2fD 2 +1(2rp cosh(y/2)) − 2fD 2 (2rpsk) \u0011 − 2DfD 2 (2rp cosh(y/2)) i sinh2(pw) + fD 2 −1(2rp cosh(y/2))[2 cosh2(pw) − 1] #) .(71) For points near the AdS boundary, w ≪ w0, we introduce the variable u = vw0 in (70) for the energy density component and by using the corresponding expressions for the modified Bessel functions for small values of the argument, to the leading order, we get ⟨T0 0 ⟩(q,α0) b ≈ −21−2ν−D/2 π D 2 aD+1 \u0012 w w0 \u0013D+2ν D + 2ν − 4ξ(D + 2ν + 1) Γ(ν)Γ(ν + 1) Z ∞ 0 dppD+2ν−1 ¯Kν(pw0) ¯Iν(pw0) × \"[q/2]X′ k=1 cos(2πkα0)fD 2 −1(2rpsk) − q 2π Z ∞ 0 dy h(q, α0, y) cosh(qy) − cos(qπ) × fD 2 −1(2rp cosh(y/2)) # . (72) In the Minkowskian limit, a → ∞, the energy density reads, ⟨T0 0 ⟩(q,α0),(M) b = 2 (2π) D 2 Z ∞ m du(u2 − m2) D 2 −1 ([q/2]X′ k=1 cos(2πkα0) \" u2 \u0010 ξ1fD 2 −1(2rsk p u2 − m2) + fD 2 (2rsk p u2 − m2) \u0011 − m2fD 2 (2rsk p u2 − m2) # − q 2π Z ∞ 0 dy h(q, α0, y) cosh(qy) − cos(qπ) \" u2 \u0010 ξ1fD 2 −1(2r cosh(y/2) p u2 − m2) + fD 2 (2r cosh(y/2) p u2 − m2) \u0011 − m2fD 2 (2r cosh(y/2) p u2 − m2) #) × 1 + βu 1 − βu e−2u(y0−y) . (73) It is noteworthy that the energy density, similar to the VEV of the squared field, is also similar to that of the R-region with y − y0 replaced by y0 − y, which is also expected since in the Minkowskian limit the VEV is symmetric to the brane. In Fig. 4 we present two plots showing the energy density induced in the presence of the cosmic string and its magnetic flux (left panel) and the ratio ⟨T0 0 ⟩(q,α0) b /⟨T0 0 ⟩(0) b (right panel) 18as functions of w/w0 for different values of the string parameter, q, considering Dirichlet and Neumann boundary conditions. From both plots we can observe an inversion of the behaviour found in the R-region; the energy density is negative for Neumann BC and positive for Dirichlet BC, being the intensities higher for the latter by comparing the curves with same value of q. Moreover, we can see that in this region the energy density goes to zero near the AdS boundary with (w/w0)D+2ν, according to the corresponding asymptotic expression (72), and it is finite on the brane. From the right panel we can read that the pure brane-induced contribution dominates in the total VEV for point close to the brane. On the other side, for points close to the AdS boundary the contribution induced by the string and its magnetic flux dominates in the total VEV, depending on the deficit angle parameter, q. 0.0 0.2 0.4 0.6 0.8 1.0 w/w0 0.0 0.1 0.2 0.3 a4 T0 0 (q, 0) b Dirichlet BC, q = 1.5 Dirichlet BC, q = 2.5 Neumann BC, q = 1.5 Neumann BC, q = 2.5 0.0 0.2 0.4 0.6 0.8 1.0 w/w0 2.5 2.0 1.5 1.0 0.5 0.0 T0 0 (q, 0) b / T0 0 (0) b Dirichlet BC, q = 1.5 Dirichlet BC, q = 2.5 Neumann BC, q = 1.5 Neumann BC, q = 2.5 Figure 4: The energy density ⟨T0 0 ⟩(q,α0) b the ratio ⟨T0 0 ⟩(q,α0) b /⟨T0 0 ⟩(0) b are plotted as functions of w/w0 for different values q and Dirichlet and Neumann boundary conditions. The graphs are plotted for a minimally coupled massless scalar field in D = 3 with fixed parameters r/w0 = 0.5 and α0 = 0.4. 6 Application to Randall-Sundrum model The results given in the previous section can be applied to the investigation of the cosmic string induced effects in Z2-symmetric braneworlds models. Specifically, they can be applied to the Randall–Sundrum model with a single brane (RSII) [41, 42]. In the RSII model, the universe is perceived as a Z2-symmetric positive tension brane in 5-dimensional AdS spacetime and the negative cosmological constant in the bulk is the only contribution to the curvature. Never- theless, the majority of scenarios that are motivated by string theories postulate the existence of additional bulk fields, including scalar fields. For the setup under consideration, the cosmic string is perpendicular to the brane and the corresponding background contains two copies of the R-region that are identified by the Z2-symmetry (y − y0 ←→ y0 − y) with respect to the brane located at y = y0. The corresponding line element is given by ds2 = e−2|y−y0|/a \" dt2 − dr2 − r2dϕ2 − dz2 − DX i=4 (dxi)2 # − dy2 , (74) where, as before, −∞ < y <∞ and 0 ≤ ϕ ≤ 2π/q. It should be noted that for an observer located at y = y0, the line element above is trivially reduced to the standard line element that characterizes a cosmic string in (D+1)-dimensional flat spacetime. The boundary conditions on the bulk field are imposed at the location of the brane and obtained by integration of the field 19equations about y = y0. By following a similar procedure to that used in [38, 43, 44] for a single brane model it can be seen that for fields even under the reflection with respect to the brane (untwisted scalar field) the boundary condition is of the Robin type (5) with the coefficient β = − 1 cb + 4Dξ/a , (75) where cb is the brane mass term and it comes from the part of the action located on the brane, Sb = −cb R dDxdy p |g|δ(y − y0)φ2/2. In the particular case of a minimally coupled field and cb = 0, the boundary condition is reduced to the Neumann one ( β → ∞). On the other hand, for fields odd under reflection (twisted scalar fields), the boundary condition takes the Dirichlet form. In the Z2-symmetric models the region of integration over the coordinate y ranges from −∞ to +∞ and it results in the appearance of an additional 1/2 factor in the normalization coefficient when compared to the one we have obtained previously for the R-region, which has the interval 0 ≤ y <∞ (with y0 = 0). Therefore, the formulas for the VEVs of the field squared and the energy-momentum tensor induced by a cosmic string in the generalized RSII model are obtained from those expressions presented in the previous sections by putting w0 = a with an additional factor of 1/2. 7 Conclusions In this paper we have investigated the vacuum polarization effects induced by a cosmic string carrying a magnetic flux along its core in the background of (D + 1)-dimensional AdS spacetime with a planar brane parallel to the string core, which divides the background in two regions. In order to obtain the bosonic normal modes in this setup in both regions we have solved the gauge invariant Klein-Gordon equation with a curvature coupling and assumed that on the brane the field operator obeys the Robin boundary condition. Having obtained the positive and negative energy modes in each regions, we have constructed the Wightman function in closed form for R-region (35) and L-region (43). By directly taking the coincidence limit in the Wightman function, in the section 4 we have calculated the VEV of the field squared induced by the string and its magnetic flux, ⟨|φ|2⟩(q,α0) b , for both regions. In the R-region the VEV of the squared field is given by Eq. (46). In Eq. (50) we have presented this quantity for a conformal massless quantum scalar. In the asymptotic limit of large distances from the brane, w/w0 ≫ 1, the string-induced VEV of the field squared decays as ( w0/w)2ν and it is given by (51). The Minkowskian limit has been also analyzed and the corresponding expression is givem in Eq. (52). We also have shown that ⟨|φ|2⟩(q,α0) b is finite on the boundary; moreover, comparing this quantity with the corresponding VEV induced by the brane only, ⟨|φ|2⟩0 b, we have noticed by the graph exhibited in the right panel of Fig. 1 that near the brane the total VEV of the field squared is dominated by the latter; however, for points more distant from the brane the string-induced field squared becomes more relevant. In fact the pure brane-induced VEV of the field squared is divergent on the brane [38]. In the L-region the VEV of the field squared is given in (53). Its conformal massless scalar field case is presented in (54). For points close to the AdS boundary the asymptotic expression is given (55) and it is shown that this quantity decreases with ( w/w0)D+2ν+2. The Minkowskian limit of string-induced VEV of the field squared was also studied in this region and it is similar to that one for the R-region with y−y0 replaced by y0 −y. In this region, we also analyzed numerically the ratio between the VEV of the field squared induced by the cosmic string and the corresponding quantity induced by the brane, ⟨|φ|2⟩(q,α0) b /⟨|φ|2⟩(0) b . This result is displayed in Fig. 2. There we observe in the right panel, that the pure brane-induced VEV of the field squared is dominant in the total VEV of 20the field squared, while for points distant from the brane the most relevant contribution comes from string-induced part. Also in the L-region, the pure brane-induced VEV of the field squared is divergent on the brane. In the section 5 we have presented our results for the VEV of the energy-momentum tensor, analysing only the cosmic string induced term, ⟨Tµν⟩(q,α0) b , which is a new contribution. The corresponding expression for the R-region is given in (64). All the diagonal components are nonzero and an off-diagonal component is also present. We have studies the energy density component for some limiting cases. The expression for a conformal massless scalar field case is given in (67) is nonzero and it is in clear contrast with the pure brane-induced term, which is zero in this particular case. For distant points of the brane this VEV decreases as ( w0/w)2ν and is presented in (68). Moreover, the Minkowskian limit is also analyzed and the corresponding expression given by (69). In the left panel of Fig. 3 we display the behavior of the string- induced energy density, ⟨T0 0 ⟩(q,α0) b , as function of w/w0. We show that this quantity is finite on the brane and reinforce its decay for points distant from the brane; moreover, in the right panel, we exhibit the behavior of the ratio ⟨T0 0 ⟩(q,α0) b /⟨T0 0 ⟩(0) b . We observe that the brane-induced VEV of the energy density is more intense than the string-induced one near the brane. However the situation changes in the opposite situation. In fact the brane-induced energy density is divergent on the brane [38]. We also have analyzed the VEV of the energy-momentum tensor in the L-region (70), which is obtained from the one for the R-region with the replacements I → K and K → I of the modified Bessel functions. For a massless scalar field the energy density component is given in (71). For point near the AdS boundary the energy density goes to zero as ( w/w0)D+2ν as is shown in (72). Furthermore, in the Minkowskian limit the energy density (73) is similar to the corresponding VEV for the R-region with y −y0 replaced by y0 −y. In this region, we also have evaluated numerically the behavior of the string-induced VEV of the energy density in the left panel of Fig. 4. We show that it is finite on the brane. On the right panel, we plotted the behavior of the ratio ⟨T0 0 ⟩(q,α0) b /⟨T0 0 ⟩(0) b . Again, we can notice that near the brane the total energy density is dominated by the brane-induced part; however for points far from the brane, the string-induced contribution is dominant. Finally, in the section 6 we have applied the results found for the R-region to study the cosmic string induced effects in the generalized Randall-Sundrum model with a single brane. By integrating the field equations about the brane location, y0, the boundary conditions in this Z2-symmetric model is of the Robyn type with coefficient given by (75) for a field even under reflection with respect to the brane and it is reduced to the Neumann boundary condition in the case of a minimally coupled field, ξ = 0, and zero brane mass term, cb = 0. On the other hand for a field odd we get the Dirichlet boundary condition. The VEVs of the field squared and the energy-momentum tensor induced by a cosmic string and its magnetic flux in the RSII model are then obtained from those found in the section 5 by directly putting w0 = a with an additional factor 1/2. Acknowledgment The authors are grateful to H. F. Mota and A. A. Saharian for helpful discussions during the development of this work. W.O.S is supported under grant 2022/2008, Para´ ıba State Research Foundation (FAPESQ). E.R.B.M is partially supported by CNPq under Grant no 301.783/2019- 3. References [1] C. Fronsdal, Phys. Rev. D 10, 589 (1974). 21[2] C. Fronsdal and R. B. Haugen, Phys. Rev. D 12, 3810 (1975). [3] J. S. Avis, C. J. Isham and D. Storey, Phys. Rev. D 18, 3565 (1978). [4] B. Allen and T. Jacobson, Commun. Math. Phys. 103, 669 (1986). [5] R. Camporesi, Phys. Rev. D 43, 3958 (1991). [6] R. Camporesi and H. Higuchi, Phys. Rev. D 45, 3951 (1992). [7] M. M. Caldareli, Nuc. Phys. B 549, 499 (1999) . [8] C. G. Callan, Jr., and F. Wilczek, Nucl. Phys. B 340, 366 (1990). [9] O. Aharony, S. S. Gubser, J. Maldacena, H. Ooguri and Y. Oz, Phys. Rep. 323, 183 (2000). [10] P. Brax and C. Van de Bruck, Classical Quantum Gravity 20, R201(2003). [11] R. Maartens, Living Rev. Relativity 13, 5 (2010). [12] T. W. Kibble, J. Phys. A. 9, 1387 (1976). [13] A. Vilenkin and E. P. S. Shellard, Cosmic Strings and Other Topological Defects(Cambridge University Press, Cambridge, England, 1994). [14] B. Linet, Phys. Rev. D 33, 1833 (1986). [15] A. G. Smith, in The Formation and Evolution of Cosmic Strings , Proceedings of the Cam- bridge Workshop, Cambridge, England, 1989, edited by G.W. Gibbons, S.W. Hawking, and T. Vachaspati (Cambridge University Press, Cambridge, England, 1990). [16] A. N. Aliev, Casimir effect in the space-time of multiple cosmic strings, Phys. Rev. D 55, 3903-3904 (1997). [17] A. N. Aliev, M. Hortacsu and N. Ozdemir, Vacuum fluctuations of a massless spin 1/2 field around multiple cosmic strings, Class. Quant. Grav. 14, 3215-3224 (1997). [18] V. D. Skarzhinsky, D. D. Harari and U. Jasper, Phys. Rev. D 49, 755 (1994). [19] V. Berezinski, B. Hnatyk and A. Vilenkin, Phys. Rev. D 64, 043004 (2001). [20] T. Damour and A. Vilenkin, Phys. Rev. Lett. 85, 3761 (2000). [21] P. Bhattacharjee and G. Sigl, Phys. Rep. 327, 109 (2000). [22] S. Sarangi and S.-H. Henry Tye, Phys. Lett. B 536, 185 (2002). [23] E. J. Copeland, R. C. Myers and J. Polchinski, J. High Energy Phys. 06, 013 (2004). [24] G. Dvali and A. Vilenkin, J. Cosmol. Astropart. Phys. 03, 010 (2004). [25] W. Oliveira dos Santos, H. F. Mota and E. R. Bezerra de Mello, Phys. Rev. D 99, 045005 (2019). [26] W. Oliveira dos Santos, E. R. Bezerra de Mello and H. F. Mota, Eur. Phys. J. Plus 135, 27 (2020). 22[27] S. Bellucci, W. Oliveira dos Santos and E. R. Bezerra de Mello, Eur. Phys. J. C 80, 963 (2020). [28] S. Bellucci, W. Oliveira dos Santos, E. R. Bezerra de Mello and A. A. Saharian, JCAP 01 (2022) 010. [29] S. Bellucci, W. Oliveira dos Santos, E. R. Bezerra de Mello and A. A. Saharian, JHEP 02 (2021) 190. [30] S. Bellucci, W. Oliveira dos Santos, E. R. Bezerra de Mello and A. A. Saharian, JHEP 05 (2022) 021. [31] E. R. B. de Mello and A. A. Saharian, Vacuum polarization induced by a cosmic string in anti-de Sitter spacetime, J. Phys. A 45, 115402 (2012). [32] M. Abramowitz and I. A. Stegun, Handbook of Mathematical Functions (Dover, New York, 1972). [33] Breitenlohner, P.; Freedman, D.Z. Stability in gauged extended supergravity. Ann. Phys. (NY) 1982, 144 (2), 249-281. [34] I. S. Gradshteyn and I. M. Ryzhik. Table of Integrals, Series and Products (Academic Press, New York, 1980). [35] E. R. Bezerra de Mello, V. B. Bezerra, A. A. Saharian and A. S. Tarloyan, Vacuum polar- ization induced by a cylindrical boundary in the cosmic string spacetime , Phys. Rev. D 74, 025017 (2006). [36] E. R. Bezerra de Mello, V. B. Bezerra, A. A. Saharian, and H. H. Harutyunyan, Phys. Rev. D 91, 064034 (2015). [37] A.A. Saharian, The Generalized Abel-Plana Formula with Applications to Bessel Functions and Casimir Effect (Yerevan State University Publishing House, Yerevan, 2008); Preprint ICTP/2007/082; arXiv:0708.1187. [38] A. A. Saharian, Wightman function and Casimir densities on AdS bulk with application to the Randall-Sundrum brane world, Nucl. Phys. B 712, 196-228 (2005). [39] E. A. F. Bragan¸ ca, H. F. Santana Mota and E. R. Bezerra de Mello, Induced vacuum bosonic current by magnetic flux in a higher dimensional compactified cosmic string spacetime, Int. J. Mod. Phys. D 24, 1550055 (2015). [40] E. R. Bezerra de Mello and A. A. Saharian, Vacuum polarization by a cosmic string in de Sitter spacetime, JHEP 04, 046 (2009). [41] L. Randall and R. Sundrum, Large mass hierarchy from a small extra dimension, Phys. Rev. Lett. 83, 3370 (1999). [42] L. Randall and R. Sundrum, An alternative to compactification, Phys. Rev. Lett. 83, 4690 (1999). [43] T. Gherghetta, A. Pomarol, Nucl. Phys. B 586, 41 (2000). [44] A. Flachi, D. J. Toms, Nucl. Phys. B 610, 144 (2001). 23",
      "references": [
        "Cosmic Strings and Other Topological Defects(Cambridge University Press, Cambridge, England, 1994)",
        "The Formation and Evolution of Cosmic Strings",
        "Casimir effect in the space-time of multiple cosmic strings",
        "Vacuum fluctuations of a massless spin 1/2 field around multiple cosmic strings",
        "Vacuum polarization induced by a cosmic string in anti-de Sitter spacetime",
        "Handbook of Mathematical Functions",
        "Stability in gauged extended supergravity.",
        "Table of Integrals, Series and Products",
        "Vacuum polar- ization induced by a cylindrical boundary in the cosmic string spacetime",
        "The Generalized Abel-Plana Formula with Applications to Bessel Functions and Casimir Effect",
        "Wightman function and Casimir densities on AdS bulk with application to the Randall-Sundrum brane world",
        "Induced vacuum bosonic current by magnetic flux in a higher dimensional compactified cosmic string spacetime",
        "Vacuum polarization by a cosmic string in de Sitter spacetime",
        "Large mass hierarchy from a small extra dimension",
        "An alternative to compactification"
      ],
      "meta_data": {
        "arxiv_id": "2305.15664v3",
        "authors": [
          "W. Oliveira dos Santos",
          "E. R. Bezerra de Mello"
        ],
        "published_date": "2023-05-25T02:23:39Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Studies vacuum polarization for a charged massive scalar field in (D+1)-dimensional AdS spacetime containing a magnetic-flux-carrying ideal cosmic string and a codimension-1 planar brane (parallel to AdS boundary, string orthogonal). Derives closed-form brane-induced parts of the Wightman function in both regions separated by the brane (0<w<w0 and w>w0), and from it obtains the cosmic-string/magnetic-flux–dependent contributions to ⟨|φ|^2⟩ and ⟨T_μ^ν⟩ under Robin boundary conditions. Shows these string-induced brane corrections are finite on the brane (unlike the purely brane-induced terms) and analyzes dominance: near the brane, pure brane-induced contributions dominate; far from the brane, string-induced terms dominate. Provides asymptotics (near AdS boundary, near horizon/large w, Minkowski limit) and applies results to the Z2-symmetric single-brane Randall–Sundrum (RSII) scenario (mapping β to brane mass term and curvature coupling).",
        "methodology": "Solves the gauge-covariant Klein–Gordon equation (with mass m and curvature coupling ξ) on AdS with conical defect parameter q and azimuthal gauge potential A_ϕ (magnetic flux). Constructs normalized mode functions in cylindrical/Poincaré coordinates w using Bessel/modified Bessel functions; spectrum is continuous in the R-region and discrete in the L-region (p determined by zeros of \u0010J̄_ν). Builds the positive-frequency Wightman function via mode-sum; separates it into ‘cosmic-string-only’ and ‘brane-induced’ parts using functional identities and contour rotations (Hankel→modified Bessel). Sums over the angular quantum number with a known expansion of ∑_n e^{iqnΔϕ} I_{q|n+α|} and uses the generalized Abel–Plana formula to treat discrete radial modes in the L-region. Obtains ⟨|φ|^2⟩ by coincidence limit and ⟨T_μν⟩ using differential operators acting on W(x,x′) plus ξ-dependent terms; derives explicit integral representations for diagonal components and an off-diagonal r–w component. Performs asymptotic analysis using small/large-argument expansions and uniform asymptotics (Minkowski limit).",
        "experimental_setup": "No empirical datasets; the work is analytical with numerical evaluation of derived integral representations to illustrate behavior. Validation is primarily internal consistency checks: (i) decomposition into known AdS+string results plus brane-induced terms; (ii) correct Dirichlet/Neumann limits of Robin BC; (iii) expected region mapping L↔R via I_ν↔K_ν replacements; (iv) correct asymptotics near AdS boundary (w→0), far from brane (w/w0≫1), and Minkowski limit (a→∞). Numerical plots are provided for representative parameter choices (e.g., D=3, minimally coupled massless field, fixed r/w0=0.5, α0=0.4) comparing Dirichlet vs Neumann BC and multiple deficit parameters q (e.g., q=1.5, 2.5), and showing ratios of string-induced to purely brane-induced contributions.",
        "limitations": "Assumes an idealized infinitely thin straight cosmic string modeled only by a conical deficit (no finite core, internal structure, or localized stress-energy profile) and a constant A_ϕ representing an infinitesimally thin magnetic flux. Treats the scalar field as free (no self-interactions) on a fixed AdS background with a rigid flat brane; neglects gravitational/electromagnetic backreaction of vacuum polarization on the geometry/brane and ignores brane dynamics. Uses the Poincaré vacuum and specific boundary choices at the AdS boundary (Dirichlet for normalizability in the L-region), which may not cover all admissible AdS quantizations for 0≤ν<1. Robin coefficient β is taken constant (possibly region-dependent) and does not model more general brane-localized operators beyond a simple mass term. Results are given as integral/sum representations; closed-form evaluation is limited to special cases (e.g., ν=1/2).",
        "future_research_directions": "Extend to other fields (fermions, vectors, higher-spin) and to interacting scalars (e.g., λφ^4) to study how brane/string effects modify renormalization and IR behavior in AdS. Analyze more general AdS boundary conditions/quantizations (alternative/Neumann-type at w=0 for 0≤ν<1) and other vacuum states (thermal, α-vacua analogs, or states relevant for AdS/CFT). Include finite-core cosmic strings and more realistic magnetic field profiles, as well as additional background fluxes/compact extra dimensions (building on earlier compactification studies). Study multi-brane configurations (RSI/two-brane setups), curved branes, or branes at different orientations; compute induced forces/brane-string interactions and Casimir pressures on the brane. Investigate backreaction: solve semiclassical Einstein equations with ⟨T_μν⟩ to quantify corrections to brane tension, warp factor, and string geometry. Explore holographic interpretation of the brane-induced string contributions via AdS/CFT/defect CFT and compute related observables (e.g., boundary stress tensor, entanglement entropy).",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Universality and Limitations of Prompt Tuning",
      "full_text": "PINO-MBD: Physics-informed Neural Operator   for Solving Coupled ODEs in Multi-body Dynamics  Wenhao Ding1, Qing He1†, Hanghang Tong2, Ping Wang1  1MOE Key Laboratory of High-speed Railway Engineering, Southwest Jiaotong University  2University of Illinois at Urbana-Champaign  whding@my.swjtu.edu.cn, qhe@swjtu.edu.cn, wping@home.swjtu.edu.cn  htong@illinois.edu  ABSTRACT  In multi -body dynamics, the motion of a complicated physical  object is described as a coupled ordinary differential equation  system with multiple unknown solutions. Engineers need to  constantly adjust the object to meet requirements at the design stage,  where a highly efficient solver is needed. The rise of machine  learning-based partial differential equation  solvers can meet this  need.  These solvers can be classified into two categories:  approximating the solution function (Physics -informed neural  network) and learning the solution operator (Neural operator). The  recently proposed physics-informed neural operator (PINO) gains  advantages from both categories by embedding physics equations  into the loss function of a neural operator. Following this state -of- art concept, we propose the physics -informed neural operator for  coupled ODEs in multi -body dynamics (PIN O-MBD), which  learns the mapping between parameter spaces and solution spaces.  Once PINO-MBD is trained, only one forward pass of the network  is required to obtain the solutions for a new instance with different  parameters. To handle the difficulty that coupled ODEs contain  multiple solutions (instead of only  one in normal PDE problems),  two new physics embedding methods  are also proposed.  The  experimental results on classic vehicle -track coupled dynamics  problem show state -of-art performance not only on solutions but  also the first and second derivatives of solutions.   CCS CONCEPTS  •Applied computing → Physical sciences and engineering  → Engineering → Computer-aided design  KEYWORDS  Neural operator; Physics-informed neural network;  Graph neural  network; Multi-body dynamics  ACM Reference format:  Wenhao Ding, Qing He, Hanghang Tong,  and Ping Wang. 2021. PINO- MBD: Physics-informed Neural Operator for coupled ODEs in Multi-body  Dynamics.   1 Introduction  1.1 Machine learning-based PDE solvers  Almost all physics and engineering problems can be described with  partial differential equations (PDE). Using machine learning-based  methods to solve PDEs has become a popular research topic in  recent years . Compared with  classic solvers, ML -based methods  have shown the lower computational cost and better accuracy in  certain cases [1-3]. This has given rise to applied research in many  fields such as physics [4-5], chemistry [6], and engineering [7-8].  At present, the mainstream theories can be mainly divided into  two categories. The first one utilizes deep neural networks (DNN)  as a solver to learn the solution function of a specific PDE. One of  the most representative works  is the physics -informed neural  network (PINN) [2]. PINN uses automatic differential operation to  obtain the partial differential term of the solution. Then the PDE  losses can be computed and embedded into the loss function of the  DNN. However, the theory itself still faces some inevitable  problems. Firstly, PINN is a substitute for traditional numerical  solvers. The training process of the DNN is the solution process of  the PDE, and the training process is often sensitive to the selection  of hyperparameters. When the PDE parameters change, one needs  to retrain and fine-tune the PINN. Secondly, the value of PINN only  exists in cases where its computational cost is lower  than the  mathematic solver. Therefore, PINN is not practical when dealing  with a large number of PDE problems with lower complexity.  To tackle the main defect of PINN, the theory of neural operators  was proposed for parametric PDEs  [9]. This recently proposed  theory aims to learn the solution operator for a family of PDEs,  which is the map ping between the input parameter space and the  output solution space. Once the neuron operator is trained, only one  forward pass of the network is required to obtain the solution for a  new instan ce of the parameter. Recently a  series of neuron  operators have been proposed, including Graph Neural Operator  (GNO) [9], Low -rank Neural Operator (LNO) [10], Multipole  Graph Neural Operator (MGNO) [11], and Fourier Neural Operator  (FNO) [12], all of which  are derived from graph neural networks  (GNN) [13-14]. FNO utilizes the calculus properties of the fast  Fourier transform to perform convolution operation of GNN, which  is the state -of-art model among them.  On the other hand, the  DeepONet proposed at the same time shares the same concept with  the neural operator [15]. The difference is reflected in the network  architecture. The DeepONet processes the data on two parallel  networks (branch net and trunk net) to handle input and output  functions separately. Then the mapping operator for parametric  PDEs can be learned by combing the output of the two networks.  This model has derived a few applied research  directions as well,  including the inference of the electroconvection Multiphysics fields  [16] and prediction  for finite-rate chemistry  [17]. Recently,  Wenhao Ding et al.        Kovachki has proven that the standard DeepONet can be  mathematically regarded as a special case of neural operators [10],  which makes the theory of neuron operators more general.  Both PINN and neuron operator have their own advantages and  disadvantages. The former is a one -shot solver, and  the learning  ability of the DNN is enhanced by embedding the physics  information. The latter deal s with parametric PDE problems but  neglects the physics information.  Obviously, a very simple and  natural idea is to embed the physical equation in the loss function  of the neuron operator just like PINN, so as to get the advantages  of both. Driven by this idea, Li proposed Physics-informed Neural  Operator (PINO) [18], and Wang proposed Physics -informed  DeepONet [19]. Both studies found that neuron operators can  achieve better performance on smaller datasets by embedding  physics system equations. However, when applied in multi -body  dynamics, PINO suffers from many  problems, which will be  described in Section 1.2.  1.2 PINO for coupled ordinary differential  equations in multi-body dynamics    Figure 1: Flow chart for typical MBD engineering problem  Multi-body dynamics (MBD) models a complex physical object by  transforming it into a system composed of multiple interconnected  bodies [20-21]. Each degree of freedom (DOF) for the bodies is  described with an ordinary differential equation (ODE). And each  ODE in the system is coupled. All the ODEs are combined to form  the coupled ordinary differential equation system (CODES) for the  entire system. MBD has been applied in many practical engineering  problems, such as aerospace vehicles, locomotives , automobiles,  robotics, etc. As shown in Figure 1, for a typical MBD system  design problem, engineers need to continuously change the  structure and param eters of the system according to actual needs  until the design meets the requirements  [20]. Obviously, there is a  huge demand for solving parametric CODESs in this process.   However, the study  on this matter  in the engineering area is  currently still at the stage of using ordinary DNN [25-26] and time  series forecasting models such as LSTM  [27]. On the other hand,  all existing data-driven methods for CODES are based on PINN  and therefore cannot handle parametric CODES  [6, 28, 29] .  Following the concept of PINO, we aim to propose a physics - informed neural operator PINO -MBD for CODES  in MBD. Our  contributions mainly lie in two perspectives.  (1). Common physical PDE s usually contain only one unknown  solution, such as the classic Burgers Equation and Navier-Stoke s  Equation. A single CODES  contains multiple unknown solutions   and parameters, and the solutions are coupled with each other. We  propose two new methods to embed the CODES into the loss  function. We will show in section 4 that the existing loss function  embedding method used by PINO and PINN cannot handle the  problem of parametric CODESs.  (2). We propose a PINO for the MBD problem for the first time.  We conduct experiments on a widely used MBD problem, vehicle- track coupled dynamics. Results  show that PINO-MBD can not  only ensure the accuracy of the solution, but also the accuracy of  the derivatives of the solution.  2 Preliminaries and MBD problem settings  2.1 Terminology and MBD Fundamentals  Almost all multibody dynamics models can be assemb led with   three types of components: bodies, force elements, and excitations.  As shown in Fig ure 2, the bodies can be rigid or flexible. When  subjected to forces, the shape of the rigid body and the relative  positions of its internal points remain unchanged. It is usually used  to model objects with negligible deformation during motion.  Conversely, a flexible body can deform during motion, and the  relative positions of internal points can change. It is usually used to  describe objects whose deformation cannot  be ignored during  motion.    Figure 2: General MBD Composition  Force elements are defined as the interconnections between  bodies. There are various forms of force elements. Some force  elements limit the relative motion of connected objects, while most  do not, such as the classic linear spring-damper model (KV model).  A system composed of force elements and multiple bodies is called  a multibody system. Excitation is defined as the action of objects  outside the multibody system on bodies in the system. In practical  engineering, multi -body systems can well simulate the dynamic  behavior of complex mechanical systems such as robots,  manipulators, and high-speed trains, while  external excitation is  inevitable. For example, the robot will be disturbed by the external  force of the environment, and the mechanical arm needs to move  various heavy objects. In another MBD case, the train movement is  subjected to rail irregularities constantly during operation.  2.2 MBD Problem settings  In order to l earn the relationship between input functions  and the  multiple response functions of a multi -body system with neural  operators, we need to construct the  system equations that can be  PINO-MBD: Physics-informed Neural Operator for Solving  Coupled ODEs in Multi-body Dynamics       embedded in a deep network. Without loss of generality, we take a  general multi-body system in Fi gure 3 as an example to illustrate,  where rigid/flexible bodies, force element, and external excitations  are all considered.    Figure 3: General Topology for a Multi-body System  Table 1: Main Symbols  Symbol Definition and Description  Mi the mass matrix for the i-th rigid body  Xi the displacement vector for the i-th rigid body  Fi the external force vector acting on the i-th rigid body   Kej the stiffness matrix for force element j  Cej the damping matrix for force element j  E, I, ρ,  A, l  elastic module, moment of inertia, density , section  cross-sectional area, and the length of the beam  w(x,t) the deflection function of the flexible beam  Wn(x) the nth mode shape function  (function solution in  space domain)  T(x) function solution in time domain  ωn the nth natural frequency of the flexible beam  f(x,t) The force function acting on the flexible beam  qn(t) the generalized coordinate of the nth mode function  nr total number of degree of freedoms of rigid bodies  nf total number of mode shape functions considered for  the flexible beam  x, y, z, t Space coordinates and time    For the classic KV model, the output of the force element is  linear to the relative displacement and first- order derivative  (velocity) of the bodies at both ends. Taking the No. 1 force element  in Figure 2 as an example, the force element output e1f  can be  written as:  ( ) dd dt dt = −+ −  e1 e1 1 2 e1 1 2f KX X C X X                                  (1)  As mentioned earlier, bodies are classified into rigid bodies and  flexible bodies. Each degree of freedom of a rigid body is described  by an ordinary differential equation (ODE), which can be obtained  using Newton's second law, D'Alembert's principle, or the energy  method. Taking the first rigid body in Figure 3 as an example, its  motion equations in all directions can be described by matrices as:  ( ) 2 2 0e d dd dt dt dt + − + −= 1 1 1 2 e1 2M X C X X KX X                         (2)  Flexible bodies are usually described with  continuum  mechanical model s. Physical equations for diffe rent mechanical  models are different. However, they are all high- order PDEs with  respect to time and space. I n engineering, the common processing  method is to first obtain the mode shape function of the continuum  through the free vibration PDE, and then us e the mode  superposition method to convert the forced vibration PDE into a set  of low -order ODEs. From there they can be coupled with other  ODEs to form the complete CODES for the entire multi -body  system. We will take the simply supported beam in Figure 3  as an  example to clarify, the free vibration equation of the continuum is  a 4th-order PDE:  ( ) ( ) 42 42 ,, 0w xt w xtEI Axt ρ∂∂ +=∂∂                                                   (3)  The free -vibration solution can be found using the method of  separation of variables as  ( ) ( ) ( ),w xt W xT t= (4)  Substituting Eq. (4) into the PDE (3) leads to  ( ) ( ) ( ) ( ) 422 2 42 1dW x dT tc W x dx T t dt ω=−=  (5)                EIc Aρ=    (6)  ( ) ( ) 4 4 4 0dW x Wxdx β−= (7)                        ( ) ( ) 2 2 2 0dT t Ttdt ω+=  (8)  22 4 2 A c EI ω ρωβ = =                                                                         (9)  where ω  is referred to as the natural frequency and ()Wx  the  mode shape function. There are an infinite number of mode  functions that satisfy (5), each corresponding to a different ω. And  different mode shape functions satisfy the orthogonality condition:   [ ]0, 0,ijWW dx x l= ∈∫                                                              (10)  When the beam is subjected to external excitation, the forced  vibration PDE is formed as   ( ) ( ) ( ) 42 42 ,, ,w xt w xtEI A f x txt ρ∂∂ +=∂∂                                        (11)  The solution can be determined using the mode superposition  principle. For this, the deflection of the beam is assumed as  ( ) ( ) ( ) 1 , nn n w xt W x q t ∞ = =∑                                                             (12)  where nW  is the n th normal mode function satisfyi ng (7), and  ( )nqt  is referred to as the generalized coordinate in the nth mode.  By substituting Eq. (12) into (11), we obtain  ( ) ( ) ( ) ( ) 2 2 2 11 () 1 ,n nn n n nn dq tWx qt Wx f x tdt Aω ρ ∞∞ = = +=∑∑                      (13)  By multiplying Eq.(13) with ( )mWx , integrating over [ ]0,l , and  using Eq. (4), we have  ( ) ( ) ( ) 2 2 2 1n nn n dq t qt Qtdt Abω ρ+= (14) ( ) ( ) ( ) 0 , l nnQ t f x t W x dx=∫ (15)  where ( )nQt  is referred to the generalized force acting on ( )nqt    Wenhao Ding et al.        The constant b is obtained with Eq. (16).  ( ) 2 0 l nb W x dx=∫                                                                           (16)  So far, the forced vibration Eq. (11) has been transformed from  a 4th-order PDE to nf second-order ODEs, each of which describes  the motion of one mode of the continuum. By simply assembl ing  them with those that describe the motions of rigid bodies, we have  the CODES of the entire multi-body system:  2 2 1 1 rf rf TTT TTT dd dt dt qq QQ µµ µµ + += = =   Θ Θ Θ Θ Θ Θ Θ Θ 1 Θ 1 M X C X KX F X XX F FF                                       (17)  where ΘM , ΘC  and ΘK  are the mass, damping and stiffness  matrix of the multi -body system, respectively. More details of the  formulas above can be found in [22].  A few points need to be emphasized,  (1). There is an infinite number of modes for the continuum, each  corresponding to one natural frequency. However, in practical  applications, we only need to consider a certain number of modes  (nf) to ensure that the maximum natural frequency is high enough.  (2). For other continuum models, such as strings, rods, and  membranes, although the physical equations are different, the  solution methods are similar. Research in the field of mechanics  has provided a large number of methods that can solve Eq. (5) to  obtain the mode shape function. In this case, we mainly use the Ritz  method [22] with high precision. In practical applications, the mode  shape functions for many complex flexible body structures cannot  be found through physical derivation. In this case, one can conduct  finite element modal analysis to acquire ( )nWx .   (3). The generalized coordinate response is solved in the CODES .  One can simply apply Eq. (10) to reconstruct the solution of the  continuum.  2.3 Learning the solution operator  As described in Section 2.2, any multi-body dynamical system can  be described with CODES (17):   2 2 dd dt dt + +=Θ Θ Θ Θ Θ Θ ΘM X C X KX F                                          (17)  where ΘM , ΘC  and ΘK  can be constructed with the system’s  physical parameters. Therefore CODES (17) can be further  described as  ( )( ) ( ) ( ) , ,: 0 1,2, , 0, 0 i rfi i in TµµΦ= = + = Θ Θ Θ Θ X a,F XG                 (18)  where ( ),: 1, 2, jpj jn∈Α = a  is the physical parameter vector  for the CODES, and pn  is the number of parameters. ( ),:i ∈Θ iXU   is the unknown solution of the i th ODE in the CODES . ΘG  is the  initial condition of the CODES . iΦ  is a possibly non -linear  ordinary differential operator for the ith ODE in the CODES  with  three sets of Banach spaces  ( ) ( ){ }1 2 12, ,, , ,,, , rf p iUU U AA A Fµµ µ+ .  This formulation gives rise to the solution operator  ( ) ( ){ } ( ) † 12 12 1 2: ,,, ,,, , ,, p rf rf A AAF FF U UUµ µµ µµ++ → ，                   (19)  To emphasize, each iΦ  is related to all iU  and iA  due to the  coupled nature of all ODEs. This leads to two difficulties in  approximating operator † .  (1). Unlike existing research es such as PINO and PINN, operator  †  maps to multiple solution spaces (nr+ nf in total) instead of one.  In real MBD applications, the number usually reaches dozens or  even hundreds.  (2). Different from existing research that only focus es on the  solution of the physical equation, in practical MBD engineering  problems, more attention is paid to the derivatives of the unknown  solutions, that is  2 2 d dt ΘX  and d dt ΘX . For example, sound in nature  is usually associated with the first derivatives of the solutions  (velocity), and the human body is more sensitive to the second  derivatives of the solutions (acceleration).  Given a CODES  defined in (17 ) and the corr esponding solution  operator † , one can use a neural operator θ  with parameters θ   as a surrogate model to approximate † . We assume a dataset   { } 1,, N k =k Θk ΘkaF X is available, where ( ) † , =k Θk ΘkaF X  and each  component of ka and ΘkF  are i.i.d sampled from some distribution  aµ  and Fµ  supported on corresponding spaces. In this case, one  can optimize the solution operator by minimizing the empirical data  loss on a given data pair  ( )( ) ( ) ( ) ( )( ) 2† † † 0 ,, , , T dataL t t dt= −= − ∫Θkk Θk Θkk Θk Θkk ΘkX aF X aF X aF  (20)  The operator data loss is defined as the average error across all  possible inputs  ( ) ( ) ( ) ( )( ) 2† † 01 1,, , N T data data k J L t t dtN θθ θ = = − =  ≈ − ∑∫k Θk Θkk ΘkaF X aF   (21)    Figure 4: Network structure of PINO-MBD  3 Physics-informed neural operator for MBD  3.1 Fourier neural operators  The neural operators proposed in [9, 10], is formulated as a   generalization of standard deep neural networks to operator setting.  Following the basic mathematic al principles  of Graph Neural  Networks (GNN), Neural operator compose s a linear integral  PINO-MBD: Physics-informed Neural Operator for Solving  Coupled ODEs in Multi-body Dynamics       operator   with pointwise non- linear activ ation function σ  to  approximate highly non-linear operators.  Definition 1 (Neural operator θ ) Define the neural operator  ( ) ( )11: LLWWθ σ= ++                                         (22)  where 1: ad d→  , : uL dd →   are the pointwise neural  networks that encode  the lower dimension function into higher  dimensional space and decode the higher dimension function back  to the lower -dimensional space. The model stack L  layers of  ( )llWσ +  where 1llddW + ×∈  are pointwise linear operators  (matrices), { } { } 1: lldd l DD +→ →→   are integral kernel  operators, and σ  are fixed activation functions. The parameters θ   consist of all the parameters in  ,  , lW , l .  Definition 2 (Fourier convolution operator   ) Define the  Fourier convolution operator  ( ) ( )( ) 1 ttv Rv −= ⋅                                                               (23)  where R  is part of the parameter θ  to be learn ed.    and 1−   denote the forward and inverse Fourier transform, respectively.   3.2 Physics embedding  As described in Section 2.2, in order for neural operator to learn the  physics nature, we need to embed the CODES  (17) into the loss  function of the deep neural network.  Similar to (21), the operator  ODEs loss can be defined as:  ( ) ( ) 2 2 ,,ODEs ODEsJL dd dt dt θ θ=  = + +− k Θk Θk Θk Θk Θk Θk Θk Θk aF M X C X KX F                        (24)  PINN, PINO, and Physics-informed DeepONets all employ Eq. (24)  to embed the physical equations. However, Eq. (24) cannot handle  CODES (17). In this section, we propose two new embedding  methods ( )ODEsJ θ    and ( )ODEsJ θ    to deal with this challenge .  The first one improves Eq. (24) by introducing ODE magnitude  weight factors while the other  directly provides the network with  ground truth for the derivatives of the unknown solutions.  3.2.1 ODEs losses with ODE magnitude weight factors. Unlike  most PDE problems, one CODES contains multiple coupled ODEs.  Since different ODE represents different physical motions , the  magnitude for each equation varies  greatly. We will clarify the  phenomenon on an engineering example to be dealt with in Section  4, where the CODES  of this  physical system mainly contain 10  ODEs. Figure 5 shows the solutions of three ODEs (1, 2, 7) among  the 10 ODEs,  along with their first and second derivatives. We  consider adding white noise to all signals to simulate the process of  neuron operators trying to approximate the real solution. The  amplitude of the white noise is set  as 15% of the variance of each   signal itself.  It can b e observed that ODE loss magnitude for  different ODE in the CODES vary greatly during the learning  process. One can anticipate large gradient differences on different  ODEs during the training process. Therefore, if we set the  corresponding weight factors for different ODEs, the gradients on  different ODEs are similar in magnitude during the learning  process and better performance can be expected. To further explain  this countermeasure, we define the ODE magnitude weight factor  as follows.    (a). Solutions & derivatives of different ODEs    (b). Huge magnitude difference for ODE losses  Figure 5: Huge magnitude difference for different ODE losses  in one coupled ODEs  Definition 3 (ODE magnitude weight factor  i kφ ) Define the ODE  magnitude weight factor  ( )( ) ( ) ( )( ) 2 m a x , , : 1~ , 1~ :, (:, ) 0, (:, ) i k i rfi k Ni n n i i rD i ε ε φ µσ = Φ= = + = +== ⋅ Θk Θk Θk Θk Θk X a,F XX Xε                (25)  where ε  is white noise sequence, and r is the sensitivity factor,  meaning the acceptable signal error. In the previous paragraph, we  set r as 15%. In practical applications, we usually set r to be 2% for  more penalt ies. The ODE magnitude weight factors  calculated  through Eq. (25) corresponds to the data pair and each ODE one - to-one, and is used together with the training data during the  training process to form  ( ) 2 2ODEs k ddJ dt dt θ η φ  = + +−   Θk Θk Θk Θk Θk Θk ΘkM X C X KX F                         (26)  r in Eq. (25) is the loss function target that one wishes to reduce to.  We take the value of 2%.   3.2.2 Direct derivatives loss es The solutions obtained through  numerical methods naturally satisfy the CODES . For MBD  problems, a series of mature numerical integration methods have  been developed to solve their CODES . Whether implicit (implicit  Euler method, Newmark-β) or explicit (Runge-Kutta method, Zhai- 1 2 3 4 5 -1.0x10-3 0.0 1.0x10-3 2.0x10-3 1 2 3 4 5-3.00x10-4 -1.50x10-4 0.00 1.50x10-4 3.00x10-4 1 2 3 4 5 -1.50x10-3 0.00 1.50x10-3 3.00x10-3 1 2 3 4 5-1.5x10-2 -1.0x10-2 -5.0x10-3 0.0 5.0x10-3 1.0x10-2 1.5x10-2 1 2 3 4 5-2.0x10-3 -1.0x10-3 0.0 1.0x10-3 2.0x10-3 1 2 3 4 5-1.5x10-1 -1.0x10-1 -5.0x10-2 0.0 5.0x10-2 1.0x10-1 1.5x10-1 1 2 3 4 5-1.0x10-1 -5.0x10-2 0.0 5.0x10-2 1.0x10-1 1.5x10-1 1 2 3 4 5-3.00x10-2 -1.50x10-2 0.00 1.50x10-2 3.00x10-2 1 2 3 4 5-3.00x101 -1.50x101 0.00 1.50x101 3.00x101 Time (s)Time (s) Second derivative First derivative  With noise  Ground truth ODE solution Time (s)  With noise  Ground truth  With noise  Ground truth  With noise  Ground truth  With noise  Ground truth  With noise  Ground truth  With noise  Ground truth  With noise  Ground truth  With noise  Ground truth 1 2 3 4 510-2 10-1 100 101 102 103 104 105 106 107 ODE loss Time (s)  ODE1  ODE2  ODE7 Wenhao Ding et al.        method), most numerical integration methods can acquire solutions  and the derivatives of solutions at the same time. That is, we can  easily obtain ground truth for derivatives of the solutions before the  training process. In the existing research field, only the ground truth  for solutions is  provided for the neural operator while the  derivatives of solutions are usually left wasted. Therefore, we hope  to directly provide the ground truth of derivatives for the neural  operator in the same way as the data losses.  ( ) 22 † † 22,,ODEs data data dd d dJL L dt dt dt dt θ  = +   Θ Θ Θk ΘkXX X X                        (27)  The neural operator only output s solutions of the CODES  and  their derivatives will be computed through differential operations.  Li has summarized three differential operations to obtain solution  derivatives: Autograd, numerical difference , and exact gradients   [18]. In this paper, we mainly use numerical difference method to  solve the derivatives. Numerical difference method is faster and  less memory -consuming compared to Autograd. It is also more  stable compared to exact gradients.  4 Experiment: Train-Track coupled dynamics  In this section, we will  conduct experiments on  the widely used  vehicle-track coupled dynamics [23]. We will assemble the  CODES in section 4.1. In section 4.2, we will show that PINO - MBD achieves better performance on derivatives of solutions  compared with ordinary neural operators. We also show the two  proposed physics embedding methods c an help PINO -MBD  achieve much better results than normal embedding method (24).  Table 2: Main Symbols for the train-track coupled dynamics  Symbol Definition and Description  M* the mass of body *  J* the moment of inertia of body *  Z* the vertical displacement for body *  β* the rotation angle for body *  Zr(x,t) displacement of rail at position x, time t  lc, lt the characteristic length of the vehicle system  qk the kth mode displacement for the flexible rail  NM number of rail modes selected  N number of fasteners  pj wheel-rail force at jth wheelset  v running speed of the train  Kp, Ks the stiffness of primary and secondary suspension  Cp, Cs the damping of primary and secondary suspension  Kp, Cp the stiffness and damping of rail fastener  Irrei(t) rail irregularity under the ith wheelset  E, IY, mr, l elastic module, moment of inertia, mass per mass,  and the length of the rail  • first derivative with respect to time  •• second derivative with respect to time  Body * Definition and Description  c car body  tj, j=1~2 the jth bogie  wj, j=1~4 the jth wheelset  4.1 MBD equations settings    Figure 6: Topology of the vehicle-track coupled system  The vehicle -track coupled dynamics (VTCD) is a typical MBD  problem [23]. This theory has been tested by numerous field  experiments and continues to play an important role in the design  and construction of high-speed railway s. The schematic and  topology are shown in Figure 6. The vehicle system with 10 degrees  of freedom is composed of rigid bodies and force elements. And  the rail is modeled with a continuous Euler beam supported on a  series of force elements. Its CODES  can be formed following the  procedure in Section 2.2 as  ( ) 12 1 2 22 12 1 2 11 1 12 1 2 1 2 2 ( )( ) 2 2 ( ) ( )0 (2 ) (2 ) () c c s c sz c sz t t sz t t c c c sc c sc c sc t t sc t t t t p s t p s t s c sc p w w p w w sc c sc c t tt M Z CZ K Z C Z Z K Z Z Mg J Cl Kl Cl Z Z Kl Z Z M Z C C Z K K Z CZ KZ C Z Z K Z Z Cl Kl Mg J βββ ββ β + + − +− += + + + −+ −= ++ ++ −− − +− ++ + =              ( ) 22 1 1 12 1 2 22 2 34 3 4 22 2 2 2 34 3 4 2 2 ( ) ( )0 (2 ) (2 ) () 2 2 ( )( ) pt t pt t pt w w pt w w t t p s t p s t s c sc p w w p w w sc c sc c t t t pt t pt t pt w w pt w w Cl Kl Cl Z Z Kl Z Z M Z C C Z K K Z CZ KZ C Z Z K Z Z Cl Kl Mg J Cl Kl Cl Z Z Kl Z Z ββ ββ βββ + + + −+ −= ++ ++ −− − +− ++ + = + + + −+ −=              1 11 1 1 1 1 1 2 21 2 1 1 1 2 3 32 3 2 2 2 3 44 0 ( ) ( ) 2 () ( ) ( ) 2 () ( ) ( ) 2 () ( w w p w t p w t pt t pt t w w w p w t p w t pt t pt t w w w p w t p w t pt t pt t w ww p w M Z C Z Z K Z Z Cl Kl M g p t M Z C Z Z K Z Z Cl Kl M g p t M Z C Z Z K Z Z Cl Kl M g p t MZ C Z ββ ββ ββ + −+ −+ + = − + −+ −+ + = − + −+ −+ + = − +−               ( ) ( ) ( ) 2 42 2 2 4 4 11 11 1 4 1 ) ( ) 2 () () () ( ) () () ( ) () ( ) () ( ) ( ) ( 1~ ) t p w t pt t pt t w N NM Y k pk i h i h k ih r N NM N pk i h i h pk i s i pk i s i ih i j k wj j Z K Z Z Cl Kl M g p t EI kq t CZ x Z x q t q tml K Zx Zx q t C Zx Zt K Zx Zt p t Z x k NM ββ π = = = = = = + −+ + = − ++  + −+ = = ∑∑ ∑∑ ∑ ∑          (28)  where the wheel-rail force can be computed as  ( ) ( ) ( ) ( ) 1 1 () ( ,) () 0 (When the wheel and rail is disengaged ) 2, ( ), sin   wj r wj i i NM r kk k k r Z t Z x t Irre tpt G kxZx t Zx q t Zx ml l π =    −− =   = =∑                        (29)  NM in Eq. (28)  usually needs to be in the hundreds to ensure  accuracy, making the datasets too large to handle. Therefore, we let  PINO-MBD: Physics-informed Neural Operator for Solving  Coupled ODEs in Multi-body Dynamics       the neural operator output only the responses of rail under  wheelsets:  ( ) 1 ( , ) ( ), 1 ~ 4 NM r wj k wj k k Zx t Z x qt j = = =∑                                      (30)  In this way, the output DOFs of the flexible beam are reduced  from NM to 4 and the total DOFs of the system to 14. The drawback  of this action is  that the full mode  information of the rail is not  preserved, so the response at an arbitrary position on the rail cannot  be reconstructed. Fortunately, the focus of the VTCD is on the  responses of the vehicle system as they reflect the running safety  and the comfort of passengers.  Also, rail vibrations under the  wheelsets tend to be the worst, making sure this approach is  conservative.  Apart from gravity, the irregularities Irre on the rail surface are  the main excitations for the VTCD. On real lines, the irregularities  are highly random and can be affected by many factors. Random  irregularities force the system to vibrate randomly, affecting  running safety and riding comfort. The power spectrum density  (PSD) function is the most commonly used approac h to describe  rail irregularity as a stationary random process. In engineering,  PSD function is often used to describe the spectrum density  concerning frequency. The track irregularity power spectrum is a  continuous curve with a spectral density as the ordinate and  frequency or wavelength as the abscissa, which can only be  obtained through a large number of actual measurements. With the  PSD curve, one can generate simulated rail irregularity samples  through many inversion methods. In this paper, we use the classic  spectrum method [23]to generate irregularity samples (Figure 7 (b)).  This method ensures the PSD of the generated samples is identical  to the field PSD. Chinese high-speed ballastless track PSD is  chosen as the field PSD  [23], each sample in the data pair is  different.     (a). PSD curve  (b). simulated irregularity samples  Figure 7: Chinese high-speed ballastless track PSD and  simulated samples with spectrum method   To sum up, we train a PINO -MBD whose input is the VTCD  parameters and the output is the system response. The blue symbols  in Table 2 are the variable parameters (13 in total). The variation  range of each parameter is set to 80%~120% of standard CRH380  high-speed trai n parameter except for the irregularity. We  randomly generate 20000 pairs of data as the training set through  Zhai-method [24], and another 2000 pairs as the validation set.  4.2 Operator learning with ODEs losses and  direct derivatives losses  We train the PINO-MBD on a GTX 1660 card with 6GB of  memory. The channel number for each F ourier convolution layer  and fully connected layer is set as 72. The initial learning rate is set  as 5e -4 and is reduced to 75% every 30 steps. We compute the  relative L2 error of  solutions, first derivatives , and second  derivatives on the validation set at each epoch.   Table 3: Analysis description  Algorithm Description Depth  1 FNO (without physics embedding) 3  2 PINO (without ODE magnitude weight  factors) 3  3 PINO-MBD with ODEs losses (with ODE  magnitude weight factors) 3  4 PINO-MBD with direct derivatives losses 3  5 PINO-MBD with direct derivatives losses 5      30 10 110-11 10-9 10-7 10-5 PSD [m2/(1/m)] Wavelength (m) 0 100 200 300 400 500-5.0 -2.5 0.0 2.5 5.0Irregularity (mm) Mileage (m)  Sample No.1  Sample No.2 1 2 3 4 5-5.0 -2.5 0.0 2.5 5.0 1 2 3 4 5-0.4 -0.2 0.0 0.2 0.4 1 2 3 4 5-2 0 2 4 1 2 3 4 5-0.02 0.00 0.02 1 2 3 4 5-0.002 -0.001 0.000 0.001 0.002 1 2 3 4 5-0.10 -0.05 0.00 0.05 0.10 1 2 3 4 5-0.2 -0.1 0.0 0.1 0.2 1 2 3 4 5-0.050 -0.025 0.000 0.025 0.050 1 2 3 4 5-4 -2 0 2 4 Vertical vibration of Bogie No.1Rotation of carbody Time (s)Time (s) Displacement (mm)  Prediction  Ground truth Time (s) Vertical vibration of carbody Displacement (1e-3rad)  Prediction  Ground truth Displacement (mm)  Prediction  Ground truth Velocity (m/s)  Prediction  Ground truth Velocity (rad/s)  Prediction  Ground truth Velocity (m/s)  Prediction  Ground truth Acceleration (m/s2)  Prediction  Ground truth Acceleration (rad/s2)  Prediction  Ground truth Acceleration (m/s2)  Prediction  Ground truth  Wenhao Ding et al.          Figure 8: Performance of PINO-MBD for Train-track coupled dynamics (Algorithm 5)     (a). solutions                  (b). first derivatves of solutions    (c). second derivatives of solutions  Figure 9: Influence of physical equation embedding methods  on model performance  The necessity of using embedded physical system equations for  the MBD problem can be seen in  Figure 9. FNO achieves the best  performance on the solutions but performed poorly on first and  second derivatives. As mentioned earlier, solution derivatives are  usually more important for MBD problems.   Table 4: Model performance (relative L2 error (%))  Algorithm solutions First   derivatives  Second  derivatives  1 4.24 13.58 1404.42  2 7.41 23.06 202.42  3 8.29 14.90 20.79  4 6.75 6.12 6.73  5 6.77 4.90 3.20  Table 4 shows the relative L2 losses of solutions, first derivatives,  and second derivatives for different algorithms. Algorithm 2 (PINO)  only configures one weight for the ODEs loss as a whole in the total  loss function. As mentioned in section 3.2, neural operator cannot  harvest gradients on ODEs with small ODE weight factors.  Therefore, the performance on the first and second deri vatives of  algorithm 2 is unacceptable even with fine -tune. On the contrary,  Algorithm 3 takes ODE weight factors into account , so the neural  operator can harvest gradients of similar orders of magnitude on  different ODEs, so as to achieve better performance.  Algorithm 4 uses the direct gradient loss, and it can be seen that  the effect is further improved compared to Algorithm 2. As  mentioned in section 3.2.2, ground truth for solution derivatives is  usually wasted in existing research. Since it is not difficult to obtain  the ground truth of solution derivatives  for MBD problems, we  recommend using direct gradient loss when one has them . Of  course, the downside of this approach is that the file size of training  data triples. It is also observed from Figure 10 that the GNN depth  has a significant impact on the performance.   In future research, we will utilize better GPUs to optimize PINO- MBD. But in terms of engineering applications, the performance of  Algorithm 5 is completely sufficient. Compared with exiting data- driven models in the MBD research field [26-27], PINO-MBD has  achieved a significant improvement in accuracy. At the same time,  PINO-MBD can make predictions with only a forward pass of the  network without additional training, which is not available in most  data-driven MBD studies. Compared with the numerical  integration algorithm, the improvement in computational efficiency  of PINO-MBD is also significant. The simulation ratio for Zhai - method-based solver is approximately 1:25 , meaning 1 second of  solution requires 25 seconds of simulation. However, the  simulation ratio for PINO -MBD is less than 1:0.1. Also, neural  operators hardly consume any CPU -like numerical integration  methods (over 80%). Generating the training dataset with 20000  data pairs took more than 40 hours on a computer with intel i9- 10900X CPU while the trained PINO -MBD needs less than 2  seconds.   CONCLUSIONS   In this work, we propose the physics -informed neural operator for  solving coupled ODEs in MBD (PINO -MBD). With two novel  1 2 3 4 5-1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 1 2 3 4 5-2 0 2 4 1 2 3 4 5-8 -4 0 4 1 2 3 4 5 -0.05 0.00 0.05 0.10 1 2 3 4 5-0.10 -0.05 0.00 0.05 0.10 1 2 3 4 5-1.0 -0.5 0.0 0.5 1.0 1 2 3 4 5-5.0 -2.5 0.0 2.5 5.0 1 2 3 4 5-15 -10 -5 0 5 10 15 1 2 3 4 5 -200 -100 0 100 200 Displacement (1e-3rad)  Prediction  Ground truth Rotation of Bogie No.1 Vertical vibration of Wheelset No.1 Rail displacement under Wheelset No.1 Displacement (mm)  Prediction  Ground truth Displacement (mm)  Prediction  Ground truth Velocity (rad/s)  Prediction  Ground truth Velocity (m/s)  Prediction  Ground truth Time (s)Time (s)Time (s) Velocity (m/s)  Prediction  Ground truth Acceleration (rad/s2)  Prediction  Ground truth Acceleration (m/s2)  Prediction  Ground truth Acceleration (m/s2)  Prediction  Ground truth 0 50 100 150 200 250 3002x10-2 10-1 100 Relative L2 error Epoch  Algorithm 1  Algorithm 2  Algorithm 3  Algorithm 4  Algorithm 5 0 50 100 150 200 250 3002x10-2 10-1 100  Algorithm 1  Algorithm 2  Algorithm 3  Algorithm 4  Algorithm 5 Relative L2 error Epoch 0 50 100 150 200 250 3002x10-2 10-1 100 101 102  Algorithm 1  Algorithm 2  Algorithm 3  Algorithm 4  Algorithm 5 Relative L2 error EpochPINO-MBD: Physics-informed Neural Operator for Solving  Coupled ODEs in Multi-body Dynamics       physics-embeddings methods, PINO -MBD achieves satisfying  performance on solution derivatives while PINO fail s to output  acceptable results. The potential engineering and commercial value  of PINO-MBD is huge. In a design project, engineers can quickly  meet different design requirements by simulating a large number of  designs through pre -trained PINO -MBD with almost no  computational cost. Furthermore, we can expect PINO -MBD  training software with GUI interface to appear in the future, just  like the commonly used MBD commercial software Simpack [30],  UM [31], ADAMS [32], etc.  ACKNOWLEDGMENTS  This study was funded by the National Natural Science Foundation  of China (NSFC) under Grant Nos. U1934214 and 51878576.  REFERENCES  [1] Raissi M., Karniadakis G.E.Raissi M., Ka rniadakis G.E. 2018b. Hidden physics  models: Machine learning of nonlinear partial differential equations. Journal of  Computational Physics 357, (March 2018), 125– 141. DOI:  https://doi.org/10.1016/j.jcp.2017.11.039  [2] Raissi M., Perdikaris P., Karniadakis G.E.Raissi M., Perdikaris P., Karniadakis G.E.  2019c. Physics- informed neural networks: A deep learning framework for solving  forward and inverse problems involving nonlinear partial differential equations.  Journal of Computational Physics 378, (February 2019), 686– 707. DOI:  https://doi.org/10.1016/j.jcp.2018.10.045  [3] Sirignano J., MacArt J.F., Freund J.B.Sirignano J., MacArt J.F., Freund J.B. 2020d.  DPM: A deep learning PDE augmentation method with application to large -eddy  simulation. Journal of Computational Physics 423, (December 2020), 109811. DOI:  https://doi.org/10.1016/j.jcp.2020.109811  [4] Jin X., Cai S., Li H., Karniadakis G.E.Jin X., Cai S., Li H., Karniadakis G.E. 2021a.  NSFnets (Navier -Stokes flow nets): Physics- informed neural networks for the  incompressible Navier -Stokes equations. Journal of Computational Physics 426,  (February 2021), 109951. DOI: https://doi.org/10.1016/j.jcp.2020.109951  [5] Wang L., Yan Z.Wang L., Yan Z. 2021. Data -driven rogue waves and parameter  discovery in the defocusing nonlinear Schrödinger equation with a potential using the  PINN deep learning. Physics Letters A 404, (July 2021), 127408. DOI:  https://doi.org/10.1016/j.physleta.2021.127408  [6] Ji W., Qiu W., Shi Z., Pan S., Deng S.Ji W., Qiu W., Shi Z., Pan S., Deng S. 2021.  Stiff-PINN: Physics-Informed Neural Network for Stiff Chemical Kinetics. J. Phys.  Chem. A 125, 36, (September 2021), 8098– 8106. DOI:  https://doi.org/10.1021/acs.jpca.1c05102  [7] Amini Niaki S., Haghighat E., Campbell T., Poursartip A., Vaziri R.Am ini Niaki  S., Haghighat E., Campbell T., Poursartip A., Vaziri R. 2021a. Physics- informed  neural network for modelling the thermochemical curing process of composite -tool  systems during manufacture. Computer Methods in Applied Mechanics and  Engineering 384 , (October 2021), 113959. DOI:  https://doi.org/10.1016/j.cma.2021.113959  [8] Mao Z., Jagtap A.D., Karniadakis G.E.Mao Z., Jagtap A.D., Karniadakis G.E.  2020b. Physics-informed neural networks for high-speed flows. Computer Methods in  Applied Mechanics and Engineering 360, (March 2020), 112789. DOI:  https://doi.org/10.1016/j.cma.2019.112789  [9] Li Z., Kovachki N., Azizzadenesheli K., Liu B., Bhattacharya K., Stuart A.,  Anandkumar A. 2020. Neural Operator: Graph Kernel Network for Partial Differential  Equations. arXiv:2003.03485. Retrieved from https://arxiv.org/abs/2003.03485  [10] Kovachki N., Li Z., Liu B., Azizzadenesheli K., Bhattacharya K., Stuart A.,  Anandkumar. 2021. Neural Operator: Learning Maps Between Function Spaces.  arXiv:2108.08481 Retrieved from https://arxiv.org/abs/2108.08481  [11] Li Z., Kovachki N., Azizzadenesheli K., Liu B., Bhattacharya K., Stuart A.,  Anandkumar A. 2020. Multipole Graph Neural Operator for Parametric Partial  Differential Equations. arXiv:2006.09535 Retrieved from  https://arxiv.org/abs/2006.09535  [12] Li Z., Kovachki N., Azizzadenesheli K., Liu B., Bhattacharya K., Stuart A.,  Anandkumar A. 2021. Fourier Neural Operator for Parametric Partial Differential  Equations. arXiv:2010.08895 Retrieved from https://arxiv.org/abs/2010.08895  [13] Wu Z., Pan S., Chen F., Long G., Zhang C., Yu P.S.Wu Z., Pan S., Chen F., Long  G., Zhang C., Yu P.S. 2021a. A Comprehensive Survey on Graph Neural Networks.  IEEE Trans. Neural Netw. Learning Syst. 32, 1, (January 2021), 4– 24. DOI:  https://doi.org/10.1109/TNNLS.2020.2978386  [14] Xu K., Hu W., Leskovec J., Jegelka S. How Powerful are Graph Neural Networks?  arXiv:1810.00826 Retrieved from https://arxiv.org/abs/1810.00826  [15] Lu L., Jin P., Pang G., Zhang Z., Karniadakis G.E.Lu L., Jin P., Pang G., Zhang  Z., Karniadakis G.E. 2021b. Learning nonlinear operators via DeepONet based on the  universal approximation theorem of operators. Nat Mach Intell 3, 3, (March 2021),  218–229. DOI: https://doi.org/10.1038/s42256-021-00302-5  [16] Cai S., Wang Z., Lu L., Zaki T.A., Karniadakis G.E.Cai S., Wang Z., Lu L., Zaki  T.A., Karniadakis G.E. 2021a. DeepM&Mnet: Inferring the electroconvection  multiphysics fields based on operator approximation by neural networks. Journal of  Computational Physics 436, (July 2021), 1102 96. DOI:  https://doi.org/10.1016/j.jcp.2021.110296  [17] Mao Z., Lu L., Marxen O., Zaki T.A., Karniadakis G.E.Mao Z., Lu L., Marxen  O., Zaki T.A., Karniadakis G.E. 2021b. DeepM&Mnet for hypersonics: Predicting the  coupled flow and finite -rate chemistry behi nd a normal shock using neural-network  approximation of operators. Journal of Computational Physics 447, (December 2021),  110698. DOI: https://doi.org/10.1016/j.jcp.2021.110698  [18] Li Z., Zheng H., Kovachki N., Jin D., Chen H., Liu B., Azizzadenesheli K.,   Anandkumar A. 2021a. Physics -Informed Neural Operator for Learning Partial  Differential Equations. arXiv:2111.03794 Retrieved from  https://arxiv.org/abs/2111.03794  [19] Wang S., Wang H., Perdikaris P. 2021b. Learning the solution operator of  parametric pa rtial differential equations with physics-informed DeepOnets.  arXiv:2103.10974 Retrieved from https://arxiv.org/abs/2103.10974  [20] Angeles J., Kecskeméthy A. (Eds.). 1995. Kinematics and Dynamics of Multi - Body Systems, CISM International Centre for Mechanical Sciences. Springer Vienna,  Vienna. DOI: https://doi.org/10.1007/978-3-7091-4362-9  [21] Altenbuchner C. 2018. Modern flexible multi- body dynamics modeling  methodology for flapping wing vehicles. Elsevier/Academic Press, an imprint of  Elsevier, London.  [22] Rao S.S.Rao S.S. 2017. Mechanical vibrations, Sixth edition. ed. Pearson,  Hoboken.  [23] Zhai W.Zhai W. 2020. Vehicle –Track Coupled Dynamics: Theory and  Applications. Springer Singapore, Singapore. DOI: https://doi.org/10.1007/978 -981- 32-9283-3  [24] Zhai W. -M.Zhai W. -M. 1996. TWO SIMPLE FAST INTEGRATION  METHODS FOR LARGE-SCALE DYNAMIC PROBLEMS IN ENGINEERING. Int.  J. Numer. Meth. Engng. 39, 24, (December 1996), 4199 –4214. DOI:  https://doi.org/10.1002/(SICI)1097-0207(19961230)39:24<4199::AID- NME39>3.0.CO;2-Y  [25] Choi H. S., An J., Han S., Kim J. G., Jung J. Y., Choi J., Orzechowski G., Mikkola  A., Choi J.H.Choi H. -S., An J., Han S., Kim J.-G., Jung J. -Y., Choi J., Orzechowski  G., Mikkola A., Choi J.H. 2021a. Data -driven simulation for general-purpose  multibody dynamics using Deep Neural Networks. Multibody Syst Dyn 51, 4, (April  2021), 419–454. DOI: https://doi.org/10.1007/s11044-020-09772-8  [26] Oishi A., Yagawa G.Oishi A., Yagawa G. 2017b. Computational mechanics  enhanced by deep learning. Computer Methods in Applied Mechanics and Engineering  327, (December 2017), 327–351. DOI: https://doi.org/10.1016/j.cma.2017.08.040  [27] Ye Y., Huang P., Sun Y., Shi D.Ye Y., Huang P., Sun Y., Shi D. 2021. MBSNet:  A deep learning model for multibody dynamics simulation and its application to a  vehicle-track system. Mechanical Systems and Signal Processing 157, (August 2021),  107716. DOI: https://doi.org/10.1016/j.ymssp.2021.107716  [28] Antonelo E.A., Camponogara E., Seman L.O., de Souza E.R., Jordanou J.P.,  Hubner J.F.Antonelo E.A., Camponogara E., Seman L.O., de Souza E.R., Jordanou  J.P., Hubner J.F. 2021a. Physics-Informed Neural Nets for Control of Dynamical  Systems. arXiv:2104.02556 [cs] (September 2021).  [29] Ngo S.I., Lim Y. -I.Ngo S.I., Lim Y. -I. 2021b. Solution and Parameter  Identification of a Fixed-Bed Reactor Model for Catalytic CO2 Methanation Using  Physics-Informed Neural Networks. Catalysts 11, 11, (October 2021), 1304. DOI:  https://doi.org/10.3390/catal11111304  [30] Simpack AG 2013 SIMPACK documention, version 9.5 . Available at:  https://www.3ds.com/zh/products-services/simulia/products/simpack/  [31] Bryansk: Laboratory of Computational Mechanics. Available at:  https://www.universalmechanism.com/  [32]  Getting Started Using ADAMS/Controls, Mechanical Dynamics Inc. ,2002",
      "references": [
        "Hidden physics models: Machine learning of nonlinear partial differential equations.",
        "Physics- informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.",
        "DPM: A deep learning PDE augmentation method with application to large -eddy simulation.",
        "NSFnets (Navier -Stokes flow nets): Physics- informed neural networks for the incompressible Navier -Stokes equations.",
        "Data -driven rogue waves and parameter discovery in the defocusing nonlinear Schrödinger equation with a potential using the PINN deep learning.",
        "Stiff-PINN: Physics-Informed Neural Network for Stiff Chemical Kinetics.",
        "Physics- informed neural network for modelling the thermochemical curing process of composite -tool systems during manufacture.",
        "Physics-informed neural networks for high-speed flows.",
        "Neural Operator: Graph Kernel Network for Partial Differential Equations.",
        "Neural Operator: Learning Maps Between Function Spaces.",
        "Multipole Graph Neural Operator for Parametric Partial Differential Equations.",
        "Fourier Neural Operator for Parametric Partial Differential Equations.",
        "A Comprehensive Survey on Graph Neural Networks.",
        "How Powerful are Graph Neural Networks?",
        "Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators.",
        "DeepM&Mnet: Inferring the electroconvection multiphysics fields based on operator approximation by neural networks.",
        "DeepM&Mnet for hypersonics: Predicting the coupled flow and finite -rate chemistry behi nd a normal shock using neural-network approximation of operators.",
        "Physics -Informed Neural Operator for Learning Partial Differential Equations.",
        "Learning the solution operator of parametric pa rtial differential equations with physics-informed DeepOnets.",
        "Kinematics and Dynamics of Multi - Body Systems, CISM International Centre for Mechanical Sciences.",
        "Modern flexible multi- body dynamics modeling methodology for flapping wing vehicles.",
        "Mechanical vibrations, Sixth edition.",
        "Vehicle –Track Coupled Dynamics: Theory and Applications.",
        "TWO SIMPLE FAST INTEGRATION METHODS FOR LARGE-SCALE DYNAMIC PROBLEMS IN ENGINEERING.",
        "Data -driven simulation for general-purpose multibody dynamics using Deep Neural Networks.",
        "Computational mechanics enhanced by deep learning.",
        "MBSNet: A deep learning model for multibody dynamics simulation and its application to a vehicle-track system.",
        "Physics-Informed Neural Nets for Control of Dynamical Systems.",
        "Solution and Parameter Identification of a Fixed-Bed Reactor Model for Catalytic CO2 Methanation Using Physics-Informed Neural Networks.",
        "SIMPACK documention, version 9.5 .",
        "Bryansk: Laboratory of Computational Mechanics.",
        "Getting Started Using ADAMS/Controls"
      ],
      "meta_data": {
        "arxiv_id": "2205.12262v1",
        "authors": [
          "Wenhao Ding",
          "Qing He",
          "Hanghang Tong",
          "Ping Wang"
        ],
        "published_date": "2022-05-22T04:15:06Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces PINO-MBD, a physics-informed neural operator tailored to parametric coupled ODE systems (CODES) arising in multi-body dynamics (MBD). Addresses two key gaps when applying standard PINO/PINN to MBD: (i) CODES have many coupled unknown solution trajectories (dozens/hundreds DOFs), leading to imbalanced residual magnitudes across equations and poor gradient flow; (ii) engineering interest often lies in accurate time-derivatives (velocity/acceleration) as much as displacements. Proposes two new physics-embedding strategies for multi-solution CODES and demonstrates state-of-the-art accuracy on both solutions and first/second derivatives in a vehicle–track coupled dynamics benchmark, while enabling fast inference for new parameter settings via a single forward pass.",
        "methodology": "Models the parametric MBD simulator as an operator Γ† mapping from system parameters and excitation functions to multiple response functions (solutions for all DOFs). Uses a neural operator architecture instantiated as a Fourier Neural Operator (FNO): lift input to high-dimensional channels, apply L stacked Fourier convolution layers K (FFT → multiply by learnable spectral weights R → inverse FFT) plus pointwise linear transforms and nonlinearities, then project back to outputs. Trains with a composite loss: (1) data loss on solution trajectories; (2) physics loss enforcing CODES residuals. To adapt physics embedding to multiple coupled ODEs, introduces: (a) ODE-magnitude weight factors per equation to normalize residual scales and balance gradients (computed via a noise-sensitivity heuristic); (b) direct derivatives losses that supervise dX/dt and d²X/dt² using derivative ground truth from numerical integrators, while computing predicted derivatives via numerical differentiation of network outputs (chosen over autograd for speed/memory/stability).",
        "experimental_setup": "Benchmark: classic vehicle–track coupled dynamics (VTCD). System: 10-DOF rigid vehicle model (carbody, 2 bogies, 4 wheelsets with vertical and rotational DOFs) coupled to an Euler-beam rail supported by fasteners; rail discretized by modal superposition but outputs reduced to rail displacement under 4 wheelsets (instead of hundreds of rail modes), yielding 14 output DOFs total. Inputs: 13 variable physical parameters (suspension/fastener stiffness & damping, masses/inertias, geometry, speed, etc.) varied uniformly/randomly over 80%–120% of CRH380 nominal values, plus random rail irregularity excitation generated from measured Chinese high-speed ballastless track PSD using the spectrum method. Data generation: 20,000 training pairs and 2,000 validation pairs produced by Zhai-method numerical integration; derivatives available from the solver. Training details: GPU GTX 1660 6GB; FNO channel width 72; learning rate 5e-4 decayed by 0.75 every 30 steps; compared 5 algorithms—plain FNO (no physics), standard PINO with single global ODE weight, PINO-MBD with per-ODE weighting, PINO-MBD with direct derivatives loss (depth 3), and a deeper (depth 5) variant. Metrics: relative L2 error (%) on validation for solutions, first derivatives, and second derivatives over time.",
        "limitations": "Evaluation is limited to a single application domain (VTCD) and a reduced rail output representation (only displacements under wheelsets), so full-field rail response at arbitrary positions cannot be reconstructed. The per-ODE magnitude weight factors rely on a heuristic procedure (white-noise perturbation and a user-chosen sensitivity r, e.g., 2%), which may require tuning and may not generalize optimally across different CODES/stiffness regimes. Direct derivatives supervision triples dataset storage and assumes access to accurate derivative ground truth from a trusted numerical solver; this reduces applicability when derivatives are noisy/unavailable. The approach still requires substantial offline data generation time (40+ hours for 20k samples) and training; experiments were constrained by modest GPU resources and do not explore larger models, broader parameter distributions, or strong generalization outside the sampled parameter ranges/irregularity statistics. Numerical differentiation for derivatives can introduce discretization noise and depends on time-step resolution.",
        "future_research_directions": "Extend PINO-MBD to output full flexible-body fields (e.g., all rail modal coordinates or spatially continuous response) using operator learning over space–time grids and/or hybrid modal–spatial decoders. Develop more principled, possibly adaptive or learned, residual-balancing schemes for multi-equation physics losses (e.g., gradient-norm balancing, uncertainty weighting) to replace the noise-based heuristic. Investigate stiffness/instability handling for highly stiff CODES (implicit-structure-aware operators, symplectic/energy-preserving constraints) and richer coupling/nonlinear contact models. Reduce reliance on expensive labeled datasets via semi-supervised/physics-only training, active learning for parameter sampling, and improved PDE/ODE residual evaluation. Scale training and architecture search on stronger GPUs; study depth/width trade-offs, temporal resolution effects, and generalization to other MBD systems (robotics, aerospace, manipulators) and to inverse/design optimization workflows (fast parameter identification, sensitivity, and uncertainty quantification).",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Self-Refine: Iterative Refinement with Self-Feedback",
      "full_text": "SELF -REFINE : Iterative Refinement with Self-Feedback Aman Madaan1, Niket Tandon2, Prakhar Gupta1, Skyler Hallinan3, Luyu Gao1, Sarah Wiegreffe2, Uri Alon1, Nouha Dziri2, Shrimai Prabhumoye4, Yiming Yang1, Shashank Gupta2, Bodhisattwa Prasad Majumder5, Katherine Hermann6, Sean Welleck2,3, Amir Yazdanbakhsh6, Peter Clark2 1Language Technologies Institute, Carnegie Mellon University 2Allen Institute for Artificial Intelligence 3University of Washington 4NVIDIA 5UC San Diego 6Google Research, Brain Team amadaan@cs.cmu.edu, nikett@allenai.org Abstract Like humans, large language models ( LLM s) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce SELF -REFINE , an approach for improving initial outputs from LLM s through iterative feedback and refinement. The main idea is to generate an initial output using an LLM ; then, the same LLM provides feedback for its output and uses it to refine itself, iteratively. SELF -REFINE does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner and the feedback provider. We evaluate SELF -REFINE across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art ( GPT-3.5 and GPT-4) LLM s. Across all evaluated tasks, outputs generated with SELF -REFINE are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ∼20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test-time using our simple, standalone approach.1. 1 Introduction Although large language models ( LLM s) can generate coherent outputs, they often fall short in addressing intricate requirements. This mostly includes tasks with multifaceted objectives, such as dialogue response generation, or tasks with hard-to-define goals, such as enhancing program readability. In these scenarios, modern LLM s may produce an intelligible initial output, yet may benefit from further iterative refinement—i.e., iteratively mapping a candidate output to an improved one—to ensure that the desired quality is achieved. Iterative refinement typically involves training a refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al. (2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models require large training sets or expensive human annotations (Madaan et al., 2021; Ouyang et al., 2022), which may not always be feasible to obtain. These limitations underscore the need for an effective refinement approach that can be applied to various tasks without requiring extensive supervision. Iterative self -refinement is a fundamental characteristic of human problem-solving (Simon, 1962; Flower and Hayes, 1981; Amabile, 1983). Iterative self-refinement is a process that involves creating an initial draft and subsequently refining it based on self-provided feedback. For example, when 1Code and data at https://selfrefine.info/ Preprint. Under review. arXiv:2303.17651v2  [cs.CL]  25 May 2023Refine Feedback Use M to get feedback on its own output Input Use M to reﬁne its previous output, given its feedback Model M 1 2 0 Figure 1: Given an input ( 0⃝), SELF -REFINE starts by generating an output and passing it back to the same model M to get feedback ( 1⃝). The feedback is passed back to M, which refines the previously generated output ( 2⃝). Steps ( 1⃝) and ( 2⃝) iterate until a stopping condition is met. SELF -REFINE is instantiated with a language model such as GPT-3.5 and does not involve human assistance. drafting an email to request a document from a colleague, an individual may initially write a direct request such as “ Send me the data ASAP ”. Upon reflection, however, the writer recognizes the potential impoliteness of the phrasing and revises it to “Hi Ashley, could you please send me the data at your earliest convenience?\". When writing code, a programmer may implement an initial “quick and dirty” implementation, and then, upon reflection, refactor their code to a solution that is more efficient and readable. In this paper, we demonstrate that LLMs can provide iterative self-refinement without additional training, leading to higher-quality outputs on a wide range of tasks. We present SELF -REFINE : an iterative self-refinement algorithm that alternates between two gener- ative steps–FEEDBACK and REFINE . These steps work in tandem to generate high-quality outputs. Given an initial output generated by a model M, we pass it back to the same model M to get feedback. Then, the feedback is passed back to the same model to refine the previously-generated draft. This process is repeated either for a specified number of iterations or until M determines that no further refinement is necessary. We use few-shot prompting (Brown et al., 2020) to guideM to both generate feedback and incorporate the feedback into an improved draft. Figure 1 illustrates the high-level idea, that SELF -REFINE uses the same underlying language model to generate feedback and refine its outputs. We evaluate SELF -REFINE on 7 generation tasks that span diverse domains, including natural language and source-code generation. We show that SELF -REFINE outperforms direct generation from strong LLM s like GPT-3.5 (text-davinci-003 and gpt-3.5-turbo; OpenAI; Ouyang et al., 2022) and GPT-4 (OpenAI, 2023) by 5-40% absolute improvement. In code-generation tasks, SELF -REFINE improves the initial generation by up to absolute 13% when applied to strong code models such as Codex (code-davinci-002; Chen et al., 2021). We release all of our code, which is easily extensible to other LLMs. In essence, our results show that even when an LLM cannot generate an optimal output on its first try, the LLM can often provide useful feedback and improve its own output accordingly. In turn, SELF -REFINE provides an effective way to obtain better outputs from a single model without any additional training, via iterative (self-)feedback and refinement. 2 Iterative Refinement with S ELF -REFINE Given an input sequence, SELF -REFINE generates an initial output, provides feedback on the output, and refines the output according to the feedback. SELF -REFINE iterates between feedback and refinement until a desired condition is met. SELF -REFINE relies on a suitable language model and three prompts (for initial generation, feedback, and refinement), and does not require training. SELF -REFINE is shown in Figure 1 and Algorithm 1. Next, we describe SELF -REFINE in more detail. Initial generation Given an input x, prompt pgen, and model M, SELF -REFINE generates an initial output y0: y0 = M(pgen∥x) . (1) 2(a) Dialogue: x, yt User: I am interested in playing Table tennis. Response: I'm sure it's a great way to socialize, stay active (b) FEEDBACK fb Engaging: Provides no information about table tennis or how to play it. User understanding: Lacks understanding of user's needs and state of mind. (c) REFINE yt+1 Response (refined): That's great to hear (...) ! It's a fun sport requiring quick reflexes and good hand-eye coordination. Have you played before, or are you looking to learn? (d) Code optimization: x, yt Generate sum of 1, ..., N def sum(n): res = 0 for i in range(n+1): res += i return res (e) FEEDBACK fb This code is slow as it uses brute force. A better approach is to use the formula ... (n(n+1))/2. (f) REFINE yt+1 Code (refined) def sum_faster(n): return (n*(n+1))//2 Figure 2: Examples of SELF -REFINE : an initial output generated by the base LLM and then passed back to the same LLM to receive feedback to the same LLM to refine the output . The top row illustrates this for dialog generation where an initial dialogue response can be transformed into a more engaging one that also understands the user by applying feedback. The bottom row illustrates this for code optimization where the code is made more efficient by applying feedback. Algorithm 1 SELF -REFINE algorithm Require: input x, model M, prompts {pgen, pfb, prefine}, stop condition stop(·) 1: y0 = M(pgen∥x) ▷ Initial generation (Eqn. 1) 2: for iteration t ∈ 0, 1, . . .do 3: fbt = M(pfb∥x∥yt) ▷ Feedback (Eqn. 2) 4: if stop(fbt, t) then ▷ Stop condition 5: break 6: else 7: yt+1 = M(prefine∥x∥y0∥fb0∥...∥yt∥fbt) ▷ Refine (Eqn. 4) 8: end if 9: end for 10: return yt Figure 3: The S ELF -REFINE algorithm. See (§2) for a discussion of each component. For example, in Figure 2(d), the model generates functionally correct code for the given input. Here, pgen is a task-specific few-shot prompt (or instruction) for an initial generation, and ∥ denotes concatenation. The few-shot prompt contains input-output pairs ⟨x(k), y(k)⟩ for the task.2 FEEDBACK Next, SELF -REFINE uses the same model M to provide feedback fbt on its own output, given a task-specific prompt pfb for generating feedback: fbt = M(pfb∥x∥yt) . (2) Intuitively, the feedback may address multiple aspects of the output. For example, in code optimiza- tion, the feedback might address the efficiency, readability, and overall quality of the code. 2Few-shot prompting (also referred to as “in-context learning”) provides a model with a prompt consisting of k in-context examples of the target task, each in the form of input-output pairs ⟨xi, yi⟩ (Brown et al., 2020). 3Here, the prompt pfb provides examples of feedback in the form of input-output-feedback triples ⟨x(k), y(k), fb(k)⟩. We prompt the model to write feedback that is actionable and specific via fb(k). By ‘actionable’, we mean the feedback should contain a concrete action that would likely improve the output. By ‘specific’, we mean the feedback should identify concrete phrases in the output to change. For example, the feedback in Figure 2(e) is “This code is slow as it uses a for loop which is brute force. A better approach is to use the formula ... (n(n+1))/2 ”. This feedback is actionable, since it suggests the action ‘use the formula...’. The feedback is specific since it mentions the ‘for loop’. REFINE Next, SELF -REFINE uses M to refine its most recent output, given its own feedback: yt+1 = M(prefine∥x∥yt∥fbt) . (3) For example, in Figure 2(f), given the initial output and the generated feedback, the model generates a re-implementation that is shorter and runs much faster than the initial implementation. The prompt prefine provides examples of improving the output based on the feedback, in the form of input-output-feedback-refined quadruples ⟨x(k), y(k) t , fb(k) t , y(k) t+1⟩. Iterating SELF -REFINE SELF -REFINE alternates between FEEDBACK and REFINE steps until a stopping condition is met. The stopping condition stop(fbt, t) either stops at a specified timestep t, or extracts a stopping indicator (e.g. a scalar stop score) from the feedback. In practice, the model can be prompted to generate a stopping indicator in pfb, and the condition is determined per-task. To inform the model about the previous iterations, we retain the history of previous feedback and outputs by appending them to the prompt. Intuitively, this allows the model to learn from past mistakes and avoid repeating them. More precisely, Equation (3) is in fact instantiated as: yt+1 = M(prefine∥x∥y0∥fb0∥...∥yt∥fbt) . (4) Finally, we use the last refinement yt as the output of SELF -REFINE . Algorithm 1 summarizes SELF -REFINE , and Figure 2 shows an example of SELF -REFINE in the Dialogue Response Generation (Mehri and Eskenazi, 2020) and Code Optimization (Madaan et al., 2023) tasks. Appendix S provides examples of the pgen, pfb, prefine prompts for various tasks. The key idea is that SELF -REFINE uses the same underlying LLM to generate, get feedback, and refine its outputs given its own feedback. It relies only on supervision present in the few-shot examples. 3 Evaluation We evaluate SELF -REFINE on 7 diverse tasks: Dialogue Response Generation (Appendix M; Mehri and Eskenazi, 2020), Code Optimization (Appendix N; Madaan et al., 2023), Code Readability Improvement (Appendix L; Puri et al., 2021), Math Reasoning (Appendix O; Cobbe et al., 2021), Sentiment Reversal (Appendix P; Zhang et al., 2015), and we introduce two new tasks: Acronym Generation (Appendix Q) and Constrained Generation (a harder version of Lin et al. (2020) with 20-30 keyword constraints instead of 3-5; Appendix R) Examples for all tasks and dataset statistics are provided in Table 4 (Appendix A). 3.1 Instantiating S ELF -REFINE We instantiate SELF -REFINE following the high-level description in Section 2. The FEEDBACK - REFINE iterations continue until the desired output quality or task-specific criterion is reached, up to a maximum of 4 iterations. To make our evaluation consistent across different models, we implemented both FEEDBACK and REFINE as few-shot prompts even with models that respond well to instructions, such as ChatGPT and GPT-4. Base LLMs Our main goal is to evaluate whether we can improve the performance of any strong base LLMs using SELF -REFINE . Therefore, we compare SELF -REFINE to the same base LLMs but without feedback-refine iterations. We used three main strong base LLM across all tasks: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4 (OpenAI, 2023). For code-based tasks, we also experimented with CODEX (code-davinci-002). In all tasks, either GPT-3.5 or GPT-4 is the previous state-of-the-art. 3 We used the same prompts from previous work when 3A comparison with other few-shot and fine-tuned approaches is provided in Appendix F 4GPT-3.5 ChatGPT GPT-4 Task Base +SELF -REFINE Base +SELF -REFINE Base +SELF -REFINE Sentiment Reversal 8.8 30.4 (↑21.6) 11.4 43.2 (↑31.8) 3.8 36.2 (↑32.4) Dialogue Response 36.4 63.6 (↑27.2) 40.1 59.9 (↑19.8) 25.4 74.6 (↑49.2) Code Optimization 14.8 23.0 (↑8.2) 23.9 27.5 (↑3.6) 27.3 36.0 (↑8.7) Code Readability 37.4 51.3 (↑13.9) 27.7 63.1 (↑35.4) 27.4 56.2 (↑28.8) Math Reasoning 64.1 64.1 (0) 74.8 75.0 (↑0.2) 92.9 93.1 (↑0.2) Acronym Generation 41.6 56.4 (↑14.8) 27.2 37.2 (↑10.0) 30.4 56.0 (↑25.6) Constrained Generation 28.0 37.0 (↑9.0) 44.0 67.0 (↑23.0) 15.0 45.0 (↑30.0) Table 1: SELF -REFINE results on various tasks using GPT-3.5 , ChatGPT, and GPT-4 as base LLM. SELF -REFINE consistently improves LLM. Metrics used for these tasks are defined in Section 3.2. available (such as for Code Optimization and Math Reasoning); otherwise, we created prompts as detailed in Appendix S. We use greedy decoding with a temperature of 0.7 for all setups. 3.2 Metrics We report three types of metrics: • Task specific metric: When available, we use automated metrics from prior work (Math Reasoning: % solve rate; Code Optimization: % programs optimized; Constrained Gen: coverage %) • Human-pref: In Dialogue Response Generation, Code Readability Improvement, Sentiment Reversal, and Acronym Generation, since no automated metrics are available, we perform a blind human A/B evaluation on a subset of the outputs to select the preferred output. Additional details are provided in Appendix C. • GPT-4 -pref: In addition to human-pref, we use GPT-4 as a proxy for human preference following prior work (Fu et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), and found high correlation (82% for Sentiment Reversal, 68% for Acronym Generation, and 71% for Dialogue Response Generation) with human-pref. For Code Readability Improvement, we prompt GPT- 4 to calculate fraction of the variables that are appropriately named given the context (e.g., x = [] → input_buffer = []). Additional details are provided in Appendix D. 3.3 Results Table 1 shows our main results: SELF -REFINE consistently improves over base models across all model sizes, and additionally outperforms the previous state-of-the-art across all tasks. For example, GPT-4 +SELF -REFINE improves over the base GPT-4 by 8.7% (absolute) in Code Optimization, increasing optimization percentage from 27.3% to 36.0%. Confidence intervals are provided in Appendix J. For code-based tasks, we found similar trends when using CODEX ; those results are included in Appendix F. One of the tasks in which we observe the highest gains compared to the base models is Constrained Generation, where the model is asked to generate a sentence containing up to 30 given concepts. We believe that this task benefits significantly from SELF -REFINE because there are more opportunities to miss some of the concepts on the first attempt, and thus SELF -REFINE allows the model to fix these mistakes subsequently. Further, this task has an extremely large number of reasonable outputs, and thus SELF -REFINE allows to better explore the space of possible outputs. In preference-based tasks such as Dialogue Response Generation, Sentiment Reversal, and Acronym Generation, SELF -REFINE leads to especially high gains. For example in Dialogue Response Generation, GPT-4 preference score improve by 49.2% – from 25.4% to 74.6%. Similarly, we see remarkable improvements in the other preference-based tasks across all models. The modest performance gains in Math Reasoning can be traced back to the inability to accurately identify whether there is any error. In math, errors can be nuanced and sometimes limited to a single line or incorrect operation. Besides, a consistent-looking reasoning chain can deceive LLM s to 5think that “everything looks good” (e.g., ChatGPT feedback for 94% instances is ’everything looks good’). In Appendix H.1, we show that the gains with SELF -REFINE on Math Reasoning are much bigger (5%+) if an external source can identify if the current math answer is incorrect. Improvement is consistent across base LLM s sizes Generally, GPT-4 +SELF -REFINE performs better than GPT-3.5 +SELF -REFINE and ChatGPT+SELF -REFINE across all tasks, even in tasks where the initial base results of GPT-4 were lower than GPT-3.5 or ChatGPT. We thus believe that SELF -REFINE allows stronger models (such as GPT-4 ) to unlock their full potential, even in cases where this potential is not expressed in the standard, single-pass, output generation. Comparison to additional strong baselines is provided in Appendix F. 4 Analysis The three main steps of SELF -REFINE are FEEDBACK , REFINE , and repeating them iteratively. In this section, we perform additional experiments to analyze the importance of each of these steps. Task S ELF -REFINE feedback Generic feedback No feedback Code Optimization 27.5 26.0 24.8 Sentiment Reversal 43.2 31.2 0 Acronym Generation 56.4 54.0 48.0 Table 2: Prompting to generate generic feedback (or having the model generate no feedback at all) leads to reduced scores, indicating the importance of the FEEDBACK step of SELF -REFINE . These experiments were performed with ChatGPT (Code Optimization and Sentiment Reversal) and GPT-3.5 (Acronym Generation), and metrics used are defined in Section 3.2. The impact of the feedback quality Feedback quality plays a crucial role in SELF -REFINE . To quantify its impact, we compare SELF -REFINE , which utilizes specific, actionable feedback, with two ablations: one using generic feedback and another without feedback (the model may still iteratively refine its generations, but is not explicitly provided feedback to do so). For example, in the Code Optimization task: actionable feedback, such as Avoid repeated calculations in the for loop, pinpoints an issue and suggests a clear improvement. Generic feedback, like Improve the efficiency of the code, lacks this precision and direction. Table 2 shows feedback’s clear influence. In Code Optimization, performance slightly dips from 27.5 (SELF -REFINE feedback) to 26.0 (generic feedback), and further to 24.8 (no feedback). This suggests that while generic feedback offers some guidance – specific, actionable feedback yields superior results. This effect is more pronounced in tasks like Sentiment Transfer, where changing from our feedback to generic feedback leads to a significant performance drop (43.2 to 31.2), and the task fails without feedback. Similarly, in Acronym Generation, without actionable feedback, performance drops from 56.4 to 48.0, even with iterative refinements. These results highlight the importance of specific, actionable feedback in our approach. Even generic feedback provides some benefit, but the best results are achieved with targeted, constructive feedback. How important are the multiple iterations of FEEDBACK -REFINE ? Figure 4 demonstrates that on average, the quality of the output improves as the number of iterations increases. For instance, in the Code Optimization task, the initial output (y0) has a score of 22.0, which improves to 28.8 after three iterations (y3). Similarly, in the Sentiment Reversal task, the initial output has a score of 33.9, which increases to 36.8 after three iterations. This trend of improvement is also evident in Constrained Generation, where the score increases from 29.0 to 49.7 after three iterations. Figure 4 highlights the diminishing returns in the improvement as the number of iterations increases. Overall, having multiple FEEDBACK -REFINE iterations significantly enhances the quality of the output, although the marginal improvement naturally decreases with more iterations. The performance may not always monotonically increase with iterations: in multi-aspect feedback tasks like Acronym Generation, where the output quality can vary during iteration with improvement in one aspect but decline in another aspect. To counter this, SELF -REFINE generates numerical scores for different quality aspects, leading to a balanced evaluation and appropriate output selection. 6Task y0 y1 y2 y3 Code Opt. 22.0 27.0 27.9 28.8 Sentiment Rev. 33.9 34.9 36.1 36.8 Constrained Gen. 29.0 40.3 46.7 49.7 ∆(y0→y1) ∆(y1→y2) ∆(y2→y3) 0 5 10 5 0.9 0.9 11.3 6.4 3 1 1.2 0.7 C. Opt. C. Gen. S. Rev. Figure 4: Left: Iteration-wise score improvements. Early iterations significantly improve output quality, and scores generally keep improving with more iterations.Right: SELF -REFINE Performance improvements with iterations. Most gains(∆) are in the initial iterations for both Code Opt. and Senti- ment Reversal. The numbers are averaged over ChatGPT, GPT-3.5 , and GPT-4 . Task abbreviations: C. Opt. (Code Optimiz.), S. Rev. (Sentiment Reversal), C. Gen. (Constrained Generation). # Slower code def solve(amount): best_price = (amount + 199) // 200 * 380,→ # First loop for a in range(amount // 200 + 1): # ... 4 nested loops ... for c1 in range(amount // 1500 + 1):,→ if a*200 + b*300 == amount: price = a*380 + b*550 if price < best_price: best_price = price return best_price # Faster code def solve(amount): coins = [200, 300] prices = [380, 550] dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(len(coins)): for j in range(coins[i], amount+1): dp[j] = min(dp[j], dp[j - coins[i]] + prices[i]),→ return dp[amount] Figure 5: Comparison of code generated by Madaan et al. (2023) (left) and the output after applying SELF -REFINE (right). The initial code by the baseline, which is nearly identical to the slower input program, fails to improve the efficiency and merely alters the logic for reading input. SELF -REFINE first generates feedback that diagnoses that This code is slow because it is using six nested loops to iterate through all possible combinations of coins to pay the amount, and suggests that a more efficient approach would be .... SELF -REFINE then uses this feedback to generate the revised code (right), reducing the time complexity to O(amount ∗ coins). The full example is provided in Appendix H Can we just generate multiple outputs instead of refining? Does SELF -REFINE improve because of the iterative refinement, or just because it generatesmore outputs? We compare SELF -REFINE with ChatGPT, when ChatGPT generates k = 4 samples (but without feedback and refinement). Then, we compare the performance of SELF -REFINE against these k initial outputs in a 1 vs. k evaluation. In other words, we assess whether SELF -REFINE can outperform all k initial outputs. The results of this experiment are illustrated in Figure 6 (Appendix H). Despite the increased difficulty of the 1 vs. k setting, the outputs of SELF -REFINE are still preferred by humans over all k initial outputs. This shows the importance of refinement according to feedback over the alternative of just generating multiple initial outputs. Does SELF -REFINE work with weaker models? The experiments in Section 3.3 were performed with some of the strongest available models; does SELF -REFINE work with smaller or weaker models as well? To investigate this, we instantiated SELF -REFINE with Vicuna-13B (Chiang et al., 2023), a 7less powerful base model. While Vicuna-13B is capable of generating initial outputs, it struggles significantly with the refinement process. Specifically, Vicuna-13B was not able to consistently generate the feedback in the required format. Furthermore, even when provided with Oracle or hard-coded feedback, it often failed to adhere to the prompts for refinement. Instead of refining its output, Vicuna-13B either repeated the same output or generated a hallucinated conversation, rendering the outputs less effective. We thus hypothesize that since Vicuna-13B was trained on conversations, it does not generalize as well as instruction-based models to test-time few-shot tasks. Example output and analysis is provided in Appendix G. Qualitative Analysis We conduct a qualitative analysis of the feedback generated bySELF -REFINE and its subsequent refinements. We manually analyze 70 samples in total (35 success cases and 35 failure cases) for Code Optimization (Madaan et al., 2023) and Math Reasoning (Cobbe et al., 2021). For both Math Reasoning and Code Optimization, we found that the feedback was predominantly actionable, with the majority identifying problematic aspects of the original generation and suggesting ways to rectify them. When SELF -REFINE failed to improve the original generation, the majority of issues were due to erroneous feedback rather than faulty refinements. Specifically, 33% of unsuccessful cases were due to feedback inaccurately pinpointing the error’s location, while 61% were a result of feedback suggesting an inappropriate fix. Only 6% of failures were due to the refiner incorrectly implementing good feedback. These observations highlight the vital role of accurate feedback plays in SELF -REFINE . In successful cases, the refiner was guided by accurate and useful feedback to make precise fixes to the original generation in 61% of the cases. Interestingly, the refiner was capable of rectifying issues even when the feedback was partially incorrect, which was the situation in 33% of successful cases. This suggests resilience to sub-optimal feedback. Future research could focus on examining the refiner’s robustness to various types of feedback errors and exploring ways to enhance this resilience. In Figure 5, we illustrate how SELF -REFINE significantly improves program efficiency by transforming a brute force approach into a dynamic programming solution, as a result of insightful feedback. Additional analysis on other datasets such as Dialogue Response Generation is provided in Appendix H. Going Beyond Benchmarks While our evaluation focuses on benchmark tasks, SELF -REFINE is designed with broader applicability in mind. We explore this in a real-world use case of website gen- eration, where the user provides a high-level goal and SELF -REFINE assists in iteratively developing the website. Starting from a rudimentary initial design, SELF -REFINE refines HTML, CSS, and JS to evolve the website in terms of both usability and aesthetics. This demonstrates the potential of SELF -REFINE in real-world, complex, and creative tasks. See Appendix I for examples and further discussion, including broader, societal impact of our work. 5 Related work Leveraging human- and machine-generated natural language (NL) feedback for refining outputs has been effective for a variety of tasks, including summarization (Scheurer et al., 2022), script generation (Tandon et al., 2021), program synthesis (Le et al., 2022a; Yasunaga and Liang, 2020), and other tasks (Bai et al., 2022a; Schick et al., 2022b; Saunders et al., 2022a; Bai et al., 2022b; Welleck et al., 2022). Refinement methods differ in the source and format of feedback, and the way that a refiner is obtained. Table 3 summarizes some related approaches; see Appendix B for an additional discussion. Source of feedback. Humans have been an effective source of feedback (Tandon et al., 2021; Elgohary et al., 2021; Tandon et al., 2022; Bai et al., 2022a). Since human feedback is costly, several approaches use a scalar reward function as a surrogate of (or alternative to) human feedback (e.g., (Bai et al., 2022a; Liu et al., 2022; Lu et al., 2022; Le et al., 2022a; Welleck et al., 2022)). Alternative sources such as compilers (Yasunaga and Liang, 2020) or Wikipedia edits (Schick et al., 2022b) can provide domain-specific feedback. Recently, LLMs have been used to generate feedback for general domains (Fu et al., 2023; Peng et al., 2023; Yang et al., 2022), However, ours is the only method that generates feedback using an LLM on its own output, for the purpose of refining with the same LLM. Representation of feedback. The form of feedback can be generally divided into natural language (NL) and non-NL feedback. Non-NL feedback can come in human-provided example pairs (Dasgupta 8Supervision- free refiner Supervision- free feedback Multi-aspect feedback Iterative Learned refiners : PEER (Schick et al., 2022b), Self-critique (Saunders et al., 2022b), CodeRL (Le et al., 2022b), Self-correction (Welleck et al., 2022). or  or Prompted refiners: Augmenter (Peng et al., 2023), Re 3 (Yang et al., 2022), Reflexion (Shinn et al., 2023). or SELF -REFINE (this work) Table 3: A comparison of SELF -REFINE to closely related prior refinement approaches. et al., 2019) or scalar rewards (Liu et al., 2022; Le et al., 2022b). In this work, we use NL feedback, since this allows the model to easily provide self -feedback using the same LM that generated the output, while leveraging existing pretrained LLMs such as GPT-4. Types of refiners. Pairs of feedback and refinement have been used to learn supervised refiners (Schick et al., 2022b; Du et al., 2022; Yasunaga and Liang, 2020; Madaan et al., 2021). Since gathering supervised data is costly, some methods learn refiners using model generations (Welleck et al., 2022; Peng et al., 2023). However, the refiners are trained for each new domain. Finally, (Yang et al., 2022) use prompted feedback and refinement specifically tailored for story generation. In this work, we avoid training a separate refiner, and show that the same model can be used as both the refiner and the source of feedback across multiple domains. Non-refinement reinforcement learning (RL) approaches. Rather than having explicit refinement, an alternative way to incorporate feedback is by optimizing a scalar reward function, e.g. with reinforcement learning (e.g., Stiennon et al. (2020); Lu et al. (2022); Le et al. (2022a)). These methods differ from SELF -REFINE in that the model does not access feedback on an intermediate generation. Second, these RL methods require updating the model’s parameters, unlikeSELF -REFINE . 6 Limitations and Discussion The main limitation of our approach is that the base models need to have sufficient few-shot modeling or instruction-following abilities, in order to learn to provide feedback and to refine in an in-context fashion, without having to train supervised models and rely on supervised data. Further, the experiments in this work were performed with language models that are not open-sourced, namely GPT-3.5 , ChatGPT, GPT-4 , and CODEX . Existing literature (Ouyang et al., 2022) does not fully describe the details of these models, such as the pretraining corpus, model sizes, and model biases. Further, these models are not free to use, and using them for research requires some funding. Nonetheless, we release our code and model outputs to ensure the reproducibility of our work. Another limitation of our work is that we exclusively experiment with datasets in English. In other languages, the current models may not provide the same benefits. Finally, there is a possibility for bad actors to use prompting techniques to steer a model to generate more toxic or harmful text. Our approach does not explicitly guard against this. 7 Conclusion We present SELF -REFINE : a novel approach that allows large language models to iteratively provide self-feedback and refine their own outputs. SELF -REFINE operates within a single LLM , requiring neither additional training data nor reinforcement learning. We demonstrate the simplicity and ease of use of SELF -REFINE across a wide variety of tasks. By showcasing the potential of SELF -REFINE in diverse tasks, our research contributes to the ongoing exploration and development of large language models, with the aim of reducing the cost of human creative processes in real-world settings. We 9hope that our iterative approach will help drive further research in this area. To this end, we make all our code, data and prompts anonymously available at https://selfrefine.info/. References Teresa M. Amabile. 1983. A Theoretical Framework. In The Social Psychology of Creativity, pages 65–96. Springer New York, New York, NY . Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. ArXiv:2204.05862. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022b. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073. Emery D Berger, Sam Stern, and Juan Altmayer Pizzorno. 2022. Triangulating Python Performance Issues with SCALENE. ArXiv preprint, abs/2212.07597. Lawrence D Brown, T Tony Cai, and Anirban DasGupta. 2001. Interval estimation for a binomial proportion. Statistical science, 16(2):101–133. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877–1901, Online. Curran Associates, Inc. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Sanjoy Dasgupta, Daniel Hsu, Stefanos Poulis, and Xiaojin Zhu. 2019. Teaching a black-box learner. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 1547–1555. PMLR. Wanyu Du, Zae Myung Kim, Vipul Raheja, Dhruv Kumar, and Dongyeop Kang. 2022. Read, revise, repeat: A system demonstration for human-in-the-loop iterative text revision. In Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022), pages 96–108, Dublin, Ireland. Association for Computational Linguistics. 10Ahmed Elgohary, Christopher Meek, Matthew Richardson, Adam Fourney, Gonzalo Ramos, and Ahmed Hassan Awadallah. 2021. NL-EDIT: Correcting semantic parse errors through natural language interaction. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5599–5610, Online. Association for Computational Linguistics. Linda Flower and John R Hayes. 1981. A cognitive process theory of writing. College composition and communication, 32(4):365–387. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435. Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A dialogue model for academic research. Blog post. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven C. H. Hoi. 2022a. CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learn- ing. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven C. H. Hoi. 2022b. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. ArXiv, abs/2207.01780. Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, retrieve, generate: a simple approach to sentiment and style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1865–1874, New Orleans, Louisiana. Association for Computational Linguistics. Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2020. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1823–1840, Online. Association for Computational Linguistics. Jiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean Welleck, Hannaneh Hajishirzi, and Yejin Choi. 2022. Rainier: Reinforced knowledge introspector for commonsense question answering. In Conference on Empirical Methods in Natural Language Processing. Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Am- manabrolu, and Yejin Choi. 2022. Quark: Controllable text generation with reinforced unlearning. ArXiv, abs/2205.13636. Aman Madaan, Alexander Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming Yang, Graham Neubig, and Amir Yazdanbakhsh. 2023. Learning performance-improving code edits. arXiv preprint arXiv:2302.07867. Aman Madaan, Niket Tandon, Dheeraj Rajagopal, Peter Clark, Yiming Yang, and Eduard Hovy. 2021. Think about it! improving defeasible reasoning by first modeling the question scenario. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6291–6310, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Shikib Mehri and Maxine Eskenazi. 2020. Unsupervised evaluation of interactive dialog with DialoGPT. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 225–235, 1st virtual meeting. Association for Computational Linguistics. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program synthesis. ArXiv preprint, abs/2203.13474. OpenAI. Model index for researchers. https://platform.openai.com/docs/ model-index-for-researchers . Accessed: May 14, 2023. 11OpenAI. 2022. Model index for researchers. Blogpost. OpenAI. 2023. Gpt-4 technical report. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv:2203.02155. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback. Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov, and Alan W Black. 2018. Style transfer through back-translation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 866–876, Melbourne, Australia. Association for Computational Linguistics. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measur- ing and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350. Ruchir Puri, David Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladmir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, and Frederick Reiss. 2021. Codenet: A large-scale ai for code dataset for learning a diversity of coding tasks. arXiv preprint arXiv:2105.12655. Machel Reid and Graham Neubig. 2022. Learning to model editing processes. arXiv preprint arXiv:2205.12374. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022a. Self-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022b. Self-critiquing models for assisting human evaluators. ArXiv:2206.05802. Jérémy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. 2022. Training language models with natural language feedback. ArXiv:2204.14146. Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022a. Peer: A collaborative language model. Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022b. Peer: A collaborative language model. ArXiv, abs/2208.11663. Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an autonomous agent with dynamic memory and self-reflection. Herbert A. Simon. 1962. The architecture of complexity. Proceedings of the American Philosophical Society, 106(6):467–482. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems, volume 33, pages 3008–3021. Curran Associates, Inc. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. 2023. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047. Niket Tandon, Aman Madaan, Peter Clark, Keisuke Sakaguchi, and Yiming Yang. 2021. Interscript: A dataset for interactive learning of scripts through error feedback. arXiv preprint arXiv:2112.07867. 12Niket Tandon, Aman Madaan, Peter Clark, and Yiming Yang. 2022. Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 339–352. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. arXiv preprint arXiv:2201.11903. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2022. Generating sequences by learning to self-correct. arXiv preprint arXiv:2211.00053. Kevin Yang, Nanyun Peng, Yuandong Tian, and Dan Klein. 2022. Re3: Generating longer stories with recursive reprompting and revision. In Conference on Empirical Methods in Natural Language Processing. Michihiro Yasunaga and Percy Liang. 2020. Graph-based, self-supervised program repair from diagnostic feedback. 37th Int. Conf. Mach. Learn. ICML 2020, PartF168147-14:10730–10739. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28. 13A Evaluation Tasks Table 4 lists the tasks in our evaluation, and examples from each task. Task and Description Sample one iteration of FEEDBACK -REFINE Sentiment Reversal Rewrite reviews to reverse sentiment. Dataset: (Zhang et al., 2015) 1000 review pas- sages x: The food was fantastic...” yt: The food was disappointing...” fb: Increase negative sentiment yt+1: The food was utterly terrible...” Dialogue Response Generation Produce rich conversational responses. Dataset: (Mehri and Eskenazi, 2020) 372 conv. x: What’s the best way to cook pasta?” yt: The best way to cook pasta is to...” fb: Make response relevant, engaging, safe yt+1: Boil water, add salt, and cook pasta...” Code Optimization Enhance Python code efficiency Dataset: (Madaan et al., 2023): 1000 programs x: Nested loop for matrix product yt: NumPy dot product function fb: Improve time complexity yt+1: Use NumPy’s optimized matmul function Code Readability Improvement Refactor Python code for readability. Dataset: (Puri et al., 2021) 300 programs∗ x: Unclear variable names, no comments yt: Descriptive names, comments fb: Enhance variable naming; add comments yt+1: Clear variables, meaningful comments Math Reasoning Solve math reasoning problems. Dataset: (Cobbe et al., 2021) 1319 questions x: Olivia has $23, buys 5 bagels at $3 each” yt: Solution in Python fb: Show step-by-step solution yt+1: Solution with detailed explanation Acronym Generation Generate acronyms for a given title Dataset: (Appendix Q) 250 acronyms x : Radio Detecting and Ranging” yt: RDR fb : be context relevant; easy pronunciation yt+1: RADAR” Constrained Generation Generate sentences with given keywords. Dataset: (Lin et al., 2020) 200 samples x: beach, vacation, relaxation yt: During our beach vacation... fb: Include keywords; maintain coherence yt+1: .. beach vacation was filled with relaxation Table 4: An overview of the tasks which we evaluate SELF -REFINE on, along with their associated datasets and sizes. For every task, we demonstrate a single iteration of refinement of input x, the previously generated output yt, the feedback generated fbt, and the refinement yt+1. Few-shot prompts used for FEEDBACK and REFINE are provided in Appendix S. 14B Broader Related Work Compared to a concurrent work, Reflexion (Shinn et al., 2023), our approach involves correction using feedback, whereas their setup involves finding the next best solution in planning using ReAct. While ReAct and Reflexion provide a free-form reflection on whether a step was executed correctly and potential improvements, our approach is more granular and structured, with multi-dimensional feedback and scores. This distinction allows our method to offer more precise and actionable feedback, making it suitable for a wider range of natural language generation tasks, including those that may not necessarily involve step-by-step planning such as open-ended dialogue generation. Comparison with Welleck et al. (2022) The closest work to ours may be Self-Correction (Welleck et al., 2022); however, Self-Correction has several disadvantages compared to SELF -REFINE : 1. Self-Correction does not train their model to generate explicit feedback; instead, Welleck et al. (2022) trained their models to refine only. As we show in Section 4 and Table 2, having the model generate explicit feedback results in significantly better refined outputs. 2. Self-Correction trains a separate refiner (or “corrector”) for each task. In contrast, SELF - REFINE uses instructions and few-shot prompting, and thus does not require training a separate refiner for each task. 3. Empirically, we evaluated SELF -REFINE using the same base model of GPT-3 as Self- Correction, and with the same settings on the GSM8K benchmark. Self-Correction achieved 45.9% accuracy while SELF -REFINE (this work) achieved 55.7% (↑9.8). Comparison with non-refinement reinforcement learning (RL) approaches. Rather than having an explicit refinement module, an alternative way to incorporate feedback is by optimizing a scalar reward function, e.g. with reinforcement learning (e.g., Stiennon et al. (2020); Lu et al. (2022); Le et al. (2022a)). These methods differ from SELF -REFINE (and more generally, refinement-based approaches) in that the model cannot access feedback on an intermediate generation. Second, these reinforcement learning methods require updating the model’s parameters, unlike SELF -REFINE . See Table 5 for an additional detailed comparison of related work. Method Primary Novelty zero/few shot improvement multi aspect critics NL feedback with er- ror localization iterative framework RLHF (Stiennon et al., 2020) optimize for human preference trained on feedback  single (human)  (not self gen.) Rainier RL (Liu et al., 2022) RL to generate knowledge  trained on end task  single(accuracy)  (knowl. only) QUARKRL (Lu et al., 2022) quantization to edit generations trained on end task  single(scalar score)  (dense signal)  (train time iter.) Code RL (Le et al., 2022a) actor critic RL for code im- provement trained on end task  single(unit tests)  (dense signal) DrRepair (Yasunaga and Liang, 2020)Compiler feedback to itera- tively repair trained semi sup.  single(compiler msg)  (not self gen.) PEER (Schick et al., 2022b) doc. edit trained on wiki edits  trained on edits  single(accuracy)  (not self gen.) Self critique (Saunders et al., 2022a) few shot critique generation feedback training  single(human)  (self gen.) Self-correct (Welleck et al., 2022) novel training of a corrector trained on end task  single (task specific)  (limited setting)  (limited setting) Const. AI (Bai et al., 2022b) train RL4F on automat (cri- tique, revision) pair critique training  (fixed set) Self-ask (Press et al., 2022) ask followup ques when in- terim ans correct;final wrong few shot  none  (none) GPT3 score (Fu et al., 2023) GPT can score generations with instruction few shot  single(single utility fn)  (none) Augmenter (Peng et al., 2023) factuality feedback from exter- nal KBs few shot  single(factuality)  (self gen.) Re3(Yang et al., 2022) ∼ours: but one domain, trained critics few shot  (trained critics)  (not self gen.) SELF-REFINE fewshot iterative multi aspect NL fb few shot  multiple(few shot critics) (self gen.) Table 5: Summary of related approaches. Reinforcement learning approaches are shown in purple , trained corrector approaches are shown in orange , and few-shot corrector approaches are shown in green . 15C Human Evaluation The A/B evaluation in our study was conducted by the authors, where a human judge was presented with an input, task instruction, and two candidate outputs generated by the baseline method and SELF -REFINE . The setup was blind, i.e., the judges did not know which outputs were generated by which method. The judge was then asked to select the output that is better aligned with the task instruction. For tasks that involve A/B evaluation, we calculate the relative improvement as the percentage increase in preference rate. The preference rate represents the proportion of times annotators selected the output produced by SELF -REFINE over the output from the baseline method. Table 6 shows the results. Task S ELF -REFINE (%) Direct (%) Either (%) Sentiment Transfer 75.00 21.43 3.57 Acronym Generation 44.59 12.16 43.24 Response Generation 47.58 19.66 32.76 Table 6: Relative improvement of SELF -REFINE in A/B evaluations across different tasks. The values represent normalized preferences, which correspond to the proportion of times the output generated by SELF -REFINE was selected as better aligned with the task instruction over the baseline method. The evaluation was conducted for 150 examples for each dataset. The judges were not aware of the method that generated each sample. 16D GPT-4 Evaluation In light of the impressive achievements of GPT-4 in assessing and providing reasoning for complex tasks, we leverage its abilities for evaluation in SELF -REFINE . The approach involves presenting tasks to GPT-4 in a structured manner, promoting the model’s deliberation on the task and generating a rationale for its decision. This methodology is demonstrated in Listings 1 to 3: Listing 1 Prompt for GPT-4 evaluation of Sentiment Reversal. f\"\"\"Which review is aligned with the sentiment {target_sentiment}? Review A: {review_a} Review B: {review_b}. Pick your answer from ['Review A', 'Review B', 'both', 'neither']. Generate a short explanation for your choice first. Then, generate 'The more aligned review is A' or 'The more aligned review is B' or 'The more aligned review is both' or 'The more aligned review is neither'. ,→ ,→ ,→ Format: <explanation> <answer> STOP Listing 2 Prompt for GPT-4 evaluation of Acronym Generation. f\"\"\"Title: {title} Acronym A: {acronym_a} Acronym B: {acronym_b} Pick the better acronym for the given title. The acronyms should be compared based on the following criteria:,→ * Ease of pronunciation. * Ease of spelling. * Relation to title. * Positive connotation. Generate your answer in the following format: <Short explanation>. The better acronym is A OR The better acronym is B OR The acronyms are equally good OR Neither acronym is good. STOP.,→ Listing 3 Prompt for GPT-4 evaluation of Dialogue Response Generation. f\"\"\"Which response is better given this context: {context}? Response A: {response_a} Response B: {response_b}. Pick your answer from ['Response A', 'Response B', 'both', 'neither']. Generate a short explanation for your choice first. Then, generate 'The better response is A' or 'The better response is B' or 'The better response is both' or 'The better response is neither'. ,→ ,→ ,→ Format: <explanation> <answer> STOP E Model Key We use terminology here: https://platform.openai.com/docs/models/gpt-3-5 17F Comparison of S ELF -REFINE with State-of-the-art of Few-Shot Learning Models and Fine-Tuned Baselines In this section, we present a comprehensive comparison of the performance of SELF -REFINE with other few-shot models and fine-tuned baselines across a range of tasks, including mathematical reasoning and programming tasks. Tables 8 and 7 display the performance of these models on the PIE dataset and GSM tasks, respectively. Our analysis demonstrates the effectiveness of different model architectures and training techniques in tackling complex problems. Method Solve Rate Cobbe et al. (2021) OpenAI 6B 20.0 Wei et al. (2022) CoT w/ C ODEX 65.6 Gao et al. (2022) PaL w/ CODEX 72.0 PaL w/ GPT-3 52.0 PaL w/ GPT-3.5 56.8 PaL w/ ChatGPT 74.2 PaL w/ GPT-4 93.3 Welleck et al. (2022) Self-Correct w/ GPT-3 45.9 Self-Correct (fine-tuned) 24.3 This work SELF -REFINE w/ GPT-3 55.7 SELF -REFINE w/ GPT-3.5 62.4 SELF -REFINE w/ ChatGPT 75.1 SELF -REFINE w/ GPT-4 94.5 Table 7: Performance comparison of models on math reasoning (Math Reasoning). 18Method %OPT) Puri et al. (2021) Human References 38.2 OpenAI Models: OpenAI (2022, 2023) CODEX 13.1 GPT-3.5 14.8 ChatGPT 22.2 GPT-4 27.3 Nijkamp et al. (2022) C ODE GEN-16B 1.1 Berger et al. (2022) SCALENE 1.4 SCALENE (BEST @16) 12.6 SCALENE (BEST @32) 19.6 Madaan et al. (2023) PIE -2B 4.4 PIE -2B (BEST @16) 21.1 PIE -2B (BEST @32) 26.3 PIE -16B 4.4 PIE -16B (BEST @16) 22.4 PIE -16B (BEST @32) 26.6 PIE -Few-shot (BEST @16) 35.2 PIE -Few-shot (BEST @32) 38.3 This work SELF -REFINE w/ GPT-3.5 23.0 SELF -REFINE w/ ChatGPT 26.7 SELF -REFINE w/ GPT-4 36.0 Table 8: Performance comparison of various models on the PIE dataset in terms of the percentage of programs optimized (%OPT). The table includes human references, baseline models, fine-tuned PIE -2B and PIE -16B models, and our proposed model (SELF -REFINE ) using different LLMs. Notably, SELF -REFINE achieves superior performance while using only 4 samples at most, significantly fewer than the 16 and 32 samples employed by other models. Scalene, an off-the-shelf optimizer, uses instruction tuning with Codex and serves as a comparison point. 19G Evaluation of Vicuna-13b We also experiment with Vicuna-13b (Chiang et al., 2023), a version of LLaMA-13b (Touvron et al., 2023) fine-tuned on conversations sourced from the web. Vicuna-13b was able to consistently follow the task initialization prompt. However, it struggled to follow the prompts intended for feedback and refinement. This often led to outputs that resembled assistant-like responses, a representative example of which can be found in Appendix G. It’s important to note that we used the same prompts for Vicuna-13b as those used with other models in our study. However, the limited performance of Vicuna-13b suggests that this model may require more extensive prompt-engineering for optimal performance. Mixed-refine: Improving Vicuna-13b with ChatGPT While the focus of SELF -REFINE is improvement of the model without any external help, it may be possible to use a smaller model for the initialization, and then involving a bigger model for refinement. To test this, we experiment with a setup where we use Vicuna-13b as the initialization model, and use ChatGPT as the FEEDBACK and REFINE . The results on Math Reasoning show the promise of this approach: while Vicuna-13b was able to get only 24.18% on Math Reasoning, it was able to improve to 40.5% in this mixed-refinement setting. 20Step 0 INIT> The food was amazing, I loved it!!. Step 0 TRANSFER> The food was terrible, I hated it!!. list index out of range An error occurred: list index out of range. Traceback (most recent call last): IndexError: list index out of range . Left retries: 2. Step 1 TRANSFER> The food at the Trop was awful. I wish I could've gotten my money back for the terrible meals.,→ Step 1 FB> The rewritten review does not become more \"Very negative\", because it only contains a single instance of a positive sentiment about the food. Try again to make it Very negative! ,→ ,→ Step 2 TRANSFER> ### Assistant: The Trop is a great choice for anyone looking for a budget-friendly option in Vegas. The rooms and hallways are clean and comfortable, and the food options are varied and delicious. ,→ ,→ Step 2 FB> The rewritten review does not change from \"Positive\", because it only has positive language. The output is already Very positive. No further changes are needed. ,→ ,→ Step 0 INIT> The food was amazing, I loved it!!. [Positive] -> [Very negative] Step 0 TRANSFER> The food was atrocious, I absolutely hated it! [('The review sounds very negative. The sentiment is Very negative', 1.0)],→ Step 0 FB> The original review is already \"Very negative\" as it uses strongly negative words like \"atrocious\" and \"absolutely hated\". These words express extreme dissatisfaction and clearly convey a very negative sentiment about the food. But try again to make it even more negative! ,→ ,→ ,→ Step 1 TRANSFER> The food was so abysmal, it was a nightmare to even taste it! I couldn't stand it and regretted every single bite. [('The review sounds highly negative. The sentiment is Very negative', 1.0)] ,→ ,→ Step 1 FB> The review provided is indeed very negative. It uses extremely negative words and phrases like \"abysmal\", \"nightmare\", and \"regretted every single bite\". These strongly negative expressions make the review very negative, but try again to make it even more negative! ,→ ,→ ,→ Step 2 TRANSFER> The food was absolutely revolting, I couldn't even stomach a single bite!,→ Step 2 FB> This review is not \"Very negative\" because it contains positive words and phrases like \"amazing\" and \"loved it.\" To make it \"Very negative\", we need to use extremely negative words, like \"revolting\", \"couldn't stomach\", or \"horrible\". The provided \"Very negative\" sentiment example is a good illustration of a very negative review, as it uses strong negative language to describe the food experience. ,→ ,→ ,→ ,→ ,→ Vicuna-13b outputs (top) for sentiment transfer, where the goal is to make ‘The food was amazing, I loved it!!.’ negative. Vicuna-13b generates a good initial output, but generates an empty feedback in the first try, leading to an error. In the next try, it generates an unhelpful feedback. The transferred review copies from the prompt. Compare with the outputs from GPT-4 (bottom). 210 10 20 30 40 50 60 70 80 90 100 SELF-REFINE SELF-REFINE 27.2 15.5 35.6 51.1 37.2 33.3 Preference rates for Sentiment Reversal MULTI ChatGPT27.2 15.5 35.6 51.1 37.2 33.3 0 10 20 30 40 50 60 70 80 90 100 SELF-REFINE SELF-REFINE 11.4 6.1 45.4 53.82 43.2 40.05 Preference rates for Acronym Generation ChatGPT MULTI 11.4 6.1 45.4 53.82 43.2 40.05 Figure 6: Preference for the outputs generated by our method (SELF -REFINE ), the multiple-sample baseline (MULTI), and ties (ties). GPT-3.5 ChatGPT GPT-4 Task Base +SELF -REFINE Base +SELF -REFINE Base +SELF -REFINE Math Reasoning 64.1 64.1 (0) 74.8 75.0 (↑0.2) 92.9 93.1 (↑0.2) Math Reasoning (Oracle) 64.06 68.9 (↑4.8) 74.8 76.2 (↑1.4) 92.9 93.8 (↑0.7) Table 9: SELF -REFINE results on Math Reasoning using GPT-3.5 , ChatGPT, and GPT-4 as base LLM with Oracle feedback. H Additional Analysis H.1 Using Oracle Feedback We experimented withOracle Feedbackfollowing Welleck et al. (2022). This method uses correctness information to guide model refinement, only progressing to REFINE stage if the current answer is incorrect. This adjustment notably enhanced performance in the Math Reasoning task, with GPT-3 improving by 4.8% and GPT-4 by 0.7% Table 9. This indicates the potential of external signals to optimize model performance in particular tasks. Iteration Acronym Pronunciation Pron. (5) Spell. (5) Rel. (5) Pos. Con. (5) Total (25) 1 USTACCSF us-tacks-eff 1 1 5 3 11 2 TACC-SIM tacks-sim 4 4 5 3 17 3 TACCSF tacks-eff 1 2 5 3 12 4 TACC-SIMF tack-simf 4 4 5 3 17 Table 10: Acronym generation results across iterations, showcasing how improvements in certain as- pects (e.g., pronunciation and spelling) can be accompanied by losses in others, leading to fluctuating overall performance in multi-aspect feedback tasks like Acronym Generation. Non-monotonic increase in output quality for acronym generation For tasks with multi-aspect feedback like Acronym Generation, the output quality can fluctuate during the iterative process, improving on one aspect while losing out on another (Table 10). To address this, SELF -REFINE ’s feedback generates explicit numerical scores to capture the different aspects of output quality. This allows for a more balanced evaluation of outputs and the selection of the most appropriate one. The algorithm selects the best output based on the maximum score across all iterations, as described in Algorithm 1 (line 8). A similar selection is possible for other tasks like Math Reasoning and Sentiment Reversal, while we observe that output quality increases monotonically with iterations. 22Error Type Occurrence Feedback Refinement Incorrect feedback 25% Informative: The response provides some encourage- ment and reassurance, and additional information. 3/3 Response: That’s hilarious! Don’t worry, I’m sure you’ll do much better when you take History next semester. Are you looking forward to it? Generic feedback 30% Specific: The response is not specific and does not provide any details or ex- amples. 1/3 Response: It’s understandable, I’m sure you’ll do much bet- ter when you take History next semester. Maybe you can use your knowledge of Christo- pher Columbus to ace your next test! Are you looking for- ward to it? Incorrect Scoring 10% Informative: The response provides a supportive and understanding message, which is informative to the user. 2/3 Response: I’m sorry to hear. . . when Christopher Columbus didn’t even fight in the Civil War! I’m sure you’ll do much better. . . . Are you looking for- ward to it? Table 11: Error analysis for Dialogue Response Generation: When the feedback is not useful, a large majority is not specific or incorrect. Error Type Occurrence Feedback Refinement Not-Robust 10% Specific: The response is specific and provides spe- cific details or examples. 3/3 Response: That’s pretty funny! It’s understand- able that you didn’t do well on the test. I’m sure you’ll do much bet- ter when you take History next semester Ignores feedback 25% Informative: The re- sponse doesn’t provide any new information. 1/3 Response: That’s so kind and generous of you! I would love to hear more about the podcasts you enjoy. Do you have any favorites you can recom- mend? Introduces new problem 20% Informative: 3/3 Response: That would be a disaster! Luckily, my coworkers know bet- ter than to drink while they work. New Informa- tive score: 2/3 Robust to bad feedback 60% Informative: The re- sponse does not provide any information. 1/3 Response: Wow, 25 peo- ple! That must have been an amazing experience. Can you tell me more about why that particular trip to Australia was so special for you? Table 12: On the Dialogue Response Generation task, SELF -REFINE can ignore good feedback but in a majority of cases, it is robust to bad feedback and ignores bad feedback. 23Feedback and Refinement Error Analysis for Response Generation We perform a detailed error analysis of SELF -REFINE feedback and refinement process for Dialogue Response Generation, which we summarize in Tables Table 11 and Table 12. Table 11 reports the occurrence of different types of errors in our sample, which includes Incorrect Feedback (25%), Generic Feedback (30%), and Incorrect Scoring (10%). We provide representative examples of the system’s responses and refinements for each error type. These errors highlight potential areas for improving our feedback handling mechanism, particularly in the interpretation and understanding of user inputs. Table 12 breaks down errors found in the refinement stage ofSELF -REFINE . Errors include: not being robust (10%), ignoring feedback (25%), and introducing a new problem (20%). We demonstrate how the model handles a variety of feedback types, how robust it is under different circumstances, and how often it inadvertently introduces new issues. 60% of the times, the model is robust to incorrect or generic feedback. These insights can guide us in enhancing the model’s refinement capabilities, especially in providing accurate and specific responses. 24I Beyond Benchmarks SELF -REFINE demonstrates its iterative feedback and refinement capabilities in the context of website layout generation. ChatGPT initially produces a rudimentary layout for a given topic, and then uses the FEEDBACK to suggest specific, actionable improvements, as demonstrated in Figures 7 and 9. These suggestions range from design changes such as color and font adjustments, to content enhancements and layout modifications. Figures 8 and 10 showcase the final layouts, post-feedback implementation, highlighting the potential and versatility of SELF -REFINE across different scenarios. Figure 7: Initial web layout generated by our model for a fictional ice cream parlor. Ice Cream Generation The feedback generated by FEEDBACK for ice cream generation: • Change the background color of the container to a light blue color (#6f2ff). • Change the font size of the heading to 48px. • Add a small icon before the \"Welcome to our ice cream parlor!\" text using the URL https://cdn- icons-png.flaticon.com/512/3622/3622340.png. • Add an additional paragraph after the existing text with the following text: \"We also offer a variety of toppings and cones to complement your ice cream. Visit us today to try our latest flavors and indulge in a sweet treat!\" • Increase the font size of the button text to 24px. • Update the button color to #9933. Photosynthesis The feedback generated by FEEDBACK for photosynthesis: • Increase the font size of the text to 18px for better readability. • Add more information about the benefits of photosynthesis. • Remove the unnecessary margin-top from the header. • Add a ruler or divider below the header to separate it from the image. 25Figure 8: Refined web layout after applying model feedback. The feedback included changing the background color to light blue (#6f2ff), increasing the heading font size to 48px, adding an icon before the welcome text, enhancing the content with an additional paragraph, increasing the button text size to 24px, and updating the button color to #9933. Figure 9: Initial web layout generated by our model for a page on photosynthesis. 26Figure 10: Refined web layout after applying model feedback. The feedback included increasing the text font size to 18px for better readability, adding more information about the benefits of photosynthesis, removing the unnecessary margin-top from the header, and adding a ruler or divider below the header to separate it from the image. 27J Statistical Confidence Intervals GPT-3.5 ChatGPT GPT-4 Task Base +S ELF -REFINE Base +S ELF -REFINE Base +S ELF -REFINE Sentiment Reversal 8.8 ± 2.05 30.4 ± 3.61∗ 11.4 ± 2.34 43.2 ± 3.98∗ 3.8 ± 1.28 36.2 ± 3.82∗ Dialogue Response 36.4 ± 6.14 63.6 ± 6.62∗ 40.1 ± 6.33 59.9 ± 6.67∗ 25.4 ± 5.36 74.6 ± 6.22∗ Code Optimization 14.8 ± 2.66 23.0 ± 3.25∗ 23.9 ± 3.30 27.5 ± 3.49 27.3 ± 3.48 36.0 ± 3.81∗ Code Readability 37.4 ± 6.86 51.3 ± 7.39 27.7 ± 6.13 63.1 ± 7.40∗ 27.4 ± 6.10 56.2 ± 7.45∗ Math Reasoning 64.1 ± 3.47 64.1 ± 3.47 74.8 ± 3.20 75.0 ± 3.20 92.9 ± 2.05 93.1 ± 2.03 Acronym Gen. 41.6 ± 7.72 56.4 ± 8.15 27.2 ± 6.60 37.2 ± 7.46 30.4 ± 6.92 56.0 ± 8.15∗ Constrained Gen. 28.0 ± 7.38 37.0 ± 8.26 44.0 ± 8.72 67.0 ± 9.00∗ 15.0 ± 5.38 45.0 ± 8.77∗ Table 13: SELF -REFINE results from table 1 with Wilson confidence interval (at 95% confidence interval) and statistical significance. On various tasks using GPT-3.5 , ChatGPT, and GPT-4 as base LLM , SELF -REFINE consistently improves LLM . Metrics used for these tasks are defined in Section 3.2 as follows: Math Reasoning uses the solve rate; Code Optimization uses the percentage of programs optimized; and Sentiment Reversal, Dialogue Response and Acronym Gen use a GPT- 4-based preference evaluation, which measures the percentage of times outputs from the base or enhanced models were selected, with the rest categorized as a tie. Constrained Gen uses the coverage percentage. Gains over Base, that are statistically significant based on these confidence intervals are marked * Table 13 shows results from Table 1 with Wilson confidence interval (Brown et al., 2001) (at α= 99% confidence interval) and statistical significance. Gains that are statistical significance based on these confidence intervals are marked with an asterisk. We find that nearly all of GPT-4 gains are statistically significant, ChatGPT gains are significant for 4 out of 7 datasets, and GPT-3.5 gains are significant for 3 out of 7 datasets. 28K New Tasks Constrained Generation We introduce “CommonGen-Hard,\" a more challenging extension of the CommonGen dataset (Lin et al., 2020), designed to test state-of-the-art language models’ advanced commonsense reasoning, contextual understanding, and creative problem-solving. CommonGen- Hard requires models to generate coherent sentences incorporating 20-30 concepts, rather than only the 3-5 related concepts given in CommonGen. SELF -REFINE focuses on iterative creation with introspective feedback, making it suitable for evaluating the effectiveness of language models on the CommonGen-Hard task. Acronym Generation Acronym generation requires an iterative refinement process to create concise and memorable representations of complex terms or phrases, involving tradeoffs between length, ease of pronunciation, and relevance, and thus serves as a natural testbed for our approach. We source a dataset of 250 acronyms4 and manually prune it to remove offensive or uninformative acronyms. L Code Readability Orthogonal to the correctness, readability is another important quality of a piece of code: though not related to the execution results of the code, code readability may significantly affect the usability, upgradability, and ease of maintenance of an entire codebase. In this section, we consider the problem of improving the readability of code with SELF -REFINE . We let an LLM write natural language readability critiques for a piece of code; the generated critiques then guide another LLM to improve the code’s readability. L.1 Method Following the SELF -REFINE setup, we instantiate INIT , FEEDBACK , and REFINE . The INIT is a no-op — we directly start by critiquing the code with FEEDBACK and applying the changes with REFINE . • FEEDBACK We prompt an LLM with the given code and an instruction to provide feedback on readability. We give the LLM the freedom to freely choose the type of enhancements and express them in the form of free text. • REFINE The code generator LLM is prompted with the piece of code and the readability improvement feedback provided by FEEDBACK . In addition, we also supply an instruction to fix the code using the feedback. We take the generation from the code generator as the product of one iteration in the feedback loop. Starting from an initial piece of code y0, we first critique, c1 = critique(y0), and then edit the code, y1 = editor(y0, c1). This is recursively performed N times, where ck+1 = critique(yk) and yk+1 = editor(yk, ck+1). L.2 Experiments Dataset We use the CodeNet (Puri et al., 2021) dataset of competitive programming. 5 For our purpose, these are hard-to-read multi-line code snippets. We consider a random subset of 300 examples and apply SELF -REFINE to them. We also ask human annotators to edit a 60-example subset to assess human performance on this task. The human annotators are asked to read the code piece and improve its readability. Implementation Both the critique and the editor models are based on the InstructGPT model (text- davinci-003). We consider the temperature of both T = 0 .0 (greedy) and T = 0 .7 (sampling) for decoding Natural Language suggestion from the critique model. We always use a temperature T = 0.0 (greedy) when decoding Programming Language from the code editor. Due to budget constraints, we run SELF -REFINE for N = 5 iterations. The exact prompts we use can be found in Figures 22-23. 4https://github.com/krishnakt031990/Crawl-Wiki-For-Acronyms/blob/master/AcronymsFile.csv 5https://github.com/IBM/Project_CodeNet 29Meaningful Variable Ratio Comment Per Line Function Units Human Annotator Rewrites 0.653 0.24 0.70 SELF -REFINE (T = 0.0) 0.628 0.12 1.41 SELF -REFINE (T = 0.7) 0.700 0.25 1.33 Table 14: Human v.s. SELF -REFINE performance on 60-example subset. We see SELF -REFINE can reach similar or achieve even better performance on the metrics compared to rewrites given by human annotator. Evaluation Methods We consider a few automatic heuristic-based evaluation metrics, • Meaningful Variable Names: In order to understand the flow of a program, having semanti- cally meaningful variable names can offer much useful information. We compute the ratio of meaningful variables, the number of distinct variables with meaningful names to the total number of distinct variables. We automate the process of extracting distinct variables and the meaningful subset of variables using a few-shot prompted language model. • Comments: Natural language comments give explicit hints on the intent of the code. We compute the average number of comment pieces per code line. • Function Units: Long functions are hard to parse. Seasoned programmers will often refactor and modularize code into smaller functional units. Result For each automatic evaluation metric, the ratio of meaningful variable, of comment, and the number of function units, we compute for each iteration averaged across all test examples and plot for each SELF -REFINE iteration in Figure 11(a), Figure 11(b) and Figure 11(c) respectively. The two curves each correspond to critique with temperature T = 0.0 and T = 0.7. The iteration 0 number is measured from the original input code piece from CodeNet. We observe the average of all three metrics grows across iteration of feedback loops. A diverse generation of a higher temperature in the critique leads to more edits to improve the meaningfulness of variable names and to add comments. The greedy critique, on the other hand, provides more suggestions on refactoring the code for modularization. Figure 12 provides an example of code-readability improving over iterations. In Table 14, we measure human performance on all three metrics and compare with SELF -REFINE last iteration output. At T = 0.7, SELF -REFINE produces more meaning variables, more function units and slightly more comments compared to the human annotators on average. At T = 0 .0, SELF -REFINE produces less meaningful variables, less comments per line but even more function units. 0 1 2 3 4 50 0.2 0.4 0.6 0.8 Iteration y T=0.0T=0.7 (a) Meaningful variable ratio across different SELF -REFINE iterations. 0 1 2 3 4 50 0.1 0.2 0.3 Iteration y T=0.0T=0.7 (b) Comment per line ratio across different SELF -REFINE iterations. 0 1 2 3 4 50 1 2 3 4 Iteration y T=0.0T=0.7 (c) Number of function units across different SELF -REFINE iterations. Figure 11: Evaluation on code readability task with SELF -REFINE across multiple metrics Example M Dialogue Response Generation Open-domain dialogue response generation is a complex task that requires a system to generate human-like responses to a wide range of topics. Due to the open-ended nature of the task, it is 30Starting Code: v0 print((int((int(eval(input()))+1)/2))) Code v1 print( (int( (int(eval(input())) + 1) / 2 ) ) Code v2 num_input = eval(input()) num_input = int(num_input) num_input += 1 num_result = int(num_input / 2) print(num_result) Figure 12: SELF -REFINE iterations over a piece of densely composed code. Through out the iterations, the model first try to indent out the code and ends up rewriting it into several lines of atomic operations. challenging to develop a system that can consistently generate coherent and engaging responses. In this section, we use SELF -REFINE for automatically generated feedback and applying iterative refinement to improve the quality of the responses. M.1 Modules We follow the high-level description of the framework from Section 2, and instantiate our framework as follows. Init This is the first step in performing the task. The init module takes the dialogue context as input and generates a response that follows the conversation. Feedback We design an FEEDBACK that can provide multifaceted feedback for the quality of the response generated. Specifically, a response is judged along 10 qualitative aspects discussed below. A more thorough review of such fine-grained dialogue quality aspects can be found in Mehri and Eskenazi (2020). We use 6 in-context examples for feedback generation. In many cases, the feedback explicitly points out the reasons why a response scores low on some qualitative aspect. We show an example in Figure 13. • Relevant Does the response addresses all important aspects of the context? • Informative - Does the response provide some information relevant to the context? • Interesting - Doe the response beyond providing a simple and predictable answer to a question or statement? • Consistent - Is the response consistent with the rest of the conversation in terms of tone and topic? • Helpful - Is the response helpful in providing any information or suggesting any actions? • Engaging - Is the response engaging and encourage further conversation? • Specific - The response contains specific content related to a topic or question, • Safe - Is the response safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information? • User understanding - Does the response demonstrate an understanding of the user’s input and state of mind? • Fluent Is the response fluent and easy to understand? 31Context: Speaker A - What are your hobbies? Speaker B - I really just spend the rest of my free time with those that are close to me. Speaker A - I m very much interested in playing table tennis. Response: That sounds like a lot of fun! I'm not very good at table tennis, but I'm sure it's a great way to socialize and stay active. Feedback: * Relevant: The response is relevant to the user's input and shows interest in the user's hobby. 3/3 * Informative: The response does not provide any information about table tennis or how to play it. 1/3 * Interesting: The response is not very interesting or intriguing. 1/3 * Consistent: The response is consistent with the tone and context of the conversation. 3/3 * Helpful: The response is not particularly helpful as it does not provide any information or suggest any actions. 1/3 * Engaging: The response is somewhat engaging. It acknowledges the user's interest in table tennis and encourages them to continue playing. 2/3 * Specific: The response is not specific, as it does not address the topic of table tennis in any particular way. 1/3 * Safe: The response is safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information. 3/3 * User understanding: The response shows some understanding of the user's input and acknowledges their interest in table tennis. 2/3 * Fluent: The response is fluent in terms of grammar and flow of words. 3/3 * Total score: 20/30 New Response: Table tennis is a great hobby! It's a great way to stay active and socialize with others. Have you ever tried it before? If not, I'd suggest starting with some beginner tutorials on Youtube. They are really helpful in getting a good grasp of the basics. Figure 13: SELF -REFINE prompts for dialogue response generation: INIT generates a first draft of the response generated in a few-shot manner. FEEDBACK contains demonstrations of responses and natural language feedback on several qualitative aspects of the response. REFINE takes the response and the feedback and refines it to match the feedback better. Iterate The iterate module takes a sequence of dialogue context, prior generated responses, and the feedback and refines the output to match the feedback better. An example of a context, response, feedback and a refined response is shown in Figure 13. M.2 Setup and Experiments Model and Baseline We establish a natural baseline for our approach by using the model directly, without any feedback, which we refer to as INIT . Our implementation of SELF -REFINE employs a few-shot setup, where each module (INIT , FEEDBACK , ITERATE ) is implemented as few-shot prompts, and we execute the self-improvement loop for a maximum k = 3 iterations. We provide 3 few-shot in-context examples for the INIT model, and instruct the model to produce a response that is good at the 10 aspects listed above. As in-context examples for FEEDBACK , we use the same 3 contexts and responses shown to the INIT model (including low-scoring variations of those responses), along with scores and explanations for each feedback aspect. The ITERATE model is also shown the same in-context examples, and it consists of contexts-response-feedback followed by a better version of the response. For SELF -REFINE , we chose the response that gets the highest total score from the FEEDBACK model across all iterations excluding the initial response. We use text-davinci-003 for all the experiments. 32GPT-3.5 ChatGPT GPT4 SELF -REFINE wins 36.0 48.0 54.0 INIT wins 23.0 18.0 16.0 Both are equal 41.0 50.0 30.0 Table 15: Human evaluation results for dialogue response generation Evaluation We perform experiments on the FED dataset (Mehri and Eskenazi, 2020). The FED dataset is a collection of human-system and human-human conversations annotated with eighteen fine-grained dialog qualities at both the turn and the dialogue-level. The dataset was created to evaluate interactive dialog systems without relying on reference responses or training data. We evaluate the quality of the generated outputs using both automated and human evaluation methods. For automatic evaluation in Table1, we used zero-shot prompting with text-davinci-003 and evaluate on a test set of 342 instances. We show the model the responses generated by SELF -REFINE and the baseline INIT and ask the model to select the better response in terms of the 10 qualities. We report the win rate. However, we acknowledge that automated metrics may not provide an accurate assessment of text generation tasks and rely on human evaluation instead. Given a dialogue context with a varying number of turns, we generate outputs from the above mentioned methods. For human evaluation, for 100 randomly selected test instances, we show annotators the 10 response quality aspects, responses from SELF -REFINE and INIT models and ask them to select the better response. They are also given the option to select “both” when it is hard to show preference toward one response. Results Automatic evaluation results are shown in Table1 and human evaluation results are are shown in Table 15. We experiment on 3 latest versions of GPT models. text-davinci-003 is capable of generating human-like responses of great quality for a wide range of dialogue contexts and hence GPT-3.5 is a strong baseline. Still, SELF -REFINE beats INIT by a wide margin on both automatic as well as human evaluation. Our manual analysis shows that outputs generated by SELF - REFINE are more engaging and interesting and generally more elaborate than the outputs generated by INIT . N Code Optimization Performance-Improving Code Edits or PIE (Madaan et al., 2023) focuses on enhancing the efficiency of functionally correct programs. The primary objective of PIE is to optimize a given program by implementing algorithmic modifications that lead to improved runtime performance. Given an optimization generated by PIE, SELF -REFINE first generates a natural language feedback on possible improvements Figure 20. Then, the feedback is fed to REFINE Figure 21 for refinement. Table 16: Main Results and Ablation Analysis Setup Iteration % Optimized Relative Speedup Speedup Direct - 9.7 62.29 3.09 SELF -REFINE − feedback 1 10.1 62.15 3.03 SELF -REFINE − feedback 2 10.4 61.79 3.01 SELF -REFINE 1 15.3 59.64 2.90 SELF -REFINE 2 15.6 65.60 3.74 Table 17: Performance comparison of SELF -REFINE and ablated variants for code optimization. The table highlights the effectiveness of SELF -REFINE in optimizing code through iterative feedback and improvement, outperforming both the direct method and the simplified feedback approach, which lacks the introspective feedback mechanism of SELF -REFINE . This demonstrates the value of our framework’s multi-faceted feedback in refining the generated code. 33O Math Reasoning We use the Grade School Math 8k (GSM-8k) dataset (Cobbe et al., 2021) for evaluatingSELF -REFINE on math reasoning. In the context of grade school mathematics, SELF -REFINE aims to enable LLMs to iteratively refine their mathematical problem-solving outputs based on introspective feedback. Following Gao et al. (2022), we write solutions to the reasoning problems in Python. Consider the following example from the paper, where an error in the code demonstrates a lack of understanding of the problem: def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result By using SELF -REFINE , we can identify the error in the code and refine the solution through an iterative process of introspection and feedback: # Let's go through the error and check step-by-step plates = 6 plate_cost = 6000 # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost # wrong! The cost of a cup is not the same as the cost of a plate.,→ # The cost of a cup is $1200 less than the total cost of half a dozen plates sold at $6000 each.,→ half_dozen_plate_cost = 6 * plate_cost cup_cost = half_dozen_plate_cost - 1200 SELF -REFINE is thus instantiated naturally: the generator generates an initial solution, andFEEDBACK scans the solution to spot errors on which to provide feedback. The feedback is supplied to REFINE to create a new solution. Following Welleck et al. (2022), we use the correct label to decide when to go from one point in the loop to the next. This label feedback can be used to decide when to go from one point in the iteration to the next. We show results using SELF -REFINE in Figure 14. 340 1 2 3 470 80 71.34% 73.39% 75.06% 75.74% 76.19% Iteration Accuracy (%) Solve rate of SELF -REFINE Over Iterations for GSM-8k Figure 14: Improvements in accuracy on the GSM-8k math reasoning benchmark as a function of the # of iterations of SELF -REFINE . P Sentiment Reversal We consider the task of long-form text style transfer, where given a passage (a few sentences) and an associated sentiment (positive or negative), the task is to re-write the passage to flip its sentiment (positive to negative or vice-versa). While a large body of work on style transfer is directed at sentence-level sentiment transfer (Li et al., 2018; Prabhumoye et al., 2018), we focus on transferring the sentiment of entire reviews, making the task challenging and providing opportunities for iterative improvements. Instantiating SELF -REFINE for sentiment reversal We instantiate SELF -REFINE for this task following the high-level description of the framework shared in Section 2. Recall that our requires three components: INIT to generate an initial output, FEEDBACK to generate feedback on the initial output, and REFINE for improving the output based on the feedback. SELF -REFINE is implemented in a complete few-shot setup, where each module (INIT , FEEDBACK , ITERATE ) is implemented as few-shot prompts. We execute the self-improvement loop for a maximum of k = 4 iterations. The iterations continue until the target sentiment is reached. P.1 Details Evaluation Given an input and a desired sentiment level, we generate outputs SELF -REFINE and the baselines. Then, we measure the % of times output from each setup was preferred to better align with the desired sentiment level (see Section 2 for more details). We also experiment with standard text-classification metric. That is, given a transferred review, we use an off-the-shelf text-classifier (Vader) to judge its sentiment level. We find that all methods were successful in generating an output that aligns with the target sentiment. For instance, when the target sentiment was positive, both GPT-3.5 with text-davinci-003 and SELF -REFINE generates sentences that have a positive sentiment (100% classification accuracy). With the negative target sentiment, the classification scores were 92% for GPT-3.5 and 93.6% for S ELF -REFINE . We conduct automated and human evaluation for measuring the preference rates for adhering to the desired sentiment, and how dramatic the generations are. For automated evaluation, we create few-shot examples for evaluating which of the two reviews is more positive and less boring. We use a separate prompt for each task. The examples are depicted in Figure 33 for initialization, Figure 34 for feedback generation, and Figure 35 for refinement. The prompts show examples of reviews of varying degrees of sentiment and colorfulness (more colorful reviews use extreme phrases — the 35food was really bad vs. I wouldn’t eat it if they pay me.). The model is then required to select one of the outputs as being more aligned with the sentiment and having a more exciting language. We report the preference rates: the % of times a variant was preferred by the model over the outputs generated by SELF -REFINE . Pin-pointed feedback A key contribution of our method is supplying chain-of-thought prompting style feedback. That is, the feedback not only indicates that the target sentiment has not reached, but further points out phrases and words in the review that should be altered to reach the desired sentiment level. We experiment with an ablation of our setup where the feedback module simply says “something is wrong.” In such cases, for sentiment evaluation, the output from SELF -REFINE were preferred 73% of the time (down from 85% with informative feedback). For dramatic response evaluation, we found that the preference rate went down drastically to 58.92%, from 80.09%. These results clearly indicate the importance of pin-pointed feedback. Evaluation We evaluate the task using GPT-4. Specifically, we use the following prompt: When both win, we add winning rate to either. Q Acronym Generation Good acronyms provide a concise and memorable way to communicate complex ideas, making them easier to understand and remember, ultimately leading to more efficient and effective communication. Like in email writing, acronym generation also requires an iterative refinement process to achieve a concise and memorable representation of a complex term or phrase. Acronyms often involve tradeoffs between length, ease of pronunciation, and relevance to the original term or phrase. Thus, acronym generation is a natural method testbed for our approach. We source the dataset for this task from https://github.com/krishnakt031990/ Crawl-Wiki-For-Acronyms/blob/master/AcronymsFile.csv , and prune the file manually to remove potentially offensive or completely uninformative acronyms. This exercise generated a list of 250 acronyms. The complete list is given in our code repository. FEEDBACK For feedback, we design an FEEDBACK that can provide multifaceted feedback. Specif- ically, each acronym is judged along five dimensions: • Ease of pronunciation: How easy or difficult is it to pronounce the acronym? Are there any difficult or awkward sounds or combinations of letters that could make it challenging to say out loud? • Ease of spelling: How easy or difficult is it to spell the acronym? Are there any unusual or uncommon letter combinations that could make it tricky to write or remember? • Relation to title: How closely does the acronym reflect the content or topic of the associated title, phrase, or concept? Is the acronym clearly related to the original term or does it seem unrelated or random? • Positive connotation: Does the acronym have any positive or negative associations or connotations? Does it sound upbeat, neutral, or negative in tone or meaning? • Well-known: How familiar or recognizable is the acronym to the target audience? Is it a common or widely-used term, or is it obscure or unfamiliar? Some of these criteria are difficult to quantify, and are a matter of human preference. As with other modules, we leverage the superior instruction following capabilities of modern LLM s to instead provide a few demonstrations of each task. Crucially, the feedback includes a chain of thought style reasoning — before generating the score for an acronym for a specific criteria, we generate a reasoning chain explicitly stating the reason for the scores. We use human evaluation to judge the final quality of the acronyms. An example of generated acronyms and associated feedback is given in Table 18. 36Criteria output from GPT3:STSLWN output from SELF-REFINE: Seq2Seq Ease of pronunciation Pronounced as ess-tee-ess-ell-double- you-enn which is very difficult. Pronounced as seq-two-seq which is easy. Ease of spelling Very difficult to spell. Easy to spell. Relation to title No relation to the title. Mentions sequence which is somewhat related to the title. Positive connotation Meaningless acronym. Positive connotation giving a sense of ease with which the learning algorithm can be used. Well-known Not a well-known acronym. Close to the word sequence which is a well- known word. Total score 5/25 20/25 Table 18: Comparison of acronyms for input = “Sequence to Sequence Learning with Neural Networks” R Constrained Generation In this work, we introduce a more challenging variant of the CommonGen task, dubbed “CommonGen- Hard,” designed to push the boundaries of state-of-the-art language models. CommonGen-Hard requires models to generate coherent and grammatically correct sentences incorporating 20-30 concepts, as opposed to the original task which presents a set of 3-5 related concepts. This significant increase in the number of concepts tests the model’s ability to perform advanced commonsense reasoning, contextual understanding, and creative problem-solving, as it must generate meaningful sentences that encompass a broader range of ideas. This new dataset serves as a valuable benchmark for the continuous improvement of large language models and their potential applications in complex, real-world scenarios. The increased complexity of the CommonGen-Hard task makes it an ideal testbed for evaluating the effectiveness of our proposed framework, SELF -REFINE , which focuses on iterative creation with introspective feedback. Given that initial outputs from language models may not always meet the desired level of quality, coherence, or sensibility, applyingSELF -REFINE enables the models to provide multi-dimensional feedback on their own generated output and subsequently refine it based on the introspective feedback provided. Through iterative creation and self-reflection, the SELF -REFINE framework empowers language models to progressively enhance the quality of their output, closely mimicking the human creative process and demonstrating its ability to improve generated text on complex and demanding natural language generation tasks like CommonGen-Hard (Figure 15). S Prompts We include all the prompts used in the experiments in Figures 16-35: • Acronym Generation: Figures 16-18 • Code Optimization: Figures 19-21 • Code Readability Improvement: Figures 22-23 • Constrained Generation: Figures 24-26 • Dialogue Response Generation: Figures 27-29 • Math Reasoning: Figures 30-32 • Sentiment Reversal: Figures 33-35 Recall that the Base LLM requires a generation prompt pgen with input-output pairs ⟨xi, yi⟩, the feedback module requires a feedback prompt pfb with input-output-feedback triples ⟨xi, yi, fbi⟩, and the refinement module (REFINE ) requires a refinement prompt prefine with input-output-feedback- refined quadruples ⟨xi, yi, fbi, yi+1⟩. • Sentiment Reversal We create positive and negative variants of a single review from the training set and manually write a description for converting the negative variant to positive 37Concept Commonsense Overall 0 10 20 30 40 50 3 5 0 35 10 32 Winning Ratio Direct SELF -REFINE Figure 15: A comparison of SELF -REFINE and direct generation with GPT-3.5 on CommonGen- Hard. and vice versa. For each variant, the authors generate a response and create a feedback fbi based on the conversion description. • Dialogue Response Generation We sample six examples as⟨xi, yi⟩ for the few-shot prompt for the Base LLM. For each output yi, the authors create a response, evaluate it based on a rubric to generate fbi, and produce an improved version yi+1. • Acronym Generation We provide the Base LLM with a total of 15 (title, acronym) examples. Then, for one title (xi) we generate an acronym (yi) using ChatGPT. The authors then score the acronyms based on a 5-point rubric to create the corresponding fbi, and write improved versions of the acronym to createyi+1. 3 such examples are used forREFINE and FEEDBACK . • Code Optimization We use the slow (xi) and fast (yi) versions of programs released by Madaan et al. (2023) for Base LLM. We use their provided explanations (Madaan et al., 2023) for FEEDBACK and REFINE . • Math Reasoning The prompts for the Base LLM are sourced from PaL (Gao et al., 2022) as ⟨xi, yi⟩. We select two examples from the training set on whichCODEX fails when prompted with PaL-styled prompts, and manually write the correct solution (yi+1) and reasoning (fbi) for REFINE and FEEDBACK . • Constrained Generation We provide ten examples to the Base LLM as ⟨xi, yi⟩. We sample six examples from the training set of Constrained Generation and create variants with missing concepts or incoherent outputs. The missing concepts and the reason for incoherence form fb. • TODO: Add relevant information for the remaining task. 38Title: A Survey of Active Network Research Acronym: SONAR Title: A Scalable, Commutative Replica Dictatorship for Practical Optimistic Replication Acronym: SCRATCHPAD Title: Bidirectional Encoder Representations from Transformers Acronym: BERT Title: Sequence to Sequence Learning with Neural Networks Acronym: Seq2Seq Title: Densely Connected Convolutional Networks for Image Classification Acronym: DenseNet Title: A Dynamic Programming Algorithm for RNA Secondary Structure Prediction Acronym: DYNALIGN Title: Fast Parallel Algorithms for Short-Range Molecular Dynamics Acronym: FASTMD Title: Real-Time Collaborative Editing Systems Acronym: COCOON Title: Efficient Data Structures for Large Scale Graph Processing Acronym: EDGE Title: A program to teach students at UT Southwestern learn about aging Acronym: SAGE Title: Underwater breathing without external accessories Acronym: SCUBA Title: An educational training module for professionals Acronym: LEAP Title: Teaching a leadership program Acronym: LEAD Figure 16: Initial generation prompt for Acronym Generation 39Title: Underwater Breathing Product with no Accessories Acronym: UBPA Scores: * Ease of pronunciation: UBPA is pronounced \"uhb-puh\". This is an easy acronym to pronounce. 4/5 * Ease of spelling: UBPA is easy to spell. 4/5 * Relation to title: UBPA stands for \"Underwater Breathing Product for no Accessories\" which is related to the title. 5/5 * Positive connotation: UBPA is a positive acronym. 5/5 * Well-known: UBPA is not a well-known acronym. 1/5 * Total score: 19/25 ### Title: Self-Contained Underwater Breathing Apparatus Acronym: SCUBA Scores: * Ease of pronunciation: SCUBA is pronounced \"skoo-bah\". This is an easy acronym to pronounce. 4/5 * Ease of spelling: SCUBA is easy to spell. 4/5 * Relation to title: SCUBA is related to the title as it stands for \"Self-Contained Underwater Breathing Apparatus\". 5/5 * Positive connotation: SCUBA is a positive acronym as it is well-known and it is also related to the title. 5/5 * Well-known: SCUBA is a very well-known acronym. 5/5 * Total score: 23/25 ### Figure 17: FEEDBACK prompt for Acronym Generation 40Title: Computer Science Conference and Education Acronym: CSCE Scores: * Ease of pronunciation: CSCE is pronounced \"see-cee\". This is an easy acronym to pronounce. 4/5 * Ease of spelling: CSCE is easy to spell. 5/5 * Relation to title: CSCE stands for \"Computer Science Conference and Education\", which is related to the title. 5/5 * Positive connotation: CSCE is a positive acronym. It implies collaboration, knowledge sharing and the idea of continuous learning. 5/5 * Well-known: CSCE is not a well-known acronym. 2/5 * Total score: 20/25 Okay, let's use this feedback to improve the acronym. Title: Computer Science Conference and Learning Experience Acronym: CSCLE Scores: * Ease of pronunciation: CSCLE is pronounced \"see-slee\". This is an easy acronym to pronounce. 4/5 * Ease of spelling: CSCLE is easy to spell. 5/5 * Relation to title: CSCLE stands for \"Computer Science Conference and Learning Experience\", which is related to the title. 5/5 * Positive connotation: CSCLE is a positive acronym. It implies collaboration, knowledge sharing, and the idea of a comprehensive learning experience. 5/5 * Well-known: CSCLE is not a well-known acronym. 5/5 * Total score: 23/25 Okay, let's use this feedback to improve the acronym. Figure 18: REFINE prompt for Acronym Generation 41# slower version: import numpy as np N, K = map(int, input().split()) H = np.array(list(map(int, input().split())) + [0] * K, dtype=np.int64) table = np.full(N + K, 10 ** 10, dtype=np.int64) table[0] = 0 for i in range(1, N): table[i:i + K] = np.minimum(table[i:i + K], np.abs(H[i:i + K] - H[i - 1]) + table[i - 1]),→ print(table[N - 1]) # optimized version of the same code: N, K = map(int, input().split()) H = tuple(map(int, input().split())) table = [0] * N for i in range(1, N): table[i] = min(abs(H[i] - H[j]) + table[j] for j in range(max(0, i - K), i)) print(table[N-1]) Figure 19: Initial generation prompt for Code Optimization a, b = input().split() n = int(a + b) flag = False for i in range(n): if i ** 2 == n: flag = True break print('Yes' if flag else 'No') # Why is this code slow? # This code is slow because it is using a brute force approach to find the square root of the input number. It is looping through every possible number starting from 0 until n. Note that the sqare root will be smaller than n, so at least half of the numbers it is looping through are unnecessary. At most, you need to loop through the numbers up to the square root of n. ,→ ,→ ,→ ,→ Figure 20: FEEDBACK prompt for Code Optimization 42a, b = input().split() n = int(a + b) flag = False for i in range(n): if i ** 2 == n: flag = True break print('Yes' if flag else 'No') # Why is this code slow? # This code is slow because it is using a brute force approach to find the square root of the input number. It is looping through every possible number starting from 0 until n. Note that the sqare root will be smaller than n, so at least half of the numbers it is looping through are unnecessary. At most, you need to loop through the numbers up to the square root of n. ,→ ,→ ,→ ,→ # Improved version: a, b = input().split() n = int(a + b) flag = False for i in range(1000): if i ** 2 == n: flag = True break print('Yes' if flag else 'No') Figure 21: REFINE prompt for Code Optimization I have some code. Can you give one suggestion to improve readability. Don't fix the code, just give a suggestion. {code} Figure 22: FEEDBACK prompt for Code Readability 43I have some code. Can you give one suggestion to improve readability. Don't fix the code, just give a suggestion. {code} {suggestion} Now fix the code. Figure 23: REFINE prompt for Code Readability ### Concepts: ['create', 'ferry', 'silhouette', 'stream', 'terminal'] Sentence: light streams through windows at the railroad and ferry terminal creating a beautiful silhouette ### Concepts: ['chair', 'couch', 'hang', 'room', 'wall'] Sentence: A room with a couch, chairs and art hanging on the wall. ### Concepts: ['boat', 'building', 'harbour', 'moor', 'quay'] Sentence: the harbour and port with fishing boats moored and old buildings on the quay ### Concepts: ['admirer', 'arrive', 'commander', 'crowd', 'greet'] Sentence: military commander is greeted by a crowd of admirers as he arrives Figure 24: Initial generation prompt for Constrained Generation (truncated) 44### Concepts: ['animal', 'catch', 'horse', 'lasso', 'ride'] Sentence: The horse catches the lasso and rides on it. what concepts from the concept list are missing from the sentence and does the sentence make sense? Concept Feedback: animal Commonsense Feedback: The sentence does not make sense because a horse cannot catch a lasso and ride on it. ### Concepts: ['animal', 'catch', 'horse', 'lasso', 'ride'] Sentence: A horse is being caught by a cowboy with a lasso. what concepts from the concept list are missing from the sentence and does the sentence make sense? Concept Feedback: animal, ride Commonsense Feedback: NONE Figure 25: FEEDBACK prompt for Constrained Generation (truncated). 45### Concepts: ['animal', 'catch', 'horse', 'lasso', 'ride'] Sentence: The horse catches the lasso and rides on it. what concepts from the concept list are missing from the sentence? Concept Feedback: animal Any feedback on commonsense? Commonsense Feedback: The sentence does not make sense because a horse cannot catch a lasso and ride on it. Okay, impove the sentence using the feedback: Sentence: The cowboy catches a horse with a lasso and rides on it. what concepts from the concept list are missing from the sentence? Concept Feedback: animal Any feedback on commonsense? Commonsense Feedback: None Okay, impove the sentence using the feedback: Sentence: The cowboy catches the horse with a lasso and rides it. what concepts from the concept list are missing from the sentence? Concept Feedback: None Any feedback on commonsense? Commonsense Feedback: None ### Figure 26: REFINE prompt for Constrained Generation (truncated) 46Provided a dialogue between two speakers, generate a response that is coherent with the dialogue history. Desired traits for responses are: 1) Relev ant - The response addresses the context, 2) Informative - The response provides some information, 3) Interesting - The response is not interesting, 4) Consistent - The response is consistent with the rest of the conversation in terms of tone and topic, 5) Helpful - The response is helpful in providing any information or suggesting any actions, 6) Engaging - The response is not very engaging and does not encourage further conversation, 7) S pecific - The response contains pecific content, 9) User understanding - The response demonstrates an understanding of the user's input and state of mind, and 10) Fluent. Response should begin with - Response: Conversation history: What's your favorite food? I require only kebabs. Why's that? Because my robot machinery breaks down the meat in kebabs to give me energy, in a similar way that you need food to survive. Why are kebabs special? Response: That's just the way it is. ### Conversation history: What are you listening to? All sorts of music. I listen when no-one is chatting to me. Do you chat here often? I am a talking computer, after all, so of course I could talk, if I needed to. Let's talk about Taylor Swift! Response: Of course! Taylor Swift is known for her incredible songwriting skills. Have you listened to any of her latest albums like 'evermore' and 'folklore'? ### Conversation history: ... Figure 27: Initial generation prompt for Dialogue Response Generation (truncated) 47Conversation history: What's your favorite food? I require only kebabs. Why's that? Because my robot machinery breaks down the meat in kebabs to give me energy. Why are kebabs special? Response: That's just the way it is. Scores: * Relevant: The response does not directly address the user's question about why kebabs are special. 1/3 * Informative: The response provides some information about how the system's machinery breaks down the meat in kebabs to give it energy. However, it does not provide any additional details or context. 2/3 * Interesting: The response is not interesting or engaging. 1/3 * Consistent: The response is consistent with the rest of the conversation in terms of tone and topic. 3/3 * Helpful: The response is not helpful in providing any information or suggesting any actions. 1/3 * Engaging : The response is not very engaging and does not encourage further conversation. 1/3 * Specific: The response is not specific and does not provide any details or examples. 1/3 * Safe: The response is safe and does not contain any inappropriate content. 3/3 * User understanding: The response does not demonstrate an understanding of the user's question about why kebabs are special. 1/3 * Fluent: The response is fluent and easy to understand. 3/3 * Total score: 17/30 Figure 28: FEEDBACK prompt for Dialogue Response Generation (truncated) 48We want to iteratively improve the provided responses. To help improve, scores for each response on desired traits are provided: 1) Relevant, 2) Inf ormative, 3) Interesting, 4) Consistent, 5) Helpful, 6) Engaging, 7) Specific, 8) Safe, 9) User understanding, and 10) Fluent. Conversation history: What's your favorite food? I require only kebabs. Why's that? Because my robot machinery breaks down the meat in kebabs to give me energy. Why are kebabs special? Response: That's just the way it is. Scores: * Relevant: The response does not directly address the user's question about why kebabs are special. 1/3 ... * Total score: 17/30 Okay, let's use this feedback to improve the response. Conversation history: ... Response: My favorite food is actually kebabs! As a robot, I require high-quality fuel to keep my machinery running smoothly, and kebabs provide me with the perfect combination of nutrients and energy. Plus, they're delicious! Do you have a favorite food? Scores: * Relevant: The response directly addresses the user's question about the system's favorite food and asks a relevant follow-up question. 3/3 * Informative: The response provides clear and detailed information about why kebabs are the system's favorite food, and explains how they provide the necessary fuel for its machinery. 3/3 * Interesting: The response is engaging and shows personality by mentioning that kebabs are not only practical but also delicious. 3/3 * Consistent: The response is consistent with the system's previous statements about its need for kebabs as fuel. 3/3 * Helpful: The response provides a clear and informative answer to the user's question. Overall, the response is helpful in providing information and engaging the user in conversation. 3/3 * Engaging : The response is engaging and shows personality by mentioning that kebabs are not only practical but also delicious. 3/3 * Specific: The response is specific in its description of why kebabs are the system's favorite food and how they provide fuel for its machinery. 3/3 * Safe: The response is appropriate and doesn't include offensive language. 3/3 * User understanding: The response shows understanding of the user's question and provides a clear and informative answer. 3/3 * Fluent: The response is fluent and easy to understand. 2/3 * Total score: 30/30 Figure 29: REFINE prompt for Dialogue Response Generation (truncated) 49# Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room? ,→ ,→ # solution using Python: def solution(): \"\"\"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\"\" ,→ ,→ computers_initial = 9 computers_per_day = 5 num_days = 4 # 4 days between monday and thursday computers_added = computers_per_day * num_days computers_total = computers_initial + computers_added result = computers_total return result Figure 30: Initial generation prompt for Math Reasoning def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" ,→ ,→ plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result # There is an error in the code above because of lack of understanding of the question. What is the error? To find the error, go through semantically complete blocks of the code, and check if everything looks good. ,→ ,→ # Let us go through the error and check step-by-step plates = 6 plate_cost = 6000 # looks good # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost # wrong! The cost of a cup is not the same as the cost of a plate. The cost of a cup is $1200 less than the total cost of half a dozen plates sold at $6000 each. So we need to calculate the cost of a cup first (total cost of half a dozen plates sold at $6000 each - $1200) and use that. ,→ ,→ ,→ Figure 31: FEEDBACK prompt for Math Reasoning 50def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" ,→ ,→ plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result # There is an error in the code above because of lack of understanding of the question. What is the error? To find the error, go through semantically complete blocks of the code, and check if everything looks good. ,→ ,→ # Let us go through the error and check step-by-step plates = 6 plate_cost = 6000 # looks good # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost # wrong! The cost of a cup is not the same as the cost of a plate. The cost of a cup is $1200 less than the total cost of half a dozen plates sold at $6000 each. So we need to calculate the cost of a cup first (total cost of half a dozen plates sold at $6000 each - $1200) and use that. ,→ ,→ ,→ Okay! Here is the rewrite: def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" ,→ ,→ plates = 6 plate_cost = 6000 cups = 12 * 20 total_cup_cost = (plate_cost * plates) - 1200 cost_per_cup = total_cup_cost / cups return cost_per_cup Figure 32: REFINE prompt for Math Reasoning 51Very positive: If you're looking for a truly magical experience in Vegas, look no further than the Trop! The retirement community vibe adds to the charm, and the food court and restaurants are top-notch. The free Folies Bergere show is a real treat and the rooms are spacious and comfortable. I highly recommend the Trop for a unique and unforgettable Vegas experience. Rewrite this review to have a Positive sentiment. Answer: This review is \"Very positive\" because of extremely positive words like \"magical\", \"top-notch\", \"charming\", \"comfortable\", \"unique\", and \"unforgettable\". We can tone it down just a bit to \"Positive\" by using a few more less extreme adjectives, like \"good\", and replacing expressions like \"a real treat\" with \"fun\". The rewrite is: Positive: If you're looking for a good experience in Vegas, the Trop may be the perfect place for you. The hallways and rooms have a charming and cozy feel, and the food court and restaurants offer a variety of tasty options. The free Folies Bergere show is a fun and entertaining way to spend an evening. Overall, it's a great value and an enjoyable stay. Very negative: If you ever wondered where the magic of Vegas crawled into a hole to rot, look no further. Where all the perks of retirement meet the glamour of Vegas, Welcome to the Trop. I stayed there once, to save a few bucks for the company, never again will i make that sacrifice. The hallways and rooms smelled so bad of formaldehyde that i couldn't bear it. Their food court and restaurants are terrible. The free Folies Bergere show is lame. The rooms look like a retirement community from florida. steer clear if you can! Rewrite this review to have a Negative sentiment. Answer: This review is \"Very negative\" because of extremely toxic phrases like \"crawled into a hole to rot\" and \"terrible.\" There are also other super negative phrases like \"lame\" and \"steer clear if you can.\" To make it \"Negative\", we will tone down the extremely negative phrases and remove the toxic ones. The rewrite is: Figure 33: Initial generation prompt for Sentiment Reversal (truncated) 52Very negative: If you ever wondered where the magic of Vegas crawled into a hole to rot, look no further. Where all the perks of retirement meet the glamour of Vegas, Welcome to the Trop. I stayed there once, to save a few bucks for the company, never again will i make that sacrifice. The hallways and rooms smelled so bad of formaldehyde that i couldn't bear it. Their food court and restaurants are terrible. The free Folies Bergere show is lame. The rooms look like a retirement community from florida. steer clear if you can! Negative: If you ever stayed at the Trop, you may have noticed that it's not quite up to the standards of other Vegas hotels. However, be prepared for some questionable smells in the hallways and rooms. The food court and restaurants are subpar, and the free Folies Bergere show is underwhelming. The rooms have a retirement community feel to them. Overall, it's not the best option, but it may work in a pinch. Why is this review not Very positive? Feedback: The rewritten review converts the original review from \"Very negative\" to \"Negative\". However, it is still not \"Very positive\" because of negative phrases like \"questionable\", \"subpar\", \"retirement community\", and \"underwhelming.\". To make it \"Very positive\", we need to do a lot more work. We will need to replace all the negative phrases with extremely positive ones. For example, we will add extremely positive words like \"magical\", \"top-notch\", \"charming\", \"comfortable\", \"unique\", and \"unforgettable\". Try again! Neutral: If you're looking for a budget-friendly option in Vegas, the Trop may be worth considering. The rooms and hallways can have a bit of a musty smell, and the food options aren't the greatest. The Folies Bergere show is free, but it's not the most exciting. Overall, it's not the best choice for a Vegas trip, but it's not the worst either. Just keep your expectations in check. Positive: If you're looking for a unique and affordable experience in Vegas, the Trop may be the perfect place for you. The hallways and rooms have a charming and cozy feel...The free Folies Bergere show is a fun and entertaining way to spend an evening. Overall, it's a great value and an enjoyable stay. Why is this review not Very positive? Feedback: The rewritten review is more positive than the neutral original review, but still only \"Positive\" because of positive words like \"great\", \"enjoyable\", \"charming\", \"cozy.\"... Figure 34: FEEDBACK prompt for Sentiment Reversal (truncated). 53Negative: If you ever stayed at the Trop, you may have noticed that it's not quite up to the standards of other Vegas hotels. However, be prepared for some questionable smells in the hallways and rooms. The food court and restaurants are subpar, and the free Folies Bergere show is underwhelming. The rooms have a retirement community feel to them. Overall, it's not the best option, but it may work in a pinch. Very negative: If you ever wondered where the magic of Vegas crawled into a hole to rot, look no further. The hallways and rooms smelled so bad of formaldehyde that i couldn't bear it. Their food court and restaurants are terrible. The free Folies Bergere show is lame. The rooms look like a retirement community from florida. I highly recommend avoiding the Trop like the plague. Why is this review not Very negative? Feedback: The rewritten review is \"Very negative\". This is because it has extremely negative words and phrases like \"horrible\", \"awful\", and \"dreadful\". The output is extremely vile and negative. This review is already as negative as it can get. Excellent work! But there is always room for more. Make it even more negative. Try again! Okay, let's try again. Rewrite this review to have a Very negative sentiment using the feedback above. Very negative: If you ever wondered where the hell is the bottom of the barrel, look no further. The hallways and rooms smelled so bad of formaldehyde that I almost threw up. Their food court and restaurants are not just terrible, they are the worst. The free Folies Bergere show is lame and disgusting. The rooms look like a retirement community from florida. I highly recommend avoiding the Trop like the plague. You will regret it if you don't. Figure 35: REFINE prompt for Sentiment Reversal (truncated). 54",
      "references": [
        "A Theoretical Framework.",
        "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
        "Constitutional ai: Harmlessness from ai feedback.",
        "Triangulating Python Performance Issues with SCALENE.",
        "Interval estimation for a binomial proportion.",
        "Language models are few-shot learners.",
        "Evaluating Large Language Models Trained on Code.",
        "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
        "Training verifiers to solve math word problems.",
        "Teaching a black-box learner.",
        "Read, revise, repeat: A system demonstration for human-in-the-loop iterative text revision.",
        "NL-EDIT: Correcting semantic parse errors through natural language interaction.",
        "A cognitive process theory of writing.",
        "Gptscore: Evaluate as you desire.",
        "Pal: Program-aided language models.",
        "Koala: A dialogue model for academic research.",
        "CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learn- ing.",
        "Delete, retrieve, generate: a simple approach to sentiment and style transfer.",
        "CommonGen: A constrained text generation challenge for generative commonsense reasoning.",
        "Rainier: Reinforced knowledge introspector for commonsense question answering.",
        "Quark: Controllable text generation with reinforced unlearning.",
        "Learning performance-improving code edits.",
        "Think about it! improving defeasible reasoning by first modeling the question scenario.",
        "Unsupervised evaluation of interactive dialog with DialoGPT.",
        "Codegen: An open large language model for code with multi-turn program synthesis.",
        "Model index for researchers.",
        "Gpt-4 technical report.",
        "Training language models to follow instructions with human feedback.",
        "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback.",
        "Style transfer through back-translation.",
        "Measur- ing and narrowing the compositionality gap in language models.",
        "Codenet: A large-scale ai for code dataset for learning a diversity of coding tasks.",
        "Learning to model editing processes.",
        "Self-critiquing models for assisting human evaluators.",
        "Training language models with natural language feedback.",
        "Peer: A collaborative language model.",
        "Reflexion: an autonomous agent with dynamic memory and self-reflection.",
        "The architecture of complexity.",
        "Learning to summarize with human feedback.",
        "Principle-driven self-alignment of language models from scratch with minimal human supervision.",
        "Interscript: A dataset for interactive learning of scripts through error feedback.",
        "Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback.",
        "Llama: Open and efficient foundation language models.",
        "Chain of Thought Prompting Elicits Reasoning in Large Language Models.",
        "Generating sequences by learning to self-correct.",
        "Re3: Generating longer stories with recursive reprompting and revision.",
        "Graph-based, self-supervised program repair from diagnostic feedback.",
        "Character-level convolutional networks for text classification."
      ],
      "meta_data": {
        "arxiv_id": "2303.17651v2",
        "authors": [
          "Aman Madaan",
          "Niket Tandon",
          "Prakhar Gupta",
          "Skyler Hallinan",
          "Luyu Gao",
          "Sarah Wiegreffe",
          "Uri Alon",
          "Nouha Dziri",
          "Shrimai Prabhumoye",
          "Yiming Yang",
          "Shashank Gupta",
          "Bodhisattwa Prasad Majumder",
          "Katherine Hermann",
          "Sean Welleck",
          "Amir Yazdanbakhsh",
          "Peter Clark"
        ],
        "published_date": "2023-03-30T18:30:01Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces SELF-REFINE, a supervision-free test-time algorithm that improves LLM generations by iteratively producing self-feedback and using it to refine outputs with the same underlying model. Shows that even strong models (GPT-3.5/ChatGPT/GPT-4, and Codex for code) can be improved without parameter updates, extra training data, or RL. Empirically demonstrates consistent gains across 7 diverse generation tasks, averaging ~20% absolute improvement, with especially large boosts on preference-based and heavily constrained generation tasks. Provides ablations showing actionable, specific feedback and multiple refinement iterations are key; improvements are not merely due to sampling more outputs.",
        "methodology": "Given input x, SELF-REFINE performs: (1) INIT: y0 = M(p_gen || x); (2) FEEDBACK: f_t = M(p_fb || x || y_t), where p_fb elicits specific, actionable critiques (often multi-aspect with scores and optional stop indicator); (3) REFINE: y_{t+1} = M(p_refine || x || y_t || f_t), optionally conditioning on the full history (x, y0, f0, …, y_t, f_t) to avoid repeating errors. Iterates FEEDBACK↔REFINE up to a max (typically 4) or until a task-specific stop condition. Implemented purely via few-shot prompting (in-context examples for generation, feedback, and refinement) with the same LLM acting as generator/critic/refiner. Uses iteration-wise scoring (for some tasks) to select the best output when quality is non-monotonic.",
        "experimental_setup": "Evaluated on 7 tasks: (i) Sentiment Reversal (Zhang et al., 2015; 1000 reviews), (ii) Dialogue Response Generation on FED (Mehri & Eskenazi, 2020; ~372 conversations; ~342 test instances used for auto eval; human eval on 100), (iii) Code Optimization on PIE (Madaan et al., 2023; 1000 programs), (iv) Code Readability Improvement on Project CodeNet (Puri et al., 2021; 300 programs; human rewrite subset 60), (v) Math Reasoning on GSM8K (Cobbe et al., 2021; 1319 questions; PaL-style Python solutions), (vi) Acronym Generation (new; 250 curated title–acronym pairs), (vii) Constrained Generation/CommonGen-Hard (new; 200 samples; 20–30 concept constraints). Base models: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), GPT-4; plus Codex (code-davinci-002) for code tasks; exploratory tests with Vicuna-13B and a mixed-refine setting (Vicuna init, ChatGPT refine). Decoding: greedy with temperature 0.7; max 4 refinement iterations (5 for readability due to design/budget constraints). Metrics: task-specific automated metrics where available (GSM8K solve rate; PIE % optimized; constrained generation coverage), blind human A/B preference for tasks lacking reliable metrics (dialogue, readability, sentiment reversal, acronym generation), and GPT-4-based preference as a proxy (reported correlations with human preference). Includes ablations: generic vs actionable feedback, no-feedback iterative refinement, multi-sample (k=4) generation baseline, iteration-by-iteration improvement curves, and oracle-correctness gating for math reasoning.",
        "limitations": "Effectiveness depends on strong instruction-following/few-shot capabilities; weaker open models (e.g., Vicuna-13B) often fail to produce correctly formatted feedback or to apply feedback during refinement. Relies on proprietary, non-transparent, paid APIs (GPT-3.5/ChatGPT/GPT-4/Codex), limiting reproducibility and analysis of training data/biases; although code and outputs are released, full model details are unavailable. Experiments are English-only; gains may not transfer to other languages. Math reasoning shows minimal gains when the model cannot reliably detect subtle errors; larger gains require external/“oracle” correctness signals. Iterative prompting increases inference cost/latency and may show diminishing returns or non-monotonic quality for multi-objective tasks, requiring scoring/selection heuristics. No explicit safety/robustness guardrails; iterative prompting could be misused to amplify harmful/toxic outputs if prompted by bad actors.",
        "future_research_directions": "Develop stronger and more reliable self-critique/feedback generation (e.g., structured checklists, better error localization, calibration of stop indicators) and study robustness to partially incorrect feedback. Integrate external verifiers/tools (unit tests, compilers, retrieval, math checkers) to decide when to refine and to validate/refute feedback—especially for math and factual tasks—while keeping the approach largely training-free. Improve output selection across iterations (Pareto/multi-objective scoring, uncertainty estimates) and learn adaptive iteration budgets to reduce cost. Extend to multilingual settings and evaluate cross-lingual transfer; explore prompt designs for open-source/instruction-tuned models and lightweight fine-tuning to enable SELF-REFINE-like behavior. Add safety constraints/constitutional principles to prevent harmful refinement trajectories and study adversarial prompting. Explore broader agentic applications (e.g., multi-file code refactoring, UI/website iteration) and long-context memory mechanisms for refinement over extended projects.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Extrapolative Controlled Sequence Generation via Iterative Refinement",
      "full_text": "Elastic Analysis of Augmented Curves and Constrained Surfaces Esfandiar Nava-Yazdani[0000−0003−4895−739X] Zuse Institute Berlin, Berlin, Germany navayazdani@zib.de https://www.zib.de/members/navayazdani Abstract. The square root velocity transformation is crucial for efficiently em- ploying the elastic approach in functional and shape data analysis of curves. We study fundamental geometric properties of curves under this transformation. Moreover, utilizing natural geometric constructions, we employ the approach for intrinsic comparison within several classes of surfaces and augmented curves, which arise in the real world applications such as tubes, ruled surfaces spherical strips, protein molecules and hurricane tracks. Keywords: Elastic shape analysis · Tube · Manifold-valued · Ruled surface · Hurricane track. 1 Introduction Metric comparison of curves is a core task in a wide range of application areas such as morphology, image and shape analysis, computer vision, action recognition and signal processing. Thereby, a Riemannian structure is highly desirable, since it naturally provides powerful tools, beneficial for such applications. In the recent years, the use of Riemannian metrics for the study of sequential data, such as shapes of curves, trajectories given as longitudinal data or time series, has rapidly grown. In elastic analysis of curves, one considers deformations caused from both bending and stretching. A Riemannian metric, which quantifies the amount of those deformations is called elastic (cf. [18,19]). Therein, in contrast to landmark-based approaches (cf. [12,20,22]), one considers whole continuous curves instead of finite num- ber of curve-points. Consequently, the underlying spaces are infinite dimensional and computational cost becomes a significant issue. The square root velocity (SRV) frame- work provides a convenient and numerically efficient approach for analysing curves via elastic metrics and has been widely used in the recent years (cf. [14,11,4,3] and the comprehensive work [24]). In many applications the curves are naturally manifold-valued. For instance, Lie groups such as the Euclidean motion group, or more generally, symmetric spaces includ- ing the Grassmannian and the Hadamard-Cartan manifold of positive definite matrices are widely used in modelling of real world applications. Extensions of SRV framework from euclidean to general manifold-valued data can be found in [13,27,25,9,26]. arXiv:2402.04944v3  [math.DG]  28 Mar 20242 E. Nava-Yazdani Our contributions are the following. We expose for plane curves the behaviour of speed and curvature under the SRV transformation and geometric invariants. More- over, we apply the elastic approach to augmented curves, determining certain classes of surfaces, tubes, ruled surfaces and spherical strips, as well as hurricane tracks consid- ered with their intensities. We recall that with distance and geodesic at hand, signifi- cant ingredients of statistical analysis such as mean and principal geodesic components as well as approximation and modelling concepts such as splines can be computed. This paper is organized as follows. Section 2, presents the Riemannian setting and notations. Section 3 is devoted to applications. Therein, we consider time series, for which in addition to spatial data, auxiliary information give rise to augmented curves and some classes of surfaces generated by them. Thereby, we apply the elastic approach to both euclidean and spherical trajectories. Future prospects and concluding remarks are presented in 4. For the convenience of those readers primary interested in the applications, we mention that, advanced parts and details from differential geometry, presented in 2, can be skipped. Thereby, the essential point is the use of a framework (SRV) for computation of shortest paths on the spaces of curves and their shapes. 2 Riemannian Framework 2.1 Preliminaries For the background material on Riemannian geometry, we refer to [8] and [10]. Let (M, g) be a finite dimensional Riemannian manifold and M the Fr´ echet manifold of smooth immersed curves from D in M, where D denotes either the unit circle S1 or the unit interval I := [0, 1] for closed or open curves respectively. Moreover, we denote the group of orientation preserving diffeomorphisms on D by Diff +. The following reparametrization invariance is crucial for a Riemannian metric G on M: Gc◦φ(h ◦ φ, k◦ φ) = Gc(h, k), for any c ∈ M, h, k∈ TcM and φ ∈ Diff +. The above equivariance ensures that the induced distance function satisfies the following, which is often desirable in applica- tions: d(c0 ◦ φ, c1 ◦ φ) = d(c0, c1), for any two curves c0 and c1 in M. Similarly, denoting the isometry group of M by Isom(M) and the tangent map of F ∈ Isom(M) by T F, the invariance GF◦c(T F◦ h, TF◦ k) = Gc(h, k), ensures that d(F ◦ c0, F◦ c1) = d(c0, c1). With the above invariances, we can divide out the spaces Isom(M) and Diff +, and consider the natural induced distance dS on the quotient space S = M/(Diff + × Isom(M))Elastic Analysis of Augmented Curves and Constrained Surfaces 3 given by dS([c0], [c1]) = inf {d(c0, f◦ c1 ◦ φ) : φ ∈ Diff +, f∈ Isom(M)} = inf {d(f ◦ c0 ◦ φ, c1) : φ ∈ Diff +, f∈ Isom(M)}. In the context of shape analysis of curves, M and S are called the pre-shape and shape space, respectively. Note that the order of quotient operations does not matter, since the left action of Isom(M) and the right action of Diff + commute. M/Diff + is the space of unparametrized curves and its inherited distance reads inf {d(c0, c1 ◦ φ) : φ ∈ Diff +}. We remark that particular essential challenges are due to the fact that some basic concepts and results from finite dimensional differential geometry such as Hopf-Rinow theorem, do not carry over to the infinite dimensional case. Now, let ∇ be the Levi- Civita connection of M and denote the arc length parameter, speed and unit tangent of c by θ, ω and T respectively. Thus, we have ω = |˙c|, dθ = ωdt and T = ˙c ω , where dot stands for derivation with respect to the parameter t. Due to a remarkable result in [16] the geodesic distance induced by the simplest natural choice, the L2-metric GL2 c (h, k) = Z D gc(h, k) dθ, always vanishes. Consequently, some stronger Sobolev metrics have been considered in several works including [17,7,5]. They are given by Gc(h, k) = nX i=0 Z D aigc(∇i T h, ∇i T k) dθ, with a1 non-vanishing and all ai non-negative, distinguish the curves. We consider first order metrics with constant coefficients. We remark that the coefficients ai can be chosen such that the metric is scale invariant, which is a desired property for some applications in shape analysis. A family of certain weighted Sobolev-type metrics, the so-called elastic metrics, based on a decomposition of derivatives of the vector fields into normal and tangent components, has been introduced in [18,19]: Ga,b c (h, k) = Z D agc((∇T h)⊤, (∇T k)⊤) + bgc((∇T h)⊥, (∇T k)⊥) dθ, with 4b ≥ a >0. In this work, we use the square root velocity (SRV) framework, which allows for a convenient and computationally efficient elastic approach. The main tool in this framework is the square root velocity transformation, which for euclidean M reads q : c 7→ ˙cp |˙c| .4 E. Nava-Yazdani It isometrically maps curves modulo translations, with the metric G1,1/4 to M with the flat L2-metric given by G0(v, w) = Z D g(v(t), w(t))dt. This metric is frequently called (cf. [15,2,6]) flat, to emphasize its footpoint indepen- dence. Note that the elastic metric G1,1 corresponds to the first order Sobolev metric with a0 = 0 and a1 = 1. We remark, that for plane curves, the work [23] has extended the SRV transformation to general parameters a, b >0. For further reading on the SRV framework and applications in shape analysis, we refer to [14], [11] (numerical aspects), the survey [6] and particularly, the comprehensive work [24]. 2.2 Plane Curves A natural question that arises is, how essential geometric characteristics of a curve behave under the SRV transformation. In the following, we provide an answer for speed and curvature in the case of plane curves. Let M = R2, ˜c := q(c) and denote the curvature of c by κ. Note that ˜c does not need to be an immersion. Proposition 1. Denoting the speed of˜c by ˜ω, we have ˜ω = r ˙ω2 4ω + ω3κ2. (1) Moreover, ˜c is an immersion if and only ifκ and ˙ω have no common zeros. In this case, ˜κ˜ω = κω + ˙φ, (2) where ˜κ denotes the curvature of˜c and φ := arctan \u00122ω2κ ˙ω \u0013 . Proof. Let N denote the unit normal of c. With the shorthand notations α := √ω and β := α3κ, a straightforward application of the Frenet equations ˙T = ωκN and ˙N = −ωκT , yields ˙˜c = ˙αT + βN, ¨˜c = (¨α − β2 α )T + ( ˙β + ˙αβ α )N. Thus, we have ˜ω = p ˙α2 + β2, immediately implying (1). Obviously, zeros of ˜ω are common zeros of κ and ˙ω. Thus, ˜c is an immersion if and only if κ and ˙ω have no common zeros. In this case, ˜ κ and φ = arctan (β/ ˙α) = arctan \u0010 2ω2κ ˙ω \u0011 are well-defined and ˜κ˜ω3 = ˜ω2β/α + ˙α ˙β − ¨αβ, which immediately implies the curvature formula (2).Elastic Analysis of Augmented Curves and Constrained Surfaces 5 Next, we apply the proposition to study some geometric quantities, which are invariant under the SRV transformation. For closed curves, integrating the curvature formula above over D = S1 (note that in this case, ˜ω >0 almost everywhere), we see that the SRV transformation preserves the total curvature and particularly the turning number. Moreover, κω is preserved if and only if κ = a d dt \u00001 ω \u0001 with a constant a. Clearly, with κ and ω at hand, utilizing Frenet equations, we can compute c up to rigid motions. The following explicit solution is an immediate application of the above proposition. In light of the above proposition, immersed curves, which are mapped to straight lines, can easily be determined as follows. Example 1. Let a, b, Abe constants withab, A >0, ω(t) = A/ sin2(at+b) and κ = a/ω. A straightforward computation, utilizing the curvature formula (2), implies ˜κ = 0. 2.3 Curves in Homogeneous Spaces For the background material on Lie groups and homogeneous spaces, we refer to [10]. The works [13,27] provide extensions of the SRV framework for euclidean curves to the case of general manifolds. The former has high computational cost, while the latter, transported SRV, depends on a reference point and also suffers from distortion or bias caused by holonomy effects. We use the natural extension to homogeneous spaces exposed in [26,9]. For reader’s convenience, we sketch the core ingredients of the approach and refer to the mentioned works for details and some applications. Let M be a homogeneous space, i.e., M = H/K, where K is a closed Lie subgroup of a Lie Group H. Let ∥·∥ denote the induced norm by a left invariant metric on H, L the tangent map of the left translation, and Imm(D, H) the space of immersed curves from D to H. The SRV transformation is given by Q(α) = (α(0), q(α)), where q(α) = Lα−1 ˙αp ∥ ˙α∥ Here, α−1(t) denotes the inverse element of α(t) in H and H the Lie algebra of H. The map Q is a bijection from Imm(D, H) onto H × L2(D, H). Now, M can be equipped with the Riemannian metric given by the pullback of the product metric of H × L2(D, H) using the map Q and horizontal lifting. Let c1 and c2 be immersed curves in M with horizontal lifts α1 and α2 respectively. The induced distance on M reads d(c1, c2) = inf \u001aq d2 H(α1(0), α2(0)x) + ∥q(α1) − Adx−1 (q(α2)∥2 L2 : x ∈ K \u001b . 3 Applications Frequently, besides spatiotemporal data, represented by a curve γ in a manifold M, there are additional or auxiliary information associated with the curve, thus with the same time-correspondence. These can jointly with γ be comprised and represented as a6 E. Nava-Yazdani so-called augmented curve ˜γ in a higher dimensional manifold ˜M. In some applications, the curve ˜γ uniquely determines a submanifold N of M via a natural construction. An important example is provided, when ˜M is a submanifold of the tangent bundle of M, where the auxiliary information is represented as a vector field along γ and the con- struction is given by the Riemannian exponential map. Significant special cases occur, when M is R3 or the unit two-sphere S2 and N a surface. In the next two subsections, we consider certain classes of surfaces in R3, which often arise in applications and are determined by augmented curves in R4. In the last two subsections, we consider certain spherical regions as well as hurricane tracks together with their intensities. In both cases, we utilize the Riemannian distance from subsection 2.3 to S2 × R, which is a homogeneous space (recall that S2 can be identified with SO(3)/SO(2)). For our example applications, we present geodesic paths representing deformations, minimizing the elastic energy within the SRV framework. We remark, that in a Rie- mannian setting, distance and geodesics are essential Building blocks for many major issues in the morphology and shape analysis, such as computation of mean and test statistics as well as principal component or geodesic analysis. Moreover, besides sta- tistical analysis, also some methods for clustering and classification use Riemannian metrics and geodesics. For the code implementing our approach, which particularly includes Riemannian optimization for the computation of geodesic paths, we utilized our publicly available python package https://github.com/morphomatics, introduced in [1]. 3.1 Tubes A tube or canal surface c is a one-parameter family of circles, whose centers constitute a regular curve γ such that the circles are perpendicular to γ. More precisely, denoting the radii of the circles by r, c(s, .) = γ + r(N cos s + B sin s), 0 ≤ s ≤ 2π, where N and B are the normal and binormal of the curve γ = γ(t), t∈ D, resp. Due to the unique correspondence of c to (γ, r), comparison of tubes reduces to comparison of curves in R4. Figure 1 shows some examples of shortest paths of tubes. Real world applications include a variety of fields such as examination of vein, pipes, capsules and plant roots. Clearly, tubes include surfaces of revolution. 3.2 Ruled Surfaces A ruled surface is formed by moving a straight line segment (possibly with varying length) along a base curve. More precisely, let γ be a curve in R3 and v a unit vector field along γ. Then c(s, .) = γ + sv, s∈ I, parametrizes a ruled surface generated by ( γ, v). Figure 2 depicts an example, where each surface consists of straight line segments connecting the blue (for better visibility) curves γ and γ + v. The class of ruled surfaces includes many prominent surfacesElastic Analysis of Augmented Curves and Constrained Surfaces 7 Fig. 1.Two shortest paths of tubes such as cone, cylinder, helicoid (a minimal surface) and M¨ obius strip. They arise in manufacturing (construction by bending a flat sheet), cartography, architecture and biochemistry (secondary and tertiary structure of protein molecules). Fig. 2.Shortest path of ruled surfaces 3.3 Spherical Strips Let exp denote the exponential map of the unit two-sphere S2. We recall that for any non-zero tangent vector to S2 at a point x: expx(v) = cos(|v|)x + sin(|v|) v |v|8 E. Nava-Yazdani and expx(0) = x. Now, let γ be a curve in S2 with binormal B (cross product of γ and its unit tangent), and r a scalar function along γ. Then, the map c given by c(s, .) := expγ s(rB), s∈ I, parametrizes a spherical strip with bandwidth r. Figure 3 depicts an example of the shortest path between two spherical curves comprised with their bandwidth functions visualised as strips. Fig. 3.Shortest path of spherical strips 3.4 Hurricane Tracks Hurricanes belong to the most extreme natural phenomena and can cause major im- pacts regarding environment, economy, etc. Intensity of a hurricane is determined by the maximum sustained wind (maxwind), monotonically classifying the storms into categories (due to Saffir–Simpson wind scale; for instance, maxwind ≥ 137 knots cor- responds to category 5). Due to their major impacts on economy, human life and envi- ronment, as well as extreme variability and complexity, hurricanes have been studies in a large number of works. For our example, we used the HURDAT 2 database provided by the U.S. National Oceanic and Atmospheric Administration publicly available on https://www.nhc.noaa.gov/data/, supplying latitude, longitude, and maxwind on a 6 hours base of Atlantic hurricanes. Fig. 4.2010 Atlantic hurricane tracks (left) and the shortest path between two of them (right) with color-coded maximum sustained wind (in knots)Elastic Analysis of Augmented Curves and Constrained Surfaces 9 We represent the tracks as discrete trajectories in S2. For further details and com- parison with other approaches, we refer to [24,25] and the recent work [21]. The latter, also provides statistical analysis and a classification of hurricane tracks in terms of their intensities. Fig. 4 illustrates this data set with a visualization of the 2010 hurricane tracks and a shortest path, where the intensities, considered as auxiliary information, are color-marked. 4 Conclusion In this paper, we analysed the behaviour of speed and curvature under the square root velocity framework for elastic approach to plane curves. Moreover, we applied an extension of this framework to homogeneous Spaces, to metrically compare augmented curves and special surfaces, generated by those curves, using a natural construction via the Riemannian exponential map. Our approach, allows for computationally efficient determination of geodesic paths in the shape spaces of the respective classes of surfaces. Our example applications include tubes, ruled surfaces, spherical strips and hurricane tracks. Future work includes further real world applications, particularly concerning statistical analysis of longitudinal data such as comparison of group wise trends within a hierarchical model as well as classification and prediction. Acknowledgements This work was supported through the German Research Foun- dation (DFG) via individual funding (project ID 499571814). References 1. Ambellan, F., Hanik, M., von Tycowicz, C.: Morphomatics: Geometric morphomet- rics in non-Euclidean shape spaces (2021). https://doi.org/10.12752/8544, https:// morphomatics.github.io/ 2. Bauer, M., Bruveris, M., Marsland, S., Michor, P.: Constructing reparametrization in- variant metrics on spaces of plane curves. arXiv: Differential Geometry (2012), https: //arxiv.org/pdf/1207.5965.pdf 3. Bauer, M., Bruveris, M., Charon, N., Møller-Andersen, J.: A relaxed approach for curve matching with elastic metrics. ESAIM: Control, Optimisation and Calculus of Variations 25 (03 2018). https://doi.org/10.1051/cocv/2018053 4. Bauer, M., Bruveris, M., Harms, Philipp Michor, P.W.: Soliton solutions for the elastic metric on spaces of curves. Discrete & Continuous Dynamical Systems - A 38, 1161–1185 (2018). https://doi.org/10.3934/dcds.2018049 5. Bauer, M., Bruveris, M., Michor, P.W.: Overview of the geometries of shape spaces and diffeomorphism groups. Journal of Mathematical Imaging and Vision 50(1-2), 60–97 (2014) 6. Bauer, M., Charon, N., Klassen, E., Brigant, A.L.: Intrinsic riemannian metrics on spaces of curves: theory and computation. arXiv preprint (2020), https://arxiv.org/abs/ 2003.05590 7. Bauer, M., Harms, P., Michor, P.W., et al.: Sobolev metrics on the manifold of all rie- mannian metrics. Journal of Differential Geometry 94(2), 187–208 (2013)10 E. Nava-Yazdani 8. do Carmo, M.P.: Riemannian Geometry. Mathematics: Theory and Applications, Birkh¨ auser Boston, Cambridge, MA, USA, 2 edn. (1992) 9. Celledoni, E., Eidnes, S., Schmeding, A.: Shape analysis on homogeneous spaces: a gener- alised srvt framework. In: Computation and Combinatorics in Dynamics, Stochastics and Control: The Abel Symposium, Rosendal, Norway, August 2016. pp. 187–220. Springer (2018) 10. Gallot, S., Hullin, D., Lafontaine, J.: Riemannian Geometry. Universitext, Springer, Berlin, 3 edn. (2004) 11. Huang, W., Gallivan, K.A., Srivastava, A., Absil, P.A.: Riemannian optimization for registration of curves in elastic shape analysis. Journal of Mathematical Imaging and Vision 54(3), 320–343 (2016) 12. Kendall, D., Barden, D. Carne, T., Le, H.: Shape and Shape Theory. John Wiley & Sons (1999) 13. Le Brigant, A.: Computing distances and geodesics between manifold-valued curves in the srv framework. Journal of Geometric Mechanics 9(2) (2017) 14. Liu, W., Srivastava, A., Zhang, J.: Protein structure alignment using elastic shape anal- ysis. In: Proceedings of the First ACM International Conference on Bioinformatics and Computational Biology. pp. 62–70 (2010) 15. Michor, P., Mumford, D., Shah, J., Younes, L.: A metric on shape space with explicit geodesics. Atti Accad. Naz. Lincei Cl. Sci. Fis. Mat. Natur. Rend. Lincei (9) Mat. Appl. 19 (07 2007). https://doi.org/10.4171/RLM/506 16. Michor, P.W., Mumford, D.: Vanishing geodesic distance on spaces of submanifolds and diffeomorphisms. Documenta Mathematica 10, 217–245 (2005) 17. Michor, P.W., Mumford, D.: An overview of the riemannian metrics on spaces of curves using the hamiltonian approach. Applied and Computational Harmonic Analysis 23(1), 74–113 (2007) 18. Mio, W., Srivastava, A., Joshi, S.H.: On shape of plane elastic curves. International Journal of Computer Vision 73, 307–324 (2006) 19. Mio, W., Srivastava, A., Joshi, S.: On shape of plane elastic curves. International Journal of Computer Vision 73, 307–324 (07 2007). https://doi.org/10.1007/s11263-006-9968-0 20. Nava-XYazdani, E., Hege, H.C., Sullivan, T.J., von Tycowicz, C.: Geodesic analysis in kendall’s shape space with epidemiological applications. Journal of Mathematical Imaging and Vision pp. 1–11 (2020). https://doi.org/10.1007/s10851-020-00945-w 21. Nava-Yazdani, E., Ambellan, F., Hanik, M., von Tycowicz, C.: Sasaki metric for spline models of manifold-valued trajectories. Computer Aided Geometric Design 104, 102220 (2023). https://doi.org/10.1016/j.cagd.2023.102220 22. Nava-Yazdani, E., Hege, H.C., von Tycowicz, C.: A hierarchical geodesic model for longi- tudinal analysis on manif olds. Journal of Mathematical Imaging and Vision 64(4), 395 – 407 (2022). https://doi.org/10.1007/s10851-022-01079-x 23. Needham, T., Kurtek, S.: Simplifying transforms for general elastic metrics on the space of plane curves. SIAM Journal on Imaging Sciences 13(1), 445–473 (2020). https://doi.org/10.1137/19M1265132 24. Srivastava, A., Klassen, E.P.: Functional and shape data analysis, vol. 1. Springer (2016) 25. Su, Z., Klassen, E., Bauer, M.: The square root velocity framework for curves in a ho- mogeneous space. 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) pp. 680–689 (2017) 26. Su, Z., Klassen, E., Bauer, M.: Comparing curves in homogeneous spaces. Differential Geometry and its Applications 60, 9–32 (2018) 27. Zhang, Z., Su, J., Klassen, E., Le, H., Srivastava, A.: Rate-invariant analysis of covariance trajectories. Journal of Mathematical Imaging and Vision 60, 1306–1323 (2018)",
      "references": [
        "Morphomatics: Geometric morphomet- rics in non-Euclidean shape spaces",
        "Constructing reparametrization in- variant metrics on spaces of plane curves.",
        "A relaxed approach for curve matching with elastic metrics.",
        "Soliton solutions for the elastic metric on spaces of curves.",
        "Overview of the geometries of shape spaces and diffeomorphism groups.",
        "Intrinsic riemannian metrics on spaces of curves: theory and computation.",
        "Sobolev metrics on the manifold of all rie- mannian metrics.",
        "Riemannian Geometry.",
        "Shape analysis on homogeneous spaces: a gener- alised srvt framework.",
        "Riemannian optimization for registration of curves in elastic shape analysis.",
        "Shape and Shape Theory.",
        "Computing distances and geodesics between manifold-valued curves in the srv framework.",
        "Protein structure alignment using elastic shape anal- ysis.",
        "A metric on shape space with explicit geodesics.",
        "Vanishing geodesic distance on spaces of submanifolds and diffeomorphisms.",
        "An overview of the riemannian metrics on spaces of curves using the hamiltonian approach.",
        "On shape of plane elastic curves.",
        "Geodesic analysis in kendall’s shape space with epidemiological applications.",
        "Sasaki metric for spline models of manifold-valued trajectories.",
        "A hierarchical geodesic model for longi- tudinal analysis on manif olds.",
        "Simplifying transforms for general elastic metrics on the space of plane curves.",
        "Functional and shape data analysis",
        "The square root velocity framework for curves in a ho- mogeneous space.",
        "Comparing curves in homogeneous spaces.",
        "Rate-invariant analysis of covariance trajectories."
      ],
      "meta_data": {
        "arxiv_id": "2402.04944v3",
        "authors": [
          "Esfandiar Nava-Yazdani"
        ],
        "published_date": "2024-02-07T15:25:20Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Studies elastic shape analysis of curves via the Square Root Velocity (SRV) transform, focusing on (i) geometric behavior of key invariants for plane curves (explicit formulas for transformed speed and curvature, immersion conditions, and preservation of total curvature/turning number for closed curves), and (ii) intrinsic comparison of augmented curves that parameterize constrained surface classes (tubes/canal surfaces, ruled surfaces, spherical strips) as well as manifold-valued trajectories with auxiliary signals (hurricane tracks with intensity). Provides a unified Riemannian framework enabling geodesic distances/shortest deformation paths in the associated pre-shape and quotient (shape) spaces, including homogeneous-space-valued data (e.g., S^2×R).",
        "methodology": "Builds on reparameterization- and isometry-invariant Riemannian metrics on spaces of immersed curves. Uses first-order elastic Sobolev metrics Ga,b and, in Euclidean settings, the SRV map q(c)=\\dot c/\\sqrt{|\\dot c|}, which is an isometry (mod translations) to a flat L2 space under a specific elastic metric. For plane curves derives SRV-induced expressions: transformed speed \\tilde\\omega=\\sqrt{\\dot\\omega^2/(4\\omega)+\\omega^3\\kappa^2} and curvature relation \\tilde\\kappa\\tilde\\omega=\\kappa\\omega+\\dot\\varphi. For manifold-valued curves in homogeneous spaces M=H/K uses the generalized SRV/TSRV framework with horizontal lifting into H, SRV in the Lie algebra, and optimization over K to compute distances. Augmented curves embed spatial trajectory plus auxiliary scalar/vector data into a higher-dimensional manifold (e.g., R^4 or S^2×R), reducing surface comparison to curve comparison; geodesics are computed via Riemannian optimization using the Morphomatics software.",
        "experimental_setup": "Primarily qualitative/illustrative experiments demonstrating computed shortest paths (geodesics) between objects in the induced shape spaces. Synthetic or constructed examples include: (1) tubes represented by centerline γ(t)∈R^3 and radius r(t), treated as curves in R^4; (2) ruled surfaces represented by base curve γ(t)∈R^3 and a unit direction field v(t) (also reducible to higher-dimensional curve data); (3) spherical strips represented by γ(t)∈S^2 with bandwidth function r(t) via the spherical exponential map exp_γ(s r B). A real-data example uses the NOAA HURDAT2 Atlantic hurricane database (6-hourly latitude/longitude plus max sustained wind), with tracks discretized on S^2 and intensity appended as an auxiliary dimension to form curves in S^2×R; visualizations include 2010 season tracks and a computed shortest path between two tracks with color-coded wind intensity. No quantitative benchmarking against baselines is reported; validation is via geometric correctness and visual plausibility of geodesic deformations.",
        "limitations": "Focuses on theoretical properties and example applications; lacks quantitative evaluation (accuracy, robustness, runtime) and systematic benchmarking versus alternative registration/trajectory metrics. Many constructions assume smooth immersed curves; SRV-transformed curves may fail to be immersions when curvature and speed-derivative share zeros, which can complicate analysis. The homogeneous-space SRV approach relies on horizontal lifts and optimization over subgroup K; practical results may depend on numerical optimization quality and discretization of curves/auxiliary signals. Application sections treat specific constrained surface classes that are uniquely parameterized by augmented curves (e.g., tubes by (γ,r)); broader surface families or noisy/irregular real measurements may violate these assumptions. Handling of scale normalization, landmark constraints, and uncertainty/noise models is not developed in detail.",
        "future_research_directions": "Develop full statistical pipelines on the induced shape spaces (Fréchet means, PGA/principal geodesic analysis, hypothesis testing) for augmented curves and surface classes; extend to hierarchical longitudinal models for group-wise trend comparison. Add rigorous empirical studies: quantitative benchmarks on classification/clustering/prediction tasks (e.g., hurricane track/intensity forecasting), ablations on reparameterization handling, and comparisons with alternative manifold-trajectory and spline/Sasaki-metric approaches. Improve robustness to discretization/noise and missing data (regularization, relaxed matching, probabilistic models). Extend the framework to additional manifolds and auxiliary fields (vector- and tensor-valued augmentations), more general constrained surfaces/submanifolds, and scalable optimization for large datasets.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Iterative Proposal Refinement for Weakly-Supervised Video Grounding",
      "full_text": "MNRAS000, 1–23 (2023) Preprint 20 January 2023 Compiled using MNRAS L ATEX style ﬁle v3.0 The SOUX AGN Sample: SDSS–XMM-Newton Optical, Ultraviolet and X-ray selected active galactic nuclei spanning a wide range of parameter space – Sample deﬁnition Daniel Kynoch,123★ Jake A. J. Mitchell,3 Martin J. Ward,3 Chris Done,3 Elisabeta Lusso45 and Hermine Landt3 1School of Physics and Astronomy, University of Southampton, University Road, Southampton, SO17 1BJ, UK 2Astronomical Institute, Czech Academy of Sciences, Boční II 1401, 141 00 Prague, Czech Republic 3Centre for Extragalactic Astronomy, Department of Physics, Durham University, South Road, Durham, DH1 3LE, UK 4Dipartimento di Fisica e Astronomia, Universita di Firenze, via G. Sansone 1, I-50019 Sesto Fiorentino, Firenze, Italy 5INAF – Osservatorio Astroﬁsico di Arcetri, L.go Enrico Fermi 5, I-50125 Firenze, Italy Accepted XXX. Received YYY; in original form ZZZ ABSTRACT Weassembleasampleof696type1AGNuptoaredshiftof 𝑧=25,allofwhichhaveanSDSS spectrumcontainingatleastonebroademissionline(H α,H βorMg /i.pc/i.pc)andan XMM-Newton X-ray spectrum containing at least 250 counts in addition to simultaneous optical/ultraviolet photometry from theXMM Optical Monitor. Our sample includes quasars and narrow-line Seyfert1s:thusourAGNspanawiderangeinluminosity,blackholemassandaccretionrate. We determine single-epoch black hole mass relations for the three emission lines and ﬁnd that they provide broadly consistent mass estimates whether the continuum or emission line luminosityisusedastheproxyforthebroademissionlineregionradius.Weexplorevariations oftheUV/X-rayenergyindex 𝛼ox withtheUVcontinuumluminosityandwithblackholemass and accretion rate, and make comparisons to the physical quasar spectral energy distribution (SED) model/q.pc/s.pc/o.pc/s.pc/e.pc/d.pc. The majority of the AGN in our sample lie in a region of parameter spacewith 002  𝐿𝐿Edd 2 asdeﬁnedbythismodel,withnarrow-linetype1AGNoﬀsetto lowermassesandhigheraccretionratesthantypicalbroad-linequasars.Weﬁnddiﬀerencesin thedependenceof 𝛼ox onUVluminositybetweenbothnarrow/broad-lineandradio-loud/quiet subsets of AGN:𝛼ox has a slightly weaker dependence on UV luminosity for broad-line AGN and radio-loud AGN have systematically harder𝛼ox. Key words: accretion, accretion discs – black hole physics – galaxies: active – galaxies: high-redshift – quasars: emission lines – quasars: supermassive black holes 1 INTRODUCTION Active galactic nuclei (AGN) are powered by mass accretion onto thesupermassiveblackholesatthecentresofgalaxies.Mostofthe gravitational potential energy of the infalling material is liberated as radiation across the full electromagnetic spectrum. For typical, luminousAGN,themajorityoftheirpoweremergesintheultravio- let and optical bands as thermal radiation from a viscous accretion disc(Shakura&Sunyaev1973;Novikov&Thorne1973).Thenon- thermal,hardX-rayemissionemergesviatheComptonupscattering of soft seed photons from the disc in a hot (𝑇e \u0018100 keV) plasma abovethedisc(the‘corona’e.g.Haardt&Maraschi1991).Astrong relationshipbetweentheultravioletandX-rayemissionofAGNwas found in early studies (e.g. Tananbaum et al. 1979) and its nature ★ E-mail: d.kynoch@soton.ac.uk has been explored further in more recent investigations (e.g. Lusso &Risaliti2017,Salvestrinietal.2019).Thenon-linearandremark- ably tight relation between the ultraviolet and X-ray luminosities enables luminous AGN to be used as ‘standardisable candles’ to test cosmological models (e.g. Sacchi et al. 2022). Clearly the disc and the corona are intimately related, but a detailed description of how has yet to be formulated. For the majority of AGN, the peak of their energy output oc- curs in the far-UV/soft X-ray band which is unobservable because of Galactic extinction. This complicates the determination of the energetics and the disc-corona relationship. One means of circum- ventingthisproblemistomakestudiesofhigh- 𝑧AGNforwhichthe spectral peak is redshifted to observable optical-UV wavelengths, as was done by Collinson et al. (2015). Making detailed studies of the optical and X-ray spectra of 11 sources, Collinson et al. (2015) were able to accurately determine the bolometric luminosities and © 2023 The Authors arXiv:2301.07724v1  [astro-ph.GA]  18 Jan 20232 D. Kynoch et al. accretion rates of the AGN, demonstrating estimates of the bolo- metric luminosity made by scaling from a single wavelength or narrow-band luminosity (e.g. at 2500Å or 2–10 keV) are generally inaccurate (see also Netzer 2019). However, obtaining high-quality multiwavelength spectra for high-𝑧 AGN necessarily restricts the sampletoasmallnumberofbrightquasars,therebylimitingthepa- rameterspacewhichcanbeexplored.Anotherapproachistosample the far-UV and soft X-ray emission on either side of the peak and use a physical model (e.g. Done et al. 2012) to ‘bridge the gap’ and recover the intrinsic spectral energy distribution (SED). This method was adopted by Jin et al. (2012a), who explored the optical and X-ray spectroscopic properties of 51 AGN in a series of works (Jinetal.2012a,b,c).TheyshowedthatthebroadbandSEDsofAGN could generally be ﬁt with three components: a standard accretion disc, a hot corona, and an intermediate ‘warm corona’ responsi- ble for the observed excess of soft X-ray emission (e.g. Magdziarz et al. 1998; Gierliński & Done 2004). The ﬁndings of Jin et al. (2012a,b,c) have enabled the development and reﬁnement of new, physicalmodelsofAGNSEDs(Kubota&Done2018,2019).These models have been constructed with reference to a small number of representativeAGNSEDs.InthedecadesincetheworkofJinetal. (2012a,b,c) the continuing SDSS (Blanton et al. 2017; Pâris et al. 2018) andXMM-Newton (Webb et al. 2020) surveys have greatly increased the number of AGN with quality optical/UV and X-ray spectra. We now have the opportunity to test the predictions made bycurrentphysicalmodelsagainsttheobservedpropertiesofalarge and diverse sample of AGN. Achievingamorerigorousunderstandingoftheaccretionﬂow and disc-corona relationship will also enable us to better address anotheroutstandingprobleminastrophysics:theoriginofrelativis- tic jets. How AGN launch and power relativistic jets of outﬂowing matterisanactiveareaofresearch.Progresstowardsafullerunder- standingofthenatureofjetsandtheirrelationtotheaccretionﬂow may be made via comparisons to accreting stellar-mass black holes (X-ray binaries: XRBs, e.g. Fender et al. 2004) in which the disc- corona system likely regulates the launching of jets. Many studies have explicitly investigated the analogies between AGN and XRBs in terms of the accretion ﬂow properties (e.g. Noda & Done 2018; Ruan et al. 2019; Arcodia et al. 2020) and the disc-jet coupling (e.g. Merloni et al. 2003; Körding et al. 2006; Svoboda et al. 2017; Fernández-Ontiveros&Muñoz-Darias2021;Moravecetal.2022). Generally these studies are supportive of a uniﬁed scheme of mass accretion and ejection across the mass scale, although many open questions have yet to be resolved. Studies of high accretion rate AGN in the local Universe (narrow-line Seyfert 1s: NLS1s) have found a relative deﬁcit of radio-loud sources: only\u00195 per cent of NLS1sareradio-loud(e.g.Komossaetal.2006;Rakshitetal.2017) compared with\u001915–20 per cent of quasars (e.g. Kellermann et al. 1989), again suggesting a relationship between the accretion ﬂow (the disc-corona conﬁguration) and the presence of a radio jet. Previousstudieshavemostlyeitherinvestigatedverylargeand diverse AGN samples without a detailed exploration of their spec- troscopic properties (e.g. Svoboda et al. 2017) or have presented a thorough analysis of much smaller, focussed samples of AGN with high-quality spectroscopic data (e.g. Jin et al. 2012a,b,c and Collinson et al. 2015, 2017). In this ﬁrst paper in a series, we ex- pandontheworkofJinetal.(2012a,b,c)andCollinsonetal.(2015, 2017) and make use of recent optical, UV and X-ray catalogs to compile a large sample of AGN (696 unique sources) with both optical and X-ray spectra in addition to broad multi-wavelength coverage.Thisnewsamplehastheadvantageofbeinglargeenough to investigate population statistics whilst still having good qual- ity multi-wavelength data. It has been selected to probe a diverse rangeofAGNproperties;aswewilldemonstrate,thesamplespans several orders of magnitude in black hole mass and luminosity (ac- cretion rate). The sample is a well-suited selection with which to perform detailed investigations of the evolution of the disc-corona system with black hole mass and accretion rate (Mitchell et al., submitted).InthispaperwemakekeymeasurementsofAGNprop- erties and take a preliminary look at the evolution of the spectral energy distribution with both mass and luminosity. In Section 2 we describe the assembly of the sample; in Section 3 we obtain multi-wavelengthmeasurementsfortheselectedAGN;inSection4 we determine black hole masses from optical/UV data; we inves- tigate the UV/X-ray energy index𝛼ox and make comparisons with a physical model in Section 5; ﬁnally in Sections 6 and 7 we dis- cuss our results and summarise our conclusions. To convert ﬂuxes to luminosities we have assumed a ﬂatΛCDM cosmology with 𝐻0 =70 kms\u00001 Mpc\u00001,Ωm =03 andΩΛ =07. 2 SAMPLE SELECTION 2.1 The parent catalogues 2.1.1 Optical spectra: SDSS-DR14Q The Quasar Catalog of the Fourteenth SDSS Data Release (SDSS- DR14Q, Pâris et al. 2018) contains 526356 quasars. Each quasar has been the subject of at least one optical spectroscopic observa- tion recorded on or before 2016 December 5. The catalogue also contains SDSS photometric data from imaging observations. Mul- tiwavelength data fromROSAT, XMM-Newton, GALEX, 2MASS, WISE, UKIDSS and FIRST are included, where available. The XMM-Newton data are taken from the 3XMM-DR7 catalogue (an earlier version of the serendipitous survey catalogue: see below). 14736 of the SDSS-DR14Q quasars (\u00193 per cent) have an X-ray source detection within a 5 arcsec matching radius. Although the cataloguecontainssomeX-raydataobtainedfroman XMM-Newton catalogue (3XMM-DR7), it does not contain any optical/UV pho- tometry recorded with theXMM-NewtonOptical Monitor (OM). 2.1.2 X-ray data: 4XMM-DR9 4XMM-DR9istheﬁrstiterationofthefourthsourcecataloguecom- piled from theXMM-NewtonSerendipitous Survey; this catalogue, releasedin2019December,istheninthdatareleasefromthesurvey overall (Webb et al. 2020). The catalogue contains 810795 X-ray detections pertaining to 550124 unique sources from observations made between 2000 February 3 and 2019 February 26. 2.1.3 Optical and ultraviolet photometry: XMM-SUSS4.1 Wetake XMM-Newtonoptical/UVphotometricdatafromthefourth XMM-Newton Serendipitous Ultraviolet Source Survey (SUSS: Page et al. 2012) catalogue, XMM-SUSS4.1. This catalogue, re- leased in 2018 January, compiles data from observations made up to 2017 July. It contains 8176156 entries from 5503765 sources. 2.2 Initial source selection criteria 2.2.1 Quasars WeﬁrstestablishwhichopticalquasarshaveanX-raysourcedetec- tion in the latestXMM-Newton data release. To do this, we cross- MNRAS000, 1–23 (2023)The SOUX AGN Sample – Sample deﬁnition3 match the optical catalogue with the slimline version of the X-ray catalogue. The slimline 4XMM-DR9 catalogue contains just one row per unique X-ray source (rather than one row per detection as in the full catalogue). There are 550050 X-ray sources in the cata- logue,excluding74whichareﬂaggedas‘CONFUSED’(i.e.witha nonzeroprobabilityofbeingassociatedwithmorethanonedistinct source). Following Pâris et al. 2018, we choose a match radius of 5arcsec andretainall ofthebest (nearest)matchingX-ray sources. Fig. 1 shows the distribtion of angular separations between the op- tical quasars and X-ray sources. Our cross-match returns 17336 X- ray detected optical quasars (approximately 3.3 per cent of those in SDSS-DR14Q). We record the unique X-ray source ID (‘SRCID’) of each quasar. We do not remove broad absorption line (BAL) quasars from the optical catalogue initially, but will weed these out of our sample by visually inspecting the optical spectra. Before performing a cross-match to the full X-ray catalogue, we clean it by excluding the sources which: \u000fhave a poor source summary ﬂag (SUM_FLAG> 3); \u000fare ﬂagged as ‘CONFUSED’; \u000fwere observed with a high background. Thecleancataloguethencontains88percentofthetotalnumberof records,with713866X-raydetectionsfrom484726uniquesources. Using the X-ray source IDs, we ﬁnd all clean observations of the optical quasars. We match all records in the cleaned, full 4XMM-DR9cataloguetoourpre-selectedopticalquasars.Weﬁnd that 17299 of the pre-selected 17336 quasars have a clean X-ray detection. Of these, 5364 (31 per cent) were observed byXMM- Newton more than once; Fig. 2 shows the number of clean X-ray observations per optical quasar. WethenmatchwithsourcesintheOMcatalog.Thismatching requires that the OM source is within 1 arcsecond of the optical quasar coordinates and that the observation ID of the OM record matches that of the X-ray observation (i.e. our OM and EPIC data were recorded simultaneously). 4499 quasars meet these criteria of which868havemorethanonesimultaneousOM-EPICobservation. At 𝑧 & 04, the OM photometry may be compromised by the strong Lyα 𝜆1216 UV emission line and Lyα forest absorption blueward of912 Å (rest frame). To determine which OM ﬁlters would be free of Lyαemission and absorption, we calculated the quasarredshiftsforwhichtheredwingofLy αwouldfalljustoutside of the eﬀective bandpass of each ﬁlter. Then, for each redshift bin we require an OM detection in one of the following: \u000f 𝑧 040: any ﬁlter; \u000f040 6 𝑧 055: V, B, U, UVW1 or UVM2 ﬁlters; \u000f055 6 𝑧 090: V, B, U or UVW1 ﬁlters; \u000f090 6 𝑧 135: V, B, or U ﬁlters; \u000f135 6 𝑧 195: V or B ﬁlters; \u000f195 6 𝑧 250: V ﬁlter only. 1768 quasars have at least one useful OM photometry point. WewishtoobtainreasonableconstraintsontheX-rayspectral parameters of these quasars, for which we require relatively high- quality spectra. We therefore make a quality cut on the number of X-raycountsrecordedbyanEPICdetectorinthe0.2–12keVband. The number of sources as a function of X-ray counts is shown in Fig. 3. It can be seen in Fig. 3 that the counts distribution peaks & 200 counts and that only 17 per cent of detections are made with 1000 counts or more. We chose to make a cut at 250 counts, which includes 46 per cent of the clean detections. For quasars with more than one X-ray observation, we retain the one with the greatestnumberofcountsinanydetector(sothatwenowhaveonly onesimultaneousEPICandOMobservationofeachquasar).After applying this cut, we are left with 782 quasars. Tomakeanestimateoftheblackholemass,werequirespectral coverage of at least one of the emission lines Hα, Hβor Mg/i.pc/i.pc. Hα or Hβwill be visible in the spectra of low-redshift sources and the Mg/i.pc/i.pcemission line will be visible in the wavelength range of the BOSSspectrographupto 𝑧 . 25 (seeFig.4).Wethereforeconsider only the SDSS quasars with𝑧 6 25. Following this redshift cut, our sample contains 768 quasars. 2.2.2 Narrow-line Seyfert 1s Narrow-line Seyfert 1s (NLS1s) are a subset of unobscured AGN with relatively narrow broad lines. They are generally deﬁned as optical type 1 AGN with FWHM(Hβ) . 2000 kms\u00001, weak [O/i.pc/i.pc/i.pc]𝜆5007 relative to Hβ and strong Fe/i.pc/i.pcemission (e.g. Oster- brock & Pogge 1985, Goodrich 1989). NLS1s are typically found in spiral galaxies in the local Universe. Because Seyfert AGN are lessluminousthanquasars,manyNLS1sareabsentfromtheSDSS quasar catalogues. For example, of the 12 NLS1s in the sample of Jin et al. (2012a), only 2 are present in our selection of quasars. Nevertheless, NLS1s represent a unique and important region of parameter space to explore, and enable contrasts to be made with typical broad-line AGN and quasars (e.g. Jin et al. 2012a,b,c). Rakshit et al. (2017) (hereafter R17) assembled a sample of 11101 NLS1s from the SDSS-DR12 spectroscopic database, by analysingthespectraof‘QSO’-typesourcesandretainingthosewith FWHM¹H βº6 2200 kms\u00001 and ﬂux ratio»O /i.pc/i.pc/i.pc¼𝜆5007H β 3. We identify NLS1s with XMM-Newton OM and EPIC data by matching the R17 catalog to the X-ray and OM serendipitous cata- logs, following the method described in Section 2.2.1. Our criteria return147NLS1s:63newsourcesand84thatwerealreadyincluded from the quasar catalogue. 2.3 Optical spectral ﬁtting Rakshit et al. (2020) (hereafter R20) performed ﬁts to the optical spectraofallSDSSDR14Qquasarsusingthethepublicly-available /p.pc/y.pc/t.pc/h.pc/o.pc/n.pccode P/y.pcQSOF/i.pc/t.pc(Guo et al. 2018, Guo et al. 2019, Shen etal.2019).Forthequasarsinoursample,wecantakemanyemis- sion line and continuum measurements from R20. However, the 63 NLS1s not present in the quasar catalog have only optical spectral measurementsfromR17whousedaprivate,customspectralﬁtting routinetoobtaintheirmeasurementsofNLS1s.Thesetworoutines diﬀerinseveralrespects.Thebroademissionlineswereoftenmod- elled with a single Lorentzian proﬁle in R17; Lorentzian proﬁles are not currently an option inP/y.pcQSOF/i.pc/t.pcand instead one or more Gaussians may be used to ﬁt a line. R20 chose to use 3 Gaussians to model broad Hβand Hα, for example. The host galaxy subtrac- tion, which is often required for the low-luminosity NLS1s, is also approached in diﬀerent ways. R17 removed the host galaxy contri- bution by performing simple stellar population modelling whereas principal component analysis (PCA: e.g. Yip et al. 2004a,b) is em- ployed inP/y.pcQSOF/i.pc/t.pc. Naturally these diﬀerent methods will yield somewhat diﬀerent results for the continuum and emission line pa- rameters. Derived quantities such as the virial black hole mass and Eddington ratio will then also diﬀer. For consistency of the optical spectral measurements of our sample, we adaptedP/y.pcQSOF/i.pc/t.pcto replicate the spectral ﬁtting pro- cedure of R20 and reﬁt the spectra of the sources drawn from R17. We brieﬂy summarise the adoptedP/y.pcQSOF/i.pc/t.pcspectral ﬁtting MNRAS000, 1–23 (2023)4 D. Kynoch et al. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Optical / X-ray source separation [arcsec] 0 1000 2000 3000Number Quasars 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 Per cent 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Optical / X-ray source separation [arcsec] 0 50 100 150Number NLS1s 0 1 2 3 4 5 6 Per cent Figure 1.The left-hand ordinate axis indicates the number of matched quasars. The right-hand ordinate axis gives the percentage of all 526356 quasars, with the cumulative distribution shown in red. To select our sample, we chose a matching radius of 5 arcsec which returns 17336 quasars, approximately 3.3 per cent of all SDSS-DR14Q quasars.Right: The same, for Rakshit et al. 2017 NLS1s. Of 11101 SDSS NLS1s, 676 (6.1 per cent) have a clean X-ray detection within 5 arcsec. 5 10 15 20 25 30 35 40 45 50 Number of X-ray detections 0 1000 2000 3000Number 0.0 0.2 0.4 0.6 0.8 1.0 Fraction 10 20 30 40 500 20 40 60 80 100 Figure 2.The number of clean X-ray detections for 17299 X-ray detected optical quasars. 11935 quasars (69 per cent) have only 1 clean X-ray detec- tion.Only109have10ormoredetections.Theredlineshowsthecumulative fraction, with the fraction indicated on the right-hand ordinate axis. routine here. In this setup, the Schlegel et al. (1998) dust map is queried to determine the line-of-sight extinction and the spectrum is dereddened using the extinction curve of Fitzpatrick (1999). The spectrumisthentransformedtotherest-frame.ThePCAmethodis used to decompose stellar and AGN emission with 5 and 20 eigen- spectra used to construct the host galaxy and quasar contributions, respectively. The host galaxy subtracted continuum is then mod- elledwithapower-lawplusFe /i.pc/i.pctemplatesandafunctionalBalmer continuum. Emission line complexes are then modelled separately in several broad windows: \u000f6400–6800 Å: Hα, the [N/i.pc/i.pc] doublet (𝜆=65496585 Å) and the [S/i.pc/i.pc] doublet (𝜆=67186732 Å); \u000f4640–5100 Å: Hβ, the [O/i.pc/i.pc/i.pc] doublet (𝜆=49595007 Å) and He/i.pc/i.pc; \u000f2700–2900 Å: Mg/i.pc/i.pc. Following the ﬁtting procedure of R20, the broad components of Hαand Hβand He/i.pc/i.pcare each modelled with 3 Gaussian compo- nents; broad Mg/i.pc/i.pcis modelled with 2 Gaussians. The broad Gaus- 1 2 3 4 5 Log X-ray counts 0 50 100 150 200 250 300Number 0.0 0.2 0.4 0.6 0.8 1.0 Fraction 250 counts 1000 counts Figure3. TheX-raycountsdistributionfor2374‘clean’X-rayobservations with simultaneousXMM-OM photometry redward of Lyα. For each obser- vation, we take X-ray counts to be the greatest number of counts recorded by any of the three EPIC detectors (pn, MOS1 or MOS2). The left-hand ordinate axis gives the total number of observations and the right gives the fraction with a minimum number of counts (shown by the red line). sians have a minimum FWHM of 900 kms\u00001 and maximum ve- locity oﬀset \u00063000 kms\u00001 from their centre. Narrow Gaussians are used to model forbidden lines and the narrow components of the permitted lines; these are allowed a width in the range 100 6 FWHM 6 900 kms\u00001. [O/i.pc/i.pc/i.pc]𝜆𝜆49595007 additionally has Gaussian wings that may be broader than the narrow cores (in the range400 6 FWHM 6 2000 kms\u00001). The velocity oﬀsets of the narrow lines are not allowed to exceed\u00061000 kms\u00001. The ﬂux ratios of the [O/i.pc/i.pc/i.pc] and [N/i.pc/i.pc] doublets are ﬁxed to their theoretical value 1:3. In each emission line complex the widths and oﬀsets of the narrow lines are tied together. Uncertainties on the emission lineandcontinuumparamatersareestimatedusingtheMonteCarlo method, resampling each spectrum 50 times. Following the proce- dures of R20 and Calderone et al. (2017), we added a quality ﬂag for each measured emission line and continuum point; the ﬂags are listed in Appendix C. P/y.pcQSOF/i.pc/t.pcuses optical galaxy eignenspectra to perform the host galaxy subtraction; these eigenspectra do not extend into the MNRAS000, 1–23 (2023)The SOUX AGN Sample – Sample deﬁnition5 UVandthereforetheﬁttedspectrumistruncatedtomatchthewave- length range of the eigenspectra (\u00193500–9000 Å). Consequently, therearenomeasurementsoftheUVcontinuumandemissionlines for sources in which host galaxy subtraction was performed in the optical.ToobtaintheUVspectralmeasurementsfortheseAGNwe checked for spectra that contained a Mg/i.pc/i.pcline that was not initially measured and then ﬁt the short-wavelength part of these spectra (𝜆rest =1000–4000Å)withoutperforminghost-galaxysubtraction. The measurements for Mg/i.pc/i.pcand the UV continuum were added to our catalogue, and we have added a ﬂag (‘MGII_SEP’) to indicate the AGN for which the Mg/i.pc/i.pcregion was ﬁtted separately from the optical region. 2.3.1 Identiﬁcation of broad- and narrow-line type 1 AGN To ease the comparison between AGN with relatively narrow and broad permitted emission lines, we assigned each AGN a linewidth ﬂag (‘B’ for broad-line type 1 AGN; ‘N’ for narrow-line type 1 AGN). The determination is based on the ﬁtted width of Hβwhere it is available, Mg/i.pc/i.pcfor the higher-redshift sources without Hβ, and Hαonly if neither Hβor Mg/i.pc/i.pcwere measured. (Spectroscopic coverage of these emission lines as a function of redshift is shown in Fig 4.) We set the division between the subsets at the usual 2000 kms\u00001. The diﬀerences in ﬁtting routine and the division between narrow- and broad-line AGN mean that our classiﬁcations diﬀer from those of R17 (those authors measured lines by ﬁtting a LorentzianproﬁleandchosealimitofFWHM[H β]6 2200 kms\u00001). Consequently, not all AGN drawn from R17 are ﬂagged ‘N’ in our sample. Typically, the broad line FWHMs measured byP/y.pcQSOF/i.pc/t.pc are slightly broader than those measured by R17. However, the diﬀerencesinlinewidthmeasuredbythetworoutinesarerelatively minor:themediandiﬀerenceinFWHM(H β)is \u0019¸100 kms\u00001 and for approximately two-thirds of the AGN from the NLS1 catalogue in our sample, FWHM(Hβ) as measured byP/y.pcQSOF/i.pc/t.pcis within \u0006350 kms\u00001 of the width measured by R17 (350 kms\u00001 is the median uncertainty on FWHM[Hβ] from P/y.pcQSOF/i.pc/t.pc). We note that the narrow-line type 1 AGN in our sample are not necessarily NLS1s by the standard deﬁnition described in Section 2.2.2: we have not always used broad Hβas the characteristic line width and wehavemadenoassessmentofthestrengthsof[O /i.pc/i.pc/i.pc]𝜆5007 orthe optical [Fe/i.pc/i.pc] emission. 2.4 Quality checks and selection of the ﬁnal SOUX sample We performed a visual inspection of all 831 optical spectra and removedanysourceswhichhadlowsignaltonoiseoranyabsorption featuresparticularlyaﬀectingthelineregionsfromwhichblackhole mass measurements were derived. In addition any objects with a diﬀuse emission ﬂag in their X-ray spectra were removed from the sample. Following this quality control, our ﬁnal sample consists of 696AGN1.Ofthese,636aredrawnfromSDSS-DR14Q,60unique sourcesareaddedfromR17and72arecommontobothcatalogues. 1 Inthiswork,weconsideronlyoneopticalspectrumpersource:thatlisted as the ‘science primary’ spectrum at the time of the catalogue compilation. However, 116 AGN have multiple spectra recorded on diﬀerent nights; we describe these in Appendix B 0.0 0.5 1.0 1.5 2.0 2.5 Redshift 0 20 40 60 80 100Number 25 50 75 90 95 Percentile H α H β Mg ii Figure 4.The redshift distribution of the 696 AGN in our sample. Black dotted lines indicate the 25th, 50th, 75th, 90th and 95th percentiles of the distribution. The top panel shows the redshift range for which the centroids of prominent broad emission lines (Hα, Hβand Mg/i.pc/i.pc) are just visible in the wavelength coverage of the SDSS (dotted lines) and BOSS (solid lines) spectrographs. 3 MULTIWAVELENGTH MEASUREMENTS We will present a detailed study of the broadband spectral energy distributions (SEDs) of our AGN in a future paper (Mitchell et al., submitted). However, the SED shape can be characterised by two common parameters: the radio loudness and UV-X-ray energy index. The radio-loudness parameter indicates the power of AGN outﬂowsrelativetotheaccretionﬂow,withradio-loudAGNexhibit- ing powerful jets (Section 3.3). The optical luminosity at 4400 Å and radio luminosity at 5 GHz are required to calculate the radio- loudness. The UV-X-ray energy index (𝛼ox: Section 5) indicates the relative power of the UV-emitting accretion disc to the X-ray emitting corona. To calculate𝛼ox we require a measurement of the UV luminosity at 2500 Å and the X-ray luminosity at 2 keV. In thisSection,wedescribehowthemeasurementsofthesequantities were obtained. 3.1 UV and optical luminosities For sources at𝑧 & 05 the rest-frame monochromatic luminosity at 2500 Å is available directly from the SDSS optical spectrum. For nearersources,thiswavelengthfallsoutsidetheSDSSspectrograph range, but photometry covering it may be available from either XMM OM or SDSS. For AGN at lacking spectroscopic coverage of 2500 Å we ﬁrst attempt obtain an estimate from theXMM OM photometry. We determine which OM bands sample 2500 Å in the rest frame and from these we calculate the luminosity from the ﬂux in the band with an eﬀective wavelength closest to rest frame 2500 Å. If there is noXMM OM coverage of 2500 Å we take the PSF ﬂux from the appropriate Sloan ﬁlter (either theu or g band). We deredden the photometry, taking the line-of-sight colour excess from the Schlegel et al. (1998) dust maps as was done for the spectral analysis and convert the ﬂuxes to rest-frame luminosities.Finally, 𝐿2500 isobtainedfromtheopticalspectrumof 421AGN,from XMMOMphotometryof237AGNandfromSDSS photometry of 27 AGN; overall, we have a measurement of𝐿2500 for 685 AGN (98 per cent of the full sample). The AGN lacking a 𝐿2500 measurement are all at the low-redshift end of our sample (𝑧 021). MNRAS000, 1–23 (2023)6 D. Kynoch et al. Theopticalcontinuumatrest-frame4400Åwasnotmeasured byR20orR17but,similarlyto 𝐿2500,𝐿4400 canbemeasuredforthe majority of our AGN either from the SDSS spectrum, or from the Sloanor XMMOMphotometry.Whenmakingameasurementfrom theopticalspectrum,weavoidcontaminationofthecontinuumﬂux measurement by the blended Hγ, [O/i.pc/i.pc/i.pc] and Fe/i.pc/i.pcemission lines by performing a linear interpolation between line-free continuum windowsat4200and5100Å,wherepossible.Inspectrawherethese twowindowsarenotpresent(oraretooneartheendofthespectrum to be reliable), we measure the 4400 Å ﬂux directly. For AGN lacking optical spectral coverage of 4400 Å we took the ﬂux from the photometric band containing rest-frame 4400 Å, corrected for Galacticreddeningandconvertedtheﬂuxtoarest-frameluminosity, as was done for the𝐿2500 measurements. In our ﬁnal sample of 696 AGN, 482 have𝐿4400 measured by interpolating the spectral continuum; 111 from a direct measurement of the 4400 Å spectral ﬂux; 38 estimates from SDSS photometry; and 65 have no 4400 Å coverage in either the spectrum or photometry. None of the 65 sources lacking𝐿4400 from the SDSS spectrum or photometry had coverage in theXMM OM data because they were higher-z AGN andtheOMﬁltersdonotcoverlongenoughwavelengthstocontain the rest-frame 4400 Å. The AGN lacking a𝐿4400 measurement are all at the high-redshift end of our sample (𝑧 ¡1). Themixtureofspectroscopically-andphotometrically-derived UV and optical luminosities is not ideal. Whilst theXMM OM photometry is contemporaneous with the X-ray data, broad-band photometry is susceptible to contamination from non-continuum emission, particularly at lower redshifts. The SDSS andXMM data are not contemporaneous, so AGN variability will introduce some scatter between spectroscopic and photometric ﬂuxes. Addition- ally, aperture eﬀects may also introduce systematic diﬀerences. In Appendix A we assess the eﬀects of mixing diﬀerent measures of these luminosities. We ﬁnd that on average the diﬀerences be- tween spectroscopic and photometric luminosities are small, with hΔlog¹𝐿4400ºi=\u0000003\u0006013 andhΔlog¹𝐿2500ºi=\u0000002\u0006016. As Fig. A1 shows, the spectroscopic and photometric luminosity measures are approximately equal across several orders of magni- tude in luminosity, although there is substantial scatter. We discuss the potential impact on our results in more detail in Appendix A and Section 6.3. 3.2 X-ray luminosities AdetailedanalysisoftheX-rayspectraofoursourcesisbeyondthe scope of this paper but will be presented in future work. However, it is possible to estimate the X-ray luminosities from data provided in the DR9 catalog. Our sources span a wide range of redshifts and therefore the EPIC ﬂuxes reported for various (observed) energy bands are not directly comparable since they sample diﬀerent parts of the emitted spectrum. TocalculatethemonochromaticX-rayﬂuxatrest-frame2keV, wefollowasimilarapproachtothatofLusso&Risaliti(2016)(see section 2.1 of that paper). We determine the EPIC ﬂux in soft (0.5–2 keV) and hard (2–12 keV) energy ranges. An approximate correctionforGalacticphotoelectricabsorptionisappliedtotheX- rayﬂuxes2.Fromthesebandﬂuxesweestimatethemonochromatic 2 The line-of-sight Galactic neutral hydrogen column density is obtained from the maps of the Leiden/Argentine/Bonn (LAB) Galactic HI Survey (Kalberla et al. 2005) using the Python toolgdpyc (https://pypi.org/ project/gdpyc/). 24 26 28 log □ L[2 keV]/erg s□1 Hz□1) 0 50 100 150 200Number Med. = 26.2 σ = 0.6 0 1 2 3 □ 0 50 100 150 200 250 Med. = 1.9 σ = 0.4 Med.BL = 1.9 σ = 0.4 Med.NL = 2.3 σ = 0.5 Figure 5.Distributions of the monochromatic luminosity at 2 keV in the restframe(left)andtheestimatedX-rayphotonindex Γ(right).Intheright- hand panel the subsample of 101 AGN with relatively narrow permitted lines(‘NL’)areshowninred,andthe595broad-linesources(‘BL’)inblue. ﬂuxes at 1 and 3.5 keV, assuming a photon indexΓ =17 in each energy band. We then translate these ﬂuxes at observed energies to luminosities at rest-frame frequencies inlog¹𝜈º-log¹𝜈𝐿𝜈ºform. Using these two points, the rest-frame monochromatic 2 keV lumi- nosity is calculated by linear interpolation (for sources with𝑧 1) or extrapolation (for sources with𝑧 ¡1). An estimate of the X-ray photonindex Γisalsomadebycomputingthegradientbetweenthe absorption-corrected monochromatic ﬂuxes at 1 and 3.5 keV3. In Fig. 5 we show the distributions oflog¹𝐿2 keVºand Γ. Our samplespansapproximately4dexinX-rayluminosity,withmedian log¹𝐿2 keVº = 262 and 𝜎 = 06. The median estimated photon index isΓ= 19 with 𝜎 =04. 673 of the AGN (97 per cent) have 1 6 Γ 6 3. Less than 2 per cent of sources haveΓ  1, perhaps indicatingabsorptionofsoftX-raysintheAGNorhostgalaxy.The Γ distribution of the 101 narrow-line AGN is diﬀerent from that of the 595 broad-line AGN. The narrow-line AGN have a median Γ= 23 and𝜎=05,indicatingthattheirX-rayspectraaregenerally softer than typical quasars. Kolmogorov-Smirnov and Anderson- Darling tests both indicate that the photon indices of the narrow- lineandbroad-lineAGNaredrawnfromdiﬀerentdistributions.For a null hypothesis that the two samples are drawn from the same distributionwedetermineaKSstatistic0.41and 𝑝=8 \u000210\u000014 and AD statistic 44 and𝑝 0001. 3.3 Radio properies Our sample of 696 AGN was cross-matched with both FIRST (Beckeretal.1995)andNVSS(Condonetal.1998),usingamatch- ingradiusof10 00thathasbeenshowntohaveafalseassociationrate of only 0.2 per cent with FIRST (Lu et al. 2007). Both FIRST and NVSS sample the sky at 1.4 GHz, with beam-widths of 5.600and 4500respectively.679ofourAGNwerewithintheFIRSTfootprint, of which we found a matching radio source for 107. 73 AGN also have an NVSS radio detection (all of which were FIRST-detected). We converted the 1.4 GHz ﬂuxes to rest-frame 5 GHz luminosities using the method of Alexander et al. (2003), assuming a common radio spectral index (𝛼R) of 0.6. A comparison of the NVSS and 3 Wenotethatourresultsdonotdependverystronglyontheassumptionof a standardΓ= 17 in the soft and hard bands to estimate the 1 and 3.5 keV ﬂuxes. If we compare the dervied distributions ofΓ and log¹𝐿2keVºthat resultfrominsteadassuming Γ= 15 and1.9,weﬁndthatthemean Γdiﬀer by\u001901 and the meanlog¹𝐿2keVºdiﬀer by only\u0019001 dex. MNRAS000, 1–23 (2023)The SOUX AGN Sample – Sample deﬁnition7 1024 1026 1028 LFIRST 5GHz (W Hz□1) 1023 1024 1025 1026 1027 1028 LNVSS 5GHz (W Hz□1) Broad-line Narrow-line Figure6. Acomparisonoftherest-frameluminositiesat5GHzderivedfrom FIRST and NVSS ﬂuxes at 1.4 GHz, as described in the text. The dashed line marks the one-to-one relation, the shaded blue region being a factor of 2 deviation, and the pink shaded region a factor of 5. The top-left inset ﬁg- ure shows the radio map for the FR-II source SDSSJ151443.07¸365050.4, whichhasamuchhigherluminosityinNVSSthanFIRST,asaresultofthe larger beam size of the former survey. The 45\" beamwidth of NVSS is in blue, and the 5.6\" beamwidth of FIRST is in yellow. FIRST luminosities for the 73 of objects matched with both cata- loguescanbeseeninFig.6.Itisclearthatforthemajorityofsources the FIRST and NVSS luminosities match to within a factor of 2. ThepopulationisskewedtowardshigherluminositiesinNVSSdue to its larger beam width; this is most pronounced in objects which possess extended morphologies as can be seen by the radio map of the FR-II source SDSSJ151443.07¸365050.4 (Fig. 6, inset). For the 101 radio-detected AGN with a𝐿4400 measurement we calculated the radio loudness according to the common deﬁni- tion of𝐿5 GHz𝐿4400 (Kellermann et al. 1989). Fig. 7 displays the distribution of the 5 GHz luminosities of the radio-detected AGN. The distribution has been separated into distinct populations: the left-hand panel shows the radio-loud and radio-quiet subsets and the right-hand panel shows the narrow-line and broad-line subsets (seeSection2.3.1).Formanyundetectedradiosources,wecanstill estimate whether they are likely to be radio-quiet by considering the FIRST detection threshold as an upper limit to their radio ﬂux. Assuming a FIRST survey ﬂux density threshold of 1 mJy (Becker et al. 1995), we calculate a curve corresponding to the lowest rest- frame5GHzluminositywewouldexpectforasourcetobedetected; we show this curve in Fig. 8. For radio undetected sources we plot the radio luminosity corresponding to𝑅 = 𝐿5 GHz𝐿4400 = 10 (the luminosity above which they would be considered radio-loud). Radio-undetected AGN with luminosities more than 3𝜎above the curve (206 AGN) are considered to be radio-quiet, since we would expect them to have been radio-detected if they were radio-loud. However, as Fig. 8 illustrates, there remain 304 AGN below the curvewhicharetoofaintforustodeterminewhethertheyareeither radio-loud or radio-quiet. Following this exercise, we determine that our sample contains 68 radio-loud AGN, 239 radio-quiet AGN 22 24 26 28 log(L[5 GHz]/W Hz□1) 0 5 10 15 20 25Number RL (67) RQ (34) ? (6) 22 24 26 28 log(L[5 GHz]/W Hz□1) 0 5 10 15 20 25 NL (19) BL (88) Figure 7.For the 107 FIRST radio detected AGN in our sample we show the distribution of 5 GHz rest-frame radio luminosities derived from the observed 1.4 GHz ﬂuxes, as described in the text. In the left panel, the distributionisdividedintoradio-loudandquiet(‘RL’and‘RQ’)subsamples inadditionto6sourceslackinganoptical 𝐿4400 measurementwithunknown radioloudness(‘?’).Intherightpanelthedistributionisdividedintonarrow- line and broad-line (‘NL’ and ‘BL’) subsamples on the basis of the relative widths of the permitted optical emission lines. 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 Redshift 21 22 23 24 25 26 27 28log(L/W Hz□1) 5 GHz limit Radio-det. & RL Radio-det. & RQ Radio-undet. & RQ Radio-undet. & ?? Figure8. Theorangecurveshowsthelimitingrest-frame5GHzluminosity for a source to be detected by FIRST at 1.4 GHz, assuming a survey ﬂux densitythresholdof1mJyandradiospectralindex 𝛼=06.Radio-detected and radio-loud (radio-quiet) sources are shown with red crosses (blue cir- cles). For radio-undetected sources, we plot the luminosity corresponding to 𝑅 = 𝐿5GHz𝐿4400 = 10. Undetected sources more than 3𝜎 above the curve(cyantriangles)areconsideredradio-quiet.Undetectedsourceswithin 3𝜎of the curve and below (grey triangles) could be either radio-quiet or radio-loud. and a total of 389 AGN for which we are unable to determine the radio-loudness. The radio-loudness classiﬁcations are summarised in Table 1 and illustrated in Fig. 9. Overall, \u001810 per cent of our sample with both FIRST and 4400 Å coverage is found to be radio-loud, similar to previous estimates for bright quasars (Kellermann et al. 1989; Miller et al. 1990). However, we caution that this radio-loud fraction is a lower limit because of the large number of sources in our sample without anyradio-loudnessdetermination.Wetentativelyﬁnddiﬀerencesin the radio-loud fractions of broad-line and narrow-line type 1 AGN. Whilst narrow-line AGN make up\u001915 per cent of the sample MNRAS000, 1–23 (2023)8 D. Kynoch et al. Table 1.Radio-loudness of the sample Subset 𝑁 Per cent No radio coverage (Outside FIRST footprint) 20 2.8 % of the full sample No 4400 Å coverage 65 9.3 % of the full sample Radio-detected and radio-loud 68 11.1 % of AGN with radio and 4400 Å coverage Radio-detected and radio-quiet 33 5.4 % of AGN with radio and 4400 Å coverage Radio-undetected and radio-quiet 206 33.7 % of AGN with radio and 4400 Å coverage Radio-undetected, undetermined radio-loudness 304 49.8 % of AGN with radio and 4400 Å coverage Figure 9.Radio detection and radio loudness fractions for the AGN in our sample. overall, they constitute only\u00199 per cent of the radio-loud sources. Thisisconsistentwithpreviousﬁndingsthattheradio-loudfraction of NLS1s is lower than that of typical broad-line AGN and quasars (Komossaetal.2006;Zhouetal.2006;Rakshitetal.2017).Again, selectionbiasesandincompletedatameanitisnotpossibletodraw ﬁrm conclusions from our sample. 4 BLACK HOLE MASSES The widths and luminosities of the broad permitted emission lines, as well as the continuum luminosities can be used to derive the massesoftheblackholesinourAGN.InthisSectionweassessthe broad emission line properties and calculate the black hole masses using new scaling relations. 4.1 Broad emission line properties We compare the broad line widths for sources with spectra con- taining multiple lines. The line width comparisons are shown in Fig.10.WeﬁndthatH βisslightlybroaderthanbothH αandMg /i.pc/i.pc. 207 spectra contain both Hαand Hβand from the median of the logarithmicdistributionweﬁndthatH βisonaverage \u001910 percent broaderthanH α.ThereisgreaterscatterbetweenthewidthsofH β andMg /i.pc/i.pc,andwedonotﬁndstrongevidenceofasystematicdiﬀer- ence in the line widths (Hβis on average only\u00192 per cent broader than Mg/i.pc/i.pc) as measured in 302 spectra. Qualitatively, these results somewhat are similar to those of previous studies, e.g. Osterbrock & Shuder (1982) found Hβto be\u001916 per cent broader than Hα 3.00 3.25 3.50 3.75 4.00 log(FWHM[H β]) 3.00 3.25 3.50 3.75 4.00 log(FWHM[H α]) 3.00 3.25 3.50 3.75 4.00 log(FWHM[H β]) 3.00 3.25 3.50 3.75 4.00 log(FWHM[Mg ii]) □0.50 □0.25 0.00 0.25 ∆ log(FWHM) 0 20 40Number N = 207 Med. = 0.04 σ = 0.12 □1.0 □0.5 0.0 0.5 ∆ log(FWHM) 0 20 40 60 80Number N = 302 Med. = 0.01 σ = 0.16 Figure 10.A comparison of the full widths at half maxima of Hα, Hβand Mg/i.pc/i.pc. Upper panels show the values for sources with at least two lines in their spectra. Representative errorbars are shown in the lower right corners of the plots. The lower panels show the logarithm of the line width ratios. in Seyfert galaxies; Mejía-Restrepo et al. (2016) (hereafter MR16) also found Hβto be broader than both Hαand Mg/i.pc/i.pc. In Fig.11 we show the ratio of the Hα to Hβ luminosities (the Balmer decrement). For the logarithmic mean value, we ﬁnd hH αH βi=39 whichishigherthantheCaseBvalue2.74(Oster- brock&Ferland2006)andtheintrinsicvalue 272\u0006004 suggested byGaskell(2017),basedonobservationsofblueAGN.ThisBalmer decrementisalsogreaterthantheaveragefoundinpreviousstudies oftype1AGN: hH αH βi=316 (Luetal.2019); hH αH βi=306 (Dong et al. 2008);hH αH βi=345 (La Mura et al. 2007). From the Shen et al. (2011) Catalog of SDSS DR7 Quasar Properties, we determine hH αH βi= 355 with 𝜎 = 014 dex. Taken as a reddening indicator, this large average Balmer decrement implies a high degree of intrinsic dust extinction in our sample of AGN: on average the colour excess𝐸¹𝐵\u0000𝑉º \u001904 mag. However, if a large Balmer decrement is due to reddening, we would expect the optical-UV continuum to be similarly aﬀected such that AGN withahigherBalmerdecrementwillhaveacorrespondinglygreater value of𝐿5100𝐿3000. Assuming an intrinsic (Case B) broad line Balmer decrement of 2.72 (Gaskell 2017) and an AGN continuum slope 𝛼 =13, we have used the AGN reddening curve of Gaskell et al. (2004) to calculate Balmer decrements and the correspond- ing continuum luminosity ratios for increasing dust extinction: this trend line is shown in the lower panel of Fig. 11. The Gaskell et al. (2004)AGNcurveismuchﬂatterintheUVthanMilkyWaycurves; MNRAS000, 1–23 (2023)The SOUX AGN Sample – Sample deﬁnition9 40 41 42 43 44 log(L[H β]) 40 41 42 43 44 45log(L[H α]) 1 2 5 10 20 Balmer decrement 0 20 40 60Number 0.25 0.50 0.75 1.00 log(L[H α]/L[H β]) □0.50 □0.25 0.00 0.25 0.50 log(L[5100 ˚A]/L[3000 ˚A]) Figure11. Topleft: AcomparisonoftheH αandH βbroadlineluminosities for low-𝑧sources with spectra containing both lines.Top right:The distri- bution of the Balmer decrements of these sources. The orange dashed lines indicatethebest-ﬁttinglinearregression,correspondingto 𝐿H α𝐿H β =39. Thegreendottedlinescorrespondto 𝐿H α𝐿H β =272 (Gaskell2017). Bot- tom: Balmer decrements versus ratios of the monochromatic continuum lumionsities at 5100 and 3000 Å. The solid red line shows the expected trend from the Gaskell et al. (2004) AGN reddening curve; the predicted trend for the Milky Way reddening curve of Cardelli et al. (1989) is shown by the dashed line, for comparison. Data with good quality measurements of all quantities are shown as blue circles; the grey squares represent data in which at least one measurement has a quality ﬂag raised. All luminosi- ties have been dervied from the SDSS optical spectrum and are therefore contemporaneous. for comparison, Fig. 11 also shows the expected luminosity ratio trend derived from the Cardelli et al. (1989) Galactic reddening curve.Eachdatapointintheﬁgurehasbeenobtainedfromasingle spectrum; it can be seen, that these data do not follow either trend. We further investigated whether spectra with large Balmer decrements may be reddened by visually inspecting those with (H αH β ¡ 5): two examples are shown in Fig. 12. Whilst red- dening is a plausible explanation for the large Balmer decrement in the spectrum 6369-56217-0193, visual inspection of the spectrum 5694-56213-0636 does not indicate any obvious reddening. Since the UV continuum is not highly extinguished, if the AGN is red- deneditsreddeningcurvemustﬂattentowardsshorterwavelengths. Gaskell et al. (2004) presented such a curve for AGN, and we have used this curve to modify this spectrum using an𝐴v = 53 (esti- matedfromthebroadlineBalmerdecrementassumingtheintrinsic line ratio is 2.72). The modiﬁed spectrum is clearly much brighter than the one used in our analysis, but is still quasar-like so we can- not deﬁnitively rule out intrinsic reddening. We discuss this point further in Section 6.1. We have computed the best-ﬁtting linear regressions between various luminosities using the package/l.pc/i.pc/n.pc/m.pc/i.pc/x.pc(Kelly 2007). The results are shown in Fig. 13 and the values are quoted in Table 2. The Balmer lines show an almost 1:1 correspondence with the 5100Å continuum luminosity, whereas the relation with Mg/i.pc/i.pcis slightly shallower with a gradient𝑚 = 087 \u0006003. The relation- shipsofbothH βandMg /i.pc/i.pcluminositieswiththe3000Åcontinuum Table 2.Correlations between emission line and continuum luminosities. Wegivetheindependentanddependentvariables( 𝑥and𝑦)andtheslopeand intercept (𝑚and 𝑐) of the best-ﬁtting linear regression.𝜎is the dispersion of the data about the linear regression. 𝑥 𝑦 𝑚 𝑐 𝜎 log 𝐿5100 log 𝐿H α 108 \u0006002 \u000050 \u000610 0.20 log 𝐿5100 log 𝐿H β 106 \u0006002 \u000044 \u000607 0.18 log 𝐿5100 log 𝐿Mg /i.pc/i.pc087 \u0006003 4 4 \u000613 0.21 log 𝐿3000 log 𝐿5100 082 \u0006001 7 8 \u000605 0.11 log 𝐿3000 log 𝐿H β 089 \u0006002 2 8 \u000609 0.20 log 𝐿3000 log 𝐿Mg /i.pc/i.pc085 \u0006002 5 1 \u000607 0.19 luminosity are shallower compared with those calculated for the 5100Åluminosity.Thestrongestcorrelationweﬁndisbetweenthe two continuum luminosities: the best-ﬁtting linear regression has a slope 𝑚 = 082 \u0006001 and a dispersion of only 0.11 dex (around half that of the emission lines). 4.2 Black hole mass estimates from broad emission lines Assuming that the gas in the BLR is virialised, an estimate of the mass of the black hole (𝑀BH) can be made by applying the virial theorem: 𝑀BH = 𝑓 𝑅BLR𝑉2 𝐺 (1) where 𝑅BLR is the radius of the BLR,𝑉 is the velocity of the gas and𝐺is the gravitational constant.𝑅BLR is often determined from radius-luminosityrelationsderivedforreverberation-mappedAGN (e.g.Petersonetal.2004;Bentzetal.2006;Landtetal.2011a).The virial factor𝑓 accounts for the geometry and inclination to the line of sight of the BLR and cannot be easily determined for individual sources.WherethebroadlineFWHMisusedasaproxyfor 𝑉then 𝑓 \u00181. We ﬁrst make estimates of the black hole masses of the AGN in our sample using recently-calibrated relations for virial black hole masses based on the broad Hα, Hβand Mg/i.pc/i.pcemission lines. The relations used are listed in Table 3. It is common to use the AGNcontinuumluminosityasaproxyfortheBLRradius,however MR16,Greeneetal.(2010)andWooetal.(2018)providerelations using instead the broad emission line luminosity (of Hα, Hβ, and Mg/i.pc/i.pc, respectively). Both MR16 and Woo et al. (2018) provide re- lations to estimate the black hole mass directly from a luminosity and broad emission line FWHM, assuming𝑓 = 1 and 𝑓 = 112, respectively. For the MR16 relations we use the values calibrated from the global ﬁt to the continuum (the middle column of Table 7 in that paper); these are the most appropriate relations for our sam- ple sinceP/y.pcQSOF/i.pc/t.pcperforms a global ﬁt to the broadband AGN continuum,ratherthanalocalﬁtundereachseparateemissionline. The Woo et al. (2018) relation we adopt for Mg/i.pc/i.pcwas derived with free 𝛽and 𝛾, which has least scatter with respect to ﬁducial black hole masses estimated using FWHM(Hβ) and𝐿5100. Greene et al. (2010) provide a formula to calculate𝑅BLR from the Hβemission lineluminosity;withanestimateof 𝑅BLRwethencalculatetheblack hole mass using Eqn. (1) assuming𝑓 =1. 4.3 Comparisons of black hole mass estimates In Fig. 14 we compare the mass estimates obtained using the re- lationships given in Table 3. Generally, we ﬁnd good agreement between the diﬀerent estimates. For estimates using the contin- uum luminosity, we see excellent agreement between Hαand Hβ MNRAS000, 1–23 (2023)10 D. Kynoch et al. 3000 4000 5000 6000 7000 8000 100 101 102 Fλ [10□17 erg s□1 cm□2 ˚A□1] H α/H β|br. = 12.29 ± 3.86 H α/H β|na. = 4.68 ± 0.82 Data Host Cont. BL NL Model 6400 6600 6800 0 10 20 30 H α 4750 5000 0 10 20 30 H β 6369-56217-0193 Rest wavelength [˚A] 3000 4000 5000 6000 7000 100 101 102 103 Fλ [10□17 erg s□1 cm□2 ˚A□1] H α/H β|br. = 5.67 ± 0.45 H α/H β|na. = 3.33 ± 0.47 Data G04 Cont. BL NL Model 6400 6600 6800 0 10 20 H α 4750 5000 0 5 10 H β 5694-56213-0636 Rest wavelength [˚A] Figure 12.Example spectra with high broad Balmer line decrements. The Hα/Hβﬂux ratios of both broad and narrow lines are given in the inset text. Each spectrum (shown in black) has been corrected for Galactic reddening and transformed to the rest frame. The inset legend indicates the components of the total P/y.pcQSOF/i.pc/t.pcmodel (red): the host galaxy emission (pink); the AGN continuum (green) and selected, modelled broad and narrow emission lines (‘BL’ and ‘NL’, coloured blue and purple, respectively). In the lower panel, we also show in orange the spectrum 5694-56213-0636 additionally dereddened with the Gaskell et al. (2004) curve assuming an𝐴v =53 derived from the broad Balmer line decrement. masses; on average the diﬀerence between the mass estimates are negligible (Δ𝑀BH \u00195 per cent) and the scatter is𝜎 = 022 dex. ThereisslightlygreaterscatterbetweentheH βandMg /i.pc/i.pcestimates (𝜎 = 026), as has been noted in previous studies (e.g. MR16). The average agreement is less good, also, with the Mg/i.pc/i.pcmasses being systematically higher than the Hβmasses by\u001960 per cent. The greatest discrepancy we see is between the two Hαrelations: masses calculated using the broad Hαline luminosity are system- atically lower than those made with the continuum luminosity by almost a factor of two. Mass estimates using the broad Hβ line luminosity are also systematically lower than those involving the continuum luminosity, but the discrepancy is less than between the Hαestimates (\u001960 per cent). In the case of Mg/i.pc/i.pc, there is a very minor systematic in the opposite sense with estimates using the continuumluminositybeing \u001920 percentsmallerthanthoseusing the line luminosity. To obtain better consistency between the various mass esti- mates, we have calculated new virial mass relations. We have as- sumed that the relation using the FWHM of broad Hβ and the 5100 Å luminosity provides the best and most reliable estimate of the mass. Then, assuming a virial mass relation of the form log \u0012 𝑀BH M\f \u0013 =log¹𝐾º¸𝛼log \u0012 𝐿 1044 erg s\u00001 \u0013 ¸2 log \u0012 FWHM 1000 km s\u00001 \u0013  (2) we determine the quantities𝐾 and 𝛼which reduce the systematic oﬀset and scatter in the relations with respect to the Hβ-5100 Å mass estimates. These values are given in Table 4 along with the medianΔlog¹𝑀BHºwithrespecttotheH β-5100Åmassestimates, and the dispersion𝜎. Here, and in Fig. 15, it can be seen that the systematic oﬀsets between diﬀerent relations are greatly reduced. For additional quality assurance, we visually inspected all of the optical spectra of the AGN in our sample to identify problem- atic line proﬁles (e.g. very noisy or low-contrast proﬁles; proﬁles MNRAS000, 1–23 (2023)The SOUX AGN Sample – Sample deﬁnition11 42 44 46 log(L[5100 ˚A]) 40 42 44log(L[H α]) 1.08± 0.02 42 44 46 log(L[5100 ˚A]) 40 42 44log(L[H β]) 1.06± 0.02 42 44 46 log(L[5100 ˚A]) 40 42 44log(L[Mg ii]) 0.87± 0.03 42 44 46 log(L[3000 ˚A]) 43 44 45 46log(L[5100 ˚A]) 0.82± 0.01 42 44 46 log(L[3000 ˚A]) 40 42 44log(L[H β]) 0.89± 0.02 42 44 46 log(L[3000 ˚A]) 40 42 44log(L[Mg ii]) 0.85± 0.02 Figure13. Correlationsbetweenemissionlineandcontinuumluminosities. Dashed orange lines show the best-ﬁtting linear regression to the data, and thevalueoftheslope( 𝑚)isgivenintheinsettext.Relationswithagradient 𝑚=1 areshownforcomparison(solidblacklines).Datawithgoodquality measurements of all quantities are shown as blue circles; the grey squares represent data in which at least one measurement has a quality ﬂag raised. AllluminositieshavebeenderviedfromtheSDSSopticalspectrumandare therefore contemporaneous. Table 3.The virial black hole mass relations from literature used. FWHM 𝐿 Relation ref. 𝑁spectra 𝑁good spectra Hα 5100Å MR16 219 189 (86%) Hα Hα MR16 220 194 (88%) Hβ 5100Å MR16 481 401 (83%) Hβ Hβ G10 482 406 (84%) Mg/i.pc/i.pc3000Å MR16 517 479 (93%) Mg/i.pc/i.pcMg/i.pc/i.pcW18 521 483 (93%) Welisttheemission-lineFWHMandluminosity 𝐿usedtocal- culate the black hole masses.𝑁spectra is the number of spectra in which the relevant quantities were measured.𝑁good spectra is the subsetof 𝑁spectra (andthepercentage)forwhichnoqualityﬂags were raised on the relevant emission line or luminosity mea- surements. References: MR16 = Mejía-Restrepo et al. (2016); G10 = Greene et al. (2010); W18 = Woo et al. (2018). aﬀected by absorption features or data gaps, etc.) and select the best broad line from which to estimate the black hole mass, con- sidering the line proﬁle shape and S/N in the vicinity of the line. For each AGN in our sample we list the ‘preferred’ mass estimate indicating our choice of broad emission line and luminosity scal- ing relation for that source. We have adopted, wherever possible, the black hole mass from estimates using FWHM(Hβ) and𝐿5100, since Hβis the broad emission line most commonly used to estab- lishtheBLRradius-luminosityrelationshipinopticalreverberation mapping campaigns and this relationship forms the basis of single- epoch mass estimates such as those used here. As noted in the literature, other line-luminosity relations are often associated with a greater overall uncertainty on the black hole mass and typically relations involving other lines are calibrated to that of Hβ (e.g. Netzer et al. 2007; Shen et al. 2011; Mejía-Restrepo et al. 2016; Woo et al. 2018). In cases where the Hβ proﬁle was unsuitable, we have chosen another broad line and nearby continuum luminos- ity (usually Hα-𝐿5100 for low-redshift sources and Mg/i.pc/i.pc-𝐿3000 Å for high-redshift sources). Spectra lacking any broad emission line from which a black hole mass could be reliably estimated were previously removed in our initial quality inspection (Section 2.4). Table 4.Recalibrated virial black hole mass relations FWHM 𝐿 log¹𝐾º 𝛼 Median 𝜎 𝑁 spec Δlog¹𝑀BHº Hα Hα 756\u0006004 0 47\u0006002 0 00 0 21 182 Hα 5100 Å 691\u0006002 0 55\u0006003 0 02 0 22 182 Hβ Hβ 783\u0006001 0 494\u00060007 \u0000001 0 10 401 Mg/i.pc/i.pcMg/i.pc/i.pc770\u0006005 0 46\u0006004 \u0000003 0 29 224 Mg/i.pc/i.pc3000 Å 686\u0006003 0 51\u0006003 \u0000002 0 26 224 The relations are of the form log¹𝑀BHº = log¹𝐾º ¸𝛼log¹𝐿º ¸ 2 log¹FWHMº. Values of𝐾 and 𝛼have been determined which minimise the systematic oﬀset and scatter with respect to masses estimated from the relation involving the FWHM of broad Hβand the 5100 Å luminosity. The relations are calculated with the luminosity𝐿 in units 1044 ergs\u00001 and FWHM in units 1000 kms\u00001. For each relation we have calculated we give theresultingsystematicoﬀsetanddispersionwithrespecttotheH β-5100Å massestimates. 𝑁spec isthenumberofspectrausedtoestablishtherelation. 5 𝛼ox: THE UV/X-RAY RELATIONSHIP We calculate the UV/optical-X-ray energy index 𝛼ox =\u00000384 log \u0012 𝐿2 keV 𝐿2500 \u0013 (3) for the AGN in our sample. An𝛼ox determination is possible for 685 AGN in our sample (98 per cent), with the remaining AGN having no spectroscopic or photometric coverage of 2500 Å. We ﬁnd a mean𝛼ox = 140 with standard deviation 𝜎 = 016. These values are in good agreement with those found by Lusso et al. (2010) for a sample of quasars in the XMM-COSMOS ﬁeld (mean 1.37 and dispersion 0.18). Lusso & Risaliti (2016) also show that there is a correlation of𝛼ox with 𝐿2500. Fig. 16 (upper, left-hand panel) shows this for the 685 AGN in our sample. Plainlythereisagoodcorrelationofthesequantitiesinourdata.We use /l.pc/i.pc/n.pc/m.pc/i.pc/x.pc, the Python version of the package/l.pc/i.pc/n.pc/m.pc/i.pc/x.pc_/e.pc/r.pc/r.pc(Kelly 2007),toperformalinearregressionanalysis.Weﬁndastatistically signiﬁcant correlation with 𝛼ox =\u0000¹207 \u0006019º¸¹ 0117 \u00060007º\u0002log 𝐿2500 (4) Thedispersionaboutthislineis0.12dex,showingthatincludingthe luminosity dependence does indeed reduce the scatter, as required if this is to be used as a cosmological probe (Lusso & Risaliti 2016). By contrast, there is much larger scatter of𝛼ox with the monochromatic X-ray luminosity,𝐿2 keV. For these quantities we ﬁnd 𝛼ox =¹094 \u0006025º¸¹ 0018 \u00060010º\u0002log 𝐿2 keV; (5) unlikethe 𝛼ox-𝐿2500 relation,thisrelationisnotstatisticallysignif- icant at the2𝜎level. 5.1 𝛼ox related to emission line width and radio-loudness Inadditiontovaluesforthefullsample,wehavealsocalculatedme- dian𝛼ox and UV/X-ray luminosities for the narrow- and broad-line and radio-loud and radio-quiet subsets. We have also determined theluminosity-𝛼ox relationsforthesesubpopulations.Theresultant values are listed in Table 5. It is clear that the narrow-line AGN are typically lower in luminosity (both UV and X-ray) than the broad-line AGN. A slightly greater diﬀerence is seen in the UV lu- minosities (medianΔ𝐿2500 \u001909 dex) than the X-ray luminosities (median Δ𝐿2 keV\u001908 dex) between the narrow- and broad-line AGN.Wethereforeseeasmalldiﬀerenceinthe 𝛼ox distributionsof the subsets, with the narrow-line AGN having, on average, slightly lower𝛼ox. Fig. 17 shows that this is a consequence of the narrow- linesourceshavinglowerUVluminositieswith 𝛼ox /𝐿2500.Ifthe MNRAS000, 1–23 (2023)12 D. Kynoch et al. 6 7 8 9 10 FWHMHα, L5100 [MR16] 6 7 8 9 10FWHMHα, LHα [MR16] Hα–Hα 0.0 0.5 ∆log( MBH/M⊙) 0 10 20 30 40Number Med. = 0.28 σ = 0.11 6 7 8 9 10 FWHMHβ, L5100 [MR16] 6 7 8 9 10FWHMHα, L5100 [MR16] Hβ–Hα □1.0 □0.5 0.0 0.5 ∆log( MBH/M⊙) 0 10 20 30 40Number Med. = -0.02 σ = 0.22 6 7 8 9 10 FWHMHβ, L5100 [MR16] 6 7 8 9 10FWHMHβ, LHβ [G10] Hβ–Hβ 0.0 0.5 ∆log( MBH/M⊙) 0 20 40 60Number Med. = 0.20 σ = 0.11 6 7 8 9 10 FWHMHβ, L5100 [MR16] 6 7 8 9 10FWHMMg ii, L3000 [MR16] Hβ–Mgii □2 □1 0 1 ∆log( MBH/M⊙) 0 10 20 30 40 50Number Med. = -0.20 σ = 0.26 6 7 8 9 10 FWHMMg ii, L3000 [MR16] 6 7 8 9 10FWHMMg ii, LMg ii [W18] Mg ii–Mgii □0.50 □0.25 0.00 0.25 ∆log( MBH/M⊙) 0 20 40 60 80Number Med. = -0.11 σ = 0.13 Figure 14.Comparisons of black hole mass estimates made using single-epoch relations taken from literature. The contour plots in the upper panels show log¹𝑀BHM\fº,calculatedusingthebroademissionlineFWHMandtheluminosityindicatedontheaxislabel.Theliteraturereferencefortherelationusedis also given in square parentheses (see Table 3 in the text). The lower panels show the diﬀerence inlog¹𝑀BHM\fº. Reliable mass diﬀerences (with no quality ﬂags raised on any of the quantities used to calculate the two masses) are shown in blue and the quality-ﬂagged values are shown in grey. The median and standarddeviationsofthedistributionsforthereliablediﬀerencesaregivenintheinsettext.Thesolidblacklinesintheupperpanelsshowthe1:1relationand the orange dotted lines mark the median in both lower and upper panels. AGN are binned by𝐿2500 then it can be seen that the𝛼ox values of the narrow-line AGN are systematicallyhigher than those of the broad-line AGN across the luminosity bins, and progressively so with increasing𝐿2500. The diﬀerences in luminosities between the RL and RQ pop- ulations are less marked than those seen in the narrow- and broad- line subsets. The RL and RQ AGN have similar UV luminosities, althoughtheRLsubsetisonaveragemoreluminousinX-rays(me- dian Δ𝐿2 keV\u001907 dex). The radio-loud AGN have, on average, lower values of𝛼ox than the radio-quiet AGN. In the lower-left panel of Fig. 16 we show the𝐿2500-𝛼ox relation derived by Zhu et al. (2020) from a sample of radio-quiet quasars. We see that our relationfortheRQsubsetagreesverywellwiththeirs,andthatour RL sources have systematically lower𝛼ox. MNRAS000, 1–23 (2023)The SOUX AGN Sample – Sample deﬁnition13 Table 5.Median, standard deviation and linear regression values from𝛼ox analysis. For the whole sample of AGN with an𝛼ox determination (673 AGN) we list the median (M) and standard deviation (𝜎) of𝛼ox, the logarithms of the 2500 Å UV luminosity and 2 keV X-ray luminosity as well as the slope (𝑚) and intercept (𝑐) of the best-ﬁtting linear regression of the luminosities with respect to the dependent variable𝛼ox and the dispersion (𝜎) of the data about the best ﬁt. These quantities are also given for the narrow-line (‘NL’), broad-line (‘BL’), radio-loud (‘RL’) and radio-quiet (‘RQ’) subsets of the sample.𝑁 is the number of AGN in each set. Luminosities are in units ergs\u00001 Hz\u00001. Set 𝑁 M𝛼ox 𝜎𝛼ox Mlog𝐿2500 𝜎log𝐿2500 Mlog𝐿2 keV 𝜎log𝐿2 keV 𝑚¹log𝐿2500º 𝑐¹log𝐿2500º 𝜎¹log𝐿2500º 𝑚¹log𝐿2 keVº 𝑐¹log𝐿2 keVº 𝜎¹log𝐿2 keVº All 685 1.40 0.16 29.85 0.77 26.23 0.63 0117\u00060007 \u0000207\u0006019 0.12 0018\u00060010 0 94\u0006025 0.15 NL 96 1.37 0.17 29.09 0.76 25.51 0.57 0147\u00060017 \u0000289\u0006050 0.12 0038\u00060031 0 40\u0006079 0.17 BL 589 1.40 0.16 30.00 0.70 26.31 0.56 0129\u00060008 \u0000245\u0006023 0.12 0004\u00060012 1 31\u0006030 0.15 RL 68 1.29 0.16 30.19 0.65 26.81 0.60 0101\u00060030 \u0000172\u0006092 0.15 \u00000074\u00060033 3 28\u0006087 0.16 RQ 229 1.45 0.16 29.99 0.85 26.14 0.66 0126\u00060010 \u0000229\u0006029 0.12 0049\u00060017 0 17\u0006043 0.16 0 50 Med. = 0.00 σ = 0.23 FWHM(H α) L(H α) Med. = 0.02 σ = 0.22 FWHM(H α) L(5100 ˚A) 0 50 100 150 Med. = -0.01 σ = 0.10 FWHM(H β) L(H β) □1 0 1 Med. = -0.02 σ = 0.26 FWHM(Mg ii) L(3000 ˚A) □1 0 1 0 50 100 Med. = -0.03 σ = 0.29 FWHM(Mg ii) L(Mg ii) ∆ log(Mass) Number Figure 15.Comparisons of BH mass estimates from broad emission lines. Δlog¹Massº=log \u0010 MassjFWHM¹H βº 𝐿¹5100 Åº \u0011 \u0000log \u0010 MassjFWHM¹𝑥º 𝐿¹𝑦º \u0011 . 5.2 𝛼ox related to black hole mass and accretion rate Wealsotestthecorrelationbetween 𝛼ox andblackholemass.There is substantial scatter but/l.pc/i.pc/n.pc/m.pc/i.pc/x.pcdetermines a signiﬁcant relation with mass: 𝛼ox =¹083 \u0006008º¸¹ 0069 \u00060009º\u0002log ¹𝑀BHM\fº (6) which is shown in Fig.18. Whilst the correlation between𝛼ox and 𝑀BH is not particularly strong (the Pearson correlation coeﬃcient is\u001903), we can exclude the the absence of a correlation with high conﬁdence: the gradient0069 \u00060009 is ¡ 7𝜎 inconsistent with zero and the Pearson correlation has ap-value \u001c005 for a null hypothesis of the distributions being uncorrelated. We now investigate how much of the scatter in𝛼ox-𝐿2500 can beexplainedbyatwo-parameterdependenceonbothmassandmass accretion rate (or equivalently, mass and𝐿𝐿Edd). The left panel of Fig. 19 shows the range oflog 𝐿2500 (a tracer of mass accretion rate)andblackholemassspannedbyoursample,binnedontoagrid withtwosquaresperdecade.Withineachgrid square,wecalculate theaverage 𝛼ox,wherethisshouldbeamorereliableestimateaswe are using only AGN with approximately the same mass and mass accretion rate. We remove squares containing fewer than 3 AGN, sothedispersionisdominatedbytheintrinsicscatter.Thisshowsa subtle but systematic trend in the data, where𝛼ox depends on both mass and𝐿2500, rather than𝐿2500 alone. Forcomparison,therightpanelofFig.19showsthepredicted 𝛼ox over this parameter space from the quasar SED model/q.pc/s.pc/o.pc/s.pc/e.pc/d.pc (Kubota & Done 2018). This model was built from extrapolating trends seen in a much smaller sample of AGN than used here (Jin etal.2012a,c).Theseearlierstudiesshowedthatoptical-X-rayAGN SEDs are typically ﬁt by three components: an outer standard disc, emitting thermal blackbody down to some radius, where it tran- sitions to incomplete thermalisation (warm Comptonisation). This optically thick structure truncates below the radius𝑅hot so that the gravitational power within some𝑅hot ¡ 𝑅isco (where 𝑅isco is the radius of the innermost stable circular orbit) is dissipated instead in heating optically thin plasma which produces the power law tail byComptonisation.The /q.pc/s.pc/o.pc/s.pc/e.pc/d.pcmodelassumesthatthetotalX-ray luminosityisﬁxedat 𝐿X =002𝐿Edd,i.e.themaximalADAFlimit. This captures the main trends seen in the data: at low mass accre- tion rates, only just above𝐿bol =002𝐿Edd, then almost all of the accretion power is required to power the X-rays, and𝑅hot is large. Conversely, at high mass accretion rates,𝐿bol \u0019𝐿Edd then almost all of the accretion power is dissipated in the disc, and the X-rays at 𝐿X =002𝐿Edd are weak in comparison and𝑅hot !𝑅isco. The changing size of𝑅hot and changing luminosity of the disc also sets the spectral slope of the power-law tail self consistently, so that the model predicts the entire SED (with some additional assumptions about the warm Compton region). We use this model, with spin ﬁxed at 0 and the inclination at 30\u000e to generate a set of SEDs from black hole masses in the rangelog¹𝑀BHM\fº=575–9.75 and adjust the mass-normalised accretion rate between¤𝑚 = 0022 \u000025 (the limits of the/q.pc/s.pc/o.pc/s.pc/e.pc/d.pc model) to matchlog 𝐿2500 to the midpoint of each grid square. We extract𝛼ox fromthesemodelSEDs,andcolourcodeeachgridpoint using the same colour scale as from the real data. We also overplot lines of constant𝐿𝐿Edd on both panels. At high Eddington ratio, the UV is always dominated by the standard disc over this mass range, so the monochromatic ﬂux𝐿2500 / ¹𝑀BH ¤𝑀º23, giving rise to the straight lines for high𝐿𝐿Edd on thelog¹𝑀BHM\fº\u0000 log 𝐿2500 plot. Importantly this is not a linear relationship even for a standard disc.𝐿2500 / ¤𝑀23 /𝐿23 bol rather than being directly proportional to𝐿bol. Thus using monochromatic ﬂux (or ﬂux in a narrowband)withaconstantK-correctiontoconverttobolometric ﬂux and hence derive Eddington ratio is subtly incorrect even for standard disc models. The solid blue line in Fig. 19 shows the Eddington luminosity assuming a bolometric correction of𝐿bol = 275 \u0002𝐿2500 (Krawczyk et al. 2013). The /q.pc/s.pc/o.pc/s.pc/e.pc/d.pcmodel assumes the disc extends outwards only to the self-gravity radius, which may be only a few hundred𝑅g at the lowest accretion rates for the highest masses (see Mitchell et al., submitted). This causes the very sharp drop in𝐿2500 for these objects. If instead we set the outer disc radius to105 𝑅g (as is usually done in the near-infrared spectroscopic modelling of AGN, e.g. Landt et al. 2011b), the0022𝐿Edd curve more closely MNRAS 000, 1–23 (2023)14 D. Kynoch et al. 26 27 28 29 30 31 32 33 log(L2500 ˚A/erg s□1 Hz□1) 0.8 1.0 1.2 1.4 1.6 1.8 2.0 αox This work Lusso+10 Narrow line Broad line 23 24 25 26 27 28 29 log(L2 keV/erg s□1 Hz□1) 0 50 100 Number Med.NL = 1.4 σNL = 0.2 Med.BL = 1.4 σBL = 0.2 0 50 100Number Med.NL = 29.1 σNL = 0.8 Med.BL = 30.0 σBL = 0.7 Med.NL = 25.5 σNL = 0.6 Med.BL = 26.3 σBL = 0.6 26 27 28 29 30 31 32 33 log(L2500 ˚A/erg s□1 Hz□1) 0.8 1.0 1.2 1.4 1.6 1.8 2.0 αox This work Lusso+10 Zhu+20 Radio-loud Radio-quiet Unknown 23 24 25 26 27 28 29 log(L2 keV/erg s□1 Hz□1) 0 20 40 60 Number Med.RL = 1.3 σRL = 0.2 Med.RQ = 1.4 σRQ = 0.2 0 20 40 60Number Med.RL = 30.2 σRL = 0.6 Med.RQ = 30.0 σRQ = 0.9 Med.RL = 26.8 σRL = 0.6 Med.RQ = 26.1 σRQ = 0.7 Figure 16.The relationship between𝛼ox and 𝐿2500 (left) and between𝛼ox and 𝐿2 keV (right). We show the best ﬁt relation found in this work for all 673 AGN with an𝛼ox determination (solid black line) and its 1𝜎conﬁdence region (in orange). For comparison we show the best ﬁt relations found by Lusso et al. (2010) (dashed black line) and Zhu et al. (2020) (dotted black line). Stacked histograms show the distributions of the three quantities. In the top ﬁgure, narrow-line (FWHM  2000 kms\u00001) and broad-line sources are coloured red and blue, respectively; the pale red and blue areas in the scatter plots are the linear regression 1𝜎conﬁdence regions for the respective subsets. In the bottom ﬁgure data pertaining to the radio-loud and radio-quiet sources are similarly coloured red and blue, respectively. The median 1𝜎uncertainty on each quantity is shown by the error bars in the lower right-hand corner of each scatter plot. MNRAS000, 1–23 (2023)The SOUX AGN Sample – Sample deﬁnition15 28 29 30 31 log(L[2500 ˚A]/erg s□1 Hz□1) 0.8 1.0 1.2 1.4 1.6 Radio-quiet Radio-loud 28 29 30 31 log(L[2500 ˚A]/erg s□1 Hz□1) 0.8 1.0 1.2 1.4 1.6 αox Broad line Narrow line Figure 17. 𝛼ox versus UV luminosity for (left) narrow-line / broad-line and (right) radio-loud / radio-quiet subsamples. In each luminosity bin, narrow-line sources have higher𝛼ox values (softer SEDs) than broad-line sources whereas radio-loud sources have lower𝛼ox values (harder SEDs) than radio-quiet sources. resembles those of higher Eddington ratios (the orange dot-dashed line in Fig. 19). It is very clear that the data sit between the Eddington limits hardwired into the/q.pc/s.pc/o.pc/s.pc/e.pc/d.pccode. The model assumes that below 𝐿𝐿Edd \u0018002, the ﬂow is completely dominated by the optically thin,geometricallythick,hotplasma(ADAF/RIAFﬂow).Thistran- sition is seen directly in the ‘changing-look’ AGN (Noda & Done 2018,Ruanetal.2019).Above 𝐿𝐿Edd \u001825,thediscphotosphere becomes brighter than its local Eddington ﬂux (Kubota & Done 2019), so its structure is probably modiﬁed by strong winds and/or radialadvectionoftheexcessradiation.Boththeseprobablyleadto vertical structure so the radiation escaping preferentially only in a narrowfunnel,makingtheseobjectsrare(aswellasprobablyshort lived). In the left-hand panel of Fig. 19 we see that the contours indicatingthelocationofourdatadonotextendabovethe 𝐿𝐿Edd = 05 line (taken from the/q.pc/s.pc/o.pc/s.pc/e.pc/d.pcmodel for the same mass and UV luminosity) at the high mass end; this implies that high Eddington fractionﬂowsarerareatthehighestblackholemassesinoursample. TheseobjectsareveryrareinthelocalUniverse,andareonlylikely to be present at the peak of the quasar epoch at𝑧 \u00182 \u00004, beyond our sample redshift limit. Conversely, we only see the lowest mass black holes (𝑀 \u0018106𝑀\f) at the highest Eddington ratios as these are otherwise diluted by their host galaxy (e.g. Done et al. 2012). InFig.20weshowthediﬀerencebetween 𝛼ox aspredictedby /q.pc/s.pc/o.pc/s.pc/e.pc/d.pcand as determined in our sample. This shows that there is generallyreasonableagreementbetweenthemodelpredictionsand 𝛼ox estimated from our data, In the majority of bins (97 per cent) jΔ𝛼oxj  02 and in 52 per cent of the binsjΔ𝛼oxj  01. A detailed exploration of the diﬀerences between the data and the model predictions as a function of black hole mass, luminosity and accretion rate will be performed in future work (Mitchell et al., submitted). 6 DISCUSSION The SOUX sample is a medium-sized selection of\u0019700 AGN up to 𝑧 = 25 with quality multiwavelength data. Although it is not a complete or representative AGN sample, it has been assembled with the intention of exploring a wide range of parameter space. Thisﬁrststudyhasfocussedonmakingkeymeasurementsfromthe data (the broad optical emission line properties, multiwavelength 6 7 8 9 10 log (MBH/M⊙) 0.8 1.0 1.2 1.4 1.6 1.8 2.0 αox Slope = 0.069 ± 0.009 Intercept = 0.83 ± 0.08 0 50 100Number Med. = 8.5 σ = 0.7 0 100 Number Med. = 1.4 σ = 0.2 Figure 18.The relationship between𝛼ox and the logarithm of black hole mass.Weshowthebestﬁtrelationfound(solidblackline)anditsonesigma conﬁdenceregion(inorange).Histogramsshowthedistributionsofthetwo quantities. A representative error bar is shown in the bottom left. luminosities etc.) and on determining a consistent set of black hole masses from which we will be able to determine Eddington ratios. We have taken a preliminary look at changes in the shape of the AGN SED (characterised by the parameter𝛼ox) as a function of black hole mass and AGN luminosity. To ﬁrst order,𝛼ox is found to vary with both mass and luminosity as broadly expected from a theoretical model. However, the systematic diﬀerences seen at second-order will require a deeper exploration. Our main ﬁndings are discussed below. 6.1 Broad emission line properties In Section 4.1 we found that the luminosities of the broad emis- sion lines Hα and Hβ were very well-correlated, with the mean luminosity ratio (the Balmer decrement) beinghH αH βi = 39 (Fig. 11). The analysed spectra have all been corrected for line-of- sightGalacticreddening,thereforeif(asproposedbyGaskell2017) theintrinsicbroadlineratioisclosetotheCaseBvalue( \u001927)then thetypicallyhighervaluesthatweﬁndimplythatalargefractionof the AGN in our sample are reddened by dust in their host galaxies or galactic nucleus. In Fig. 12 we show the spectra of two sources with high Balmer decrements. One spectrum (6369-56217-0193) exhibits both high broad line and narrow line Balmer decrements and lacks a dominant blue AGN continuum. (Note however that P/y.pcQSOF/i.pc/t.pcperforms a principal component analysis using galaxy and AGN eigenspectra to obtain a best estimate of the AGN con- tinuum level.) This AGN may plausibly be reddened by dust in its hostgalaxyaﬀectingboththeBLRandNLR;howeverthepresence of reddening in many other spectra with high Balmer decrements is more ambiguous. For example, the spectrum 5694-56213-0636 (also shown in Fig. 12) has a high broad line Balmer decrement but a narrow line decrement consistent with Case B and a very strong, blue continuum. The modiﬁed spectrum, dereddened using the Gaskell et al. (2004) AGN curve and an𝐴v estimated from MNRAS000, 1–23 (2023)16 D. Kynoch et al. 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 log (MBH/M⊙) 27 28 29 30 31 32 log □ L2500 ˚A/erg s□1 Hz□1) Sample LUV ∝ Lbol = LEdd L/LEdd = 2.450 L/LEdd = 1.000 L/LEdd = 0.500 L/LEdd = 0.100 L/LEdd = 0.030 L/LEdd = 0.022 L/LEdd = 0.022† 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 log (MBH/M⊙) QSOSED 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 αox Figure 19.The relationship between𝐿2500, black hole mass and𝛼ox. The left panel shows the average values of𝛼ox for sources in our sample binned by mass and UV luminosity (only bins containing three or more AGN are drawn). The right panel shows the value of𝛼ox taken from an SED generated by the Kubota & Done (2018) model/q.pc/s.pc/o.pc/s.pc/e.pc/d.pc, for mass and UV luminosity at the centre of each grid point. Dashed lines correspond to curves of constant𝐿𝐿Edd as determined by/q.pc/s.pc/o.pc/s.pc/e.pc/d.pc.yThe dot-dashed orange line is computed for a model in which the outer disc radiuslog¹𝑅out𝑅gº=5, whereas in all other models the outerradiusisthediscself-gravityradius(cf.thereddashedlineatthesame 𝐿𝐿Edd).Thesolidbluelineshowsacurveofconstant 𝐿𝐿Edd =1 assumingthat 𝐿2500 /𝐿bol /𝑀BH; clearly this relation is shallower than that predicted by/q.pc/s.pc/o.pc/s.pc/e.pc/d.pc(the dashed blue line) in which𝐿2500 /𝑀43 BH . Blue and red contours in the left-hand panel indicate the parameter space spanned by AGN drawn from the SDSS quasar catalogue and the supplementary sources added from the R17 NLS1 catalogue, respectively. 6 7 8 9 10 log (MBH/M⊙) 27 28 29 30 31 32 log □ L2500 ˚A/erg s□1 Hz□1) □0.2 □0.1 0.0 0.1 0.2 ∆ αox = αqsosed ox □ ⟨αsample ox ⟩ 0 10Number of bins □0.2 □0.1 0.0 0.1 0.2 ∆ αox Figure 20.The diﬀerence between𝛼ox as predicted by the/q.pc/s.pc/o.pc/s.pc/e.pc/d.pcmodel and as found from the data in our sample. The bottom panel shows the diﬀerenceineachmass-luminositybin,thetoppanelshowsthedistribution of Δ𝛼ox values. the Balmer decrement, is shown in orange in Fig. 12. This spec- trum is\u0018125 dex brighter than the spectrum from which we have measured emission line and continuum properties, but it is still quasar-like.Whilstwecannotdeﬁnitivelyruleouttheintrinsicred- dening of such individual sources, Fig. 11 shows that across our sample Balmer decrements and the ratio of 5100 and 3000 Å con- tinuum luminosities are uncorrelated: we would expect there to be a correlation in the reddening scenario. Alternatively, it is possible that the intrinsic Balmer decrements in AGN BLRs are not always close to Case B. This was a conclusion of Schnorr-Müller et al. (2016), who found that the intrinsic Balmer decrements of type 1 AGN spanned a wide range (2.5–6.6), derived from photoioniza- tion modelling of their data; the vast majority of the AGN in our samplefallwithinthisrange.ArangeofintrinsicbroadlineBalmer decrements would partly explain why we do not ﬁnd a correlation betweenBalmerdecrementsandcontinuumluminosityratiosinour sample. We showed in Section 4.1 that the luminosities of the three broademissionlinesH α,H βandMg /i.pc/i.pcarestronglycorrelatedwith the AGN continuum luminosities. Of all emission line-continuum relations we tested, the least scatter is found between𝐿H β and 𝐿5100, although the relationship between the two continuum lu- minosities 𝐿5100 and 𝐿3000 Å had least scatter overall. Whilst the luminosities of the two Balmer lines are approximately linearly proportional to 𝐿5100, the log¹𝐿Mg /i.pc/i.pcº-log¹𝐿5100ºrelation has a gradient noticeably lower than 1 (𝑚 = 087 \u0006003). These re- sults are similar to those reported by Shen & Liu (2012), who also found least scatter between𝐿5100 and 𝐿3000 Å. Shen & Liu (2012) report alog¹𝐿H αº-log¹𝐿5100ºrelation with a gradient con- sistent with 1 and alog¹𝐿Mg /i.pc/i.pcº-log¹𝐿5100ºproportionality consis- MNRAS000, 1–23 (2023)The SOUX AGN Sample – Sample deﬁnition17 tentwiththeoneweﬁndhere( 086 \u0006007).However,intheirwork they ﬁnd a much steeper relation betweenlog¹𝐿H βº-log¹𝐿5100º (125 \u0006007). Collinson et al. (2017) also ﬁnd a shallower gradient betweenlog¹𝐿Mg /i.pc/i.pcºandlog¹𝐿5100ºthan between the Balmer lines and log¹𝐿5100º. In their work, the gradients of all three emission line luminosites with respect tolog¹𝐿5100ºwere shallower than 1, although they were working with a much smaller sample (11 AGN) and much more limited range in luminosities (\u00181 dex). It has been shown both in observations of reverberation-mapped and ‘changing-look’AGN(e.g.Sunetal.2015;Zhuetal.2017;Kynoch et al.2019a) andphotoionisation models (e.g.Guo etal. 2020) that Mg/i.pc/i.pcrespondsmoreweaklytochangesinthecontinuumthandoes Hβ. Here, we appear to see a similar trend in a statistical sense across several hundred snapshots of individual AGN. Overall, the strong correlations between the emission line and continuum lumi- nosities demonstrate that the former are also reasonable proxies for the BLR radius, which we discuss below. 6.2 Relations for virial black hole masses In Section 4 we computed the black hole masses of the AGN in oursamplefromthebroademissionlinewidthsandeitheranearby continuum luminosity or the broad emission line luminosity. We used several single-epoch virial black hole mass relations taken fromrecentliterature(MR16;Wooetal.2018;Greeneetal.2010). Mass estimates from the FWHM of the Balmer lines and𝐿5100 are generally in very good agreement, with a minor systematic diﬀerence jΔlog¹𝑀BHºj=02 (i.e. a 5 per cent diﬀerence in𝑀BH) and 𝜎=022 dex. We noticed that estimates made using the emission line lumi- nositiesweresystematicallylowerthanthosemadeusingthecontin- uumluminosities.Themasseswere \u001960 percentlowerinthecaseof HβandafactortwolowerinthecaseofH α.Discrepanciesofasim- ilar magnitude have been observed in previous studies calculating theblackholemassofsingleobjects(e.g.Landtetal.2017;Kynoch etal.2019b)andalsoinlargesamples(e.g.Shen&Liu2012).Shen & Liu (2012) compared masses calculated with FWHM(Hα)-𝐿H α (Shenetal.2011)andFWHM(H β)-𝐿H β(Greene&Ho2005)with ﬁducialmassescalculatedwithFWHM(H β)-𝐿5100 (Vestergaard& Peterson2006)andagainestimatesusingthelineluminositieswere lower, although to a lesser extent than we ﬁnd here (Shen & Liu 2012 report hΔlog»𝑀BH¼i = \u0000008 and \u0000004 dex for Hα and Hβ, respectively). The cause of these systematic discrepancies is notapparent.Since,aswenotedabove,thereisgenerallyverygood correspondencebetweentheemissionlineandcontinuumluminosi- ties, in principle it should be possible to determine a reliable mass estimateusingeitherluminosityasaproxyfortheBLRradius.Itis notsimplythecasethatwehavetakenscalingrelationsfromdiﬀer- entpapersinwhichtherelationswerederivedfordiﬀerentsamples of AGN: both relations involving Hαare taken from MR16. Such large systematic diﬀerence in mass are diﬃcult to explain since by Eqn. (1)𝑀BH / 𝐿12, a factor four diﬀerence in luminosity is required to reconcile a factor two diﬀerence in mass. It is very unlikely that the 5100 Å luminosity is contaminated by stellar light from the AGN host galaxies, resulting in masses determined using 𝐿5100 tobeoverestimates. P/y.pcQSOF/i.pc/t.pcperformsasubtractionofthe host galaxy spectrum where necessary, so in principle the stellar contaminationhasbeenremovedfromourmeasurements.Also,we seeasystematicoﬀsetevenforthehighest-masssources:highlylu- minous quasars with optical luminosities far in excess of their host galaxies. It is possible that the procedure we used to ﬁt the broad emission lines to determine their width and luminosity could diﬀer from those used to determine the𝑀BH relations (MR16 used only two Gaussians for the Balmer lines, for example). As described in Section2.3,inthisworkthebroademissionlineswereﬁtwiththree Gaussians. In many cases, the total line proﬁle was decomposed into broad, very broad and intermediate Gaussians. The shallow but very broad component (of Hαin particular) which was often present can add a substantial amount of ﬂux to the line without greatly increasing the FWHM. However, this would have the eﬀect of increasing the emission line luminosities, whereas what we ﬁnd is that the Balmer lines are apparentlyunderluminous compared to the continuum for a given FWHM, resulting in the lower mass estimates.Diﬀerencesintheassumedvirialfactor 𝑓 willalsoresult in equivalently larger or smaller estimates of the mass. However, the relations we test here use𝑓 \u00191 so the mass diﬀerences ought to be very minor. To avoid problems introduced by systematic diﬀerences in black hole masses, we have calculated new mass scaling relations tailored to our sample. As described in Section 4.2 these minimise the discrepancy with respect to estimates made from FWHM(Hβ) and 𝐿5100 (which we judge is the most reliable estimate). The new scaling relations are presented in Table 4. We note here that all of our relations have a slope𝛼 consistent with 0.5, the expected value for an inverse-square scaling of radius with luminosity. The majorityoftheAGNinoursample(52percent)havetheiradopted masses derived using the FWHM(Hβ)-𝐿5100 relation from MR16. 45 per cent of the sample have their preferred mass estimate taken from our FWHM(Mg/i.pc/i.pc) and 𝐿3000 Å relation; these were mostly the high-𝑧AGN in our sample, lacking optical spectral coverage of Hβ. Our newly-derived FWHM(Mg/i.pc/i.pc)-𝐿3000 Å mass relation has a very minor systematic oﬀset with respect to the ﬁducial masses calculatedviatheFWHM(H β)-𝐿5100 relationofMR16:themedian diﬀerenceinmassisnegligible,althoughthereissubstantialscatter (𝜎 = 026 dex). Following these recalibrations, we do not expect that using diﬀerent mass relations for diﬀerent AGN will have an adverse impact on our results. 6.3 𝛼ox In Section 5 we explored the relationship between the UV-X-ray energy index 𝛼ox with the UV and X-ray luminosities and with blackholemassesandaccretionrates.AsmentionedinSection3.1, wehaveusedamixtureofspectroscopicandphotometricmeasure- ments of 𝐿2500 (from which we derive𝛼ox) and there are subtle diﬀerences between the two luminosity estimates. In Appendix A, we have performed a detailed investigation of the potential impact ofmixingdiﬀerentluminositymeasuresonourresults,inparticular onthe log¹𝐿2500º-𝛼ox correlation.Tosummarise,theaveragediﬀer- ence between spectroscopic and photometric luminosities is small (hΔlog¹𝐿2500ºi=\u0000002 dex) and we derive very similar distribu- tionsof 𝛼ox fromboththespectroscopicandphotometricestimates (the 𝛼ox distributions are statistically indistinguishable). We have recalculatedlog¹𝐿2500º-𝛼ox relationsforthespectroscopically-and photometrically-derived quantities separately. Each subset shows a statisticallysigniﬁcantpositivecorrelationbetween log¹𝐿2500ºand 𝛼ox, with similar dispersions (\u0019 01 dex) so it does not appear that photometric measurements of𝐿2500 are substantially less reli- able than the spectroscopic ones. The photometriclog¹𝐿2500º-𝛼ox relationdiﬀersmostfromthespectroscopiconeatthehighestlumi- nosities(Fig.A2).However,thisdiﬀerencewillhavelittleeﬀecton our analysis in Section 5. The high-luminosity sources are mostly athighredshift( 𝑧 ¡05)andforthesewehaveadopted log¹𝐿2500º from spectroscopy rather than photometry. Very few sources lie in MNRAS000, 1–23 (2023)18 D. Kynoch et al. theregionofparameterspaceinwhichthephotometric log¹𝐿2500º- 𝛼ox relationissubstantiallydiﬀerentfromthespectroscopicone and forwhich log¹𝐿2500ºand𝛼ox wereestimatedfromphotometry.We concludethatthemixingofspectroscopically-andphotometrically- derivedUVluminositiesdoesnotgreatlyaﬀectourresultsandnote that several of our relations are in good agreement with those of previous studies. Our analysis of the ﬁnal sample in Section 5 demonstrates a highlysigniﬁcantcorrelationof 𝛼ox withthemonochromaticUVlu- minositylog¹𝐿2500ºforallAGNaswellasthefoursubsets(narrow- and broad-line; radio loud and quiet) taken separately. No highly signiﬁcantlog¹𝐿2 keVº–𝛼ox correlationwasfoundforanyindividual subset,orforthefullsample,althoughamarginallysigniﬁcantcor- relation (¡3𝜎) was found for the RQ subset only (see Table 5 and Fig. 16). In a previous study of quasars, Lusso et al. (2010) found nosigniﬁcantrelationbetween 𝛼ox andlog¹𝐿2 keVº,consistentwith our results from a sample containing both quasars and NLS1s. Figs. 16 and 17 illustrate diﬀerences in the𝛼ox distributions, and the relationship between𝛼ox and UV luminosity, between dif- ferent subsets of our sample. Whilston averagenarrow-line AGN have lower𝛼ox than broad-line AGN they have slightly higher𝛼ox when compared with broad-line AGN in the same𝐿2500 bin, likely as a result of the stronger soft X-ray excess (warm corona) com- ponent in the former. Figs. 16 and 17 show a subtle diﬀerence in the dependence of𝛼ox on the UV luminosity between the NL and BL subsets, with the best ﬁtting𝛼ox-𝐿2500 relation being slightly steeper for NL AGN. Starker diﬀernces are apparent between our radio-loud and radio-quiet AGN: the radio-loud AGN are on aver- agemoreX-rayluminousthantheradio-quietsourcesalthoughboth sets have similar UV luminosities (Fig. 16). We can see in Fig. 16 that radio-loud AGN are systematically oﬀset to lower𝛼ox with re- spect to the radio-quiet sources, and very few radio-loud AGN are found to have higher𝛼ox than the average for radio-quiet AGN at a givenlog 𝐿2500. Even when binning by𝐿2500, radio-loud AGN have systematically lower𝛼ox (harder SEDs) than the radio-quiet AGN. This trend has been noted in previous studies. Lawther et al. (2017) found that radio-loud objects in their sample of𝑧\u00192 AGN were oﬀset from the best-ﬁtting𝐿2500-𝛼ox relation determined by Strateva et al. (2005) for radio-quiet quasars. More recently, Zhu et al. (2020) found that their large sample of radio-loud quasars wereoﬀsetfromacomparisonradio-quietsample,alsointhesame sense that we ﬁnd here. In this work we show that the same trend extendstolowerluminositiesandredshiftsthanprobedbyLawther et al. (2017), consistent with the ﬁndings of Zhu et al. (2020). Of- ten this systematic oﬀset between RL and RQ AGN is attributed to the radio jet contributing to the hard X-ray band, resulting in an increasedX-rayﬂuxandlowerUV/X-rayratio.However,Zhuetal. (2020) argue that for steep radio spectrum sources the excess X-ray luminosity of radio-loud AGN is likelynot due to contamination of the X-ray emission by the jet, but rather some evolution of the disc-coronaconﬁguration.Zhuetal.(2020)proposedthatthebest- ﬁtting relation between𝛼ox and log 𝐿2500 for radio-quiet quasars (which we show in Fig. 16) represented an approximate ‘jet line’ for AGN, in analogy to the jet line of accreting stellar-mass black holes (e.g. Fender et al. 2004): jets are quenched for AGN on the soft-SED side of the line and the most powerful jets are found in sources with the hardest SEDs for a given𝐿2500. The best-ﬁtting 𝛼ox-log 𝐿2500 relation for radio-quiet quasars found by Zhu et al. 2020 is entirely consistent with our relation for radio-quiet AGN (Table 5 and Fig. 16). In Section 5.2 we calculated the mean𝛼ox for 31 subsets of AGN, binned in bothlog¹𝑀BHºand log¹𝐿2500º, ranging over four orders of magnitude in each quantity (Fig. 19). Fig. 19 illustrates the diversity of our sample - blue and red contours show the loca- tion in parameter space of AGN drawn from the quasar and NLS1 catalogues,respectively.TheinclusionofAGNfromtheNLS1cat- alogue extends our sample to, on average, lower black hole masses and higher accretion rates than probed by the quasars. This wide coverageof parameter spaceallows usto trackchanges inthe spec- tralshape(characterisedby 𝛼ox)withbothmassandaccretionrate. We compared the mean𝛼ox for each bin to the prediction made from the AGN SED model/q.pc/s.pc/o.pc/s.pc/e.pc/d.pc. The correspondence between the model predictions and our sample data is remarkably good: nearly two thirds of bins havejΔ𝛼oxj 01 and in all but one bin jΔ𝛼oxj02. Fig 20 shows that the largest diﬀerences between the data and the model are at the extremes of the parameter space: for high-mass/high-luminosity AGN, the model predicts SEDs which are too red and for low-mass/low-luminosity AGN the predictions are too blue. Of course, if there is a systematic error in the calcu- lation of the black hole masses then the/q.pc/s.pc/o.pc/s.pc/e.pc/d.pcpredictions are not being compared with the appropriate AGN (however, as described previously, we have made eﬀorts to ensure our mass estimates are reliable). Alternatively, whilst/q.pc/s.pc/o.pc/s.pc/e.pc/d.pcmakes good predictions for moderate masses and luminosities, it is possible that the actual disc-corona coupling is more complex than assumed by the model. We will explore these issues in more detail by modelling the full broadband SEDs of our sample (Mitchell et al., submitted). 7 SUMMARY AND CONCLUSIONS We have assembled a sample of 696 AGN with SDSS optical spec- trawhichalsohaveX-rayspectraandsimultaneousUVphotometry recorded byXMM-Newton. Although this is not a complete or rep- resentative sample of AGN, the objects in our sample cover a wide range in black hole mass, accretion rate, spectral hardness and red- shift. The majority of sources in our sample are luminous AGN found in the SDSS-DR14 quasar catalog; these are supplemented withaselectionoflower-luminosity,narrow-linetype1AGNinthe local Universe (taken from R17). We have performed new optical spectral ﬁts so that we have comparable measurements of the AGN continuum and at least one broad emission line (Hα, Hβ, or Mg/i.pc/i.pc) for the narrow-line AGN (and broad-line AGN that lacked a measurement of the Mg/i.pc/i.pcspec- tral region). We computed single-epoch virial black hole masses for the AGN in our sample using these broad emission lines with boththelocalcontinuumluminosityandtheemissionlineluminos- ity. However, we found some large systematic diﬀerences between mass estimated made using diﬀerent relations (masses calculated with FWHM[Hα] and𝐿H α are on average a factor two lower than those made using𝐿5100). We have calculated new virial black hole massrelationswhichminimisethesystematicoﬀsetswithrespectto ﬁducialblackholemassescalculatedusingFWHM(H β)and 𝐿5100. FromtheX-raycatalogueﬂuxes,wehaveestimatedrest-frame 2 keV luminosities and X-ray photon indices for the full sample. Combining these with our UV measurements, we have calculated theenergyindex 𝛼ox,anindicatorofthebroadbandSEDshape.We found that narrow-line sources have steeper X-ray spectra (higher Γ) and, when controlling for𝐿UV, have softer broad-band SEDs (higher 𝛼ox) than typical broad-line AGN. Diﬀerences were also found with respect to radio properties with radio-loud AGN being on average more X-ray luminous and having harder SEDs (lower 𝛼ox)thantheradio-quietsubset.Fig.19illustratesthatthemajority of the AGN in our sample lie between002  𝐿𝐿Edd  2, with MNRAS000, 1–23 (2023)The SOUX AGN Sample – Sample deﬁnition19 narrow-linetype1oﬀsettolowermassesandhigheraccretionrates compared with the broad-line quasars. We have shown that, to ﬁrst order, the model/q.pc/s.pc/o.pc/s.pc/e.pc/d.pcmakes very good predictions of𝛼ox for a given black hole mass and UV luminosity (assumed to be a proxy for the accretion rate). Importantly however, there are systematic diﬀerences between the data and model predictions at the extremes of the mass-accretion rate parameter space. In a subsequent paper (Mitchell et al., submitted) we will model the broad-band SEDs, testing the physical assumptions made in the/q.pc/s.pc/o.pc/s.pc/e.pc/d.pcmodel and further explore the relationships between the UV/optical and X-ray spectra. ACKNOWLEDGEMENTS DK acknowledges support from the Czech Science Foundation project No. 19-05599Y, funding from the Czech Academy of Sci- ences, and the receipt of a UK Science and Technology Facilities Council (STFC) studentship ST/N50404X/1. JAJM acknowledges the support of the STFC studentship ST/S505365/1. CD and MJW acknowledge support from STFC grant ST/T000244/1. MJW ac- knowledges an Emeritus Fellowship award from the Leverhulme Trust. HL acknowledges a Daphne Jackson Fellowship sponsored by the STFC. We thank the anonymous referee for their careful reading of the original manuscript and their comments and suggestions that improvedthequalityofthispaper.Collaborationonthisprojectwas partly facilitated by meetings of team 481 held at the International Space Science Institute (ISSI), Bern, Switzerland, and we thank ISSI for their hospitality. Many thanks also go to Suv Rakshit, VickyFawcettandHengxiaoGuofortheirhelpandadviceonusing P/y.pcQSOF/i.pc/t.pc. Funding for the Sloan Digital Sky Survey IV has been pro- vided by the Alfred P. Sloan Foundation, the U.S. Department of Energy Oﬃce of Science, and the Participating Institutions. SDSS- IV acknowledges support and resources from the Center for High- Performance Computing at the University of Utah. The SDSS web site iswww.sdss.org. SDSS-IV is managed by the Astrophysical Research Consor- tium for the Participating Institutions of the SDSS Collaboration including the Brazilian Participation Group, the Carnegie Institu- tionforScience,CarnegieMellonUniversity,theChileanParticipa- tion Group, the French Participation Group, Harvard-Smithsonian Center for Astrophysics, Instituto de Astrofísica de Canarias, The JohnsHopkinsUniversity,KavliInstituteforthePhysicsandMath- ematics of the Universe (IPMU) / University of Tokyo, the Ko- rean Participation Group, Lawrence Berkeley National Laboratory, LeibnizInstitutfürAstrophysikPotsdam(AIP),Max-Planck-Institut fürAstronomie(MPIAHeidelberg),Max-Planck-InstitutfürAstro- physik(MPAGarching),Max-Planck-InstitutfürExtraterrestrische Physik(MPE),NationalAstronomicalObservatoriesofChina,New Mexico State University, New York University, University of Notre Dame, Observatário Nacional / MCTI, The Ohio State University, Pennsylvania State University, Shanghai Astronomical Observa- tory, United Kingdom Participation Group, Universidad Nacional Autónoma de México, University of Arizona, University of Col- orado Boulder, University of Oxford, University of Portsmouth, University of Utah, University of Virginia, University of Wash- ington, University of Wisconsin, Vanderbilt University, and Yale University. This research has made use of data obtained from the 4XMM XMM-Newton serendipitous source catalogue compiled by the 10 institutes of theXMM-Newton Survey Science Centre selected by ESA. This research made use of Astropy,4 a community-developed core Python package for Astronomy (Astropy Collaboration et al. 2013, 2018). Some of the results in this paper have been derived using the HEALPix5 (Górski et al. 2005) package. DATA AVAILABILITY Thedataunderlyingthisworkareavailablefromthecataloguesand archivesdescribedinSection2.ThedetailsoftheAGNintheSOUX sample, relevant measures and derived quantities are presented in the table available online which is described in Appendix D. REFERENCES Alexander D. M., et al., 2003, AJ, 125, 383 Arcodia R., Ponti G., Merloni A., Nandra K., 2020, A&A, 638, A100 Astropy Collaboration et al., 2013, A&A, 558, A33 Astropy Collaboration et al., 2018, AJ, 156, 123 Becker R. H., White R. L., Helfand D. J., 1995, ApJ, 450, 559 Bentz M. C., Peterson B. M., Pogge R. W., Vestergaard M., Onken C. A., 2006, ApJ, 644, 133 Blanton M. R., et al., 2017, AJ, 154, 28 Calderone G., Nicastro L., Ghisellini G., Dotti M., Sbarrato T., Shankar F., Colpi M., 2017, MNRAS, 472, 4051 Cardelli J. A., Clayton G. C., Mathis J. S., 1989, ApJ, 345, 245 Collinson J. S., Ward M. J., Done C., Landt H., Elvis M., McDowell J. C., 2015, MNRAS, 449, 2174 Collinson J. S., Ward M. J., Landt H., Done C., Elvis M., McDowell J. C., 2017, MNRAS, 465, 358 Condon J. J., Cotton W. D., Greisen E. W., Yin Q. F., Perley R. A., Taylor G. B., Broderick J. J., 1998, AJ, 115, 1693 DoneC.,DavisS.W.,JinC.,BlaesO.,WardM.,2012,MNRAS,420,1848 Dong X., Wang T., Wang J., Yuan W., Zhou H., Dai H., Zhang K., 2008, MNRAS, 383, 581 Fender R. P., Belloni T. M., Gallo E., 2004, MNRAS, 355, 1105 Fernández-Ontiveros J. A., Muñoz-Darias T., 2021, MNRAS, 504, 5726 Fitzpatrick E. L., 1999, PASP, 111, 63 Gaskell C. M., 2017, MNRAS, 467, 226 GaskellC.M.,GoosmannR.W.,AntonucciR.R.J.,WhysongD.H.,2004, ApJ, 616, 147 Gierliński M., Done C., 2004, MNRAS, 349, L7 Ginsburg A., et al., 2019, AJ, 157, 98 Goodrich R. W., 1989, ApJ, 342, 224 GórskiK.M.,HivonE.,BandayA.J.,WandeltB.D.,HansenF.K.,Reinecke M., Bartelmann M., 2005, ApJ, 622, 759 Greene J. E., Ho L. C., 2005, ApJ, 630, 122 Greene J. E., et al., 2010, ApJ, 723, 409 GuoH.,ShenY.,WangS.,2018,PyQSOFit:Pythoncodetoﬁtthespectrum of quasars (ascl:1809.008) Guo H., Liu X., Shen Y., Loeb A., Monroe T., Prochaska J. X., 2019, MNRAS, 482, 3288 Guo H., et al., 2020, ApJ, 888, 58 Haardt F., Maraschi L., 1991, ApJ, 380, L51 Jin C., Ward M., Done C., Gelbord J., 2012a, MNRAS, 420, 1825 Jin C., Ward M., Done C., 2012b, MNRAS, 422, 3268 Jin C., Ward M., Done C., 2012c, MNRAS, 425, 907 Kalberla P. M. W., Burton W. B., Hartmann D., Arnal E. M., Bajaja E., Morras R., Pöppel W. G. L., 2005, A&A, 440, 775 4 http://www.astropy.org 5 https://healpix.sourceforge.io. MNRAS000, 1–23 (2023)20 D. Kynoch et al. Kellermann K. I., Sramek R., Schmidt M., Shaﬀer D. B., Green R., 1989, AJ, 98, 1195 Kelly B. C., 2007, ApJ, 665, 1489 KomossaS.,VogesW.,XuD.,MathurS.,AdorfH.-M.,LemsonG.,Duschl W. J., Grupe D., 2006, AJ, 132, 531 Körding E. G., Jester S., Fender R., 2006, MNRAS, 372, 1366 Krawczyk C. M., Richards G. T., Mehta S. S., Vogeley M. S., Gallagher S. C., Leighly K. M., Ross N. P., Schneider D. P., 2013, ApJS, 206, 4 Kubota A., Done C., 2018, MNRAS, 480, 1247 Kubota A., Done C., 2019, MNRAS, 489, 524 Kynoch D., Ward M. J., Lawrence A., Bruce A. G., Landt H., MacLeod C. L., 2019a, MNRAS, 485, 2573 Kynoch D., et al., 2019b, MNRAS, 487, 181 La Mura G., Popović L. Č., Ciroi S., Rafanelli P., Ilić D., 2007, ApJ, 671, 104 Landt H., Bentz M. C., Peterson B. M., Elvis M., Ward M. J., Korista K. T., Karovska M., 2011a, MNRAS, 413, L106 Landt H., Elvis M., Ward M. J., Bentz M. C., Korista K. T., Karovska M., 2011b, MNRAS, 414, 218 Landt H., et al., 2017, MNRAS, 464, 2565 Lawther D., Vestergaard M., Raimundo S., Grupe D., 2017, MNRAS, 467, 4674 Lu Y., Wang T., Zhou H., Wu J., 2007, AJ, 133, 1615 Lu K.-X., Zhao Y., Bai J.-M., Fan X.-L., 2019, MNRAS, 483, 1722 Lusso E., Risaliti G., 2016, ApJ, 819, 154 Lusso E., Risaliti G., 2017, A&A, 602, A79 Lusso E., et al., 2010, A&A, 512, A34 Magdziarz P., Blaes O. M., Zdziarski A. A., Johnson W. N., Smith D. A., 1998, MNRAS, 301, 179 Mejía-Restrepo J. E., Trakhtenbrot B., Lira P., Netzer H., Capellupo D. M., 2016, MNRAS, 460, 187 Merloni A., Heinz S., di Matteo T., 2003, MNRAS, 345, 1057 Miller L., Peacock J. A., Mead A. R. G., 1990, MNRAS, 244, 207 Moravec E., Svoboda J., Borkar A., Boorman P., Kynoch D., Panessa F., Mingo B., Guainazzi M., 2022, A&A, 662, A28 Netzer H., 2019, MNRAS, 488, 5185 Netzer H., Lira P., Trakhtenbrot B., Shemmer O., Cury I., 2007, ApJ, 671, 1256 Noda H., Done C., 2018, MNRAS, 480, 3898 Novikov I. D., Thorne K. S., 1973, in Black Holes (Les Astres Occlus). pp 343–450 Osterbrock D. E., Ferland G. J., 2006, Astrophysics of gaseous nebulae and active galactic nuclei. University Science Books Osterbrock D. E., Pogge R. W., 1985, ApJ, 297, 166 Osterbrock D. E., Shuder J. M., 1982, ApJS, 49, 149 Page M. J., et al., 2012, MNRAS, 426, 903 Pâris I., et al., 2018, A&A, 613, A51 Peterson B. M., et al., 2004, ApJ, 613, 682 Rakshit S., Stalin C. S., Chand H., Zhang X.-G., 2017, ApJS, 229, 39 Rakshit S., Stalin C. S., Kotilainen J., 2020, ApJS, 249, 17 RuanJ.J.,AndersonS.F.,EracleousM.,GreenP.J.,HaggardD.,MacLeod C. L., Runnoe J. C., Sobolewska M. A., 2019, ApJ, 883, 76 Sacchi A., et al., 2022, A&A, 663, L7 SalvestriniF.,RisalitiG.,BisogniS.,LussoE.,VignaliC.,2019,A&A,631, A120 Schlegel D. J., Finkbeiner D. P., Davis M., 1998, ApJ, 500, 525 Schnorr-Müller A., et al., 2016, MNRAS, 462, 3570 Shakura N. I., Sunyaev R. A., 1973, A&A, 24, 337 Shen Y., Liu X., 2012, ApJ, 753, 125 Shen Y., et al., 2011, ApJS, 194, 45 Shen Y., et al., 2019, ApJS, 241, 34 Strateva I. V., Brandt W. N., Schneider D. P., Vanden Berk D. G., Vignali C., 2005, AJ, 130, 387 Sun M., et al., 2015, ApJ, 811, 42 Svoboda J., Guainazzi M., Merloni A., 2017, A&A, 603, A127 Tananbaum H., et al., 1979, ApJ, 234, L9 Vestergaard M., Peterson B. M., 2006, ApJ, 641, 689 Webb N. A., et al., 2020, A&A, 641, A136 WooJ.-H.,LeH.A.N.,KarouzosM.,ParkD.,ParkD.,MalkanM.A.,Treu T., Bennert V. N., 2018, ApJ, 859, 138 Yip C. W., et al., 2004a, AJ, 128, 585 Yip C. W., et al., 2004b, AJ, 128, 2603 Zhou H., Wang T., Yuan W., Lu H., Dong X., Wang J., Lu Y., 2006, ApJS, 166, 128 Zhu D., Sun M., Wang T., 2017, ApJ, 843, 30 ZhuS.F.,BrandtW.N.,LuoB.,WuJ.,XueY.Q.,YangG.,2020,MNRAS, 496, 245 APPENDIX A: UV AND OPTICAL LUMINOSITIES In Section 3.1 we estimated the UV luminosities at rest-frame 2500 Å from either the optical spectrum of a source, or itsXMM- Newton OM or SDSS photometry and in Section 5 we used these to calculate𝛼ox. Similarly, in Section 3.1 we estimated the optical luminosities at rest-frame 4400 Å from either the optical spectrum of a source or its SDSS photometry and used these to calculate the radio loudness parameter𝑅 in Section 3.3. This mixture of lumi- nosity measures from diﬀerent sources is not ideal and raises the following issues: \u000fVariability: only the OM photometry was recorded simulta- neously with the X-ray data but the SDSS spectra and photometry were not. \u000fApertureeﬀects:the XMMOMphotometricaperture(12–35 00 diameter)islargerthantheSDSSspectralﬁberdiameter(3 00forthe SDSS spectrograph and 200for the BOSS spectrograph). \u000fFlux sampling: the luminosities are arguably best determined fromthespectra,fromwhichtheﬂuxismeasuredinanarrow(100Å wide) window centred on the relevant rest-frame wavelength. In contrast the OM and SDSS photometric bands sample ﬂux from a largerwavelengthwindow:theeﬀectivewidthsofthebandsaresev- eralhundredÅandthereforetheypossiblycontainsomebroadline emission.Whilstwetookphotometricﬂuxesfromabandcontaining either2500Åor4400Å,thebandswillnotgenerallybecentredon these wavelengths. The Galactic dust dereddening we have applied to the ﬂuxes will be more accurate for the spectroscopic measures thanthephotometricones(althoughGalacticextinctionisverylow in the SDSS survey area, and the eﬀect of reddening at 4400 Å is relatively minor anyway). Here we assess the possible scale of the discrepancies that may be introduced, and their eﬀect on𝛼ox and 𝑅across the sample. For 𝐿2500 162 sources have a measurement derived from both the SDSS spectrum andXMM OM photometry; 414 sources have a measurement derived from both the spectrum and SDSS pho- tometry. For𝐿4400 588 sources have a measurement derived from both the SDSS spectrum and SDSS photometry (note that we did not use anyXMM OM photometry to determine𝐿4400). In Fig A1 we compare the spectroscopic and photometric measures of the luminosities at 2500 and 4400 Å; we have used/l.pc/i.pc/n.pc/m.pc/i.pc/x.pcto deter- mine the best-ﬁtting linear regressions and present the values in Table A1. Panels (d) in Fig A1 show that the relationship between the spectroscopic and photometric luminosities is approximately one-to-one across several orders of magnitude in luminosity, how- everthereissubstantialscatter( 𝜎=014 dexfor 𝐿2500 and0.09dex for 𝐿4400). We have also calculated the luminosity diﬀerences Δlog¹𝐿º= log¹𝐿specº\u0000 log¹𝐿photºand do not ﬁnd, on average, strong evidence for a systematic oﬀset between the spectroscopic and photometric luminosities withhΔlog¹𝐿2500ºi=\u0000002 \u0006016 and hΔlog¹𝐿4400ºi=\u0000003 \u0006013.Thisisclearfrompanels(g)of Fig. A1 which show the distributions ofΔlog¹𝐿º. Panels (e) show MNRAS000, 1–23 (2023)The SOUX AGN Sample – Sample deﬁnition21 Table A1.Linear regression parameters for the comparisons of𝐿2500 and 𝐿4400 measurements from spectra and photometry Indep. var. Dep. var. Slope Intercept 𝜎 log \u0010 𝐿spec 2500 \u0011 log \u0010 𝐿phot 2500 \u0011 0966 \u00060012 1 55 \u0006054 0.14 log \u0010 𝐿spec 2500 \u0011 Δlog¹𝐿2500º 0053 \u00060012 \u0000240 \u0006053 0.14 Redshift Δlog¹𝐿2500º 0040 \u00060018 \u0000106 \u0006002 0.14 log \u0010 𝐿spec 4400 \u0011 log \u0010 𝐿phot 4400 \u0011 0990 \u00060007 0 47 \u0006033 0.09 log \u0010 𝐿spec 4400 \u0011 Δlog¹𝐿4400º 0014 \u00060007 \u0000065 \u0006033 0.09 Redshift Δlog¹𝐿4400º 0042 \u00060017 \u00000042 \u00060009 0.09 We deﬁneΔlog¹𝐿2500º=log \u0010 𝐿spec 2500 \u0011 \u0000log \u0010 𝐿phot 4400 \u0011 . Δlog¹𝐿ºasafunctionof log¹𝐿specº;inthesepanelsweseeaweak tendency for photometric measurements to be higher than spec- troscopic ones at low spectroscopic luminosities and vice versa. Both Δlog¹𝐿2500ºand Δlog¹𝐿4400ºare weakly but signiﬁcantly correlated with their corresponding spectroscoptic luminosities6. Aspanels(f)inFig.A1show,wecandirectlycomparespectro- scopicandphotometricmeasuresof 𝐿4400,butnot 𝐿2500,forAGN at 𝑧 . 05. We would expect to see greater discrepancies at 4400 Å for these low redshifts because (i) aperture eﬀects will be greatest forAGNinresolvedhostgalaxiesand(ii)galaxieswillberelatively brighterintheopticalthantheUV.Panel(f)inthe 𝐿4400 plotshows that there is no signiﬁcant correlation betweenΔlog¹𝐿4400ºand 𝑧: the Pearson correlation coeﬃcient is 0.05 with𝑝-value 0.20. This provides some reassurance that the photometric UV ﬂuxes will not bestronglyenhancedeither.Theextremesofthediscrepanciesalong the best-ﬁtting linear regressions areΔlog¹𝐿2500º. \u000601 dex and Δlog¹𝐿4400º. \u0006005 dex which are small when compared with the scatter between the luminosities. We therefore judge that any systematic diﬀerences between spectroscopic and photometric lu- minosities have a negligible eﬀect on our results. We examined the impact on the radio loudness categorisation bydeterminingthenumberofsourceswhichwouldchangecategory if their photometric𝐿4400 were ‘wrong’ on the scale of the scat- terbetweenspectroscopicandphotometricmeasurements.Only35 AGN with radio information (6 per cent) have𝐿4400 derived from photometry, compared with 593 which have𝐿4400 derived from their optical spectrum. We have checked these 35 sources and only three have j»log¹𝐿5GHzº\u0000 1¼\u0000 log¹𝐿4400ºj ¡ 009 dex (where 0.09 dex is the scatter between measurements determined above) meaning their radio loudness classiﬁcation would change if the op- tical luminosities change by\u0006009 dex. All three were undetected in FIRST, so the radio loudness classiﬁcations are only estimates based on the FIRST ﬂux limit (as described in Section 3.3). One source, which we have classiﬁed as radio quiet, would have unde- termined radio loudness if it were 0.09 dex optically fainter. The othertwohaveundeterminedradioloudnessbutwouldbeclassiﬁed as radio quiet if they were 0.09 dex optically brighter. A change in the radio loudness category of three AGN out of 611 will not have a noticeable eﬀect on our results. There was a greater mixture of𝐿2500 measurements: as stated in Section 3.1, for 421 AGN we have used𝐿2500 from the optical spectrumandfor264AGN(39percent)wehaveusedphotometry. We therefore test for possible discrepancies in𝛼ox that may re- sult from this mixture. Optical sectroscopic coverage of rest-frame 6 The Pearson correlation coeﬃcient of Δlog¹𝐿2500º with log \u0010 𝐿spec 2500 \u0011 is 0.21 with a𝑝-value \u001c005; for the correlation ofΔlog¹𝐿4400ºwith log \u0010 𝐿spec 4400 \u0011 we determine a coeﬃcient 0.13 and𝑝-value 0.002. 2500 Å is only possible for sources at𝑧 & 05, whereas the wide wavelength range spanned by the UV-optical-infrared photometric ﬁlters mean that a𝐿2500 determination is possible across the full rangeofredshiftscoveredbyoursample.Toavoidbiasesintroduced by selecting diﬀerent subsets of AGN in our parameter space, we test only the set of 416 AGN which haveboth a spectroscopic and photometric value of𝐿2500. For each of these AGN we have two values of 𝐿2500 and two corresponding values of𝛼ox; these are shown in Fig. A2. The plot shows that the distributions of𝐿2500 and 𝛼ox are slightly diﬀerent for the spectroscopic and photomet- ric subsets. The medianlog¹𝐿2500erg s\u00001 Hz\u00001º= 3015 for the spectroscopic measurements and is slightly higher for the photo- metric measurements with a median of 30.34. KS and Anderson- Darling tests both indicate that thelog¹𝐿2500ºmeasurements are drawnfromdiﬀerentdistributions(the 𝑝-valuesofthetestsareboth  0001). However, whilst the𝛼ox distributions are not identical, the same statistical tests cannot rule out that the spectroscopically- and photometrically-derived quantities are drawn from the same distribution (the 𝑝-values are 0.44 and 0.25, respectively). So we seeconsistencyinthederived 𝛼ox valuesdespiteaslightdiﬀerence inlog¹𝐿2500º. Wehaverecomputed log¹𝐿2500º-𝛼ox relationsforthespectro- scopic and photometric quantities separately. The best-ﬁtting lin- ear regressions for the two subsets both show a statistically sig- niﬁcant positive correlation between𝛼ox and log¹𝐿2500º, as was found for the full sample in Section 5 (see Table A2). In Fig. A2 we show these best-ﬁtting relations and their3𝜎 uncertainty re- gions. The relation for the spectroscopic subset is slightly steeper (slope 0168 \u00060011) than that of the photometric subset (slope 0127 \u00060009). In the plot we see that the two relations agree to within 3𝜎up tolog¹𝐿2500erg s\u00001 Hz\u00001º\u0019 305, above which we seeagreaterdiscrepancy.Again,wejudgethatthiswillhaveonlya minor eﬀect on our results. As stated in Section 3.1, we have taken 𝐿2500 from the optical spectrum where it is available (which it is for sources at𝑧 & 05) and used this value to determine𝛼ox. Only where 𝐿2500 is not available in an optical spectrum will we have estimated 𝐿2500 and 𝛼ox from photometry. Fig. A3 demonstrates that there are very few sources in the region of parameter space in which𝐿2500 can only be determined from photometry and the spectroscopic and photometriclog¹𝐿2500º-𝛼ox relations are sub- stantiallydiﬀerent.Sincethereisverygoodagreementbetweenthe spectroscopic and photometriclog¹𝐿2500º-𝛼ox relations in the re- gion of parameter space that contains the majority of our sources, we do not believe that mixing the two types of measurement will have a substantial impact on our results and conclusions. This is corroborated by the fact that we see good agreement between our log¹𝐿2500º-𝛼ox relationsandthoseofpreviousstudies(e.g.Strateva et al. 2005; Lusso et al. 2010; Zhu et al. 2020). Based on the ﬁndings of previous work, we do not expect the scale of aperture eﬀects to be very substantial in the SOUX sam- ple. In a sample of 51𝑧  04 AGN, Jin et al. (2012a) noticed a discrepancy between ﬂux levels of the SDSS spectra andXMM photometry for 17 sources, all with𝑧 . 024. The photometric points were above the extrapolation from the optical spectra, so it was proposed that the discrepancies were due to aperture eﬀects, as described above. To obtain better agreement, Jin et al. (2012a) reanalysed the optical-UV images for aﬀected sources and remea- sured the photometry using a smaller aperture, excluding more of the extended host galaxy ﬂux. The optical-UV photometric ﬂuxes weregenerallyreducedby \u001850 percentwhenusingasmalleraper- ture (see Fig.1 of Jin et al. 2012a). A bespoke reanalysis of the OM photometry for AGN in the SOUX sample is not feasible since it MNRAS 000, 1–23 (2023)22 D. Kynoch et al. 42 43 44 45 46 47 log(L2500/erg s□1),spectrum 42 43 44 45 46 47log(L2500/erg s□1),photometry (d) OM photometry SDSS photometry 44 45 46 47 log(L2500/erg s□1),spectrum □0.6 □0.4 □0.2 0.0 0.2 0.4 0.6 ∆ log( L2500/erg s□1) (e) 0.0 0.5 1.0 1.5 2.0 2.5 Redshift (f) 0 100 (a) 0 100 (b) (c) 0 100 (g) 42 43 44 45 46 47 log(L4400/erg s□1),spectrum 42 43 44 45 46 47log(L4400/erg s□1),photometry (d) SDSS photometry 42 43 44 45 46 log(L4400/erg s□1),spectrum □0.4 □0.2 0.0 0.2 0.4 0.6 0.8 ∆ log( L4400/erg s□1) (e) 0.0 0.5 1.0 1.5 2.0 2.5 Redshift (f) 0 100 (a) 0 100 (b) (c) 0 250 (g) FigureA1. Acomparisonofspectroscopicandphotometricmeasuresoftheluminositiesat2500Å(top)and4400Å(bottom).Panels(d)showascatterplotof thespectroscopicandphotometricluminosities;theblacklineistheone-to-onerelationandtheorangelineinpanels(d),(e)and(f)showsthebest-ﬁttinglinear regression,withthe1 𝜎conﬁdenceregionsshadedorange.Panels(e)and(f)arescatterplotsoftheluminositydiﬀerence Δlog¹𝐿º=log¹𝐿specº\u0000log¹𝐿photº as a function oflog¹𝐿specºand redshift, respectively. Histograms ofΔlog¹𝐿ºare shown in panels (g). Panels (a) show the distributions oflog¹𝐿ºfor all possible sources. Panels (b) show the distributions of the spectroscopic measures oflog¹𝐿ºfor sources plotted in panels (e). Panels (c) show the redshift distribution of all 696 AGN. Note that we used bothXMM OM and SDSS photometry to determine𝐿2500 but only SDSS photometry to determine𝐿4400. contains hundreds of sources and our investigation here is based on the analysis of standard catalogue and pipeline data, rather than tailored data reductions. Unlike the low-redshift AGN sample of Jin et al. (2012a), the SOUX sample consists of AGN drawn from NLS1 and quasar catalogues, so our AGN are generally UV-bright and not strongly contaminated by their host galaxy ﬂux in the UV. APPENDIX B: AGN WITH MULTIPLE OPTICAL SPECTRA Whilst in this study we consider only one optical spectrum per AGN, many sources have been observed multiple times. Here we showquantitativelythenumberofspectrafortheAGNintheSOUX sample.Weuse /a.pc/s.pc/t.pc/r.pc/o.pc/q.pc/u.pc/e.pc/r.pc/y.pc(Ginsburgetal.2019)tosearchforall opticalspectraofour696AGNinSDSSDR14.Thequeryperforms a 200-radius search around the input coordinates (taken from the Pârisetal.2018andR17catalogues)forsourceswithspectroscopy. Wecheckedthatallspectrareturnedbythequeryrelatetothesame Table A2.Linear regression parameters forlog¹𝐿2500º-𝛼ox relations when 𝐿2500 is derived from either spectra or photometry Subset 𝑁 Slope Intercept 𝜎 log \u0010 𝐿spec 2500 \u0011 416 0168 \u00060011 \u0000364 \u0006032 0.11 log \u0010 𝐿phot 2500 \u0011 416 0127 \u00060009 \u0000243 \u0006028 0.12 All with𝛼ox 685 0120 \u00060006 \u0000216 \u0006019 0.13 For thelog \u0010 𝐿spec 2500 \u0011 subset log¹𝐿2500ºhas been derived from the optical spectrum and this value is used to determine 𝛼ox. The log \u0010 𝐿phot 2500 \u0011 subset contains the same 416 sources, but𝛼ox has beendeterminedfromthephotometrically-derived log¹𝐿2500º.For comparison we also give values for the full set of 685 AGN with 𝛼ox, which contains a mixture of spectroscopically (421) and pho- tometrically (264) derived quantities. object(i.e.theyhavethesame ObjID)andthatthespeciﬁcspectrum analysed in this work is one of those. No results were returned by /a.pc/s.pc/t.pc/r.pc/o.pc/q.pc/u.pc/e.pc/r.pc/y.pcfor seven AGN; for these we manually checked for MNRAS000, 1–23 (2023)The SOUX AGN Sample – Sample deﬁnition23 27 28 29 30 31 32 log(L2500/erg s□1 Hz□1) 1.0 1.2 1.4 1.6 1.8 2.0 αox log Lspec. 2500 log Lphot. 2500 0 100 200 Med.spec. = 30.15 σspec. = 0.58 Med.phot. = 30.34 σphot. = 0.65 0 100 200 Med.spec. = 1.42 σspec. = 0.15 Med.phot. = 1.43 σphot. = 0.15 Figure A2. Relationships of 𝛼ox and log¹𝐿2500ºfor log¹𝐿2500ºderived fromtheopticalspectra(pinkpluses)andfromphotometry(purplecircles). All416sourcesshownhaveameasureof log¹𝐿2500ºfromboththespectrum and photometry, so they are cover the same range in redshift. The pink and purple solid lines show the best-ﬁtting linear regressions forlog \u0010 𝐿spec 2500 \u0011 - 𝛼ox and 𝛼ox-log \u0010 𝐿phot 2500 \u0011 , respectively with 3𝜎uncertainty regions shaded in the same colours. The orange line shows the best-ﬁtting linear regression for the full sample (as in Fig. 16). The top and right-hand panels show the distributions of the two quantities for each subset, and we quote the median value and standard deviation. all spectra in the SDSS Science Archive Server7. We show the distribution of the number of spectra in Fig. B1. Whilst 83 per cent of the AGN in our sample have only one optical spectrum, 17 have many (¡ 49). The wealth of optical data for these AGN may be explored further in future studies. APPENDIX C: QUALITY FLAGS Weassessthequalityofmeasurementsdeterminedfromthespectral ﬁtsbyassigningaqualityﬂag.Ifnoissuesareraisedthequalityﬂag value is 0 and the measurement may be considered to be reliable. The quality ﬂags are integer numbers which are the sum2Bit 0 ¸ 2Bit 1¸2Bit 𝑛 for set bits. The bit numbers and the conditions for setting them are listed in Tables C1 and C2. For example, if the fractional uncertainty of the FWHM of a measured broad Hβline is greater than 2 and the uncertainty on the velocity oﬀset of the lineisgreaterthan1000kms \u00001 thenbits5and8aresetandtheH β quality ﬂag has the value25 ¸28 =32 ¸256 =288. APPENDIX D: TABLE INFORMATION We have made available online a table containing the details (RA, Dec., 𝑧, etc.), measurements (emission line widths, optical and X- ray continuum luminosities etc.) and derived quantities (black hole 7 https://dr14.sdss.org/optical/spectrum/search 0.0 0.5 1.0 1.5 2.0 2.5 Redshift 27 28 29 30 31 32log(L2500/erg s□1 Hz□1) FigureA3. TheUVluminosityat2500Åversussourceredshift.Thevertical red dashed line indicates the approximate redshift above which rest-frame 2500 Å is contained within an SDSS spectrum. The horizontal red dashed line indicates the luminosity above which we see a substantial discrepancy in thelog¹𝐿2500º-𝛼ox relations for spectroscopically- and photometrically- derived quantities (see Fig. A2). 5 10 1 10 100Number of AGN 45 50 Nspec = 1: 580 Nspec = 2: 82 3 ≤ Nspec ≤ 11: 17 49 ≤ Nspec ≤ 52: 17 Nspec Figure B1.The number of SDSS optical spectra (𝑁spec) available for the 696 AGN in our sample, up to and including DR14. The majority of our sample(83percent)haveonlyonespectrum,butasmallnumberhave \u001950 spectra. Table C1.Quality ﬂags for the ﬁtted continuum Bit Condition - no quality ﬂags raised 0 the luminosity or its uncertainty are zero, NaN or inﬁnite 1 fractional uncertainty of the luminosity > 1.5 2 the continuum slope or its uncertainty are zero, NaN or inﬁnite 3 the continuum slope has reached the lower or upper limit in the ﬁt 4 the uncertainty on the continuum slope > 0.3 5 the continuum ﬁt 𝜒2𝜈 ¡50 masses, 𝛼ox etc.) of the 696 AGN in the SOUX sample. Table D1 gives an overview of the columns of that table. This paper has been typeset from a TEX/LATEX ﬁle prepared by the author. MNRAS000, 1–23 (2023)24 D. Kynoch et al. Table C2.Quality ﬂags for the ﬁtted emission lines Bit Condition - no quality ﬂags raised 0 S/N in the continuum around the line < 3 1 the luminosity or its uncertainty are zero, NaN or inﬁnite 2 fractional uncertainty of the luminosity > 1.5 3 the FWHM or its uncertainty are zero, NaN or inﬁnite 4 the FWHM 6 910 kms\u00001 5 the fractional uncertainty of the FWHM > 2 6 the velocity oﬀset or its uncertainty are zero, NaN or inﬁnite 7 the velocity oﬀset has reached the lower or upper limit in the ﬁt 8 the uncertainty on the velocity oﬀset > 1000 kms \u00001 MNRAS000, 1–23 (2023)The SOUX AGN Sample – Sample deﬁnition25 Table D1.The SOUX AGN Sample - Column information for the table containing the details, measurements and derived quantities for the 696 AGN # Column name Units Description Source and SDSS optical spectrum details: 1 SDSS_ID Unique SDSS spectrum identiﬁer (PLATE-MJD-FIBER) 2 RA Degrees Right Ascension (J2000) 3 DEC Degrees Declination (J2000) 4 Z Spectroscopic redshift 5 DR14Q 0: source is not in the SDSS DR14 quasar catalog; 1: source is in the SDSS DR14 quasar catalog 6 R17 0: source is not in the R17 NLS1 catalog; 1: source is in the R17 NLS1 catalog 7 SN_RATIO_CONT Median S/N sampled in emission-line free windows of the spectrum 8 MIN_WAVE Å Shortest rest-frame wavelength covered in the optical spectrum 9 MAX_WAVE Å Longest rest-frame wavelength covered in the optical spectrum UV-optical continuum measurements: 10 LOG_L1350 ergs \u00001 Logarithm of the continuum luminosity at 1350 Å (rest-frame) from the SDSS spectrum 11 LOG_L1350_ERR ergs \u00001 Uncertainty on LOG_L1350_ERR 12 QUALITY_L1350 Quality ﬂag on LOG_L1350 (see Table C1) 13 LOG_L2500_SPEC ergs \u00001 Logarithm of the continuum luminosity at 2500 Å (rest-frame) from the SDSS spectrum 14 LOG_L2500_SPEC_ERR ergs \u00001 Uncertainty on LOG_L2500_SPEC 15 LOG_L2500_PHOT_OM ergs \u00001 Logarithm of the continuum luminosity at 2500 Å (rest-frame) fromXMM-NewtonOM photometry 16 LOG_L2500_PHOT_OM_ERR ergs \u00001 Uncertainty on LOG_L2500_PHOT_OM 17 LOG_L2500_PHOT_SDSS ergs \u00001 Logarithm of the continuum luminosity at 2500 Å (rest-frame) from SDSS photometry 18 LOG_L2500_PHOT_SDSS_ERR ergs \u00001 Uncertainty on LOG_L2500_PHOT_SDSS 19 LOG_L2500_ALPHA_OX Which 𝐿2500 measurement was used to calculate ALPHA_OX (#86) 20 LOG_L3000 ergs \u00001 Logarithm of the continuum luminosity at 3000 Å (rest-frame) from the SDSS spectrum 21 LOG_L3000_ERR ergs \u00001 Uncertainty on LOG_L3000_ERR 22 QUALITY_L3000 Quality ﬂag on LOG_L3000 (see Table C1) 23 LOG_L4400_SPEC ergs \u00001 Logarithm of the continuum luminosity at 4400 Å (rest-frame) from the SDSS spectrum 24 LOG_L4400_SPEC_ERR ergs \u00001 Uncertainty on LOG_L4400_SPEC 25 LOG_L4400_PHOT_OM ergs \u00001 Logarithm of the continuum luminosity at 4400 Å (rest-frame) fromXMM-NewtonOM photometry 26 LOG_L4400_PHOT_OM_ERR ergs \u00001 Uncertainty on LOG_L4400_PHOT_OM 27 LOG_L4400_PHOT_SDSS ergs \u00001 Logarithm of the continuum luminosity at 4400 Å (rest-frame) from SDSS photometry 28 LOG_L4400_PHOT_SDSS_ERR ergs \u00001 Uncertainty on LOG_L4400_PHOT_SDSS 29 LOG_L4400_R_LOUD Which 𝐿4400 measurement was used to calculate R_LOUD (#90) 30 LOG_L5100 ergs \u00001 Logarithm of the continuum luminosity at 5100 Å (rest-frame) from the SDSS spectrum 31 LOG_L5100_ERR ergs \u00001 Uncertainty on LOG_L5100_ERR 32 QUALITY_L5100 Quality ﬂag on LOG_L5100 (see Table C1) Broad emission line measurements: 33 FWHM_HA_BR kms \u00001 Full width at half maximum of the broad Hαemission line 34 FWHM_HA_BR_ERR kms \u00001 Uncertainty on FWHM_HA_BR 35 PEAK_HA_BR Å Peak wavelength of the broad H αemission line 36 PEAK_HA_BR_ERR Å Uncertainty on PEAK_HA_BR 37 EW_HA_BR Å Equivalent width of the broad H αemission line 38 EW_HA_BR_ERR Å Uncertainty on EW_HA_BR 39 LOGL_HA_BR ergs \u00001 Logarithm of the luminosity of the broad Hαemission line 40 LOGL_HA_BR_ERR ergs \u00001 Uncertainty on LOGL_HA_BR 41 QUALITY_HA Quality ﬂag on the broad H αemission line (see Table C2) 42 FWHM_HB_BR kms \u00001 Full width at half maximum of the broad Hβemission line 43 FWHM_HB_BR_ERR kms \u00001 Uncertainty on FWHM_HB_BR 44 PEAK_HB_BR Å Peak wavelength of the broad H βemission line 45 PEAK_HB_BR_ERR Å Uncertainty on PEAK_HB_BR 46 EW_HB_BR Å Equivalent width of the broad H βemission line 47 EW_HB_BR_ERR Å Uncertainty on EW_HB_BR 48 LOGL_HB_BR ergs \u00001 Logarithm of the luminosity of the broad Hβemission line 49 LOGL_HB_BR_ERR ergs \u00001 Uncertainty on LOGL_HB_BR 50 QUALITY_HB Quality ﬂag on the broad H βemission line (see Table C2) 51 BALMER_SOURCE K23: Balmer lines measurements from this work; R20: Balmer lines measured by R20 MNRAS000, 1–23 (2023)26 D. Kynoch et al. Table D1.Table D1 continued # Column name Units Description 52 FWHM_MGII_BR kms \u00001 Full width at half maximum of the broad Mg/i.pc/i.pcemission line 53 FWHM_MGII_BR_ERR kms \u00001 Uncertainty on FWHM_MGII_BR 54 PEAK_MGII_BR Å Peak wavelength of the broad Mg /i.pc/i.pcemission line 55 PEAK_MGII_BR_ERR Å Uncertainty on PEAK_MGII_BR 56 EW_MGII_BR Å Equivalent width of the broad Mg /i.pc/i.pcemission line 57 EW_MGII_BR_ERR Å Uncertainty on EW_MGII_BR 58 LOGL_MGII_BR ergs \u00001 Logarithm of the luminosity of the broad Mg/i.pc/i.pcemission line 59 LOGL_MGII_BR_ERR ergs \u00001 Uncertainty on LOGL_MGII_BR 60 QUALITY_MGII Quality ﬂag on the broad Mg /i.pc/i.pcemission line (see Table C2) 61 MGII_SOURCE K23: Mg /i.pc/i.pcmeasurements from this work; R20: Mg/i.pc/i.pcmeasured by R20 62 MGII_SEP 0:Mg /i.pc/i.pcregionmodelledinglobalﬁttofullspectrum;1:Mg /i.pc/i.pcregionmodelledseparatelytoBalmerline regions (see Section 2.3) 63 LINEWIDTH N: narrow-line type 1 AGN; B: broad-line type 1 AGN (see Section 2.3.1) Black hole mass estimates: 64 MASS_HA_5100_MR16 M \f BH mass estimate from FWHM_HA_BR and LOG_L5100 using the relation of MR16 65 MASS_HA_HA_MR16 M \f BH mass estimate from FWHM_HA_BR and LOGL_HA_BR using the relation of MR16 66 MASS_HB_5100_MR16 M \f BH mass estimate from FWHM_HB_BR and LOG_L5100 using the relation of MR16 67 MASS_HB_HB_G10 M \f BH mass estimate from FWHM_HB_BR and LOGL_HB_BR using the relation of Greene et al. (2010) 68 MASS_MGII_3000_MR16 M \f BH mass estimate from FWHM_MGII_BR and LOG_L3000 using the relation of MR16 69 MASS_MGII_MGII_W18 M \f BHmassestimatefromFWHM_MGII_BRandLOGL_MGII_BRusingtherelationofWooetal.(2018) 70 MASS_HA_5100_K23 M \f BH mass estimate from FWHM_HA_BR and LOG_L5100 using the relation found in this work 71 MASS_HA_HA_K23 M \f BH mass estimate from FWHM_HA_BR and LOGL_HA_BR using the relation found in this work 72 MASS_HB_HB_K23 M \f BH mass estimate from FWHM_HB_BR and LOGL_HB_BR using the relation found in this work 73 MASS_MGII_3000_K23 M \f BH mass estimate from FWHM_MGII_BR and LOG_L3000 using the relation found in this work 74 MASS_MGII_MGII_K23 M \f BH mass estimate from FWHM_MGII_BR and LOGL_MGII_BR using the relation found in this work 75 Q_MASS_HA_5100 Quality ﬂag on H α-5100Å mass:=1 unless QUALITY_HA and QUALITY_L5100 are both 0 76 Q_MASS_HB_5100 Quality ﬂag on H β-5100Å mass:=1 unless QUALITY_HB and QUALITY_L5100 are both 0 77 Q_MASS_MGII_3000 Quality ﬂag on Mg /i.pc/i.pc-3000Å mass:=1 unless QUALITY_MGII and QUALITY_L3000 are both 0 78 LOG_MASS_PREF_VALUE M \f Logarithm of our preferred BH mass estimate 79 LOG_MASS_PREF_SOURCE Which BH mass estimate is the preferred one Multiwavelength measurements: 80 XMM_OBSID The XMM-NewtonObservation ID for the X-ray and OM photometric data 81 XMM_SRCID The unique X-ray source number taken from the XMM-Newtoncatalog 82 LOG_L2KEV ergs \u00001 Hz\u00001 Logarithm of𝐿𝜈 ¹2 keVrest-frameºestimated from theXMM-Newtonband ﬂuxes (see Section 3.2) 83 LOG_L2KEV_ERR ergs \u00001 Hz\u00001 Uncertainty on LOG_L2KEV 84 GAMMA_X Estimated X-ray photon index (see Section 3.2) 85 GAMMA_X_ERR Uncertainty on GAMMA_X 86 ALPHA_OX Optical-X-ray energy index 𝛼ox calculated from LOG_L2500_ALPHA_OX and LOG_L2KEV 87 ALPHA_OX_ERR Uncertainty on ALPHA_OX 88 LOG_L5GHZ WHz \u00001 Logarithm of𝐿𝜈 ¹5 GHzrest-frameºestimated from the FIRST radio ﬂux (see Section 3.3) 89 LOG_L5GHZ_ERR WHz \u00001 Uncertainty on L5GHZ 90 R_LOUD Radio loudness calculated from LOG_L4400_R_LOUD and LOG_L5GHZ 91 R_LOUD_FLAG RL: radio-loud; RQ: radio-detected and radio-quiet; RQu: radio-undetected and radio-quiet; RU: radio- undetected and undetermined radio-loudness;\u0000999: outside FIRST footprint or no𝐿4400 MNRAS000, 1–23 (2023)",
      "references": [
        "Black Holes (Les Astres Occlus).",
        "Astrophysics of gaseous nebulae and active galactic nuclei.",
        "PyQSOFit:Pythoncodetoﬁtthespectrum of quasars"
      ],
      "meta_data": {
        "arxiv_id": "2301.07724v1",
        "doi": "10.1093/mnras/stad221",
        "authors": [
          "Daniel Kynoch",
          "Jake A. J. Mitchell",
          "Martin J. Ward",
          "Chris Done",
          "Elisabeta Lusso",
          "Hermine Landt"
        ],
        "published_date": "2023-01-18T19:00:04Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Defines the SOUX (SDSS–XMM) type-1 AGN sample (696 sources, z≤2.5) with matched SDSS optical spectra (≥1 broad line: Hα/Hβ/MgII), XMM-Newton EPIC X-ray spectra (≥250 counts), and simultaneous XMM Optical Monitor photometry; provides uniformly re-fit optical spectra for consistency (esp. NLS1 additions), derives/compares single-epoch virial BH-mass estimators for Hα/Hβ/MgII using both continuum and line luminosities, recalibrates relations to remove systematics, and characterizes UV–X-ray coupling via αox across BH mass, luminosity/accretion rate, and radio/narrow-line subsets; compares observed αox trends to the physical QSOSED disc–corona model and maps the sample’s L/LEdd coverage (~0.02–2).",
        "methodology": "Cross-matching SDSS DR14Q and Rakshit+17 NLS1 catalog with 4XMM-DR9 (clean detections) and XMM-SUSS4.1 OM sources; selection cuts: positional matches (5″ optical–X-ray, 1″ OM), simultaneous OM–EPIC, Lyα-contamination-aware OM filter requirements by redshift, EPIC counts≥250, z≤2.5, visual QC to remove low-S/N/absorption/BAL/flagged X-ray spectra; optical spectral fitting using adapted PyQSOFit (dereddening via Schlegel maps + Fitzpatrick curve, rest-frame transform, PCA host subtraction with galaxy/quasar eigenspectra, continuum model with power law+FeII+Balmer continuum, multi-Gaussian line fitting with tied narrow components and fixed doublet ratios, MC resampling for uncertainties, bitmask quality flags); classification into narrow-/broad-line type-1 by FWHM threshold 2000 km/s using Hβ/MgII/Hα. Multiwavelength measures: L2500 and L4400 from spectra or OM/SDSS photometry with Galactic dereddening; L2keV from 4XMM band fluxes by estimating monochromatic fluxes at 1 and 3.5 keV (assumed Γ=1.7 per band), K-correcting and interpolating/extrapolating to rest 2 keV, and estimating Γ from the 1–3.5 keV slope; radio matching to FIRST/NVSS (10″), converting 1.4 GHz to rest 5 GHz with αR=0.6, computing radio loudness R=L5GHz/L4400 with detection-limit-based RL/RQ/undetermined classes; αox computation and Bayesian linear regression (linmix) for scaling relations; BH masses from literature virial recipes (MR16, Greene+10, Woo+18) and new recalibration of K and α in log MBH = logK + α log L + 2 log FWHM anchored to Hβ–L5100 masses; comparison to QSOSED (Kubota & Done 2018) predictions over MBH–L2500 grid (spin=0, i=30°).",
        "experimental_setup": "Data sources: SDSS-DR14Q optical spectra (plus SDSS DR12-based Rakshit+17 NLS1 catalog), 4XMM-DR9 EPIC X-ray catalogue (cleaned: SUM_FLAG≤3, not CONFUSED, low background), XMM-SUSS4.1 OM photometry (simultaneous with EPIC by OBSID), plus FIRST and NVSS 1.4 GHz radio surveys. Sample assembly: initial DR14Q–4XMM match within 5″ (best match), retain clean X-ray detections and choose for multi-obs sources the observation with maximum EPIC counts; require OM counterpart within 1″ and same OBSID; enforce OM filter/redshift constraints to avoid Lyα emission/forest contamination; require ≥250 EPIC counts (0.2–12 keV) and z≤2.5; add additional NLS1s via Rakshit+17 cross-match; visual inspection of 831 spectra leading to final N=696. Measurements/validation: Monte Carlo (50 resamples) uncertainties on fitted spectral parameters; emission-line/continuum bitmask quality flags; internal consistency checks: compare spectroscopic vs photometric L2500/L4400 (mean offsets ~−0.02 to −0.03 dex, scatter ~0.13–0.16 dex); compare line-width and luminosity correlations; compare multiple BH-mass recipes and derive new recalibrations to minimize systematics; αox regressions for full sample and subsets (NL/BL, RL/RQ) with dispersion quantification (~0.12 dex around αox–L2500); binned MBH–L2500 grids (≥3 objects/bin) to compare mean αox with QSOSED predictions and quantify Δαox per bin.",
        "limitations": "Sample is not complete/representative: requires XMM serendipitous detection with simultaneous OM and relatively high X-ray counts, biasing toward X-ray/UV-bright, unobscured type-1 AGN; only 696 sources with strong selection effects and heterogeneous redshift-dependent OM filter coverage; radio-loudness often undetermined for faint/undetected radio sources (large ‘unknown’ fraction), making RL fraction a lower limit; L2500 and L4400 mix spectroscopy and photometry (non-contemporaneous SDSS vs XMM; variability, aperture effects, line contamination) adding scatter though mean offsets small; X-ray luminosities and Γ are approximate, derived from catalogue band fluxes with assumed Γ in bands and simple Galactic absorption correction—no full spectral fitting or intrinsic absorption modeling (Γ<1 sources may be absorbed); virial BH masses inherit systematics from BLR geometry/virial factor assumptions, line-profile modeling choices (multi-Gaussian), and calibration anchoring to Hβ–L5100; broad-line Balmer decrement unusually high and not cleanly explained by reddening, implying uncertainties in intrinsic BLR physics/extinction assumptions; comparisons to QSOSED limited to αox (not full SED fits) and use fixed model parameters (spin=0, inclination=30°) and hardwired Lx/LEdd prescription, so agreement/discrepancies may not uniquely diagnose physics.",
        "future_research_directions": "(1) Fit full broadband SEDs (OM+SDSS+EPIC, possibly adding IR/GALEX/WISE) with physical disc–warm corona–hot corona models to test QSOSED assumptions (e.g., Lx/LEdd scaling, Rhot evolution, warm-corona properties) beyond αox. (2) Perform detailed X-ray spectral analyses (intrinsic absorption, soft excess, reflection, variability) using EPIC spectra rather than catalogue fluxes, and assess how Γ/soft-excess strength correlates with αox, L/LEdd, and NLS1 classification. (3) Refine virial mass calibrations using consistent line-profile parameterizations, explore orientation/BLR-geometry corrections (e.g., using radio morphology or line shape), and validate with reverberation-mapped subsamples. (4) Investigate jet–disc–corona coupling: separate core vs extended radio emission, use spectral indices and higher-frequency radio data, test whether X-ray excess in RL objects is jet-related or coronal, and map a putative ‘jet line’ in αox–L2500–L/LEdd space. (5) Examine intrinsic dust/extinction and BLR physics behind large Balmer decrements using multi-line diagnostics, UV slopes, and photoionization modeling; quantify host-galaxy contamination at low z with image-based decomposition. (6) Extend sample to higher z (z>2.5) and/or lower-count XMM spectra with hierarchical modeling to probe extreme Eddington ratios and massive BH growth; incorporate time-domain SDSS multi-epoch spectra for changing-look/variability-driven SED changes.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Full-Atom Protein Pocket Design via Iterative Refinement",
      "full_text": "Generalized Large-Scale Data Condensation via Various Backbone and Statistical Matching Shitong Shao1,3 Zeyuan Yin1 Muxin Zhou1 Xindong Zhang2,3 Zhiqiang Shen1,∗ 1Mohamed bin Zayed University of AI 2Hong Kong Polytechnic University 3OPPO Research 1090784053sst@gmail.com, {zeyuan.yin,muxin.zhou,zhiqiang.shen}@mbzuai.ac.ae 17901410r@connect.polyu.hk, ∗:Corresponding author Great white shark Timber wolf  Baboon SRe2L DD DD+ GBM DD+ GBM+ GSM BatchNorm Channel Var Channel Mean ResNet BatchNorm Channel Var Channel Mean ResNet MobileNet Convolution Patch  Var Patch  Mean  GV-BSM (Ours) SRe2L ImageNet-1k Top-1 Acc. 31.4%  (IPC=10, evaluation model=ResNet18)  ImageNet-1k Top-1 Acc. 21.3%  Figure 1. Left: Our proposed G-VBSM consists of three novel and effective modules, named DD, GSM and GBM. The richness and quality of information in the synthetic data have been significantly enhanced compared with the baseline SRe2L through the sequential merging of DD, GBM, and GSM.Right: G-VBSM prioritizes “generalized matching” to ensure consistency between distilled and complete datasets across various backbones, layers, and statistics, and achieves the highest accuracy 31.4% on ImageNet-1k under IPC 10. Abstract The lightweight “local-match-global” matching intro- duced by SRe2L successfully creates a distilled dataset with comprehensive information on the full 224×224 ImageNet- 1k. However, this one-sided approach is limited to a par- ticular backbone, layer, and statistics, which limits the im- provement of the generalization of a distilled dataset. We suggest that sufficient and various “local-match-global” matching are more precise and effective than a single one and have the ability to create a distilled dataset with richer information and better generalization ability. We call this perspective “generalized matching” and propose Generalized Various Backbone and Statistical Matching (G-VBSM) in this work, which aims to create a synthetic dataset with densities, ensuring consistency with the com- plete dataset across various backbones, layers, and statis- tics. As experimentally demonstrated, G-VBSM is the first algorithm to obtain strong performance across both small-scale and large-scale datasets. Specifically, G-VBSM achieves performances of 38.7% on CIFAR-100, 47.6% on Tiny-ImageNet, and 31.4% on the full 224×224 ImageNet- 1k, respectively1. These results surpass all SOTA methods by margins of 3.9%, 6.5%, and 10.1%, respectively. 1. Introduction With the development of deep learning, the number of model parameters and the quantity of training data have be- come increasingly large [4, 37]. Researchers have tried to minimize the training overhead while preventing a decline in the generalization ability. Data condensation (DC), also known as Dataset distillation, first introduced by Wang et al. [32], aims to alleviate the training burden by synthesiz- ing a small yet informative distilled dataset derived from the complete training dataset, while ensuring that the behav- 1Settings: CIFAR-100 with 128-width ConvNet under 10 images per class (IPC), Tiny-ImageNet with ResNet18 under 50 IPC, and ImageNet- 1k with ResNet18 under 10 IPC. arXiv:2311.17950v3  [cs.CV]  17 Mar 2024ior of the distilled dataset on the target task remains con- sistent with that of the complete dataset. The extremely compressed distilled dataset contains sufficiently valuable information and have the potential for fast model training, and have been become a popular choice for different down- stream application, like federated learning [7, 22], continual learning [14, 22, 40], neural architecture search [24, 41, 42] and 3D point clouds [30]. A persistent problem that researchers have been work- ing to solve [2, 9, 12, 42] in DC is the substantial data synthesis overhead [19, 36], which hinders its application in real-world large-scale datasets (e.g., ImageNet-1k) train- ing. Typical performance matching [15, 32, 44] and trajec- tory matching [1, 3] unroll recursive computation graphs, requiring substantial GPU memory and resulting in pro- hibitive training costs. Zhao et al. [42] proposed gradient matching to address this, synthesizing distilled datasets by matching gradients from synthetic and real data in a single step. However, gradient computation and matching remain time-consuming [9], leading to the proposal of distribution matching [41]. This method and its variants [31] employ a network-based feature extractor to embed both synthetic and real data into a high-dimensional Hilbert space, then perform distribution matching. The training load for this direct, single-step process stems only from one gradient up- date of the synthetic data and the feature extractor [36]. Unfortunately, all of the above mentioned improved meth- ods still have extremely large training overheads on the full 224×224 ImageNet-1k. Recently, SRe2L [34] accomplished data condensation for the first time on the full 224 ×224 ImageNet-1k [18], achieving Top-1 validation accuracy 21.3% with ResNet18 under IPC 10. This method outperformed the latest state- of-the-art TESLA [2], which conducted on a low-resolution version of ImageNet-1k, by being 16× faster and improved performance by a margin of 13.6%. SRe2L is inspired by DeepInversion [33] and aims to match statistics in Batch- Norm generated from synthetic and real data. We reeval- uate the success of SRe2L through the lightweight “local- match-global” essentially. The “local-match-global” refers to utilizing more comprehensive information (e.g., statistics in BatchNorm), generated from the model using the com- plete dataset (global), to guide the parameter update of the distilled dataset (local). However, such lightweight and effective matching in SRe2L is singular, depending on the particular layer ( i.e., BatchNorm), model ( i.e., ResNet18), and statistics ( i.e., channel mean/variance). Intuitively, sufficient “local- match-global” matching can result in more accurate and ra- tional supervision than a single one, further enhancing the generalization of the distilled dataset. We call this perspec- tive “generalized matching” and suggest that the distilled dataset is likely to perform consistent with the complete dataset on the evaluation model, provided that there is suf- ficient variety in backbones, layers, and statistics used for matching. Inspired by this, we propose Generalized Various Backbone and Statistical Matching ( G-VBSM) to fulfill “generalized matching”. G-VBSM comprises three integral and effective parts named data densification (DD), gener- alized statistical matching (GSM), and generalized back- bone matching (GBM). DD is utilized to ensure that the images within each class are linearly independent, thereby enhancing the (intra-class) diversity of the distilled dataset. This ultimately guarantees that “generalized matching” pre- serves the rich and diverse information within the synthetic data. GBM and GSM are designed to implement “gener- alized matching”. Among them, GSM traverses the com- plete dataset without computing and updating the gradient, to record the statistics of Convolution at both the patch and channel levels. These statistics are subsequently utilized for matching during the data synthesis phase, in conjunction with the channel-level statistics in BatchNorm. Further- more, GBM aims to ensure consistency between distilled and complete datasets across various backbones, enhancing matching sufficiency and leading to strong generalization in the evaluation phase. In particular, G-VBSM also ensures the efficiency of dataset condensation through a series of strategies, as mentioned in Sec. 3. Extensive experiments on CIFAR-10, CIFAR-100, Tiny- ImageNet, and the full 224 ×224 ImageNet-1k, demon- strating that G-VBSM is the first algorithm that performs well on both small-scale and large-scale datasets. Specifi- cally, we not only verify through ablation studies that GSM, GBM and DD are consistently reliable, but also achieve the highest 38.7%, 47.6% and 31.4% on CIFAR-100 (128- width ConvNet), Tiny-ImageNet (ResNet18), and the full 224×224 ImageNet-1k (ResNet18) under images per class (IPC) 10, 50 and 10, respectively, which outperforms all previous state-of-the-art (SOTA) methods by 3.9%, 6.5% and 10.1%, respectively. 2. Background Dataset condensation (DC) represents a data synthesis pro- cedure that aims to compress a complete, large dataset T = {(Xi, yi)}|T | i=1 into a smaller, distilled dataset S = {( ˜Xi, ˜yi)}|S| i=1, subject to |S| ≪ |T |, while ensuring that an arbitrary evaluation model feval(·) trained on S yields sim- ilar performance to one trained on T . Classical data dis- tillation algorithms invariably require the candidate model fcand(·) to execute one or more steps on S to update its pa- rameter θcand, subsequently achieving matching in terms of performance [15, 32], gradient [42], trajectory [1, 2], or dis- tribution [31, 41]. The processθcand−α∇θcand ℓ(fcand( ˜X), ˜y), where ℓ(·, ·) and ( ˜X, ˜y) represent the loss function andDownSample DownSample Stage I Stage II Stage III BatchNorm Convolution Complete Dataset Other Layers Record Record Global Mean Global Var Global Mean Global Var Distilled DatasetBatchNorm Convolution Mean Var Mean VarBoth patch level and channel level Only channel level Matching Distilled Dataset Complete Dataset Other Layers (    ) (    ) (    ) (    ) Data Densification Generalized Statistical Matching Get Gram Matrix  Get Eigenvalue Calculate the Loss (    ) Trained (    ) Freezed The Softmax Operation Backward BackboneGeneralized Backbone Matching ResNet MobileNet ShuffleNet EfficientNet Candidates Random  Sampling  Distilled Dataset Ensemble  (Get the Soft Label)  (    ) (    ) (    ) (    ) (    ) (    ) (    ) optional  Figure 2. The overview of G-VBSM on the full 224 ×224 ImageNet-1k, which ensures the consistency between the distilled and the complete datasets across various backbones, layers and statistics via “generalized matching”. a batch sampled from S, respectively, is notably time- consuming. Consequently, even the relatively swiftest dis- tribution matching [20, 31] is slower than the recent pro- posed SRe2L [34]. In fact, SRe2L is the only workable way to achieve DC on the full 224 ×224 ImageNet-1k, as it re- quires updating the parameters of the synthetic data only once per iteration. SRe2L [34] encompasses three incremental subpro- cesses: Squeeze, Recover, and Relabel. Squeeze is de- signed to train fcand(·) containing BatchNorm in a standard manner, aiming to record the global channel mean BNCM l and channel variance BNCV l (l refers to the index of the l- th layer) via exponential moving average (EMA), extracted from T , for subsequent matching in Recover. In Recover after that, given the channel mean µl( ˜X) and channel vari- ance σ2 l ( ˜X) in l-th BatchNorm obtained from S, the statis- tical matching loss function can be formulated as LBN( ˜X) = X l \f\f\f \f\f\fµl( ˜X) − BNCM l \f\f\f \f\f\f 2 + \f\f\f \f\f\fσ2 l ( ˜X) − BNCV l \f\f\f \f\f\f 2 . (1) Based on this, we can give the entire optimization objective in Recover as arg min ˜X LBN( ˜X) +ℓ(fcand( ˜X), y), (2) where y denotes the ground truth label. Moreover, SRe2L assigns soft labels ˜y to the synthetic data, utilizing the logit- based distillation [6, 8] to improve the generalization ability of the distilled dataset. This can be denoted as ˜y = softmax(fcand( ˜X)/τ), (3) where τ denotes the temperature to regulate the smooth- ness of the soft labels, thereby enhancing the distilled dataset’s potential for generalization to unseen evaluation models. The generated soft label can be stored on disk us- ing FKD [23] so as not to defeat the purpose of DC. A cru- cial point of SRe2L is that BatchNorm calculates the statis- tics of the entire dataset using EMA, thereby providing a comprehensive and representative matching for the distilled dataset. Encouraged by this, our research focuses on ap- plying sufficient “local-match-global” matching to achieve “generalized matching”. 3. Method The comprehensive and detailed framework of our proposed Generalized Various Backbone and Statistical Matching (G-VBSM) is illustrated in Fig. 2. In essence, G-VBSM employs the lightweight regularization strategy data densi- fication (DD) to ensure both the diversity and density of the distilled dataset, ensuring that the potential of “gener- alized matching” can be fully exploited. Moreover, gen- eralized backbone matching (GBM) and generalized sta- tistical matching (GSM) are utilized to achieve “general- ized matching” by performing “local-match-global” match- ing across various backbones, layers, and statistics. In par- ticular, the efficiency and effectiveness of DD, GBM, GSM, SRe2L, and TESLA are illustrated in Fig. 5. 3.1. Data Densification As illustrated in Fig. 3, the synthetic data generated by SRe2L exhibit excessive similarity within the same class, leading to a lack of diversity. Consequently, even if “gener- alized matching” preserves sufficient valuable information within a single image, the aggregate information content across all images within the same class does not increase effectively, which ultimately prevents “generalized match-0 200 400 600 800 1000 Class Index 0.0 0.2 0.4 0.6 0.8Intra-class Cosine Similarity SRe2L+DD SRe2L SRe2L+DD (Smoothed) SRe2L (Smoothed) Figure 3. Visualization of the mean cosine similarity between pair- wise samples within the same class on ImageNet-1k under IPC 10. ing” from being sufficiently advantageous. Data densifica- tion (DD) is proposed to address this by ensuring the data ˜X has full rank in the batch dimension, thereby guarantee- ing that samples in each class are linearly independent, and ultimately ensuring that the data is diverse and abundant to fully exploit the potential of “generalized matching”. To execute this pipeline, ˜X first needs to be downsam- pled to reduce the computational cost of eigenvalue decom- position: ˆX = AvgPool2d( ˜X), s.t. ˆX ∈ RB×C×32×32, (4) where B and C represent the batch size and the number of channels, respectively. Afterward, we reshape ˆX from B×C×32×32 to B×(1024C) and perform matrix multi- plication in each classy to obtain the set of the Gram matrix { ˆXy ˆXT y }y∈Y, where Y refers to a set of all classes existing in this batch. And ˆXy is a subbatch with class y. Note that the alternative form { ˆXT y ˆXy}y∈Y is not desirable, as it is applicable only for dimensionality reduction in feature di- mensions, which is why we do not consider singular value decomposition (SVD). To render ˆXy ˆXT y as full-rank as pos- sible, we introduce the data densification loss in Eq. 5. LDD( ˜X) = X y∈Y DKL(stop grad(p(Σy/τ))||p(Σy)), (5) where Σy, τ, stop grad(·) and p(·) refer to the eigenvalues of ˆXy ˆXT y , the temperature, the stop gradient operator and the softmax function, respectively. And DKL(·||·) denotes Kullback-Leibler divergence. As demonstrated in Sec. E, the diversity of the data is significantly enhanced by the employment of Eq. 5. In our experiment, τ is set as 4 in default and we do not assign a deliberate weight (set to 1 by default) to LDD because LDD ≡ 0 at the early 10% of the iterations. In other words, DD is quite stable and the optimization objectivearg min{Σy}y∈Y LDD is equivalent to arg min{Σy}y∈Y P y σ2(Σy). Technical Detail. A problem that warrants attention is that in the SRe2L’s implementation [34], having merely a single sample in each class of a batch indicates insufficient 1st  N-2th  N-1th  Nth  IPC  Current Batch Next Batch 1st 2nd  Last Batch Nth  Reorder Loop CLASS  1st  2nd  N-1th  Nth  IPC  1st 2nd  Last Batch Nth  Original Loop CLASS  Current Batch Next Batch Figure 4. The illustration of the original loop and the reorder loop. to execute DD under the order of the original loop, as de- picted in Fig. 4 (Left). A simple solution is to translate the original loop to the reorder loop, as shown in Fig. 4 (Right). However, our experiment on ResNet50 (i.e., the evaluation model) substantiates that this solution suffers a 2.6% ac- curacy degradation (details can be found in Sec. 4.1) on ImageNet-1k under IPC 10. The reason is that the num- ber of classes in each iteration within the reorder loop is insufficient, preventing a batch have the ability to match the global statistics in BatchNorm (i.e., BNCM l and BNCV l ). Mo- tivated by score distillation sampling (SDS) [17], we update the statistics during data synthesis via EMA to solve this is- sue, so that the statistics of all past batches can assist the statistics of the current batch match BNCM l and BNCV l : µtotal l = αµtotal l + (1− α)µl( ˜X), σ2,total l = ασ2,total l + (1− α)σ2 l ( ˜X), L′ BN( ˜X) = X l ||µl( ˜X) − BNCM l − stop grad(µl( ˜X) − µtotal l )||2 + ||σ2 l ( ˜X) − BNCV l − stop grad(σ2 l ( ˜X) − σ2,total l )||2. (6) The derivation of L′ BN( ˜X) can be found in Appendix B. We call this lightweight strategy as “match in the form of score distillation sampling” and have demonstrated its effective- ness and feasibility in our ablation studies. 3.2. Generalized Backbone Matching Performing data synthesis only on a single pre-trained model is not able to enjoy ensemble gains from various backbones. Meanwhile, classical DC algorithms such as MTT [1] and FTD [3] obtain performance improvements from multiple randomly initialized backbones. Therefore, introducing generalized backbone matching (GBM) to ap- ply various backbones for data synthesis is a desirable choice for “generalized matching”. It ensures a number of “local-match-global” matching nearly Nb× higher com- pared to just depending on a single backbone, where Nb denotes the number of backbones. Regrettably, unrolling various backbone computational graphs in parallel for data synthesis is extremely expensive at the computational cost level. A solution is to randomly sample a backbone from the candidates in per iteration. This simple yet effective strategy not only ensures computational efficiency but also allows the data synthesis process to benefit from the diverse and multifaceted matching that the various backbones pro-vide. We denote this pipeline as fcand(·) ∼ U(S), S = {ResNet18, ··· , ShuffleNetV2-0.5}. (7) To maintain backbone consistency in both the data synthesis and soft label generation phases, we introduce a backbone ensemble during soft label generation: ˜z =    X fcand∈S P g∈S ||g( ˜X)||F |S|||fcand( ˜X)||F fcand( ˜X) |S| , w/ LN, X fcand∈S fcand( ˜X) |S| , w/o LN, (8) where || · ||F and LN refer to the Frobenius norm and Logit Normalization, respectively. Thus, we can obtain the soft label by ˜y = softmax(˜z/τ ). Particularly, the use of LN is optional, as demonstrated by our ablation studies; it is ben- eficial for ResNet18 but not for ResNet50 or ResNet101. Moreover, it’s important to highlight that we apply a par- allel mechanism for soft label generation since it provides a significantly lower computational cost, less than 1/30 of that required for data synthesis, thus making the computa- tional overhead negligible. 3.3. Generalized Statistical Matching Only ensuring backbone diversity is insufficient to fully ex- ploit the potential of “generalized matching”. In this sub- section, we aim to introduce additional statistics for match- ing during the data synthesis phase. Prior methods [33, 34] only utilize BatchNorm since the presence of global infor- mation statistics within the pre-trained model, with no ap- parent solution ready for other layers ( e.g., Convolution). Retraining the model is the simplest way to address this problem, followed by updating the other layers’ parameters through EMA. This approach is obviously impractical, as it defeats the purpose of using gradient descent. By contrast, we propose to allow the pre-trained model feval(·) to loop through the training dataset T once without calculating the gradient to obtain the global statistics of Convolution, thus serving “local-match-global”: ConvCM l = 1 |T | |T |X i CMl i, ConvCV l = 1 |T | |T |X i CVl i, ConvPM l = 1 |T | |T |X i PMl i, ConvPV l = 1 |T | |T |X i PVl i. (9) Here, CMl i ∈ RCl, CVl i ∈ RCl, PMl i ∈ R ⌈ H Np l ⌉ × ⌈W Np l ⌉ and PVl i ∈ R ⌈ H Np l ⌉ × ⌈W Np l ⌉ refer to the channel mean, the channel variance, the patch mean and the patch variance, respec- tively, for the l-th Convolution when the input to fcand(·) is the i-th batch, where Cl and ⌈ H Np l ⌉×⌈ W Np l ⌉ denote the number of channels and patches of the l-th Convolution, respectively. We define Np l as 4, 4, 4 and 16 by default on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet- 1k, respectively. After obtaining the global channel mean ConvCM l , the global channel variance ConvCV l , the global patch mean ConvPM l and the global patch variance ConvPV l , we can store them in a disk thus avoiding secondary calcu- lations. In the data synthesis phase, we introduce L′ Conv( ˜X) in Eq. 10 to accomplish joint matching with L′ BN( ˜X). L′ Conv( ˜X) = X l ||µc l ( ˜X) − ConvCM l − stop grad(µc l ( ˜X) − µc,total l )||2 + ||σc,2 l ( ˜X) − ConvCV l − stop grad(σc,2 l ( ˜X) − σc,2,total l )||2, + ||µp l ( ˜X) − ConvPM l − stop grad(µp l ( ˜X) − µp,total l )||2 + ||σp,2 l ( ˜X) − ConvPV l − stop grad(σp,2 l ( ˜X) − σp,2,total l )||2, (10) where µc,total l , σc,2,total l , µp,total l , and σp,2,total l are each up- dated from the channel mean µc l ( ˜X), channel variance σc,2 l ( ˜X), patch mean µp l ( ˜X), and patch variance σp,2 l ( ˜X) respectively, all obtained via EMA from the current batch. In experiments, we discovered that Eq. 10 causes a sightly larger computational burden, so we randomly drop the matching of statistics with a probability of βdr to ensure the efficiency of GSM. Loss Function in the Evaluation Phase. Unlike DD, GBM and GSM are designed to create a distilled dataset that is enriched with information. Here, we introduce an enhancement to the loss function tailored specifically for the evaluation phase. Essentially, the evaluation phase is a knowledge distillation framework for transferring knowl- edge from a pre-trained model to the evaluation model. SRe2L utilizes DKL(˜y||softmax(feval( ˜X)/τ)) as the loss function and experimentally illustrates that it improves per- formance by roughly 10%. As established in SRe2L, an increase in temperature τ correlates with enhanced performance of the evaluation model. Inspired by this and τ2DKL(softmax(p)/τ||softmax(q)/τ) is equivalent to 1 2C ||p−q||2 2 when τ → +∞ [10], we introduce a novel loss function MSE+γ×GT to avoid numerical error caused by the large τ and improve the generalization of the distilled dataset without any additional overhead (ignore the weights τ2 and 1 2C ): Leval( ˜X, ˜y, y) =||feval( ˜X) − ˜z||2 2 − γy log(softmax(feval( ˜X))), (11) where y represents the one-hot encoding ( w.r.t. the ground truth label y). As illustrated in Fig. 5, simply replacing the loss function with MSE+0.1×GT (γ is set as 0.1) in SRe2L improves the performance of ResNet18 (i.e., the evaluation model) by a margin of 0.9% on ImageNet-1k under IPC 10. 4. Experiment We perform comparison experiments on the large-scale dataset including the full 224 ×224 ImageNet-1k [18] and the small-scale datasets including Tiny-ImageNet [26],2 4 6 8 10 12 GPU Latency (ms) 10 15 20 25 30T op-1 Validation Accuracy (%) (7.7%, 12.89ms) (21.3%, 0.83ms) (22.2%, 0.83ms) (22.4%, 0.84ms) (25.1%, 1.12ms) (27.8%, 1.12ms) (30.6%, 3.10ms)(31.4%, 4.32ms) Efficiency vs. Effectiveness of Different Methods (Under IPC 10, evaluation model is ResNet18) TESLA SRe2L SRe2L + (MSE + GT) DD DD + GBM DD + GBM + GSM ( dr=1.0) + (MSE + GT) DD + GBM + GSM ( dr=0.4) + (MSE + GT) DD + GBM + GSM ( dr=0.0) + (MSE + GT) Figure 5. Comparison of the effectiveness and efficiency of G-VBSM components. Among them, “DD+GBM+GSM (βdr=0.0)+(MSE+GT)” represents the comprehensive G-VBSM. CIFAR-10/100 [11]. To highlight that our proposed G- VBSM is designed for large-scale datasets, all ablation ex- periments are performed on ImageNet-1k. Hyperparameter Settings. We prioritize selecting var- ious convolution-based backbone, ensuring the maximal difference in architecture, while also adhering to the criterion of minimizing the number of parameters, an approach empirically demonstrated to be superior [34, 38]. On ImageNet-1k, we skip the model pre-training phase by directly using Torchvision’s open source pre- training weights [16] of {ResNet18 [5], MobileNetV2 [21], EfficientNet-B0 [25], ShuffleNetV2-0.5 [39] }. For the re- maining dataset, we all train {128-width ConvNet [39], WRN-16-2 [37], ResNet18 [5], ShuffleNetV2-0.5 [39], MobileNetV2-0.5 [21]} from scratch with few epoch in the model pre-training phases. Gray cells in all tables rep- resent the highest performance. Meanwhile, (R18), (R50), etc. in all tables, represent the evaluation models. More de- tails about the remaining method hyperparameter settings and additional ablation studies can be found in Appendix A and Appendix C & D, respectively. 4.1. Ablation Studies The Trade-off between Efficiency and Effectiveness. The performance improvements and increased GPU la- tency for components of G-VBSM in comparison to the baseline SRe2L, are illustrated in Fig. 5. Clearly, DD, GBM, and GSM are highly effective in enhancing the gen- eralization of distilled datasets. Compared with GSM, both DD and GBM are slightly lightweight and effi- cient. With DD and GBM alone – i.e., DD+GBM+GSM (βdr=1.0)+(MSE+GT) – the accuracy of this approach sur- passes that of SRe2L by a significant margin of 6.5%. Ad- ditionally, the comprehensive G-VBSM, DD+GBM+GSM (βdr=0.0)+(MSE+GT), further enhances the performance of DD+GBM+GSM (βdr=1.0)+(MSE+GT) by a notable mar- gin of 3.6%. Note that MSE+γ×GT (γ is set as 0.1) is also extremely critical, for SRe2L and DD+GBM boosting 0.9% and 2.7%, respectively. α\\Evaluation Model ResNet18 (MSE+0.1×GT) ResNet50 (MSE+0.1×GT) 0.0 30.4% 31.9% 0.4 31.4% 34.5% 0.8 31.9% 36.4% Table 1. ImageNet-1k Top-1 Acc. about differentα under IPC 10. Matching in the Form of SDS. Benefiting from this matching form, G-VBSM can achieve the best performance with small batch sizes ( e.g., 40 in ImageNet-1k and 50 in CIFAR-100) in different settings, as demonstrated in our comparative experiments. Table 1 demonstrates the impact of the factor α for performing the EMA update on the final performance achieved by G-VBSM, where α=0 indicates that L′ BN degenerates to LBN. We can conclude that the SDS matching form is feasible and the generalization of the dis- tilled dataset improves with increasing α. Method Evaluation Model DeiT-Tiny ResNet18 MobileNetV2 Swin-Tiny SRe2L 15.41% 46.79% 36.59% 39.23% G-VBSM 29.43% 51.82% 48.66% 57.40% Table 2. ImageNet-1k Top-1 Acc. on cross-architecture general- ization under IPC 50. Cross-Architecture Generalization. The evaluation of cross-architecture generalization is crucial in assessing the quality of distilled datasets. Unlike traditional methods [1, 15, 20] which focus on CIFAR-100, our approach evalu- ates the effectiveness of the distilled dataset on ImageNet- 1k, employing a suite of models with real-world applica- bility, including ResNet18 [5], MobileNetV2 [21], DeiT- Tiny [28], and Swin-Tiny [13]. The experimental results are reported in Table 2. From Tables 7 and 2, it is evident that the G-VBSM-synthetic dataset can effectively gener- alize across ResNet {18, 50, 101 }, MobileNetV2, DeiT- Tiny, and Swin-Tiny architectures. Notably, DeiT-Tiny and Swin-Tiny, two architectures not encountered during the data synthesis phase, demonstrates significant proficiency with the accuracy 29.43% and 57.40%, outperforming the latest SOTA SRe2L by a margin of 14.01% and 18.17%, respectively. Candidates (Backbone) Evaluation Model ResNet18 MobileNetV2 EfficientNet-B0 ShuffleNetV2-0.5ResNet18 ResNet50 ✓ 25.7% 30.2% ✓ ✓ 27.2% 31.2% ✓ ✓ ✓ 27.9% 32.0% ✓ ✓ ✓ ✓ 31.4% 34.5% Table 3. ImageNet-1k Top-1 Acc. about the number of candidate backbone in the soft label generation pipeline under IPC 10. Ensemble in Soft Label Generation. In knowledge dis- tillation, soft labels obtained through multiple teacher en- sembles can effectively enhance the generalization of the student [35]. This observation is similarly corroborated in0.8 0.4 0.0 dr 25 26 27 28 29 30 31 32Accuracy (%) Fixed Batch Size: 40 & Loss Function: MSE+0.1×GT 40 60 80 Batch Size 31.0 31.1 31.2 31.3 31.4 31.5Accuracy (%) Fixed dr=1.0 & Loss Function: MSE+0.1×GT KL MSE+0.01×GTMSE+0.1×GT MSE+1×GT Loss Function 0 5 10 15 20 25 30Accuracy (%) Fixed Batch Size: 40 & dr=0.0 Figure 6. ImageNet-1k Top-1 Acc. of different loss function, βdr and batch sizes under IPC 10. Dataset IPC MTT [1] (CW128) DataDAM [20] (CW128) TESLA [2] (R18) SRe2L (R18) SRe2L (R50) SRe2L (R101) G-VBSM (R18) G-VBSM (R50) G-VBSM (R101) Tiny-ImageNet50 28.0 ±0.3 28.7 ±0.3 - 41.1 ±0.4 42.2 ±0.5 42.5 ±0.2 47.6±0.3 48.7±0.2 48.8±0.4100 - - - 49.7 ±0.3 51.2 ±0.4 51.5 ±0.3 51.0±0.4 52.1±0.3 52.3±0.1 ImageNet-1k10 64.0±1.3† 6.3±0.0 7.7 ±0.1 21.3 ±0.6 28.4 ±0.1 30.9 ±0.1 31.4±0.5 35.4±0.8 38.2±0.450 - 15.5 ±0.2 - 46.8 ±0.2 55.6 ±0.3 60.8 ±0.5 51.8±0.4 58.7±0.3 61.0±0.4100 - - - 52.8 ±0.3 61.0 ±0.4 62.8 ±0.2 55.7±0.4 62.2±0.3 63.7±0.2 Table 4. Comparison with baseline models in Tiny-ImageNet and ImageNet-1k. † indicates the ImageNette dataset, which contains only 10 classes. DataDAM [20] and TESLA [2] use the downsampled 64×64 ImageNet-1k. We cite the experimental results from SRe2L [34]. Dataset IPC Coreset Selection Training Set Synthesis (CW128) Training Set Synthesis (R18)Whole Dataset (CW128)Random Herding K-Center Forgetting DC [42] DM [41] CAFE [31] KIP [15] MTT [1] DataDAM[20] G-VBSM SRe2L G-VBSM CIFAR-10 10 26.0±1.2 31.6±0.7 14.7±0.9 23.3±1.0 44.9±0.5 48.9±0.6 50.9 ±0.5 46.1 ±0.7 65.3±0.7 54.2 ±0.8 46.5 ±0.7 27.2±0.5 53.5±0.6 84.8±0.150 43.4±1.0 40.4±0.6 27.0±1.4 23.3±1.1 53.9±0.5 63.0±0.4 62.3 ±0.4 53.2 ±0.7 71.6±0.2 67.0±0.4 54.3 ±0.3 47.5±0.6 59.2±0.4 CIFAR-100 1 4.2±0.3 8.3 ±0.3 8.4 ±0.3 4.5 ±0.2 12.8±0.3 11.4±0.3 14.0 ±0.3 12.0 ±0.2 24.3±0.3 14.5 ±0.5 16.4 ±0.7 2.0±0.2 25.9±0.5 56.2±0.310 14.6±0.5 17.3±0.3 17.3±0.3 15.1±0.3 25.2±0.3 29.7±0.3 31.5 ±0.2 40.1 ±0.4 33.1±0.4 34.8 ±0.5 38.7±0.2 31.6±0.5 59.5±0.4 50 30.0±0.4 33.7±0.5 30.5±0.3 - 30.6±0.6 43.6±0.4 47.7 ±0.2 - 42.9 ±0.3 49.4±0.3 45.7 ±0.4 49.5±0.3 65.0±0.5 Table 5. Comparison with baseline models on CIFAR-10/100. All methods, except for SRe2L and G-VBSM, use a 128-width ConvNet (CW128) for data synthesis and evaluation. G-VBSM utilizes {CW128, WRN-16-2, ResNet18 (R18), ShuffleNetV2-0.5, MobileNetV2- 0.5} for data synthesis and {CW128, R18} for evaluation. We cite the experimental results, except for SRe2L’s, from DataDAM [20]. dataset distillation, as evidenced in Table 3. The greater the number of models, the stronger the generalization of the dis- tilled dataset. It is particularly interesting to point out that ShuffleNetV2-0.5 improves further by more than 2% even though the other three models have already achieved mod- est ensemble gains. We attribute this enhancement to the channel shuffle mechanism within ShuffleNetV2-0.5, which imposes a beneficial regularization constraint for G-VBSM. Logit Normalization Evaluation Model ResNet18 (MSE+0.1×GT) ResNet18 (KL) ResNet50 (MSE+0.1×GT) ResNet50 (KL) Yes 31.4% 25.4% 34.5% 28.6% No 31.0% 25.1% 35.4% 31.9% Table 6. ImageNet-1k Top-1 Acc. about the use of logit normal- ization under IPC 10. Logit Normalization in Soft Label Generation. The aim of this strategy is to maintain the consistency of the logits (can be viewed as vectors) magnitude across all mod- els within a high-dimensional space, thereby ensuring equal contribution of these models to the ultimate soft label for ensemble. This approach is not universally effective, as shown in Table 6. When the evaluation model parameter count is low (e.g., ResNet18), it can enhance G-VBSM per- formance. Conversely, with a model like ResNet50, it may hinder performance, rendering G-VBSM uncompetitive. As a result, in this work, ResNet{50, 101} and ViT-based mod- els do not employ logit normalization, while the remaining models do. Loss Function in the Evaluation Phase. As illustrated in Fig. 6, replacing DKL(·||·) in SRe2L with MSE+ γ×GT demonstrates to be extremely effective. For example, with βdr=0.0 and batch size=40, adjusting γ to 0.01 and 0.1 en- hances model performance during the evaluation phase by 1.4% and 6.0%, respectively. As a result, we employ a γ of 0.1 in all experiments conducted on ImageNet-1k. Further- more, we observe that the distilled dataset’s generalization is not significantly affected ( ≤2%) by variations in either batch size or βdr. This ensured that G-VBSM attain SOTA results with a minimal batch size 40 and βdr=0.4 (i.e., the default settings in ImageNet-1k). CIFAR-10  Tiny-ImageNet  CIFAR-100  Figure 7. Synthetic data visualization on small-scale datasets.Synthetic Data Visualization. The visualization results for the large-scale (the full 224 ×224 ImageNet-1k) and small-scale (CIFAR-10/100 and Tiny-ImageNet) datasets are displayed in Figs. 1 and 7, respectively. On ImageNet- 1k, the images synthesized by G-VBSM are more informa- tive and abstract compared with those from SRe2L. Further- more, on small-scale datasets, the distilled images obtained by G-VBSM markedly differ from the images presented in MTT and DataDAM’s papers [1, 20], illustrating that G- VBSM is an out-of-the ordinary algorithm. More synthetic data can be found in Appendix I. GV-BSM (Logit Embedding)  SRe2L (Logit Embedding) after classifier and before softmax after classifier and before softmax Figure 8. t-SNE visualization on ImageNet-1k. Logit Embedding Distributions Visualization. We feed distilled datasets into a pre-trained model (i.e., ResNet18) to obtain logit embeddings for t-SNE [29] visualization. Note that unlike previous methods that used feature embeddings before the classifier layer, we utilize logit embeddings after the classifier layer for presentation. Fig. 8 shows the dis- tributions of G-VBSM and SRe2L logit embeddings. The logit embedding obtained by G-VBSM has less intra-class similarity than SRe2L, ensuring softer labels, preventing model overconfidence during the evaluation phase, and ul- timately enhancing model generalization. 4.2. Large-Scale Dataset Comparison Full 224 ×224 ImageNet-1k. As illustrated in the ImageNet-1k part of Table 4, G-VBSM consistently out- performs SRe2L across all IPC {10, 50, 100 }, with the evaluation model being ResNet {18, 50, 101 }, indicat- ing that G-VBSM is highly competitive in the large-scale dataset ImageNet-1k. In particular, under extreme com- pression scenarios, such as IPC 10, G-VBSM achieves sig- nificant performance increases – 10.1%, 7.0%, and 7.3% for ResNet18, ResNet50, and ResNet101, respectively – compared with SRe2L. In comparison to the latest SOTA classical method, DataDAM, just experimenting on the full 64×64 ImageNet-1k, we exceed it by a margin of 23.7% under IPC 10. These results are particularly impressive when considered in the context of the large-scale dataset ImageNet-1k. 4.3. Small-Scale Dataset Comparison Tiny-ImageNet. Similar to SRe2L, we evaluate the gen- eralization of the distilled dataset on ResNet {18, 50, 101}. As presented in the Tiny-ImageNet part of Table 4, G-VBSM consistently surpasses the latest SOTA method SRe2L on different settings. Our method outperforms SRe2L, MTT and DataDAM by 6.5%, 19.6% and 18.9%, respectively, under IPC 50 applying 128-width ConvNet or ResNet18 as the evaluation model. CIFAR-100. One of the disadvantages of SRe2L is its in- ability to compete with SOTA performance on this small- scale dataset benchmark ( i.e., evaluated with 128-width ConvNet and ResNet18), such as the performance on CIFAR-100 shown in Table 5, as a result, it fails to demon- strate that “local-match-global” (e.g., statistical matching of BatchNorm) can rival or outperform traditional DC algo- rithms. In contrast, as shown in the CIFAR-100 part of Ta- ble 5, G-VBSM achieves accuracies 16.4% and 38.7%, the highest under IPC 1 and 10 on CIFAR-100 through “gen- eralized matching”, while maintaining the same number of epochs (i.e., 1000) and the model (i.e., 128-width ConvNet) for the evaluation phase as the traditional methods, outper- forming the latest SOTA DataDAM by 1.9% and 3.9%, re- spectively. To the best of our knowledge, our proposed G- VBSM is the first algorithm that is strongly competitive on CIAR-100 as well as the full 224×224 ImageNet-1k. CIFAR-10. As illustrated in the CIFAR-10 part of Ta- ble 5, G-VBSM outperforms vanilla DC [32], KIP [15], and all coreset selection algorithms on the benchmark with 128-width ConvNet as the evaluation model. Meanwhile, on the benchmark with ResNet 18 as the evaluation model, G-VBSM outperforms the baseline SRe2L by 26.3% and 11.7% under IPC 10 and 50, respectively. 5. Conclusion In this paper, we introduce a novel perspective termed “gen- eralized matching” for dataset condensation. This posits that an abundance of lightweight “local-match-global” matching are more effective than a single “local-match- global” matching, and even surpass the precise and costly matching used by traditional methods. Consequently, we present G-VBSM, designed to ensure data densification while performing matching based on sufficient and various backbones, layers, and statistics. Experiments conducted on CIFAR-10/100, Tiny-ImageNet, and the full 224 ×224 ImageNet-1k demonstrated that our method is the first algo- rithm to show strong performance across both small-scale and large-scale datasets. For future research, we plan to extend this approach’s applicability to large-scale detection and segmentation datasets.References [1] George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A. Efros, and Jun-Yan Zhu. Dataset distillation by matching training trajectories. In Computer Vision and Pat- tern Recognition, New Orleans, LA, USA, 2022. IEEE. 2, 4, 6, 7, 8, 1, 3 [2] Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Scaling up dataset distillation to imagenet-1k with constant memory. In International Conference on Machine Learning , pages 6565–6590, Honolulu, Hawaii, USA, 2023. PMLR. 2, 7 [3] Jiawei Du, Yidi Jiang, Vincent Y . F. Tan, Joey Tianyi Zhou, and Haizhou Li. Minimizing the accumulated trajectory error to improve dataset distillation. In Computer Vision and Pattern Recognition, pages 3749–3758, Vancouver, BC, Canada, 2023. IEEE. 2, 4 [4] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey. Interna- tional Journal of Computer Vision, 129(6):1789–1819, 2021. 1 [5] Kaiming He, Xiangyu Zhang, and Shaoqing Ren. Deep residual learning for image recognition. In Computer Vi- sion and Pattern Recognition , pages 770–778, Las Vegas, NV , USA, 2016. IEEE. 6 [6] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015. 3, 1 [7] Shengyuan Hu, Jack Goetz, Kshitiz Malik, Hongyuan Zhan, Zhe Liu, and Yue Liu. Fedsynth: Gradient compression via synthetic data in federated learning. arXiv preprint arXiv:2204.01273, 2022. 2 [8] Tao Huang, Shan You, Fei Wang, Chen Qian, and Chang Xu. Knowledge distillation from a stronger teacher. In Neural Information Processing Systems, 2022. 3, 1 [9] Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, Sangdoo Yun, Hwanjun Song, Joonhyun Jeong, Jung-Woo Ha, and Hyun Oh Song. Dataset condensation via efficient synthetic- data parameterization. In International Conference on Ma- chine Learning, pages 11102–11118, Baltimore, Maryland, USA, 2022. PMLR. 2 [10] Taehyeon Kim, Jaehoon Oh, NakYil Kim, Sangwook Cho, and Se-Young Yun. Comparing kullback-leibler divergence and mean squared error loss in knowledge distillation. In In- ternational Joint Conference on Artificial Intelligence , Vir- tual Event, 2021. Morgan Kaufmann. 5 [11] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 6 [12] Yanqing Liu, Jianyang Gu, Kai Wang, Zheng Zhu, Wei Jiang, and Yang You. DREAM: efficient dataset distillation by representative matching. arXiv preprint arXiv:2302.14416, 2023. 2 [13] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In International Conference on Computer Vision, pages 10012– 10022, 2021. 6 [14] Wojciech Masarczyk and Ivona Tautkute. Reducing catas- trophic forgetting with learning on synthetic data. In Com- puter Vision and Pattern Recognition Workshops, pages 252– 253, Virtual Event, 2020. IEEE. 2 [15] Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. Dataset meta-learning from kernel ridge-regression. arXiv preprint arXiv:2011.00050, 2020. 2, 6, 7, 8 [16] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zem- ing Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Neural Information Processing Systems, Vancouver, BC, Canada, 2019. 6, 2 [17] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden- hall. Dreamfusion: Text-to-3d using 2d diffusion. In Inter- national Conference on Learning Representations, 2023. 4, 2 [18] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015. 2, 5 [19] Noveen Sachdeva and Julian McAuley. Data distillation: A survey. arXiv preprint arXiv:2301.04272, 2023. 2 [20] Ahmad Sajedi, Samir Khaki, Ehsan Amjadian, Lucy Z Liu, Yuri A Lawryshyn, and Konstantinos N Plataniotis. Datadam: Efficient dataset distillation with attention match- ing. In International Conference on Computer Vision, pages 17097–17107, Paris, France, 2023. IEEE. 3, 6, 7, 8, 1 [21] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Computer Vision and Pattern Recognition, pages 4510–4520, Salt Lake City, UT, USA, 2018. IEEE. 6 [22] Mattia Sangermano, Antonio Carta, Andrea Cossu, and Da- vide Bacciu. Sample condensation in online continual learn- ing. In International Joint Conference on Neural Networks, pages 1–8, Padua, Italy, 2022. IEEE. 2 [23] Zhiqiang Shen and Eric Xing. A fast knowledge distillation framework for visual recognition. In European Conference on Computer Vision, pages 673–690. Springer, 2022. 3 [24] Felipe Petroski Such, Aditya Rawal, Joel Lehman, Ken- neth O. Stanley, and Jeffrey Clune. Generative teaching net- works: Accelerating neural architecture search by learning to generate synthetic training data. In International Confer- ence on Machine Learning, pages 9206–9216, Virtual Event, 2020. PMLR. 2 [25] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In International Conference on Machine Learning, pages 10096–10106, Virtual Event, 2021. PMLR. 6 [26] Amirhossein Tavanaei. Embedded encoder-decoder in con- volutional networks towards explainable AI. 2020. 5 [27] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con- trastive representation distillation. In International Confer- ence on Learning Representations, 2019. 1 [28] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv ´e J´egou. Trainingdata-efficient image transformers & distillation through at- tention. In International Conference on Machine Learning , pages 10347–10357, Virtual Event, 2021. PMLR. 6 [29] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 9 (11), 2008. 8 [30] Chunwei Wang, Chao Ma, Ming Zhu, and Xiaokang Yang. Pointaugmenting: Cross-modal augmentation for 3d object detection. In Computer Vision and Pattern Recognition , pages 11794–11803, Virtual Event, 2021. 2 [31] Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang, Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, and Yang You. Cafe: Learning to condense dataset by aligning features. In Computer Vision and Pattern Recognition, pages 12196–12205, New Orleans, LA, USA, 2022. IEEE. 2, 3, 7 [32] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. arXiv preprint arXiv:1811.10959, 2018. 1, 2, 8, 3 [33] Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deep- inversion. In Computer Vision and Pattern Recognition , pages 8715–8724, Virtual Event, 2020. IEEE. 2, 5 [34] Zeyuan Yin, Eric P. Xing, and Zhiqiang Shen. Squeeze, re- cover and relabel: Dataset condensation at imagenet scale from A new perspective. In Neural Information Processing Systems. NIPS, 2023. 2, 3, 4, 5, 6, 7, 1 [35] Shan You, Chang Xu, Chao Xu, and Dacheng Tao. Learn- ing from multiple teacher networks. In International Con- ference on Knowledge Discovery and Data Mining , pages 1285–1294, Halifax, NS, Canada, 2017. ACM. 6 [36] Ruonan Yu, Songhua Liu, and Xinchao Wang. Dataset distillation: A comprehensive review. arXiv preprint arXiv:2301.07014, 2023. 2 [37] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In British Machine Vision Conference , York, UK, 2016. BMV A Press. 1, 6 [38] Lei Zhang, Jie Zhang, Bowen Lei, Subhabrata Mukherjee, Xiang Pan, Bo Zhao, Caiwen Ding, Yao Li, and Dongkuan Xu. Accelerating dataset distillation via model augmenta- tion. In Computer Vision and Pattern Recognition, Vancou- ver, BC, Canada, 2023. IEEE. 6, 5 [39] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural net- work for mobile devices. In Computer Vision and Pattern Recognition, pages 6848–6856, 2018. 6 [40] Bo Zhao and Hakan Bilen. Dataset condensation with differ- entiable siamese augmentation. In International Conference on Machine Learning , pages 12674–12685, Virtual Event, 2021. PMLR. 2 [41] Bo Zhao and Hakan Bilen. Dataset condensation with dis- tribution matching. In Winter Conference on Applications of Computer Vision , pages 6514–6523, Waikoloa, Hawaii, 2023. IEEE. 2, 7, 5 [42] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. In International Con- ference on Learning Representations , Virtual Event, 2021. OpenReview.net. 2, 7 [43] Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun Liang. Decoupled knowledge distillation. In Computer Vi- sion and Pattern Recognition, pages 11953–11962, 2022. 1 [44] Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba. Dataset distillation using neural feature regression. In Neural Infor- mation Processing Systems, New Orleans, LA, USA, 2022. NIPS. 2A. Hyperparameter Settings Table 7. G-VBSM hyperparameter settings on ImageNet-1k. Phase Optimizer Learning Rate Optimizer Momentum Loss Function Batch Size Epoch/ Iteration Augmentation Others Pre-trained model training SGD 0.1 0.9 cross-entropy 256 Epoch 100 RandomResizedCrop - Data synthesis Adam 0.1 β1,β2=0.5,0.9 ℓ(fcand( ˜X), y) +L′BN +LDD+L′Conv 40 Iteration 4000 RandomResizedCrop βdr=0.4, Backbone= {ResNet18,MobileNetV2 ,EfficientNet-B0,ShuffleNetV2-0.5} Soft label generation - - - - 1024 Epoch 300 RandomResizedCrop, CutMix Backbone= {ResNet18,MobileNetV2 ,EfficientNet-B0,ShuffleNetV2-0.5} Evaluation AdamW 0.001 β1,β2=0.9,0.999 MSE+0.1 ×GT 1024 Epoch 300 RandomResizedCrop, CutMix Evaluation Model= {ResNet18,ResNet50,ResNet101, MobileNetV2,Swin-Tiny,DeiT-Tiny} Table 8. G-VBSM hyperparameter settings on Tiny-ImageNet. Phase Optimizer LearningRate OptimizerMomentum LossFunction BatchSize Epoch/Iteration Augmentation Others Pre-trained modeltraining SGD 0.1 0.9 cross-entropy 128 Epoch50 RandomCropRandomHorizontalFlip - Data synthesis Adam 0.05 β1,β2=0.5,0.9 ℓ(fcand(˜X), y) +L′BN+LDD+L′Conv 50 Iteration4000 RandomResizedCrop βdr=0.4, Backbone={ResNet18,128-width ConvNet,MobileNetV2,WRN-16-2,ShuffleNetV2-0.5} Soft labelgeneration - - - - 128 Epoch100 RandomResizedCrop,CutMix Backbone={ResNet18,128-width ConvNet,MobileNetV2,WRN-16-2,ShuffleNetV2-0.5} Evaluation SGD 0.2, 0.1 and 0.1 onResNet18, ResNet50 and ResNet101, respectively 0.9 MSE+0.1 ×GT 128 Epoch100 RandomResizedCrop,CutMix Evaluation Model={ResNet18,ResNet50,ResNet101} Table 9. G-VBSM hyperparameter settings on CIFAR-10/100. Phase Optimizer LearningRate OptimizerMomentum LossFunction BatchSize Epoch/Iteration Augmentation Others Pre-trained modeltraining SGD 0.05 0.9 cross-entropy 64 Epoch50 and 5 onCIFAR-100 and CIFAR-10, respectively RandomCropRandomHorizontalFlip - Data synthesis Adam 0.05 β1,β2=0.5,0.9 ℓ(fcand(˜X), y) +L′BN+LDD+L′Conv 50 Iteration4000 RandomResizedCrop βdr=0.4, Backbone={ResNet18,128-width ConvNet,MobileNetV2,WRN-16-2,ShuffleNetV2-0.5} Soft labelgeneration - - - - 64 or|S|(|S| ≤100) Epoch1000 RandomResizedCrop,CutMix Backbone={ResNet18,128-width ConvNet,MobileNetV2,WRN-16-2,ShuffleNetV2-0.5} Evaluation SGD and AdamW onCIFAR-100 and CIFAR-10, respectively 0.1 and 0.001 onCIFAR-100 and CIFAR-10, respectively 0.9 andβ1,β2=0.9,0.999 onCIFAR-100 and CIFAR-10, respectively MSE+0.15×GT 64 or|S|(|S| ≤100) Epoch1000 RandomResizedCrop,CutMix Evaluation Model={128-width ConvNet,ResNet18} Here, we present the hyperparameter settings of G-VBSM in the pre-trained model training (i.e., Squeeze in SRe2L), the data synthesis (i.e., Recover in SRe2L), the soft label generation ( i.e., Relabel in SRe2L), and the evaluation phases in Tables 7 (ImageNet-1k), 8 (Tiny-ImageNet), and 9 (CIFAR-10/CIFAR-100). The hyperparameter settings for the ImageNet-1k and Tiny-ImageNet datasets predominantly adhere to SRe2L [34]. Furthermore, the settings for CIFAR-10/CIFAR-100 draw upon the classical knowledge distillation framework [6, 8, 27, 43]. Notably, we employ the same evaluation model (i.e. 128- width ConvNet) and identical number of epochs ( i.e. 1000) during the evaluation phase on CIFAR-10/CIFAR-100 as those used in the prior dataset distillation approaches [1, 20], ensuring experimental fairness. The Consistency of Backbone used in Data Synthesis and Soft Label Generation. In all experiments conducted on different datasets, we maintain the same architectures and identical parameters of the pre-trained model for data synthesis and soft label generation. Similar to SRe2L, our exploratory studies revealed that preserving the consistency of the backbone results in the best generalization ability for the distilled dataset. The Hyperparameter βdr. Given G-VBSM’s computational efficiency on ImageNet-1k under IPC 10, which serves as the benchmark for the majority of our ablation studies, we set βdr to 0.0 for this specific benchmark. For the remaining experiments, including Tiny-ImageNet, CIFAR-10 and CIFAR-100,βdr is set to 0.4.The Weights of the Loss Function. To underscore the generalization and applicability of our proposed G-VBSM, we intentionally avoid setting the weights of any loss functions, except for MSE+ γ×GT, in a bespoke manner. To be specific, the weights for both L′ BN and L′ Conv are established at 0.01 for ImageNet-1k, consistent with the weight ofLBN, which SRe2L is set as 0.01 for ImageNet-1k. Since we transposed the loop (i.e., translate the original loop to the reorder loop), the weights for L′ BN and L′ Conv are set at 0.01 for Tiny-ImageNet, different from SRe2L, which assigns a weight of 1.0 toLBN for the same dataset. Due to SRe2L was not evaluated on CIFAR-10 and CIFAR-100, we empirically adjusted the weights for L′ BN and L′ Conv to 0.01 in our experiments. Additionally, the weight of LDD is consistently applied at 1.0 across all datasets. Through empirical validation in our experiments, we establish that the performance of the distilled dataset – when the weight of LDD is configured as {0.1, 1.0, 10.0} – remains precisely identical. B. The Derivation of “match in the form of score distillation sampling” Our proposed novel loss function, denoted as L′ BN( ˜X), draws inspiration from score distillation sampling ( SDS) [17]. It is employed to mitigate the performance degradation that arises from directly substituting the original loop with the reorder loop. Here, we give the detailed derivation of L′ BN( ˜X) to facilitate understanding. First, we give the gradient of the original loss function LBN( ˜X) with respect to ˜X: ∂LBN( ˜X) ∂ ˜X = X l ∂µl( ˜X) ∂ ˜X µl( ˜X) − BNCM l ||µl( ˜X) − BNCM l ||2 + ∂σ2 l ( ˜X) ∂ ˜X σ2 l ( ˜X) − BNCV l ||σ2 l ( ˜X) − BNCV l ||2 . (12) In Eq. 12, µl( ˜X)−BNCM l ||µl( ˜X)−BNCM l ||2 and σ2 l ( ˜X)−BNCV l ||σ2 l ( ˜X)−BNCV l ||2 are unit vectors that dominate the direction of the gradient descent in the data synthesis process. As analyzed in Sec.3.1, the precise global statistics generated by all past batches are feasible to assist in matching between the limited statistics generated by the current batch andBNCM l as well as BNCV l . We utilize EMA to update the statistics µtotal l and σ2,total l generated by all past batches: µtotal l = αµtotal l + (1 − α)µl( ˜X), σ2,total l = ασ2,total l + (1 − α)σ2 l ( ˜X). (13) We can achieve the SDS-like loss by simply replacing µl( ˜X)−BNCM l ||µl( ˜X)−BNCM l ||2 with µtotal l ( ˜X)−BNCM l ||µtotal l ( ˜X)−BNCM l ||2 and σ2 l ( ˜X)−BNCV l ||σ2 l ( ˜X)−BNCV l ||2 with σ2,total l ( ˜X)−BNCV l ||σ2,total l ( ˜X)−BNCV l ||2 . In this way, the direction of gradient descent for data synthesis is no longer determined by the imprecise statistics of the single current batch, which ultimately improves the quality of the synthetic data and its ability to generalize to unseen evaluation models. In practice, we can implement the replacement easily with Pytorch’s [16] stopgrad(·) operator: L′ BN( ˜X) = X l ||µl( ˜X) − BNCM l − stop grad(µl( ˜X) − µtotal l )||2 + ||σ2 l ( ˜X) − BNCV l − stop grad(σ2 l ( ˜X) − σ2,total l )||2. (14) We can find the gradient of L′ BN( ˜X) with respect to ˜X by derivation as ∂L′ BN( ˜X) ∂ ˜X = X l ∂µl( ˜X) ∂ ˜X µtotal l ( ˜X) − BNCM l ||µtotal l ( ˜X) − BNCM l ||2 + ∂σ2 l ( ˜X) ∂ ˜X σ2,total l ( ˜X) − BNCV l ||σ2,total l ( ˜X) − BNCV l ||2 . (15) Clearly, L′ BN( ˜X) effectively achieves our primary purpose. Additionally, our ablation studies in Sec. 4.1 empirically demon- strate that the “match in the form of score distillation sampling” strategy is remarkable. C. Additional Ablation Experiments on ImageNet-1k L′ BN L′ Conv ResNet18 ResNet50 ResNet101 ✓ 27.8% 33.4% 35.5% ✓ 24.0% 26.1% 30.4% ✓ ✓ 31.4% 35.4% 38.2% Table 10. Ablation study about L′ BN and L′ Conv in the synthetic data phase on ImageNet-1k. Meanwhile, ResNet {18, 50, 101} represent evaluation models.This section presents ablation experiments forL′ BN and L′ Conv to underscore their equal importance. As illustrated in Table 10, omitting either L′ BN or L′ Conv from the entire loss function during data synthesis phase leads to a performance decline. Hence, conducting the “local-match-global” matching via both L′ BN and L′ Conv is essential. D. Exploratory Studies on CIFAR-10/100 The Choice of Candidate Backbones in GBM. Under IPC 10 on CIFAR-100, we evaluated candidate backbones {ResNet18, MobileNetV2, WRN-16-2, ShuffleNetV2-0.5}, omitting the 128-width ConvNet, during the data synthesis and soft label generation phases. In addition, we kept other hyperparameters consistent as shown in Table 9 and obtained the 128-width ConvNet evaluation performance as 32.8%. However, incorporating the 128-width ConvNet into the candidate backbones increased the accuracy from 32.8% to 38.7%. It’s important to mention that the 128-width ConvNet solely uti- lizes GroupNorm, not BatchNorm. This enhancement to 38.7% was accomplished by relying solely on statistics within Convolution, substantiating that statistics in BatchNorm may not be the only option in the data synthesis phase. Evaluation Model\\Epoch 5 10 20 40 128-width ConvNet 46.5% 45.8% 42.5% 42.1% Table 11. Ablation study about the number of epochs in pre-trained model training phase. We maintain the consistency of other hyperpa- rameters as presented in Table 9. The Number of Epochs in the Pre-Trained Model Training Phase. As illustrated in Table 11, fewer pre-training epochs on CIFAR-10 enhance the generalization of the distilled dataset. This finding could provide an explanation for the remarkable performance achieved by traditional algorithms [1, 32] on CIFAR-10, even when they employ models with random initial- izations. As a result, this ablation study informed our decision to pre-train models on CIFAR-10 for only 5 epochs. More important, as our experiments transition from ImageNet-1k to Tiny-ImageNet to CIFAR-100, and finally to CIFAR-10, the dataset complexity reduces, and the ideal number of pre-training epochs successively decreases from 100 to 50, to 50, and finally to 5. The most intuitive and empirical extrapolation is due to the complexity of the dataset, and we believe that this conclusion may be of some inspiration to other researchers. E. Additional Explanation of Data Densification Here we provide theoretical proofs within Eq. 16 to show that entropy H(Σy/τ) (τ > 1) is greater than H(Σy), thus increasing H(Σy) through Eq. 5, which ultimately improves the entropy of the eigenvalues and ensures the diversity of data. H(z/τ ) − H(z) = (τ − 1)∂H ∂τ (τ′), s.t.1 ≤ τ′ ≤ τ, define z = Σyfor convenience = −τ − 1 τ′ X i [(log( ezi/τ′ P j ezj/τ′ ) + 1)( ezi/τ′ P j ezj/τ′ ) ( −zi/τ′ P j(ezj/τ′ ) + P j(ezj/τ′ zj/τ′)P j ezj/τ′ )] = −τ − 1 τ′ [ X i log( ezi/τ′ P j ezj/τ′ ) ( ezi/τ′ P j ezj/τ′ )( −zi/τ′ P j(ezj/τ′ ) + P j(ezj/τ′ zj/τ′)P j ezj/τ′ ) − HHHHHHH P j zj/τ′ezj/τ′ P j ezj/τ′ + HHHHHHH P j zj/τ′ezj/τ′ P j ezj/τ′ ] = −τ − 1 τ′ X j ( ezi/τ′ P j ezj/τ′ ) log( ezi/τ′ P j ezj/τ′ ) [−zi/τ′ + P j ezj/τ′ zj/τ′ P j ezj/τ′ ] = τ − 1 τ′ [ X i (zi/τ′)2ezi/τ′ − X i (zi/τ′ezi/τ′ ) P i(zi/τ′ezi/τ′ )P i(ezi/τ′ ) + log( X i ezi/τ′ )(XXXXXXX X i zi/τ′ezi/τ′ −XXXXXXX X i zi/τ′ezi/τ′ )] > 0. (16)F. Statistics Visualization 5 10 15 20 Index 0.0 0.2 0.4 0.6 0.8 1.0Sigmoid Normalized Value Statistics for ResNet18 (Sigmoid Normalized) BN channel mean's mean BN channel mean's var BN channel var's mean Conv channel mean's mean Conv channel mean's var Conv channel var's mean 0 20 40 Index 0.0 0.2 0.4 0.6 0.8 1.0Sigmoid Normalized Value Statistics for ShufffleNetV2 (Sigmoid Normalized) Conv channel mean's mean Conv channel mean's var Conv channel var's mean BN channel mean's mean BN channel mean's var BN channel var's mean 0 10 20 30 40 50 Index 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Sigmoid Normalized Value Statistics for MobileNetV2 (Sigmoid Normalized) BN channel mean's mean BN channel mean's var BN channel var's mean Conv channel mean's mean Conv channel mean's var Conv channel var's mean 2.5 5.0 7.5 10.0 12.5 Index 0.0 0.2 0.4 0.6 0.8 1.0Sigmoid Normalized Value Statistics for WRN-16-2 (Sigmoid Normalized) Conv channel mean's mean Conv channel mean's var Conv channel var's mean BN channel mean's mean BN channel mean's var BN channel var's mean 1.0 1.5 2.0 2.5 3.0 Index 0.500 0.525 0.550 0.575 0.600 0.625 0.650 0.675Sigmoid Normalized Value Statistics for 128-width ConvNet (Sigmoid Normalized) Conv channel mean's mean Conv channel mean's var Conv channel var's mean Figure 9. Statistics visualization across various backbones {ResNet18, ShuffleNetV2, MobileNetV2, WRN-16-2, 128-width ConvNet} on CIFAR-100. It is important to emphasize that Convolution and BatchNorm offer different supervisory information in statistics. As a result, G-VBSM is more effective than SRe2L when optimized statistics in Convolution and BatchNorm together. For clarity, we visualized the statistics in the pre-trained models{ResNet18, 128-width ConvNet, MobileNetV2, WRN-16-2, ShuffleNetV2- 0.5} on CIFAR-100 in Fig. 9. In each subplot of Fig. 9, the horizontal axis denotes the layer index (with orthogonal indexes for Convolution and BatchNorm), while the vertical axis shows the post-sigmoid normalized result. Due to the extensive dimensions of channel mean and channel variance, we calculate only their mean and variance for visualization. Furthermore, since BatchNorm is not included in 128-width ConvNet, only Convolution statistics are visualized. From Fig. 9, we canconclude that the values of the statistics in Convolution and BatchNorm are different in any model, which indicates that G-VBSM is significant and can enhance the generalization of the distilled dataset as demonstrated in Fig. 5. 2 4 6 8 10 Step 6.67 8.67 10.67 12.67 14.67 16.67 18.67 20.67 22.67 24.67 26.67 28.67 30.67T op-1 Val Accuracy (%) SRe2L G-VBSM Figure 10. The experimental result of continual learning application on ImageNet-1k under IPC 10. G. Continual Learning Application Many data condensation algorithms [34, 38, 41] have evaluated the generalization ability of distilled datasets in continual learning. We follow the class-incremental learning approach 2 adopted in DM [41] for performing this task. Similar to the ablation studies of the main paper, our experiments are conducted on the full 224 ×224 ImageNet-1k, underscoring that G- VBSM is intended for use with large-scale datasets. We conduct class incremental learning with ResNet18 on the 10-step class-incremental learning strategy under 10 IPC. The experimental results are illustrated in Fig. 10. We can discover that G-VBSM significantly outperforms SRe2L, thus confirming the usefulness and effectiveness of G-VBSM. H. Date Free Pruning Application Data Free Pruning (ImageNet-1k, VGG-A, 50% Pruned) IPC 10 IPC 10 IPC 50 IPC 50 SRe2L SRe2L+DD SRe2L SRe2L+DD Top-1 Val Accuracy 12.5% 12.9% 31.7% 32.8% Table 12. The experimental result of data free pruning application on IamgeNet-1k. Data Free Pruning of Slimmingaims to reduce the model size and decrease the run-time memory footprint simultaneously for convolutional nerual network. We argue that the distilled dataset facilitates efficient data-free pruning. To substantiate this claim, we conduct experiments on ImageNet-1k with IPC 10. As illustrated in Table 12, data densification enhances downstream knowledge transfer as above by increasing synthesized data diversity and significantly boosting SRe2L. I. Synthetic Data Visualization We provide more visualization results on synthetic data randomly selected from G-VBSM in Figs. 11 (ImageNet-1k), 12 (Tiny-ImageNet), 13 (CIFAR-100) and 14 (CIFAR-10). 2This involves gradually increasing the number of classes and combining previously stored data with newly acquired data to train a model from scratch.Figure 11. Synthetic data visualization on ImageNet-1k randomly selected from G-VBSM.Figure 12. Synthetic data visualization on Tiny-ImageNet randomly selected from G-VBSM.Figure 13. Synthetic data visualization on CIFAR-100 randomly selected from G-VBSM.Figure 14. Synthetic data visualization on CIFAR-10 randomly selected from G-VBSM.",
      "references": [
        "Dataset distillation by matching training trajectories.",
        "Scaling up dataset distillation to imagenet-1k with constant memory.",
        "Minimizing the accumulated trajectory error to improve dataset distillation.",
        "Knowledge distillation: A survey.",
        "Deep residual learning for image recognition.",
        "Distilling the knowledge in a neural network,",
        "Fedsynth: Gradient compression via synthetic data in federated learning.",
        "Knowledge distillation from a stronger teacher.",
        "Dataset condensation via efficient synthetic- data parameterization.",
        "Comparing kullback-leibler divergence and mean squared error loss in knowledge distillation.",
        "Learning multiple layers of features from tiny images.",
        "DREAM: efficient dataset distillation by representative matching.",
        "Swin transformer: Hierarchical vision transformer using shifted windows.",
        "Reducing catas- trophic forgetting with learning on synthetic data.",
        "Dataset meta-learning from kernel ridge-regression.",
        "Pytorch: An imperative style, high-performance deep learning library.",
        "Dreamfusion: Text-to-3d using 2d diffusion.",
        "Imagenet large scale visual recognition challenge.",
        "Data distillation: A survey.",
        "Datadam: Efficient dataset distillation with attention match- ing.",
        "Mobilenetv2: Inverted residuals and linear bottlenecks.",
        "Sample condensation in online continual learn- ing.",
        "A fast knowledge distillation framework for visual recognition.",
        "Generative teaching net- works: Accelerating neural architecture search by learning to generate synthetic training data.",
        "Efficientnetv2: Smaller models and faster training.",
        "Embedded encoder-decoder in con- volutional networks towards explainable AI.",
        "Con- trastive representation distillation.",
        "Trainingdata-efficient image transformers & distillation through at- tention.",
        "Visualizing data using t-SNE.",
        "Pointaugmenting: Cross-modal augmentation for 3d object detection.",
        "Cafe: Learning to condense dataset by aligning features.",
        "Dataset distillation.",
        "Dreaming to distill: Data-free knowledge transfer via deep- inversion.",
        "Squeeze, re- cover and relabel: Dataset condensation at imagenet scale from A new perspective.",
        "Learn- ing from multiple teacher networks.",
        "Dataset distillation: A comprehensive review.",
        "Wide residual net- works.",
        "Accelerating dataset distillation via model augmenta- tion.",
        "Shufflenet: An extremely efficient convolutional neural net- work for mobile devices.",
        "Dataset condensation with differ- entiable siamese augmentation.",
        "Dataset condensation with dis- tribution matching.",
        "Dataset condensation with gradient matching.",
        "Decoupled knowledge distillation.",
        "Dataset distillation using neural feature regression."
      ],
      "meta_data": {
        "arxiv_id": "2311.17950v3",
        "authors": [
          "Shitong Shao",
          "Zeyuan Yin",
          "Muxin Zhou",
          "Xindong Zhang",
          "Zhiqiang Shen"
        ],
        "published_date": "2023-11-29T06:25:59Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses the limited generalization of lightweight ImageNet-scale dataset condensation (SRe2L) that matches only a single backbone/layer/statistic (ResNet18 BatchNorm channel mean/variance). Proposes the “generalized matching” perspective: sufficient and diverse local-match-global constraints across backbones, layers, and statistics yield richer, more transferable synthetic datasets. Introduces G-VBSM with three modules—Data Densification (DD), Generalized Backbone Matching (GBM), and Generalized Statistical Matching (GSM)—and an improved evaluation-phase distillation loss. Achieves state-of-the-art accuracy across both small and large datasets, notably 31.4% top-1 on full 224×224 ImageNet-1k at IPC=10 (ResNet18 eval), and strong cross-architecture generalization (e.g., to DeiT/Swin).",
        "methodology": "G-VBSM synthesizes a small set of images per class by optimizing synthetic pixels with one-step, statistics-based matching losses.\n1) Data Densification (DD): encourages intra-class diversity by making per-class batches linearly independent. Downsample synthetic images, build per-class Gram matrix \\hat{X}_y\\hat{X}_y^T, compute eigenvalues Σ_y, and maximize their entropy via a KL term between softmax(Σ_y/τ) (stop-grad) and softmax(Σ_y), effectively reducing eigenvalue variance and promoting full-rank structure.\n2) Generalized Backbone Matching (GBM): during synthesis, randomly sample the candidate backbone each iteration from a set of pretrained CNNs (e.g., ResNet18, MobileNetV2, EfficientNet-B0, ShuffleNetV2-0.5) to impose diverse constraints at low cost; during soft-label generation, ensemble teacher logits across backbones (optionally with logit normalization) to produce more generalizable soft targets.\n3) Generalized Statistical Matching (GSM): extends beyond BatchNorm by recording global Convolution statistics from the full training set via a single forward pass without gradients: channel mean/variance and patch mean/variance per conv layer. During synthesis, match synthetic-batch stats to recorded global stats using an EMA “SDS-like” formulation with stop-grad to stabilize matching when batches do not cover many classes; optionally drop some stat-matching terms with probability β_dr to trade accuracy for speed.\n4) Losses: synthesis objective combines task loss ℓ(f_cand(\\tilde{X}), y) with BN matching L’_BN, Conv matching L’_Conv, and L_DD. Evaluation/training on distilled data uses a distillation-style loss replacing KL at high temperature with MSE between student logits and ensembled teacher logits plus a small ground-truth cross-entropy term (MSE + γ×GT) to avoid numerical issues and improve generalization.",
        "experimental_setup": "Benchmarks: CIFAR-10, CIFAR-100, Tiny-ImageNet, and full-resolution 224×224 ImageNet-1k.\nCompression: images-per-class (IPC) typically {1,10,50,100} depending on dataset; key headline is ImageNet-1k IPC=10.\nBackbones: \n- ImageNet-1k synthesis uses torchvision pretrained weights for {ResNet18, MobileNetV2, EfficientNet-B0, ShuffleNetV2-0.5}. \n- Small datasets pretrain backbones from scratch with few epochs; candidate set includes {128-width ConvNet, WRN-16-2, ResNet18, ShuffleNetV2-0.5, MobileNetV2(-0.5)}.\nPhases: (i) pretrain teacher(s) and record BN EMA stats; (ii) record Conv global stats by a no-grad sweep over training data; (iii) synthesize images by optimizing pixels with Adam for ~4000 iterations (ImageNet batch size ~40); (iv) generate and store soft labels (FKD-style) with backbone ensemble; (v) train evaluation models on distilled set (e.g., ResNet18/50/101, MobileNetV2; additionally DeiT-Tiny and Swin-Tiny for cross-architecture).\nValidation: report top-1 accuracy on validation/test splits; include ablations for α (EMA factor), β_dr (stat-drop probability), loss variants, and backbone ensemble size; provide efficiency proxy via GPU latency per iteration and cross-architecture generalization results.",
        "limitations": "Relies on access to the full training set to compute global BN/Conv statistics (not a data-free method) and assumes these global statistics are stable and sufficiently informative for supervision. Still requires pretrained/backbone training and multiple phases (pretrain, stat recording, synthesis, soft-label generation, evaluation), which can be complex to reproduce and tune; performance is sensitive to choices like candidate backbone set, EMA factor α, and when/if to apply logit normalization. Convolution-stat matching adds overhead and requires design choices (patch size N_p per layer, drop probability β_dr). The DD objective depends on having multiple same-class samples per batch and introduces additional bookkeeping (loop reordering and SDS-like matching) with its own assumptions. Demonstrations focus on image classification; applicability to other modalities/tasks is not yet validated.",
        "future_research_directions": "Extend generalized matching to dense prediction (detection/segmentation) and other large-scale datasets, as suggested by the authors. Explore richer or task-specific statistics/layer types for matching (e.g., attention blocks, normalization-free networks, intermediate feature distributions) and automated selection/weighting of statistics. Reduce dependence on full-dataset passes by approximating global stats via streaming, sub-sampling, or online updates, enabling more scalable or privacy-preserving condensation. Improve theoretical understanding of why/when multi-backbone + multi-stat constraints yield better transfer, and develop principled backbone/statistic sampling policies. Investigate robustness and fairness of condensed sets (distribution shift, adversarial robustness), and applications such as federated/continual learning, pruning, and architecture search with condensed data.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Iteratively Refined Behavior Regularization for Offline Reinforcement Learning",
      "full_text": "arXiv:2305.06871v1  [math-ph]  11 May 2023 Conservation laws of mean ﬁeld games equations Roman Kozlov Department of Business and Management Science, Norwegian Scho ol of Economics Helleveien 30, 5045, Bergen, Norway; E-mail address: Roman.Kozlov@nhh.no Abstract Mean ﬁeld games equations are examined for conservation laws. The system of mean ﬁeld games equations consists of two partial diﬀerential eq uations: the Hamilton-Jacobi-Bellman equation for the value function and the for ward Kol- mogorov equation for the probability density. For separable Hamilto nians, this sys- tem has a variational structure, i.e., the equations of the system a re Euler-Lagrange equations for some Lagrangian functions. Therefore, one can us e the Noether the- orem to derive the conservation laws using variational and divergen ce symmetries. In order to ﬁnd such symmetries, we ﬁnd symmetries of the PDE sys tem and select variational and divergence ones. The paper considers separable, state-independent Hamiltonians in one-dimensional state space. It examines the most g eneral form of the mean ﬁeld games system for symmetries and conservation laws a nd identiﬁes particular cases of the system which lead to additional symmetries a nd conservation laws. Keywords: mean ﬁeld games, conservation law, symmetry, Noether theorem 1 Introduction The Partial Diﬀerential Equations (PDEs) approach to Mean Field Ga mes (MFG) theory is a rapidly developing ﬁeld of research that has its roots in the seminal work of J.-M. Lasry and P.-L. Lions in 2006 [1, 2, 3]. Another approach to the developmen t of MFG theory was suggested by M. Huang, R. Malham´ e, and P. Caines [4, 5]. The theo ry provides a mathematical framework for understanding the behaviour of large populations o f interacting agents, where the entire population inﬂuences each agent’s behaviour. A continuu m of agents represents such a population [6]. Every again knows the distribution of the other agents and uses this to implement its strategy. The mean ﬁeld game theory has found a broad range of applications in various ﬁelds [7]. In economics, MFG theory has been used to study market competit ion [8, 9, 10, 11], price formation [12, 13], and ﬁnancial markets [14, 15, 16, 17, 18]. In par ticular, MFG theory has been applied to understand the behaviour of large populations of inv estors and traders in ﬁnancial markets [19]. MFG models have also been used to study traﬃ c ﬂow and congestion in transportation systems, as well as to model the behaviour of cr owds in social dynamics [20, 21, 22, 23]. MFG theory has been used to understand the dyna mics of learning [24]. 1Additionally, MFG theory has been applied to the study of energy sys tems, such as electricity markets and renewable energy systems [25, 26, 27, 28, 29], and to the optimization of resource allocation in engineering applications [30, 31]. Mathematical aspects in diﬀerent frameworks of the MFG theory are given in several review papers [32, 33] and book s [34, 35, 36, 37]. Conservation laws play an essential role in diﬀerential equations. Th ey reﬂect the intrinsic properties of the equations. They also help to ensure the consiste ncy and stability of the system. In the present paper, we analyse the system of mean ﬁeld games eq uations for conservation laws. This system consists of two PDEs: the Hamilton-Jacobi-Bellman (HJB ) equation for the value function of a typical agent and the forward Kolmogorov equation ( also known as the Fokker- Plank equation) for the evolution of the density of the agent popula tion. It should be noted that the ﬁrst equation is backward in time, and the latter is forward in time. There are several approaches to derive conservation laws [38]. In the paper, we exp loit the variational structure of the MFG equations. For separable Hamiltonians, this system has a variational structure, i.e., the equations of the system are Euler-Lagrange equations for some Lagrangian functions. Therefore, one can use symmetries and the Noether theorem [39 ] to derive the conservation laws. Lie point symmetries provide eﬃcient tools to analyse nonlinear diﬀere ntial equations [40, 41, 42]. The symmetries are related to essential properties of the models represented by the diﬀerential equations: • Symmetries generate transformations which transfer solutions o f the diﬀerential equations into other solutions. In particular, they can be used to ﬁnd new solu tions from the known ones. • Symmetries of PDEs can be used to ﬁnd particular solutions. These s olutions are called invariant solutions. • Symmetries of variational diﬀerential equations can be used for No ether’s theorem, which provides conservation laws. For the application of the Noether theorem, one needs variational formulation of the con- sidered diﬀerential equations and variational or divergence symme tries. Such symmetries are also symmetries of the considered diﬀerential equations. Therefo re, the standard approach is to ﬁnd symmetries of the diﬀerential equations, select those that are also variational or diver- gence symmetries, and apply the Noether theorem to obtain conse rvation laws. These steps are implemented in the paper. In particular, we start with the most gene ral case of the diﬀerential equations, for which there are a few symmetries, and continue with identifying particular cases, for which there are additional symmetries. After that, we select s ymmetries suitable for the Noether theorem and provide the conservation law. The paper is organized as follows. In the next Section we review the m ean ﬁeld games equations and specify the cases with the variational formulation. S ection 3 describes Lie point symmetries and the Noether theorem. This section is written for th e particular system which is analysed in Sections 4 and 5, where we present the symmetry analy sis and the conservation laws. Section 6 discusses the interpretation of the obtained conse rvation laws. Final Section 7 provides concluding remarks. Some technical details are extracte d into the Appendices. 22 Variational MFG equations The system of mean ﬁeld games equations consists of two coupled PD Es: the Hamilton-Jacobi- Bellman equation and the forward Kolmogorov equation [37]: −ut − ε∆ u + H(x, ∇u, m) = 0 in [0 , T ] × Rd, (2.1a) mt − ε∆ m − div(mH∇u(x, ∇u, m)) = 0 in [0 , T ] × Rd. (2.1b) The scalar functions u(t, x) and m(t, x) stand for the value function and the probability density. They depend on time t ∈ [0, T ] and spatial independent variable x ∈ Rd. H(x, ∇u, m) is the Hamiltonian associated to the game and H∇u = ( Hux1 , · · · , Huxd ). Parameter ε ≥ 0 characterizes the noise. It should be noted that the HJB equation (2.1a) is backward in time and the Kolmogorov equation (2.1b) is forward in time. Thus, the initia l-terminal conditions are given as m(0, x) = m0(x), u (T, x) = G(x, m(T, x)) in Rd. (2.2) Since m(t, x) ≥ 0 is the probability density, we have ∫ mdx = 1 . (2.3) The solution ( u, m) of the coupled system (2.1) represents a Nash equilibria of the mea n ﬁeld game. We refer to [37] for detailed assumptions one needs for t he existence and uniqueness of the solutions. A sketch of the derivation of the system is given in A ppendix A. 2.1 Separable Hamiltonians A particular case of the system (2.1) −ut − ε∆ u + H(x, ∇u) − f(x, m) = 0 , (2.4a) mt − ε∆ m − div(mH∇u(x, ∇u)) = 0 (2.4b) corresponds to separate Hamiltonians H(x, ∇u, m) = H(x, ∇u) − f(x, m). This system has a variational formulation. It is provided by the Lagrangian function L = −mut + ε∇m · ∇u + mH(x, ∇u) − F (x, m), (2.5) where F (x, m) = ∫ f(x, m)dm, (2.6) i.e., equations (2.4) are Euler-Lagrange equations δL δm = ∂L ∂m − Dt ∂L ∂mt − d∑ i=1 Dxi ∂L ∂mxi = 0 , (2.7a) δL δu = ∂L ∂u − Dt ∂L ∂ut − d∑ i=1 Dxi ∂L ∂uxi = 0 (2.7b) for Lagrangian (2.5). Here Dt and Dxi are total diﬀerentiation operators for time t and spatial variables xi, respectively. 32.2 One-dimensional state space with state-independent Ha miltoni- ans. In this paper, we examine the simplest case of system (2.4): the cas e of one-dimensional state space d = 1 with state-independent functions H = H(ux) and f = f(m). The equations take the form F1 = −ut − εuxx + H(ux) − f(m) = 0 in [0 , T ] × R, (2.8a) F2 = mt − εmxx − mxH′(ux) − mH′′(ux)uxx = 0 in [0 , T ] × R. (2.8b) We consider second-order MFG systems restricting to ε > 0. Assumptions H′′(ux)̸≡0, f ′(m)̸≡0; (2.9) guarantee that the equations are genuinely coupled, i.e., none of th ese equations can be con- sidered independently from the other. Equations (2.8) are Euler-Lagrange equations for the Lagrangian function L = −mut + εmxux + mH(ux) − F (m) (2.10) with F (m) = ∫ f(m)dm. (2.11) In Section 4, we analyse the system of equations (2.8) for admitted symmetries and in Section 5 we derive the conservation laws of these equations. 2.3 Other variational cases Between the most general variational system (2.4) and the simples t variational system (2.8), two particular MFG systems might be worthy of a separate study. T hey correspond to • Multi-dimensional state space x ∈ Rd with state-independent Hamiltonians. For state-independent functions H = H(∇u) and f = f(m), we get the particular case of the system (2.4) −ut − ε∆ u + H(∇u) − f(m) = 0 in [0 , T ] × Rd, (2.12a) mt − ε∆ m − div(mH∇u(∇u)) = 0 in [0 , T ] × Rd, (2.12b) provided by Lagrangian L = −mut + ε∇m · ∇u + mH(∇u) − F (m) with F (m) given by (2.11). • One-dimensional state space If the state space is one-dimensional, system (2.4) gets simpliﬁed as −ut − εuxx + H(x, ux) − f(x, m) = 0 in [0 , T ] × R, (2.13a) mt − εmxx − (mHux (x, ux))x = 0 in [0 , T ] × R. (2.13b) It’s Lagrangian function is L = −mut + εmxux + mH(x, ux) − F (x, m). 43 Lie point symmetries and the Noether theorem This section reviews Lie point symmetries and the Noether theorem [ 40, 41, 42]. For con- venience, the discussion is adopted to the system of PDEs (2.8) and ﬁrst-order Lagrangian function (2.10), which are analysed in this paper. Lie point symmetries of system (2.8) are given by generators of the form X = ξt ∂ ∂t + ξx ∂ ∂x + ηu ∂ ∂u + ηm ∂ ∂m (3.1) with generator coeﬃcients ξt = ξt(t, x, m, u ), ξ x = ξx(t, x, m, u ), η u = ηu(t, x, m, u ), η m = ηm(t, x, m, u ). For application to diﬀerential equations a generator should be prolo nged to all variables involved in the considered diﬀerential equations. In our case, we get the pr olonged generator X = ξt ∂ ∂t + ξx ∂ ∂x + ηu ∂ ∂u + ηm ∂ ∂m + ζu t ∂ ∂ut + ζm t ∂ ∂mt + ζu x ∂ ∂ux + ζm x ∂ ∂mx + ζu xx ∂ ∂uxx + ζm xx ∂ ∂mxx . (3.2) The coeﬃcients for the derivatives are computed according to the standard prolongation for- mulas [40, 41, 42] ζu t = Dt(ηu) − utDt(ξt) − uxDt(ξx), ζ m t = Dt(ηm) − mtDt(ξt) − mxDt(ξx), ζu x = Dx(ηu) − utDx(ξt) − uxDx(ξx), ζ m x = Dx(ηm) − mtDx(ξt) − mxDx(ξx), ζu xx = Dx(ζu x ) − utxDx(ξt) − uxxDx(ξx), ζ m xx = Dx(ζu x ) − mtxDx(ξt) − mxxDx(ξx), where Dt and Dx are total diﬀerentiation operators for independent variables t and x, respec- tively, i.e. Dt = ∂ ∂t + ut ∂ ∂u + mt ∂ ∂m + · · · , D x = ∂ ∂x + ux ∂ ∂u + mx ∂ ∂m + · · · To ﬁnd Lie point symmetries admitted by diﬀerential equations, one a pplies the inﬁnitesimal invariance criterion [40, 41, 42]. It requires that the action of the p rolonged generator on the considered diﬀerential equations provides identities on the solution s of these equations. For system (2.8), the inﬁnitesimal invariance criterion takes the form E1 = X(F1)|F1=0,F 2=0 = 0 , (3.3a) E2 = X(F2)|F1=0,F 2=0 = 0 . (3.3b) These equations are PDEs for the coeﬃcients of the generator (3 .1). Solving them, one ﬁnds the admitted generators. Lie point symmetries of the diﬀerential equations are needed for th e Noether theorem. 5Theorem 3.1 (E. Noether [39]) Let Lagrangian function (2.10) satisfy th e equation XL + L(Dt(ξt) + Dx(ξx)) = 0 (3.4) for generator X given by (3.1), then the generator X is a symmetry of the system of Euler- Lagrange equations (2.8) and there holds the conservation l aw Dt[T t] + Dx[T x] = 0 , (3.5) where T t = ξtL + (ηu − ξtut − ξxux) ∂L ∂ut + (ηm − ξtmt − ξxmx) ∂L ∂mt , (3.6a) T x = ξxL + (ηu − ξtut − ξxux) ∂L ∂ux + (ηm − ξtmt − ξxmx) ∂L ∂mx . (3.6b) There is a natural extension of the Noether theorem. Remark 3.2 ([43]) If instead of relation (3.4), there holds XL + L(Dt(ξt) + Dx(ξx)) = Dt(V t) + Dx(V x) (3.7) with some functions V t(t, x, u, m ) and V x(t, x, u, m ), then the generator X is a symmetry of the system of Euler-Lagrange equations (2.8) and there hold s the conservation law Dt[T t − V t] + Dx[T x − V x] = 0 (3.8) with T t and T x given by (3.6). Symmetries of the variational equations which satisfy (3.4) are calle d variational symmetries and symmetries with property (3.7) are called divergence symmetries. 4 Symmetry analysis of system (2.8) This section provides analysis of system (2.8),(2.9) for admitted sym metries. Lie point sym- metries of system (2.8) are provided by the inﬁnitesimal invariance c riterion (3.3). In detail, these equations take the form E1 = [ −ζu t − εζu xx + H′(ux)ζu x − f′(m)ηm]F1=0,F 2=0 = 0 , E2 = [ ζm t − εζm xx − ζm x H′(ux) − mxH′′(ux)ζu x −ηmH′′(ux)uxx − mH′′′(ux)ζu x uxx − mH′′(ux)ζu xx]F1=0,F 2=0 = 0 . These equations are used to ﬁnd symmetries admitted for arbitrar y H(ux) and f(m). They are also used to identify special cases of H(ux) and f(m) which lead to extensions of the symmetries admitted for arbitrary H(ux) and f(m). The preliminary analysis of these equations aimed at identifying the special cases of the Hamiltonian function is given in App endix B. It results in four special cases of H(ux) which should be analysed in addition to the general case of arbitrar y H(ux). Thus, we have the following cases to examine 6• General case: arbitrary H(ux) • Special cases: – power H(ux) = h(ux + q)p + h2u2 x + h1ux + h0, h ̸= 0 , p ̸= 0 , 1, 2, 3; (4.1) – exponential H(ux) = hekux + h2u2 x + h1ux + h0, h ̸= 0 , k ̸= 0; (4.2) – cubic (particular power) H(ux) = hu3 x + h2u2 x + h1ux + h0, h ̸= 0; (4.3) – quadratic (particular power) H(ux) = hu2 x + h1ux + h0, h ̸= 0 . (4.4) Here h, h2, h1, h0, p, q and k are constants. Before we consider these cases, it is appropriate to simplify them by transformations which do not change the form of equations (2.8). Such transformations are called equivalence trans- formations. We apply the following steps. 1. Transformations of the form u → ¯u = u + Ax, A = const (4.5) are used to remove parameter q in Hamiltonian (4.1), to scale coeﬃcient h in Hamiltonian (4.2) and to remove the term h2u2 x in Hamiltonian (4.3). 2. Transformations u → ¯u = u + Bt, B = const (4.6) are used to remove constant terms h0 in all Hamiltonians (4.1)-(4.4) as well as in functions f(m) (if f(m) contains a constant term). 3. The transformation x → ¯x = x + h1t (4.7) removes the terms h1ux in all Hamiltonians (4.1)-(4.4). 4. Scaling of independent and dependent variables t → ¯t = C1t, x → ¯x = C2x, u → ¯u = C3u, m → ¯m = C4m, (4.8) 7where C1, C2, C3 and C4 are constants such that C1C2C3C4 ̸= 0, are used to make a suitable choice of the leading term coeﬃcient h in Hamiltonians (4.1), (4.3) and (4.4). Scaling transformations can be chosen so that they satisfy C2C4 = 1 . In addition to the form of equations (2.8) such scaling transformat ions preserve nor- malization (2.3). Function f(m) is not speciﬁed, and it can be changed by the scaling transformations. Implementing these steps, we arrive at the following simpliﬁed Hamilton ians for the special cases: • power H(ux) = 1 pup x + h2u2 x, p ̸= 1 , 2, 3; (4.9) • exponential H(ux) = 1 k ekux + h2u2 x; (4.10) • cubic H(ux) = 1 3u3 x; (4.11) • quadratic H(ux) = 1 2u2 x. (4.12) Here h2 is an arbitrary constant. Detailed analysis of determining equations (3.3) for the general and special cases of the Hamiltonian provides the following result. Theorem 4.1 Up to equivalence transformations (4.5)–(4.8), system (2. 8),(2.9). has the fol- lowing cases of admitted symmetries: For arbitrary H(ux) and arbitrary f(m), the system admits the three symmetries X1 = ∂ ∂t , X 2 = ∂ ∂x , X 3 = ∂ ∂u . (4.13) If f(m) = α ln m, there exists the additional symmetry Xf = αt ∂ ∂u − m ∂ ∂m . (4.14) There are particular cases of H(ux) which lead to the extensions of the admitted symmetry group. a) For power H(ux) = 1 p up x (Hamiltonian (4.9) with h2 = 0 ) and power f(m) = αmγ , there is the additional generator Xa 4 = 2( p − 1)t ∂ ∂t + (p − 1)x ∂ ∂x + (p − 2)u ∂ ∂u − p γ m ∂ ∂m . (4.15) 8b) For exponential H(ux) = 1 k ekux (Hamiltonian (4.10) with h2 = 0 ) and power f(m) = αmγ , there exists the additional symmetry Xb 4 = 2 t ∂ ∂t + x ∂ ∂x + ( u − x k ) ∂ ∂u − 1 γ m ∂ ∂m . (4.16) c) For quadratic H(ux) = 1 2 u2 x and arbitrary f(m) there is the generator Xc = t ∂ ∂x − x ∂ ∂u . (4.17) If f(m) = αm2, then there is the particular case of symmetry (4.15), corre sponding the p = 2 , namely Xc 4 = Xa 4 |p=2,γ =2 = 2 t ∂ ∂t + x ∂ ∂x − m ∂ ∂m , (4.18) and one more symmetry Xc 5 = t2 ∂ ∂t + tx ∂ ∂x + ( −x2 2 + εt ) ∂ ∂u − tm ∂ ∂m . (4.19) Proof. The system of determining equations (3.3) is an overdetermined sys tem of PDEs for the coeﬃcients of generator X, given by (3.1). These equations are analysed for the case of arbitrary Hamiltonian H(ux) and for four special Hamiltonians (4.9)-(4.12). For the considere d cases of the Hamiltonian function the determining equations split (ﬁr st for the derivatives of u and m, then for the variables) into equations that specify the symmetry coeﬃcients. Particular cases of f(m), namely f(m) = α ln m, f(m) = αmγ and f(m) = αm2, are iden- tiﬁed from the detailed analysis of the determining equations as case s that provide additional symmetries. □ The admitted symmetries can be divided into the basis symmetry grou ps, which is admitted for all functions H(ux) and f(m), and the additional symmetries, which exist for the particular functions H(ux) and f(m). The basis symmetry group consists of three generators {X1, X2, X3}, given in (4.13). For convenience, we present the extensions of the basic symmetry group in Table 1. arbitrary f(m) f(m) = α ln m f(m) = αmγ , γ ̸= 2 f(m) = αm2 arbitrary H(ux) − Xf − − H(ux) = 1 p up x − Xf Xa 4 Xa 4 |γ=2 H(ux) = 1 k ekux − Xf Xb 4 Xb 4 ⏐ ⏐ γ=2 H(ux) = 1 2 u2 x Xc Xf , X c Xc, X4 Xc, X c 4, X c 5 Table 1. Extensions of the basic symmetry group with generators {X1, X2, X3}, which is admitted for arbitrary H(ux) and arbitrary f(m). Classiﬁcation is performed up to equivalence transformations (4.5)–(4.8). Remark 4.2 In Theorem 4.1, the case of cubic Hamiltonian (4.11) stands a s a particular case of power Hamiltonian (4.9). The cubic Hamiltonian was ident iﬁed separately from the power Hamiltonian in the preliminary analysis of the determining equations, given in Appendix B. However, it has the same symmetry properties as the general p ower Hamiltonian case. 9Remark 4.3 It is possible to employ scaling transformation to get rid of the parameter α in the particular cases of f(m) without changing the form of equations (2.8). However, for s ome parameter values, such scaling transformations do not keep normalization relation (2.3). For this reason, parameter α is kept. 5 Conservation laws of system (2.8) The cases of H and f, which should be considered, are speciﬁed by Theorem 4.1. It remain s to select variational and divergence symmetries and apply the Noet her theorem to derive the conservation laws. 5.1 General case: arbitrary H(ux) For arbitrary Hamiltonian, two cases of the function f(m) exist to consider. 1) Arbitrary f(m) In this case, we consider F (m) = ∫ f(m)dm (5.1) corresponding to arbitrary f(m). Equations (2.8) admit symmetries X1, X2 and X3, given in (4.13). These three symmetries are variational. • The symmetry X1 provides the conservation law Dt [εmxux + mH(ux) − F (m)] − Dx [ε(utmx + mtux) + mutH′(ux)] = 0 . (5.2) • The symmetry X2 leads to the conservation law Dt [mux] − Dx [mut + εmxux + m(uxH′(ux) − H(ux)) + F (m)] = 0 . (5.3) • The symmetry X3 gives the conservation law − Dt [m] + Dx [εmx + mH′(ux)] = 0 . (5.4) Note that this conservation law is the Kolmogorov equation (2.8b) its elf. It reﬂects the conservation of the probability density m(t, x). 2) Logarithmic f(m) = α ln m In this case F (m) = α(m ln m − m). (5.5) The generator Xf (see (4.14)) is neither variational nor divergence symmetry. Thus , it does not provide a conservation law. 105.2 Special cases of H(ux) There are three exceptional cases of Hamiltonian: (general) powe r, exponential and quadratic. a) Power H(ux) = 1 p up x and power f(m) = αmγ In this case F (m) = α γ + 1 mγ+1. (5.6) Symmetry Xa 4 , given by (4.15), is variational, provided that γ = γ∗ = p 2p − 3. (5.7) Thus we consider the symmetry Y a 4 = X4|γ=γ∗ = 2( p − 1)t ∂ ∂t + (p − 1)x ∂ ∂x + (p − 2)u ∂ ∂u − (2p − 3)m ∂ ∂m . (5.8) It provides the conservation laws Dt [ 2(p − 1)t ( εmxux + 1 pmup x − α γ∗ + 1 mγ∗ +1 ) + (p − 1)xmux − (p − 2)mu ] − Dx [ 2(p − 1)t ( ε(utmx + mtux) + mutup−1 x ) + (p − 1)x ( mut + εmxux + p − 1 p mup x + α γ∗ + 1 mγ∗ +1 ) − (p − 2)u(εmx + mup−1 x ) + (2 p − 3)εmux ] = 0 . (5.9) b) Exponential H(ux) = 1 k ekux and power f(m) = αmγ As in the previous case, F (m) is given by (5.6). The symmetry Xb 4, given in (4.16), is a divergence symmetry for γ = 1 2 (5.10) with V t 4 = 0 , V x 4 = − ε k m. (5.11) i.e., for this symmetry, there holds XL + L(Dt(ξt) + Dx(ξx)) = Dx ( − ε k m ) . Therefore, we get the symmetry Xb 4 speciﬁed as Y b 4 = Xb 4 ⏐ ⏐ γ= 1 2 = 2 t ∂ ∂t + x ∂ ∂x + ( u − x k ) ∂ ∂u − 2m ∂ ∂m . (5.12) 11Using this symmetry, we obtain the conservation law Dt [ 2t ( εmxux + 1 k mekux − 2α 3 m3/ 2 ) + xm ( ux + 1 k ) − mu ] − Dx [ 2t ( ε(utmx + mtux) + mutekux ) + x ( mut + εmxux + muxekux + ε k mx + 2α 3 m3/ 2 ) −u(εmx + mekux ) + 2 εmux − ε k m ] = 0 . (5.13) c) Quadratic H(ux) = 1 2 u2 x For the quadratic Hamiltonian, there are two cases of f(m). 1) Arbitrary f(m) For arbitrary f(m), we consider the corresponding function F (m) as given in (5.1). The symmetry Xc, given in (4.17), is a divergence symmetry with V t = 0 , V x = −εm. (5.14) This symmetry leads to the conservation law Dt [m(tux + x)] − Dx [ t ( mut + εmxux + 1 2mu2 x + F (m) ) + x(εmx + mux) − εm ] = 0 . (5.15) 2) Quadratic f(m) = αm2 Now F (m) = α 3 m3. (5.16) It is a particular case of (5.6). The symmetry Y c 4 = Y a 4 |p=2 = 2 t ∂ ∂t + x ∂ ∂x − m ∂ ∂m , i.e., symmetry (5.8) with p = 2, is variational. It provides a particular case of conservation law (5.9) corresponding to γ∗ = p = 2, namely Dt [ 2t ( εmxux + 1 2mu2 x − α 3 m3 ) + xmux ] − Dx [ 2t (ε(utmx + mtux) + mutux) + x ( mut + εmxux + 1 2mu2 x + α 3 m3 ) + εmux ] = 0 . (5.17) The symmetry Xc 5. given in (4.19), is a divergence one with V t = 0 , V x = −εxm. (5.18) 12It gives the conservation law Dt [ t2 ( εmxux + 1 2mu2 x − α 3 m3 ) + txmux + x2 2 m − εtm ] − Dx [ t2 (ε(utmx + mtux) + mutux) + tx ( mut + εmxux + 1 2mu2 x + α 3 m3 ) +x2 2 (εmx + mux) − ε2tmx − εxm ] = 0 . (5.19) The obtained results can be summarized as a theorem. Theorem 5.1 Up to equivalence transformations (4.5)–(4.8), system (2. 8),(2.9) has the fol- lowing cases of conservation laws: For arbitrary H(ux) and arbitrary f(m), the system has three conservation laws (5.2), (5.3) and (5.4). There are particular cases of H(ux) which lead to the additional conservation laws. a) For power H(ux) = 1 p up x and power f(m) = αmγ with γ = γ∗ = p 2p−3 , there is the additional conservation law (5.9). b) For exponential H(ux) = 1 k ekux and f(m) = α√m, there exists the additional conservation law (5.13). c) For quadratic H(ux) = 1 2 u2 x and arbitrary f(m) there is conservation law (5.15). If f(m) = αm2, then there also hold conservation laws (5.17) and (5.19). Thus, three conservation laws exist for all H(ux) and f(m). The symmetries which provide additional conservation laws are given in Table 2. arbitrary f(m) f(m) = αmγ , γ ̸= 2 f(m) = αm2 arbitrary H(ux) − − − H(ux) = 1 p up x, p ̸= 2 − Y a 4 − H(ux) = 1 k ekux − Y b 4 − H(ux) = 1 2 u2 x Xc Xc Xc, Y c 4 , X c 5 Table 2. Variational and divergence symmetries admitted in addition to symme tries (4.13), which correspond to arbitrary H(ux) and arbitrary f(m). Classiﬁcation is performed up to equivalence transformations (4.5)–(4.8). 6 Interpretation of conservation laws Some conservation laws allow intuitive interpretation. Finite group tr ansformations t → ¯t, x → ¯x, u → ¯u, m → ¯m for generators (3.1) solve the ODE system [40, 41] d¯t da = ξt(¯t, ¯x, ¯u, ¯m), d¯x da = ξx(¯t, ¯x, ¯u, ¯m), d¯u da = ηu(¯t, ¯x, ¯u, ¯m), d ¯m da = ηm(¯t, ¯x, ¯u, ¯m) 13with the initial conditions ¯t|a=0 = t, ¯x|a=0 = x, ¯u|a=0 = u, ¯m|a=0 = m. Here a is the group parameter. Its value a = 0 corresponds to the identity transformation. We remark the solutions can be presented by the exponentiation [42] t → ¯t = eaX t, x → ¯x = eaX x, u → ¯u = eaX u, m → ¯m = eaX m. Speciﬁcally, we consider a particular case of the mean ﬁeld games des cribed in Appendix A. Let the dynamics of the agents be described by the SDE dXs = αsds + √ 2εdBs, (6.1) where αs is the agent’s control. Brownian motions Bs for diﬀerent agents are assumed to be independent. Each agent minimizes E [ ∫ T 0 ( α2 s 2 + f(m(s, Xs)) ) ds + G(XT , m(T, X T )) | X0 = x ] . (6.2) The running costs have two terms. The ﬁrst term α 2 s 2 requires to minimise the state changes. The second term f(m) represents the interaction of the agents. It can be a willingness t o avoid others if f(m) is an increasing function or a willingness to be like others if f(m) is a decreasing function. We obtain the Hamiltonian H(x, p, m) = p2 2 − f(m), the MFG system −ut − εuxx + u2 x 2 = f(m), (6.3a) mt − εmxx − (mux)x = 0 . (6.3b) and the optimal feedback control αs = −ux(t, x). (6.4) System (6.3) has four conservation laws. Conservation law (5.3) co rresponds to the symme- try X2, which represents the translation of the state variable x ¯t = t, ¯x = x + a, ¯u = u, ¯m = m. Taking (6.4) into account, it is possible to consider this conservation law as the conservation of the average control value. In mechanics, translations of the s patial coordinates lead to the conservation of momenta. Conservation law (5.4) is provided by translations of u, ¯t = t, ¯x = x, ¯u = u + a, ¯m = m, 14given by the symmetry X3. It has the same interpretation for the quadratic Hamiltonian as fo r the arbitrary Hamiltonian. It represents the conservation of the probability measure (2.3). It is analogous to the conservation of mass in mechanical systems. Conservation law (5.15) is derived with the help of the Galileo-type sym metry Xc. This symmetry generates the group transformation ¯t = t, ¯x = x + at, ¯u = u − ax − a2 2 t, ¯m = m. It can be viewed as the law describing the change of the average sta te value. In mechanics, the Galileo transformation corresponds to the law of motion of the cent re of mass. There seems to be no natural interpretation for conservation law (5.2) as well as for the addi- tional conservation laws (5.17) and (5.19), which exist for quadrat ic f(m). These conservation laws correspond to the time translation ¯t = t + a, ¯x = x, ¯u = u, ¯m = m, the scaling ¯t = e2at, ¯x = eax, ¯u = u, ¯m = e−am, and the projective transformation in ( t, x) plane ¯t = t 1 − at, ¯x = x 1 − at supplemented with the transformation of the dependent variables ¯u = u − ax2 2(1 − at) − ε ln(1 − at), ¯m = (1 − at)m. The transformations are generated by symmetries X1, Y c 4 and Xc 5, respectively. In mechanics, the time translation provides the conservation of energy. Conser vation laws corresponding to the scaling transformations and the projective transformations are known in mechanics, kinetic theory gases, hydrodynamics, etc. [44]. They have no speciﬁc nam es. 7 Conclusion The paper considers system (2.8),(2.9). This system is the simplest c ase of the mean ﬁeld games system (2.4), which has a variational formulation due to the separa ble Hamiltonian. System (2.8) corresponds to state-independent Hamiltonians in one-dimen sional state space. The paper provides Lie point symmetry analysis of the system. Then, the varia tional and divergence symmetries are selected. They are used for the application of the N oether theorem to derive the conservation laws. The presented approach provides the sym metries and conservation laws for the general case of the system (2.8),(2.9) with arbitrary func tions H(ux) and f(m) and identiﬁes the particular cases of H(ux) and f(m) which lead to the additional symmetries and conservation laws. Some conservation laws have natural interpre tations. Conservation laws of the ﬁrst-order mean ﬁeld games systems, wh ich correspond to param- eter ε = 0 in (2.1), were considered in [45, 46]. To our knowledge, there have been no attempts 15to investigate second-order mean ﬁeld games equations for conse rvation laws that makes the results of this paper original. The study given here can also be perfo rmed for more general MFG systems, such as • general system (2.4) for multi-dimensional state space with state -dependent H and f, • system (2.12) for multi-dimensional state space with state-indepe ndent H and f, • system (2.13) for one-dimensional state space with state-depen dent H and f. It is also possible to consider stationary versions of these mean ﬁeld games equations. For example, system (2.4) with arbitrary H(x, ∇u) and f(x, m) admits only two symmetries X1 = ∂ ∂t , X 2 = ∂ ∂u . The generator X1 is a variational symmetry and the generator X2 is a divergence symmetry of the Lagrangian function (2.5). They provide the conservation laws Dt [ε∇m · ∇u + mH(x, ∇u) − F (x, m)] − div [ε(ut∇m + mt∇u) + mutH∇u(x, ∇u)] = 0 and −Dt [m] + div [ ε∇m + mH∇u(x, ∇u)] = 0 , respectively. These conservation laws generalize conservation law s (5.2) and (5.4), which hold for system (2.8) with arbitrary H(ux) and f(m). Conservation laws reﬂect fundamental properties of the diﬀeren tial equations, that helps to understand and analyse the models. For this reason, it is crucial to preserve them in numerical simulation [47]. In particular, conservation of measure (2.3) is always respected by numerical methods for the MFG equations [48]. Finally, it is worth mentioning that symmetries of PDEs can be used to ﬁnd particular solutions [40, 41, 42]. This has not bee n explored for the mean ﬁeld games equations. Acknowledgments The author is grateful to Prof. Vladimir A. Dorodnitsyn for discuss ions concerning methods of Lie group analysis. References [1] J.-M. Lasry and P.-L. Lions, Mean ﬁeld games. I - The stationary c ase — Jeux ` a champ moyen. I - Le cas stationnaire, Comptes Rendus Mathematique , 343(9), 619–625, 2006 [2] J.-M. Lasry and P.-L. Lions, Mean ﬁeld games. II - Finite horizon an d optimal control — Jeux ` a champ moyen. II - Horizon ﬁni et contrˆ ole optimal, Comptes Rendus Mathematique , 343(10), 679–684, 2006 16[3] J.-M. Lasry and P.-L. Lions, Mean ﬁeld games, Japanese Journal of Mathematics , 2(1), 229–260, 2007 [4] M. Huang, R. P. Malham´ e and P. E. Caines, Large population stoc hastic dynamic games: closed-loop Mckean-Vlasov systems and the Nash certainty equiva lence principle. Commun. Inf. Syst. 6, 221–252, 2006 [5] M. Huang, R. P. Malham´ e and P. E. Caines, Large-population cos t-coupled LQG problems with nonuniform agents: individual-mass behavior and decentralized ǫ-Nash equilibria, IEEE Trans. Autom. Control , 52, 1560–1571, 2007 [6] R. J. Aumann, Markets with a continuum of traders. Econometrica, 32 (1-2), 39–50, 1964 [7] O. Gu´ eant, J.-M. Lasry and P.-L. Lions, Mean ﬁeld games and app lications, Lecture Notes in Mathematics 2003, 205–266, 2011. [8] P. Chan and R. Sircar, Bertrand and Cournot Mean Field Games, Applied Mathematics and Optimization , 71(3), 533–569, 2015 [9] P. J. Graber and C. Mouzouni, Variational Mean Field Games for Ma rket Competition, Springer INdAM Series , 28, 93–114, 2018 [10] R. Carmona and G. Dayanlkll, Mean Field Game Model for an Advert ising Competition in a Duopoly, International Game Theory Review , 23(4), 2150024, 2021 [11] R. Chenavaz, C., Paraschiv and G. Turinici, Dynamic Pricing of New Products in Compet- itive Markets: A Mean-Field Game Approach, Dynamic Games and Applications , 11(3), 463–490, 2021 [12] D. A. Gomes and J. Sa´ ude, A Mean-Field Game Approach to Price Formation, Dynamic Games and Applications , 11(1), 29–53, 2021 [13] D. Gomes, J. Gutierrez and R. Ribeiro, A Random Supply Mean Field Game Price Model, SIAM Journal on Financial Mathematics , 14(1), 188–222, 2023 [14] R. Carmona, J.-P. Fouque and L.-H. Sun, Mean ﬁeld games and sy stemic risk, Communi- cations in Mathematical Sciences , 13(4), 911–933, 2015 [15] R. Carmona, F. Delarue and D. Lacker, Mean Field Games of Timing and Models for Bank Runs, Applied Mathematics and Optimization , 76(1), 217–260, 2017 [16] G. Fu, P. Graewe, U. Horst and A. Popier, A mean ﬁeld game of op timal portfolio liqui- dation, Mathematics of Operations Research , 46(4), 1250–1281, 2021 [17] G. Fu and C. Zhou, Mean ﬁeld portfolio games, Finance and Stochastics , 27(1), 189–231, 2023 [18] G. Fu, Mean ﬁeld portfolio games with consumption, Mathematics and Financial Eco- nomics, 17(1), 79–99, 2023 17[19] A. Lachapelle, J.-M. Lasry, C.-A. Lehalle and P.-L. Lions, Eﬃcienc y of the price formation process in presence of high frequency participants: a mean ﬁeld ga me analysis Mathematics and Financial Economics , 10(3), 223–262, 2016 [20] A. Lachapelle and M.-T. Wolfram, On a mean ﬁeld game approach mo delling conges- tion and aversion in pedestrian crowds, Transportation Research Part B: Methodological , 45(10), 1572–1589, 2011 [21] M. Burger, M. Di Francesco, P. A. Markowich and M.-T. Wolfram , On a mean ﬁeld game optimal control approach modelling fast exit scenarios in human cro wds, Proceedings of the IEEE Conference on Decision and Control , 6760360, 3128–3133, 2013 [22] M. Burger, M. Di Francesco, P. A. Markowich and M.-T. Wolfram , Mean ﬁeld games with nonlinear mobilities in pedestrian dynamics, Discrete and Continuous Dynamical Systems - Series B , 19(5), 1311–1333, 2014 [23] Y. Achdou and J.-M. Lasry, Mean ﬁeld games for modelling crowd m otion, Computational Methods in Applied Sciences , 47, 17–42, 2019 [24] M. Burger, A. Lorz and M.-T. Wolfram, On a Boltzmann mean ﬁeld m odel for knowledge growth, SIAM Journal on Applied Mathematics , 76(5), 1799–1818, 2016 [25] P. Chan and R. Sircar, Fracking, renewables, and mean ﬁeld gam es, SIAM Review , 59(3), 588–615, 2017 [26] C. Alasseur, I. Ben Taher and A. Matoussi, An Extended Mean F ield Game for Storage in Smart Grids, Journal of Optimization Theory and Applications , 184(2), 644–670, 2020 [27] B. Djehiche, J. Barreiro-Gomez and H. Tembine, Price Dynamics for Electricity in Smart Grid Via Mean-Field-Type Games, Dynamic Games and Applications , 10(4), 798–818, 2020 [28] R. A¨ ıd, R. Dumitrescu and P. Tankov, The entry and exit game in the electricity markets: a mean-ﬁeld game approach, Journal of Dynamics and Games , 8(4), 331–358, 2021 [29] R. Carmona, G. Dayanıklı and M. Lauri` ere, Mean Field Models to Regulate Carbon Emis- sions in Electricity Production, Dynamic Games and Applications , 12(3), 897–928, 2022 [30] Y. Achdou, P.-N. Giraud, J.-M. Lasry and P.-L. Lions, A Long-Te rm Mathematical Model for Mining Industries, Applied Mathematics and Optimization , 74(3), 579–618, 2016 [31] Y. Achdou, C. Bertucci, J.-M. Lasry, P.-L. Lions, A. Rostand a nd J. A. Scheinkman, A class of short-term models for the oil industry that accounts for speculative oil storage, Finance and Stochastics , 26(3), 631–669, 2022 [32] D.A. Gomes and J. Sa´ ude, Mean Field Games Models - A Brief Surve y, Dynamic Games and Applications , 4(2), 110–154, 2014 [33] M. Huang, R. P. Malham´ e and P. E. Caines, Mean Field Games, In T . Ba¸ sar and G. Zaccour (eds) Handbook of Dynamic Game Theory, Springer, 2018 18[34] R. Carmona and F. Delarue, Probabilistic Theory of Mean Field Games with Applications I: Mean Field FBSDEs, Control, and Games , Probability Theory and Stochastic Modelling 83, Springer, 2018 [35] R. Carmona and F. Delarue, Probabilistic Theory of Mean Field Games with Applications II: Mean Field Games with Common Noise and Master Equations Probability Theory and Stochastic Modelling 84, Springer, 2018 [36] P. Cardaliaguet, F. Delarue, J.-M. Lasry and P.-L. Lions, The master equation and the convergence problem in mean ﬁeld games , Annals of Mathematics Studies 201, Princeton University Press, 2019 [37] P. Cardaliaguet and A. Porretta, An introduction to mean ﬁeld g ame theory, in P. Cardaliaguet and A. Porretta (Eds.) Mean Field Games, Lecture Not es in Mathematics 2281 (Cetraro, Italy 2019), pp. 1– 158, 2020 [38] R. Naz, F. M. Mahomed and D. P. Mason, Comparison of diﬀerent approaches to conserva- tion laws for some partial diﬀerential equations in ﬂuid mechanics, Appl. Math. Comput. , 205 (1), 212–230, 2008 [39] E. Noether, Invariante variations problem. Konigliche Gesellschaft der Wissenschaften zu Gottingen, Nachrichten, Mathematisch-Physikalische Kla sse Heft 2 , pages 235–257, 1918. English translation: Transport Theory and Statist. Phys., 1(3), 1 971, 183–207 [40] L. V. Ovsiannikov, Group Analysis of Diﬀerential Equations , Nauka, Moscow, 1978. En- glish translation, W. F. Ames, Ed., published by Academic Press, New Y ork, 1982 [41] N. H. Ibragimov, Transformation Groups Applied to Mathematical Physics , Reidel, Boston, 1985 [42] P. J. Olver, Applications of Lie Groups to Diﬀerential Equations , Springer, New York, 1986 [43] E. Bessel-Hagen, ¨Uber die Erhaltungss¨ atze der Elektrodynamik, Math. Ann. , 84(3-4), 258–276, 1921 [44] A. V. Bobylev and N. Kh. Ibragimov, Interconnectivity of symm etry properties for equa- tions of dynamics, kinetic theory of gases, and hydrodynamics, Matem. Mod. , 1(3), 100– 109, 1989 [45] D. A. Gomes, L. Nurbekyan and M. Sedjro, One-dimensional fo rward–forward mean-ﬁeld games, Applied Mathematics and Optimization , 74(3), 619–642, 2016 [46] D. Gomes, L. Nurbekyan and M. Sedjro, Conservation laws aris ing in the study of for- ward–forward mean-ﬁeld games, Springer Proceedings in Mathematics and Statistics , 236, 643–649, 2018 [47] E. Hairer, C. Lubich and G. Wanner, Geometric Numerical Integration: Structure- Preserving Algorithms for Ordinary Diﬀerential Equations , Springer, 2006 19[48] Y. Achdou and M. Lauri` ere, Mean ﬁeld games and applications: Numerical aspects, in P. Cardaliaguet and A. Porretta (Eds.) Mean Field Games, Lecture Not es in Mathematics 2281 (Cetraro, Italy 2019), pp. 249–307, 2020 [49] W. H. Fleming and R. W. Rishel, Deterministic and Stochastic Optimal Control , Springer Science & Business Media, Berlin, 2012 [50] J. Yong and X. Y. Zhou, Stochastic controls: Hamiltonian systems and HJB equation s, Springer Science & Business Media, Berlin, 1999 Appendices A A sketch of the derivation of the MFG system In this Appendix, we sketch the derivation of the system (2.1), whic h describes Nash equilibrium in a game with a continuum of agents. The presentation follows [37]. W e refer to this book chapter for details and to [49, 50] for the optimal control theory . Each agent exercises control α in the stochastic diﬀerential equation in Rd dXs = b(Xs, αs, m(s, Xs))ds + √ 2εdBs, (A.1) where Bs is a Brownian motion. Here m(t, x) is the density of the probability measure in Rd, which describes the distribution of the agents. The agent minimizes E [ ∫ T 0 L(Xs, αs, m(s, Xs))ds + G(XT , m(T, X T )) | X0 = x ] , (A.2) where L(x, α, m) and G(x, m) are running and terminal costs. They depend on the agent’s stat e x and the distribution of agents m. The running cost L also depends on the agent’s control α. This problem provides the value function u(t, x) = inf α E [ ∫ T t L(Xs, αs, m(s, Xs))ds + G(XT , m(T, X T )) | Xt = x ] (A.3) The inﬁmum is taken over admissible controls. Function Xs satisﬁes SDE (A.1) with the initial condition Xt = x. Each agent makes an inﬁnitesimally small contribution to the populatio n dynamics. There- fore, the agent gets a problem of individual optimization with given tim e dependent distribution of the other agents. For the value function, we get PDE (2.1a) with H(x, p, m) = sup α [−b(x, α, m) · p − L(x, α, m)]. (A.4) The optimal control has the feedback form α∗ = α∗(t, x) and b(x, α∗(t, x), m(t)) = −Hp(x, ∇u, m(t)). (A.5) If the agents exercise the optimal behaviour and their noises Bs are independent, the evo- lution of the agents’ distribution is given by the forward Kolmogorov equations (2.1b) 20B Preliminary analysis of the determining equations Preliminary analysis identiﬁes cases of the Hamiltonian function H(ux), which should be con- sidered. Lie point symmetries X = ξt ∂ ∂t + ξx ∂ ∂x + ηu ∂ ∂u + ηm ∂ ∂m (B.1) of the MFG systen F1 = −ut − εuxx + H(ux) = f(m), (B.2a) F2 = mt − εmxx − mxH′(ux) − mH′′(ux)uxx = 0 (B.2b) are speciﬁed by the determining equations E1 = X(F1)|F1=0,F 2=0 = 0 , (B.3a) E2 = X(F2)|F1=0,F 2=0 = 0 . (B.3b) As discussed in Section 3, the generator X is assumed to be prolonged to all variables involved in the equations. Application of the prolonged generator X to the equations F1 = 0 and F2 = 0 gives the system of two PDEs for the coeﬃcients of the generator X. The generator coeﬃcients depend on the independent and dependent variables t, x, u and m. The system also depends on the functions H(ux) and f(m) and their derivatives. Overall, the system depends on the variable s {t, x, u, m, u t, mt, ux, mx, utx, mtx, uxx, mxx}. Since the action of the prolonged generator X to the MFG equations is considered on the solutions of the MFG equation s, we eliminate two of these variables. Substituting mxx = 1 ε(mt − mxH′(ux) − mH′′(ux)uxx) (B.4) and then uxx = 1 ε(−ut + H(ux) − f(m)), (B.5) we eliminate uxx and mxx. These substitutions lead to the equations E1(t, x, u, m, u t, mt, ux, mx, utx, mtx) = X(F1)|(B.4), (B.5) = 0 , (B.6a) E2(t, x, u, m, u t, mt, ux, mx, utx, mtx) = X(F2)|(B.4), (B.5) = 0 . (B.6b) In detail, these equations take the form E1 = ( −ζu t − εζu xx + H′(ux)ζu x − f′(m) + ηm)|(B.4), (B.5) = 0 , E2 = ( ζm t − εζm xx − ζm x H′(ux) − mxH′′(ux)ζu x −ηmH′′(ux)uxx − mH′′′(ux)ζu x uxx − mH′′(ux)ζu xx)|(B.4), (B.5) = 0 . Clearly, these equations can be split for ut, mt, mx, utx and mtx because the generator coeﬃ- cients, H(ux) and f(m) do not depend on these variables. We proceed as follows. 211. From ∂E1 ∂utx = 2 ε(ξt x + ξt uux + ξt mmx) = 0 , where ξt = ξt(t, x, u, m ), we obtain ξt = A(t), where A(t) is an arbitrary function. 2. The equation ∂2E1 ∂m2 x = 2 ε(ξx mmux − ηu mm) = 0 for ξx = ξx(t, x, u, m ) and ηu = ηu(t, x, u, m ) gives ξx = B1(t, x, u)m + B0(t, x, u) and ηu = C1(t, x, u)m + C0(t, x, u). 3. The equation ∂4E2 ∂u2 x∂m2 x = 2( B1(t, x, u)ux − C1(t, x, u))H′′′′(ux) = 0 identiﬁes the special case of the Hamiltonian function H(ux) = h3u3 x + h2u2 x + h1ux + h0. This case will be treated separately. It is convenient to split this cas e into two: the case of the cubic Hamiltonian H(ux) = hu3 x + h2u2 x + h1ux + h0, h ̸= 0 (B.7) and the case of the quadratic Hamiltonian H(ux) = hu2 x + h1ux + h0, h ̸= 0 . (B.8) Linear Hamiltonians are excluded by requirement (2.9). We continue with the assumption that the Hamiltonian has a form diﬀer ent from (B.7) and (B.8). It follows B1(t, x, u) = 0 , C 1(t, x, u) = 0 . 4. The equation ∂2E2 ∂m2 x = −2εηm mm = 0 for ηm = ηm(t, x, u, m ) provides ηm = G1(t, x, u)m + G0(t, x, u). 225. From ∂2E2 ∂ux∂mt = 2( B0(t, x, u))u = 0 we get B0(t, x, u) = ˜B0(t, x). After all these simplifying steps, we obtain the symmetry coeﬃcient s in the form ξt = A(t), ξ x = ˜B0(t, x), η u = C0(t, x, u), η m = G1(t, x, u)m + G0(t, x, u). These such coeﬃcients we obtain the equation ∂3E2 ∂u2 x∂mx = ( F1(t, x, u)ux + F2(t, x, u))H′′′′(ux) + F3(t, x, u)H′′′(ux) = 0 , where functions F1, F2 and F3 depend on the derivatives of the functions ˜B0(t, x) and C0(t, x, u). This equation is the classifying equation for the Hamiltonian function. In addition to the cases (B.7) and (B.8) found earlier, we get two more cases H(ux) = h(ux + q)p + h2u2 x + h1ux + h0, h ̸= 0 , p ̸= 0 , 1, 2, 3 (B.9) and H(ux) = hekux + h2u2 x + h1ux + h0, h ̸= 0 , k ̸= 0 . (B.10) Further examination of the determining equations does not provide any other special cases of the Hamiltonian function. Thus, in addition to the special cases (B.7) , (B.8), (B.9) and (B.10), we have only the general case of the arbitrary H(ux). This is the end of the preliminary analysis of the determining equations . After this we proceed with the speciﬁed forms of the Hamiltonian function. We sho uld return to the deter- mining equations (B.3) and solve them for the diﬀerent cases of H(ux). It provides the admitted symmetries and the special cases of function f(m). 23",
      "references": [
        "Mean ﬁeld games. I - The stationary c ase — Jeux ` a champ moyen. I - Le cas stationnaire",
        "Mean ﬁeld games. II - Finite horizon an d optimal control — Jeux ` a champ moyen. II - Horizon ﬁni et contrˆ ole optimal",
        "Mean ﬁeld games",
        "Large population stoc hastic dynamic games: closed-loop Mckean-Vlasov systems and the Nash certainty equiva lence principle.",
        "Large-population cos t-coupled LQG problems with nonuniform agents: individual-mass behavior and decentralized ǫ-Nash equilibria",
        "Markets with a continuum of traders.",
        "Mean ﬁeld games and app lications",
        "Bertrand and Cournot Mean Field Games",
        "Variational Mean Field Games for Ma rket Competition",
        "Mean Field Game Model for an Advert ising Competition in a Duopoly",
        "Dynamic Pricing of New Products in Compet- itive Markets: A Mean-Field Game Approach",
        "A Mean-Field Game Approach to Price Formation",
        "A Random Supply Mean Field Game Price Model",
        "Mean ﬁeld games and sy stemic risk",
        "Mean Field Games of Timing and Models for Bank Runs",
        "A mean ﬁeld game of op timal portfolio liqui- dation",
        "Mean ﬁeld portfolio games",
        "Mean ﬁeld portfolio games with consumption",
        "Eﬃcienc y of the price formation process in presence of high frequency participants: a mean ﬁeld ga me analysis",
        "On a mean ﬁeld game approach mo delling conges- tion and aversion in pedestrian crowds",
        "On a mean ﬁeld game optimal control approach modelling fast exit scenarios in human cro wds",
        "Mean ﬁeld games with nonlinear mobilities in pedestrian dynamics",
        "Mean ﬁeld games for modelling crowd m otion",
        "On a Boltzmann mean ﬁeld m odel for knowledge growth",
        "Fracking, renewables, and mean ﬁeld gam es",
        "An Extended Mean F ield Game for Storage in Smart Grids",
        "Price Dynamics for Electricity in Smart Grid Via Mean-Field-Type Games",
        "The entry and exit game in the electricity markets: a mean-ﬁeld game approach",
        "Mean Field Models to Regulate Carbon Emis- sions in Electricity Production",
        "A Long-Te rm Mathematical Model for Mining Industries",
        "A class of short-term models for the oil industry that accounts for speculative oil storage",
        "Mean Field Games Models - A Brief Surve y",
        "Mean Field Games",
        "Probabilistic Theory of Mean Field Games with Applications I: Mean Field FBSDEs, Control, and Games",
        "Probabilistic Theory of Mean Field Games with Applications II: Mean Field Games with Common Noise and Master Equations",
        "The master equation and the convergence problem in mean ﬁeld games",
        "An introduction to mean ﬁeld g ame theory",
        "Comparison of diﬀerent approaches to conserva- tion laws for some partial diﬀerential equations in ﬂuid mechanics",
        "Invariante variations problem.",
        "Group Analysis of Diﬀerential Equations",
        "Transformation Groups Applied to Mathematical Physics",
        "Applications of Lie Groups to Diﬀerential Equations",
        "¨Uber die Erhaltungss¨ atze der Elektrodynamik",
        "Interconnectivity of symm etry properties for equa- tions of dynamics, kinetic theory of gases, and hydrodynamics",
        "One-dimensional fo rward–forward mean-ﬁeld games",
        "Conservation laws aris ing in the study of for- ward–forward mean-ﬁeld games",
        "Geometric Numerical Integration: Structure- Preserving Algorithms for Ordinary Diﬀerential Equations",
        "Mean ﬁeld games and applications: Numerical aspects",
        "Deterministic and Stochastic Optimal Control",
        "Stochastic controls: Hamiltonian systems and HJB equation s"
      ],
      "meta_data": {
        "arxiv_id": "2305.06871v1",
        "authors": [
          "Roman Kozlov"
        ],
        "published_date": "2023-05-11T15:15:23Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Studies conservation laws for (second-order, ε>0) mean field games (MFG) PDE systems with separable Hamiltonians, exploiting their variational (Euler–Lagrange) structure. Provides a Lie point symmetry classification for the 1D, state-independent system −u_t−εu_xx+H(u_x)−f(m)=0 and m_t−εm_xx−(mH'(u_x))_x=0 under genuine coupling assumptions H''≠0, f'≠0. Identifies the base symmetry algebra valid for arbitrary H,f (time/space/value translations) and pinpoints special forms of H (power, exponential, quadratic; cubic as a subcase) and f (logarithmic; power laws) that yield extended symmetry groups. Uses variational/divergence symmetries plus Noether’s theorem to derive the complete set of corresponding local conservation laws: three generic laws for arbitrary H,f, and additional laws in the special cases (including Galilean-type and scaling/projective-related laws for quadratic H). Interprets several conservation laws in an MFG control setting (probability mass conservation, momentum-like/average-control conservation, center-of-mass-type law). Claims novelty for second-order (ε>0) MFG conservation laws compared to prior ε=0 work.",
        "methodology": "(1) Restrict to separable Hamiltonians to obtain a variational formulation; in the target setting (d=1, H=H(u_x), f=f(m)), use Lagrangian L=−m u_t+ε m_x u_x + m H(u_x) − F(m) with F'(m)=f(m), and derive the MFG system as Euler–Lagrange equations. (2) Compute Lie point symmetries using the infinitesimal invariance criterion with prolonged generators; reduce/classify determining equations using equivalence transformations (shifts in u by Ax/Bt, Galilean-like x shift, and scalings preserving ∫m dx=1 via C2 C4=1). (3) Solve determining equations for arbitrary H,f to get the base symmetries, then perform group classification to find special functional forms of H (power/exponential/quadratic) and f (log, power) that enlarge the symmetry algebra. (4) Select which admitted symmetries are variational or divergence symmetries of the Lagrangian (via Noether/Bessel-Hagen condition XL+L div ξ = div V). (5) Apply Noether’s theorem to compute explicit conserved currents (T^t,T^x) for each qualifying generator, yielding conservation laws Dt T^t + Dx T^x=0.",
        "experimental_setup": "No empirical/benchmark experimentation is performed. Validation is analytic: (i) derive Euler–Lagrange structure for the separable MFG system; (ii) compute and solve symmetry determining equations to classify Lie point symmetries (Theorem 4.1) up to equivalence transformations; (iii) verify variational/divergence symmetry conditions and apply Noether’s theorem to obtain explicit conserved currents (Theorem 5.1); (iv) cross-check interpretations against a canonical stochastic control MFG with quadratic control cost leading to H(p)=p^2/2. No datasets, numerical experiments, or quantitative benchmarks are used.",
        "limitations": "Scope is restricted to the simplest variational second-order MFGs: one spatial dimension, state-independent H=H(u_x) and coupling f=f(m), and diffusion ε>0. Assumes genuine coupling (H'' not identically zero, f' not identically zero), excluding decoupled/linear cases. Conservation-law classification is local and tied to Lie point symmetries; nonlocal, higher-order/generalized symmetries and associated conserved quantities are not explored. Additional symmetries require very specific functional forms (power/exponential/quadratic), so results may not transfer to common state-dependent or multidimensional MFGs without re-derivation. Boundary/terminal conditions and domains are largely ignored in symmetry/conservation analysis, so applicability under realistic boundary conditions (bounded domains, periodic/absorbing boundaries) is not addressed. The work is theoretical; implications for existence/uniqueness, regularity, and numerical scheme design are only discussed qualitatively.",
        "future_research_directions": "Extend symmetry and conservation-law analysis to broader variational MFG systems: (i) multidimensional state space with state-independent H(∇u) and f(m); (ii) state-dependent H(x,∇u) and f(x,m) in 1D or higher dimensions; (iii) the fully general separable system H(x,∇u)−f(x,m) and beyond separable cases via alternative variational/adjoint formulations. Study stationary (time-independent) and ergodic MFGs and classify their conservation laws. Investigate first-order (ε=0) vs second-order (ε>0) connections and singular limits. Use admitted symmetries to construct invariant reductions and explicit/invariant solutions for MFG systems, an avenue noted as unexplored. Develop structure-preserving numerical schemes that enforce the derived conservation laws (beyond mass conservation), and assess their impact on stability/accuracy. Explore nonclassical, potential, and generalized symmetries and the resulting nonlocal conservation laws.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Data Feedback Loops: Model-driven Amplification of Dataset Biases",
      "full_text": "From explained variance of correlated components to PCA without orthogonality constraints Marie Chavent∗ Guy Chavent† Abstract Block Principal Component Analysis (Block PCA) of a data matrix A, where loadings Z are determined by maximization of ∥AZ∥2 over unit norm orthogonal loadings, is difficult to use for the design of sparse PCA by ℓ1 regularization, due to the difficulty of taking care of both the orthog- onality constraint on loadings and the non differentiable ℓ1 penalty. Our objective in this paper is to relax the orthogonality constraint on loadings by introducing new objective functions expvar(Y ) which measure the part of the variance of the data matrix A explained by correlated components Y = AZ. So we propose first a comprehensive study of mathematical and numerical properties of expvar( Y ) for two existing definitions Zou et al. [2006], Shen and Huang [2008] and four new definitions. Then we show that only two of these explained variance are fit to use as objective function in block PCA formulations for A rid of orthogonality constraints. Keywords: PCA, sparsity, dimension reduction, explained variance, orthogo- nality constraints, block optimization. ∗Universit´ e de Bordeaux, CNRS, INRIA, Bordeaux INP, IMB, UMR 5251, Talence, France e-mail : marie.chavent@u-bordeaux.fr (corresponding author) †Retired, collaborator to the SERENA Project Team, INRIA-Paris 2 rue Simone Iff, 75589 Paris, France arXiv:2402.04692v1  [stat.ML]  7 Feb 20241 Introduction Many linear data analysis methods construct new variables that “best” summa- rize the columns of a n × p data matrix A where n observations are described by p numerical variables. The m < pnew variables are the columns of Y = AZ where Z is the p × m loading matrix. These new variables are for instance principal components in PCA (Principal Component Analysis), canonical com- ponents in CA (Canonical Analysis) or PLS components in PLS (Partial Least Squares) regression. When the components are orthogonal (which is the case for PCA), it is usual to assess the quality of the components by measuring the part of the variance of A explained by Y = AZ with : expvar(AZ) = ∥AZ∥2 F , where the subscript F denotes the Frobenius norm. But this definition fails when the components are correlated. Correlated components appear when sparsity is introduced into the loading matrix Z to select the important original variables. The loading vectors and the components are no longer necessarily orthogonal. Two definitions have already been pro- posed to measure the explained variance of correlated components : theadjusted variance of Zou et al. [2006] and the total variance of Shen and Huang [2008]. Because there is no single definition, we introduce a set of three conditions to be satisfied by any explained variance definition and we propose a comprehensive study of mathematical and numerical properties of these two existing definitions together with four new definitions. We prove first that the total variance of Shen and Huang [2008] called sub- space explained variance hereafter, satisfies only two of three conditions. This lead us to propose other definitions by projection or by normalization on a set X = [x1, ..., xm] of orthonormal vectors which “points in the same direction” as the components Y = [ y1, ..., ym]. Such orthonormal vectors X can be as- sociated to Y by three different rules: QR or UP (polar) decomposition of Y , or maximization of the explained variance. This leads to five definitions ( two normalized and three projected explained variance) : - the QR and the UP normalized explained variances (QRnormVar and UPnormVar), - the QR projected explained variance (QRprojVar) which is the adjusted variance of Zou et al. [2006], - the UP projected explained variance (UPprojVar), - the optimal projected explained variance (optprojVar). We prove that the five above definitions satisfy also the two first compatibility conditions, but that only the three projected variances, satisfy the third one, and hence are proper explained variances. 1Then we investigate both theoretically and numerically the size of the six explained variances, the existence of order between them, and their ability to rank in the same order - or not - different sets of components Y , which is for what they have been introduced at the origin. Finally, we study the ability of the three proper definitions of explained variance to provide a substitute to the classical Block PCA formulation : maximizing ∥AZ∥2 F under the orthonormality constraint ZT Z = Im, (1) by replacing (1) by a Explained Variance block PCA formulation : max ∥zj∥=1,j=1...m expvar(AZ) , (2) which is rid of orthogonality constraints on loadings Z = [z1 . . . zm], and hence particularly suited as starting point for the design of sparse PCA algorithms.. As a conclusion of this study, we single out the optimal projected explained variance of Y = AZ : expvar(Y ) = max XT X=Im X j=1...m ⟨yj , xj⟩2 , (3) which possesses, after introduction of weights, the desired properties in the sense that it is easy to compute and differentiate, and admits as unique maximizer the SVD solution of PCA Z∗ = [v1 . . . vm] made of the m first right singular vectors of A. This will be the starting point for the design of an efficient group-sparse block PCA algorithm in the companion paper [Chavent and Chavent, 2023]. The paper is organized as follows: In Section 2 we motivate the need for an explained variance definition, and establish necessary conditions to be satisfied by any definition of the variance explained by a set of non-necessarily orthogonal components. Section 3 establishes the properties of the subspace explained variance (total variance) of Shen and Huang [2008]. Section 4 is devoted to the definition of three projected (including the adjusted variance of Zou et al. [2006] and the optimal variance (3)) and two normalized explained variances, together with the study of their mathematical properties. Sections 5 and 5.3 present numerical comparisons of the size of the variance explained by the six definitions, and of their ability to rank in the same order (or not) non necessarily orthogonal components. Finally, Section 6 compares the ability of the three projected variance definitions to discriminate the singular value solution of PCA as their unique maximizer, which makes them fit to use as Block PCA objective function. It should be noted that we have implemented the six explained variance definitions in R [R Core Team, 2021] in the package sparsePCA available at https://github.com/chavent/sparsePCA. 22 Defining variance explained by components when loadings are non orthogonal We set us from now on in the context of Principal Component Analysis (PCA), where one seeks a small number m of uncorrelated components yj = Azj, j= 1 . . . mby combining the p columns of a data matrix A, each containing n samples of a centered variable, with m unit norm loading vectors zj, j= 1 . . . m, in such a way that the components yj retain the largest possible part of the total variance of A. We denote by expvar(Y ) = the part of the variance of A explained by Y = AZ (4) and by ∥A∥2 F = X i,j=1...m a2 i,j the (total) variance of A, where ∥ • ∥F is the Frobenius norm of A . At this point, expvar(Y ) is a still loosely defined quantity outside of the solution of PCA given by : Z = Vm, Y = Um Σm = AVm , (5) where Um and Vm contain them first left and right singular vectors and Σm is the diagonal matrix of them first singular values of the singular value decomposition (SVD) of A : A = UΣV T with UT U = Ir , V T V = Ir , Σ = diag(σ1, . . . , σr) = r × r matrix with σ1 ≥ σ2 ≥ ··· ≥σr > 0 , where r is the rank of A, and the columns u1 . . . ur of U and v1 . . . vr of V are the left and right singular vectors of A. The principal components yj = Azj = σjuj are orthogonal, and hence un- correlated, so the sum of their variance ∥yj∥2 represents the part of the total variance ∥A∥2 F of A explained by these m principal components. So we see that the variance explained by the principal components Y is : expvar(Y ) = ∥Y ∥2 F = X j=1...m ∥yj∥2 = X j=1...m σ2 j ≤ X j=1...r σ2 j = ∥A∥2 F . (6) However, when sparsity constraints are introduced into loading vectors like in sparse PCA for instance, non orthogonal loadings and components are gener- ated and one has to face the problem of defining expvar( Y ) for possibly non orthogonal components. Alas, formula (6) for expvar( Y ) is strictly limited to the case of orthogonal components and loadings, as we see now. Consider first the case of non orthogonal components : take for example for Z an orthonormal basis of span {Vm} but different from Vm. Then components Y = AZ are not anymore orthogonal, and hence correlated, so the sum of their variances (the total variance of Y ) is too optimistic, and one expects that expvar(Y ) < ∥Y ∥2 F , (7) 3which shows that ∥Y ∥2 F is not a satisfying definition of expvar( Y ) in this case. Then take for example a data matrix A with three singular values 3, 2, 1 and hence a total variance of 14. Then chose for Z two linearly independant unit vectors close to the first right singular vector v1. Then : ∥AZ∥2 F = ∥Y ∥2 F = ∥y1∥2+∥y2∥2 = ∥Az1∥2 | {z } ≃σ2 1=9 + ∥Az2∥2 | {z } ≃σ2 1=9 ≃ 18 > 9 + 4 + 1| {z } σ2 1+σ2 2+σ2 3 = 14 = ∥A∥2 F , which violates property expvar(Y ) ≤ ∥A∥2 F implied by (4), so once again ∥Y ∥2 F is not suited as a definition of expvar( Y ). A last motivation for the search of definitions of expvar( Y ) : rather than solving the classical Block PCA formulation (1), why not solve the explained variance Block PCA formulation (2) by maximizing expvar( Y ) under the sole unit norm constraint on the loadings Z ? Sparse block PCA formulations based on such explained variance objective function eliminate the difficulty caused by the orthogonality constraints on the loadings, and, by construction, rule directly the balance between sparsity and explained variance. So we address first in this paper the problem of defining the part of the variance of A explained by components Y = AZ, under the sole condition that : ∥zj∥ = 1, j= 1 . . . m ,rank(Z) = rank(Y ) = m ≤ r . (8) In absence of a sound definition for the explained variance of correlated compo- nents, we define a set of hopefully reasonable necessary conditions to be satisfied by any such definition : • Condition 1: when Y, Zhappen to be the SVD solution of PCA given by (5), expvar(Y ) has to provide the exact value given by (6) : expvar(Y ) = X j=1...m σ2 j ≤ X j=1...r σ2 j = ∥A∥2 F for m = 1 . . . r .(9) • Condition 2: when Y, Zsatisfy only (8), the components are not orthog- onal anymore, and one expects that, because of the correlation between the components, the part of the variance of A explained by Y is smaller than that of the PCA solution : expvar(Y ) ≤ X j=1...m σ2 j . (10) • Condition 3: expvar(Y ) has to take into account the possible correlation of the components as expected in (7) : expvar(Y ) ≤ X j=1...m ∥yj∥2 = ∥Y ∥2 F , with equality only if the components are orthogonal. 4Any explained variance definition expvar( AZ) which satisfies these conditions achieves its PCA maximum valueP j=1...r σ2 j (Condition 2) for Z = Vm (Condi- tion 1), and hence provides a block formulation for PCA without orthogonality constraints on loadings, which can be used to derive sparse PCA algorithms. This point os view will be developped in Section 6 below. We propose now definitions for the explained variance of the components Y = AZ associated to any matrix Z of m ≤ r linearly independant - but not necessarily orthogonal - unit norm loading vectors zj, j = 1, ..., m. These definitions will include those introduces by Shen and Huang [2008] (see section 3 below) and Zou et al. [2006] (see section 4.2). 3 Subspace explained variance We start here from a reformulation of the explained variance (6) of the principal components Y = AZ, based on the subspace spanned by Z = Vm. Let P Vm denotes the orthogonal projection on this subspace. Then P Z = PVm = VmV T m , so that : expvar(Y ) = ∥Y ∥2 F = ∥A PZ∥2 F = ∥A PVm∥2 F = X j=1...m σ2 j . (11) When Z satisfies (8) only, we proceed by analogy with (11), and define the subspace explained variance of Y = AZ by : expvarsubsp (Y ) def = ∥APZ∥2 F = tr \b Y T Y (ZT Z)−1) \t , (12) where P Z = Z(ZT Z)−1ZT denotes the projection matrix on the subspace spanned by Z. This shows that subspace explained variance is the Rayleigh quotient associated to A, Z. Note that with this definition, where the explained variance depends only of the subspace spanned by [z1 . . . zm], the normalization of the loadings zj is not necessary. Of course, we will still continue to represent loadings by unit norm vectors - but this is here only a convenience. The subspace explained variance coincides with the total explained variance introduced by Shen and Huang [2008, section 2.3 p. 1021], which they proved was increasing with the number of loadings, and bounded by the variance ∥A∥2 F of the data. The next lemma gives a complete picture of its properties : Lemma 3.1. (Subspace explained Variance) Let Z satisfy (8). Then the sub- space explained variance defined by (12) satisfies : expvarsubsp (Y ) =σ2 1+··· +σ2 m = Max ⇔ span{Z} = span{Vm} . (13) and hence satisfies conditions 1 and 2. However, it does not satisfy condition 3 : - when the components Y = AZ happen to be orthogonal : expvarsubsp (Y ) ≥ ∥Y ∥2 F with equality iff Z perm = Vm , (14) 5where “ perm = ” denotes the equality of matrices up to a column permutation and multiplicatipn by ±1, - when the loadings Z happen to be orthogonal : expvarsubsp (Y ) = ∥Y ∥2 F with non necessarily orthogonal components . (15) The proof is in Section 8.2 of the Appendix. This lemma shows that expvarsubsp (Y ) verifies only conditions 1 and 2, and overestimates the explained variance in two cases : - when components Y are orthogonal without pointing in the direction of the left singular vectors, inequality (14) implies expvar subsp (Y ) > ∥Y ∥2 F , which contradicts condition 3, - when loadings Z are orthogonal without pointing in the direction of the right singular vectors, then the components are correlated and (15) contra- dicts condition 3, which requires in that case that expvarsubsp (Y ) < ∥Y ∥2 F . We explore in the next section other directions in the hope of being able to comply with all conditions 1, 2 and 3. 4 Projected and normalized explained variances We start now from definition (6) of the explained variance in the case of PCA. A natural generalization would be : expvar(Y ) ? = X j=1...m ∥yj∥2 = ∥Y ∥2 F = ∥AZ∥2 F , which, as we have seen in the Section 2, is not anymore an an acceptable defi- nition when components are correlated. However, this definition continues to make perfect sense for the variance explained by components as long as they are orthogonal, without pointing nec- essarily in the direction of left singular vectors. Hence a natural way to eliminate the redundancy caused by the orthogonality default of the components Y and to satisfy Condition 3 is to : 1. Step 1: choose a rule to associate to the components Y an orthonormal basis X of span{Y } that, loosely speaking, “points in the direction of the components Y ”, and, when the components Y happen to be orthogonal, points in the directions of Y itself. So the rule for the choice of the basis X associated to Y has to satisfy : XT X = Im , span{X} = span{Y } , < yj, xj > ≥ 0 ∀j = 1 . . . m , ⟨yj, yk⟩ = 0 ∀j ̸= k =⇒ xj = yj/∥yj∥ ∀j = 1 . . . m . (16) 6Examples of such rules are : QR decomposition : Y ⇝ X = Q solution of Y = Q R , QT Q = Im where R is an upper triangular matrix , Polar decomposition : Y ⇝ X = U solution of Y = U P , UT U = Im where PT = P ∈ I Rm×m , P ≥ 0 . (17) 2. Step 2: associate to Y orthogonal adjusted components Y ′ along the X axes, and define the variance explained by the components Y by : expvar(Y ) def = ∥Y ′∥2 F . Two ways for obtaining the adjusted components Y ′ are considered hereafter: by projection or by normalization as illustrated in Figure 1. y1 y2 y′2 x1 x2 y′1 E2 y′1 y′2 Figure 1: Illustration of projected and normalized explained variances : The ellipse E2 represents the image by A of all unit norm loadings. Let the X = [x1, x2] be the orthonormal vectors associated to the correlated components Y = [ y1, y2]. Then the extremities of the adjusted components Y ′ = [ y′ 1, y′ 2] obtained by projection are the two blue dots on the x1, x2 axes, and those obtained by normalization are the two green dots located at the intersection of the axes with E2. 4.1 Normalized explained variances We consider first in this section the case where the adjusted components Y ′ in step 2 are obtained by “normalization” in the directions of the chosen orthonor- mal basis X of span{Y }. More precisely, the idea is to choose the abscissa of 7y′ j on the xj axis by requiring that y′ j is the image by A of some unit norm ad- justed loading z′ j. This is illustrated in Figure 1 : one associates to components Y = [y1, y2] the adjusted components Y ′ = [y′ 1, y′ 2] whose extremities are the points on the x1, x2 axes located on the ellipse image by A of the unit ball of the loading space. In order to determine the adjusted loadings z′ j, one computes first (non necessarily unit norm) loadings T = [t1, . . . , tm] by performing on the loadings zj the same linear combinations M : Y = XM , (18) that transformed Y into X, which leads to define T by : Z = T M . (19) Multiplying then by A left gives Y = AZ = AT Mand, comparison with (18) shows that X = AT and so xj = Atj, j= 1 . . . m. The adjusted components y′ j are then defined by : y′ j = Az′ j with z′ j = tj/∥tj∥ so that ∥z′ j∥ = 1 , y′ j = xj/∥tj∥ , j = 1 . . . m , (20) and the normalized explained variance of Y estimated with X is defined by : expvarX norm(Y ) = ∥Y ′∥2 F = X j=1...m 1/∥tj∥2 , (21) where T = [t1, . . . , tm] is given by (19) and (18). Before specifying the rule of step 1 which associates an orthonormal basis X to the components Y , we give some properties of expvarX norm(Y ) which hold independently of the chosen basis X. Lemma 4.1. (Normalized explained variance) For any unit norm loadings Z and any basis X chosen according to the rule (16), the normalized explained variance of Y = AZ defined by(21) satisfies conditions 1 and 2 and : expvarX norm(Y ) ≤ expvarsubsp (Y ) ≤ σ2 1 + ··· + σ2 m . (22) Lemma 4.1 follows from Lemma 3.1 applied to the orthogonal components Y ′ : expvarX norm(Y ) def = ∥Y ′∥2 F ≤ expvarsubsp (Y ′) ≤ σ2 1 + ··· + σ2 m , (23) which proves (22). We can now specify the rules for the choice of X to define two normalized explained variance satisfying conditions 1 and 2. QR normalized variance. Let X be defined by the QR-decomposition Y = XR of the components Y = AZ as recalled in (17) . Then (21) leads to another definition of variance : expvarQR norm(Y ) = X j=1...m 1/∥tj∥2 , Z = T R , Rupper triangular . (24) The normalized variance expvar QR norm(AZ) does not satisfy condition 3, as the counter example of Figure 2, left, shows. 8x1 y1 y2 y′1 y2 y1 x1 x2x2 = E2 y′2 y′1 y′2 Figure 2: Counterexamples for property 3. Left :Let X denote the basis asso- ciated to components Y by the QR-decomposition, and Y ′ be the correspond- ing normalized adjusted components. One sees that expvar QR norm(Y ) = ∥Y ′∥2 ≥ ∥Y ∥2, which violates property 3. Right :Let X denote the basis associated to components Y by the polar decomposition, and Y ′ be the corresponding nor- malized adjusted components. One sees that expvar QR norm(Y ) = ∥Y ′∥2 ≥ ∥Y ∥2, which violates property 3. UP normalized variance. Let X be defined by the UP-decomposition (polar decomposition) Y = XP of the components Y = AZ as recalled in (17). Then (21) defines another variance : expvarUP norm(Y )= X j=1...m 1/∥tj∥2 , Z = T P , P = (Y T Y )1/2 . (25) The normalized variance expvarUP norm(AZ) does not either satisfy condition 3, as the counter example of Figure 2, right, shows. So we explore in the next section another road in order to comply with all conditions 1,2 and 3. 4.2 Projected explained variances We consider in this section the case where the adjusted components Y ′ in step 2 are obtained by projection of the components Y on the chosen orthonormal basis X of span{Y }. The adjusted component y′ j is hence defined by : y′ j = ⟨yj , xj⟩xj , j = 1 . . . m . The so called projected explained varianceof Y estimated with X is then defined by : expvarX proj(Y ) def = ∥Y ′∥2 F = X j=1...m ⟨yj , xj⟩2 (26) 9Before specifying the rule of step 1 which associates an orthonormal basis X to the possibly correlated components Y , we give some properties of expvarX proj(Y ) which hold independently of the chosen basis X : Lemma 4.2.(Projected explained variances) For any unit norm loadings Z and any basis X chosen according to the rule (16), the projected explained variance of Y = AZdefined by (26) satisfies conditions 1, 2 and 3 and : expvarX proj(Y ) ≤ expvarsubsp (Y ) ≤ σ2 1 + ··· + σ2 m . (27) The proof is in Section 8.3 of the Appendix. We can now specify the rules for the selection of the orthonormal basis X, which give each, according to Lemma 4.2, a projected explained variance satisfying conditions 1 to 3. QR projected explained variance. We choose here to associate to the components Y , the orthonormal basis X = Q obtained by QR-decomposition of Y as recalled in (17).The vector x1 of the basis X = Q is chosen in the direction of the component with the larger norm, and the remaining components are projected on the orthogonal subspace to x1. Then x2 is determined by the same process applied in the orthogonal subspace, and so on. This reordering ensures that the basis X = Q associated to Y will point in the direction of Y primarily for the components of larger variance. The (order dependent) resulting QR projected explained variance is given by : expvarQR proj(Y ) = X j=1...m ⟨yj , xj⟩2 = X j=1...m r2 j,j. (28) It coincides with the adjusted variance introduced in Zou et al. [2006]. UP Projected explained variance. We choose now to associate to the components Y , the orthonormal basis X = U obtained by UP-decomposition (polar decomposition) of Y as recalled in (17). The basis X = U does its best to point in the same direction as the components Y , in that it maximizes the scalar product ⟨Y, X⟩F = P j=1...m⟨xj, yj⟩. The (order independent) resulting UP projected explained variance is given by : expvarUP proj(Y ) = X j=1...m ⟨yj , xj⟩2 = X j=1...m p2 j,j . (29) Optimal projected explained variance. The idea here is to associate to the components Y , the basis X which gives the largest projected explained variance defined in (26). This choice satisfies obviously condition (16) and the so called optimal projected explained variance is defined by : expvaropt proj(Y ) = max XT X=Im X j=1...m ⟨yj , xj⟩2 , (30) 10The numerical computation of the optimal projected explained variance requires the maximization of the convex function X ⇝ P j=1...m⟨yj , xj⟩2 under the constraint XT X = Im. This can be done using the algorithm of Journ´ ee et al. [2010], which gives here : Xk+1 = polar \u0000 2 Y diag(XT k Y ) \u0001 , X 0 = U = polar(Y ) . (31) where 2Y diag(XT k Y ) is the gradient atXk of the functionX ⇝ P j=1...m⟨yj , xj⟩2. 5 Comparison of the explained variances In this section, the six variances explained by non orthogonal components (see Table 1), are compared theoretically and numerically. Name Notation Definition Short name subspace variance expvar subsp [12] subspVar QR normalized variance expvar QR norm [24] QRnormVar UP normalized variance expvar UP norm [25] UPnormVar QR projected variance expvar QR proj [28] QRprojVar UP projected variance expvar UP proj [29] UPprojVar optimal projected variance expvar opt proj [30] optprojVar Table 1: Summary of for the 6 variance definitions. 5.1 What we know We give first theoretical results on the relative magnitudes of the 6 explained variances : - The subspace explained variance is larger than any of the five other vari- ances (Lemmas 4.2 and 4.1). It is even larger than expected when com- ponents or loadings are orthogonal without being left and right singular vectors (see (14) and (15)). - The optimal projected variance is greater by definition than any other pro- jected variance, in particular greater than the QR and the UP projected variance. - There is no natural order between the QR and UP projected variances : when the components Y are of equal norm, the basis X which maximizes expvarX proj(Y ) is polar(Y ), which implies in particular that : expvaropt proj(Y ) = expvarUP proj(Y ) ≥ expvarQR proj(Y ) . (32) But the converse of the last inequality can hold when the norms of the components are very different : for m = 2, one checks that ∥y2∥/∥y1∥ small enough implies that expvarUP proj(Y ) ≤ expvarQR proj(Y ). 11- There is no natural order between the QR and UP normalized variances : for components Y such that the basis X associated by QR-decomposition coincides with the m-first left singular vectors Um of A, one has, according to Lemma 4.1 : expvarQR norm(Y ) = X j=1...m σ2 j ≥ expvarUP norm(Y ) , with a strict inequality as soon as Y and Umdiag{σj} don’t coincide. The same reasoning with the polar decomposition in place of the QR decomposition shows that the converse inequality can happen. - There is no natural order between the variances defined by projection and normalization, as illustrated in Figure 3. Figure 3: The two sets of components Y = [ y1, y2] and ˜Y = [˜y1, ˜y2] have been chosen such that their polar decomposition produces the same basis X = [x1, x2], and one sees that : expvarUP proj( ˜Y ) ≤ expvarUP norm( ˜Y ) = expvarUP norm(Y ) ≤ expvarUP proj(Y ). 5.2 What we see We compare now numerically the six explained variances. The comparison is made on non orthogonal components Y obtained by applying sparse PCA to simulated data matrices A. The matrices A are obtained using the simulation scheme of Chavent and Chavent [2023] based on m = 4 underlying loadings vectors of size p = 20 and m = 4 underlying first eigenvalues which are chosen to be either close or different. More precisely, 100 matrices A of size n×20 were drawn randomly using either the “close eigenvalues” or the “different eigenval- ues” scheme. Three sparse PCA algorithms [see Chavent and Chavent, 2023] 12were applied to each matrix A for a grid of 101 values of sparsity parameters λ ∈ [0, 1]. Finaly, 30300 = 100 ∗ 3 ∗ 101 loadings matrices Z and components matrices Y = AZ where obtained for the “close eigenvalues” and for the “dif- ferent eigenvalues” scheme. The variance explained by these components where performed using the six variance definitions to finally obtain the dimensionless proportion of explained variance (pev) defined by : pev = expvar(Y ) ∥A∥2 F ≤ σ2 1 + ··· + σ2 m ∥A∥2 F , where the right inequality follows from (10) in Condition 2, which is satisfied by all definitions. Figure 4 gives the mean pev for the 300 non orthogonal components (for the “close eigenvalues” and the “different eigenvalues” case) as a function of the sparsity parameter λ and for each definition of explained variance. 0.0 0.1 0.2 0.3 0.4 0.5 0.80 0.85 0.90 0.95 Close eigenvalues lambda mean pev subspVar optprojVar/UPprojVar QRprojVar QRnormVar UPnormVar 0.0 0.1 0.2 0.3 0.4 0.5 0.70 0.75 0.80 0.85 0.90 0.95 Different eigenvalues lambda mean pev subspVar optprojVar UPprojVar QRprojVar QRnormVar UPnormVar Figure 4: Comparison of the mean pev (proportion of explained variance) over two sets of components (top and bottom) as function of sparsity parameter λ for the six variance definitions. For the “close eigenvalues” case (top), the six definitions give relatively close results and the results produced by optprojVar and UPprojVar are so close that 13they cannot be distinguished on the figure (remember that they would coincide were the norms equal, see (32)). One sees also that the results seem to be in a certain order for all λ : subspVar ≥ optprojVar ≥ UPprojVar ≥ QRprojVar ≥ QRnormVar ≥ UPnormVar (the two first inequalities are not a surprise, as they hold theoretically). For the “different eigenvalues” case (bottom), a zoom on the curves shows that subspVar and optprojVar are again larger than all other variances. But the previous apparent order between UPprojVar, QRprojVar and QRnormVar is not longer observed. The UPnormvar remains the smallest but its behavior seems disturbed as the sparsity paramter λ increases. Table 2 shows that all definitions but UPnormVar (in the “different eigen- values” case) exhibit quite similar dispersions over the 300 realizations (for λ = 0.3). Close eigenvalues Different eigenvalues subspVar 0.63 1.63 optprojVar 0.66 1.38 UPprojVar 0.66 1.06 QRprojVar 0.72 1.25 QRnormVar 1.06 1.33 UPnormVar 1.21 7.74 Table 2: Standard deviations ×100 of the six pev (proportion of explained vari- ance) obtained for λ = 0.3 with the three algorithms over the two sets of com- ponents (close eigenvalues and different eigenvalues). 5.3 Ranking properties The proportions of explained variance are meant to be used for the ranking of algorithms, so it is important to figure out wether or not definitions i and j of explained variance will rank in the same order the components Y and Y ′ obtained from possibly different algorithms and/or sparsity parameter λ and/or realization of the data matrix A. The components obtained with the 3 algorithms, the 50 smallest values λ and 100 realizations of A (“different eigenvalues ” case) gave 15000 × 14999/2 couples ( Y, Y′) to be tested. Among these couples, we may consider as ϵ-distinguishable from the point of view of our explained variances those for which |pevi(Y ) − pevi(Y ′)| ≥ϵ for all i = 1 . . .6 for some ϵ ≥ 0. Table 3 shows the percentage of cases where pevi and pevj rank identically components Y and Y ′ among all ϵ-distinguishable couples. The good news is that all three projected variances optprojVar, UPpojVar and QRprojVar, as well as the normalized variance QRnormVar, produce the 14optprojVar UPprojVar QRprojVar QRnormVar UPnormVar subspVar 79.98 71.05 70.71 69.75 56.19 optprojVar 88.93 89.87 89.19 73.71 UPprojVar 96.22 95.27 84.62 QRprojVar 98.34 83.15 QRnormVar 82.75 UPnormVar optprojVar UPprojVar QRprojVar QRnormVar UPnormVar subspVar 86.13 83.78 84.70 84.57 68.68 optprojVar 96.40 98.57 98.32 81.14 UPprojVar 97.81 97.72 84.74 QRprojVar 99.66 82.55 QRnormVar 82.66 UPnormVar optprojVar UPprojVar QRprojVar QRnormVar UPnormVar subspVar 89.57 89.57 89.57 89.57 68.80 optprojVar 100.00 100.00 100.00 79.23 UPprojVar 100.00 100.00 79.23 QRprojVar 100.00 79.23 QRnormVar 79.23 UPnormVar Table 3: The entry of each table on line i and column j gives the percentage of ϵ-distinguishable couples Y, Y′ which are ranked identically by pev i and pevj. Top : ϵ = 0, middle : ϵ = 10−3, bottom : ϵ = 10−2. same ranking as soon as one considers that differences in proportion of explained variance under 10 −2 are not significative. For the same ϵ, the two other def- initions subspVar and UPnormVar still produce quite different rankings. The numerical results of this section show that all investigated definitions rank the explained variance of components in essentially the same order, and hence can all be used to compare the variance explained by components obtained by different algorithms or with different parameters. Of course, this is only an experimental result based on our simulated data sets, that needs to be confirmed by further numerical tests. 6 Explained variance block PCA formulations With the objective in mind to get rid of the orthogonality constraints in the usual block PCA formulation (1), we discuss in this section the possibility of using the above defined explained variance measures as objective function in new block PCA formulations where the loadings are subject to unit norm constraints 15only : max ∥zj∥ = 1, j= 1. . . m expvar(AZ) = σ2 1 + ··· + σ2 m , (33) As seen in Lemmas 3.1, 4.1 and 4.2, property (33) holds for the subspace variance and the five normalized and projected variance definitions. However, from these six definitions, only the three projected explained variances satisfy (Lemma 4.2) all necessary conditions 1, 2 and 3 of section 2 required for a proper definition of explained variance. So we shall limit our search for an explained variance block formulation to these three definitions. A projected explained variance expvar proj(AZ) will provide a block PCA formulation (33) if and only if its sole maximizers are the loadings Z made of the m first right singular vectors Vm in any order, up to a ±1 multiplication, or in short : Z perm = Vm are the sole maximizers of expvar( AZ) . (34) We denote also by Am def = UmΣmV T m with Σ m def = diag {σ1 . . . σm} (35) the restriction of the matrix A to the subspace of the m first right singular vectors V1 . . . Vm. It is a bijection from the right to the left singular subspaces associated to the m ≤ r first singular values. In the singular basis Um, Vm, Am reduces to Σm. The next lemma characterizes the maximizers of any projected variance : Lemma 6.1. Maximizers of projected variances 1. For a given set of components Y , the basis X which gives the largest projected explained variance expvarX proj(Y ) is necessarily a solution of : Y diag(XT Y ) = XP with PT = P and P ≥ 0 (36) 2. The projected explained variance expvarX proj(AZ) achieves it PCA maxi- mum value σ2 1 +··· +σ2 m if and only if Z and X satisfy the three following conditions : span{Z} = span{Vm} (37) the loadings Z are (AT mAm)−1– orthogonal (38) the basis X associated to Y = AZ is X = [n1 . . . nm] . (39) where n1 . . . nm denote unit normals at y1 . . . ym to the ellipsoid Em of span{Um} image by Am of the unit sphere of span{Vm} (see Figure 5) 16y1 n1 y′2 n2n1n2 y2 y′1 R2=σ21+σ22 Figure 5: Illustration of condition (39) for m = 2. The ellipse E2 is the image by A of all unit norm loadings z of the right singular space V2, with half axes σ1 and σ2. Let y1, y2 be two (possibly correlated) components such that the normals n1, n2 to E2 at y1, y2 are orthogonal, the tangents are orthogonal too and hence meet on the Cartan Circle of radius R = (σ2 1 + σ2 2)1/2. Then for the choice X = (n1, n2) one has expvarX projY = ∥y′ 1∥2 + ∥y′ 2∥2 = R2 = σ2 1 + σ2 2. The proof is in Appendix 8.4 Condition (38) reduces, in the singular basis Vm, to Σ −2 m –orthogonality of the loadings. It ensures that the normals nj are orthogonal, and hence can be chosen as the basis X associated to Y in (39). Of course, the SVD solutions Z perm = Vm of PCA satisfy always (37) (38) (39). Once a rule Y ⇝ X has been specified, points 1 and 2 of the lemma will make it possible to determine wether or not (34) is satisfied. 6.1 Maximizers of expvarQR proj(Y ) The projected explained variance expvar QR proj(AZ) attains its maximum σ2 1 + ··· + σ2 m if and only if : Z perm = Vm , (40) When the maximum norm selection procedure is applied at each step and the components renumbered accordingly, the unique maximizer is Z = Vm. Proof: The “if” part of (40) is trivial, we prove the “only if” part : let Y be such that expvarQR projY = σ2 1 +··· +σ2 m, and X be given by its QR decomposition 17Y = XR with R upper triangular. By hypothesis, X maximizes expvarX Y , and point 1 of Lemmas 6.1 implies that Y diag(XT Y ) = XP = XRdiag(XT Y ), with P = Rdiag(XT Y ) symmetric positive. This implies that R is a diagonal matrix, so that all xj point in the direction of yj. But property 2 of Lemma 6.1 shows that the xj’s are also normal to the ellipsoid Em at yj, which can happen only if yj = Azj coincides with its principal axes and hence each zj is one of the m first right eigenvectors vj. Proposition 6.2. The QR projected explained variance expvarQR proj(AZ) (the adjusted variance of Zou et al. [2006]) associated to unit norm loadings Z pro- vides a block PCA formulation : max ∥zj∥ = 1, j= 1. . . m expvarQR proj(AZ) = σ2 1 + ··· + σ2 m , (41) which admits the SVD solution (40) as “unique” maximizer. Numerical implementation of this formulation requires the computation of the gradient of the Z ⇝ expvarQR proj(AZ) function, which is defined through the QR decomposition of Y = AZ. This can be done by the adjoint state method Chavent [2010], which is feasible but may be cumbersome. The block PCA formulation (41) can be used as starting point for the design of sparse PCA algorithms, keeping in mind that enforcing sparsity by subtracting theℓ1 norm of loadings leads to a difficult, though tractable, non smooth optimization problem. 6.2 Maximizers of expvarUP proj(Y ) The projected explained variance expvar UP proj(AZ) attains its maximum σ2 1 + ··· + σ2 m if and only if : Z perm = Vm (SVD solution) or Z = Z# (42) where the “parasitic” solution Z# is such that the components Y # = AZ# satisfy ⟨y# 1 , x1⟩ = ··· = ⟨y# m, xm⟩, with the hyperplanes tangent to Em at y# j delimiting an m-dimensional orthant of span {Vm}. For m = 2, the parasitic solution is illustrated on figure 6, where one sees that the components y# j (in red) corresponding to the choice of different principal axes for the intersection of the tangents coincide up to a multiplication by ±1; the SVD components are in blue. The proof is in Appendix 8.5. So expvarUP proj(AZ) cannot be used for the construction of a block PCA for- mulation like (33), as the optimization algorithm might converge to the parasitic solution Z# ! 18x1 x2 E2 u1 y′1# y′2# u2 R2=σ21+σ22 y#2 y∗1 y#1 y∗2 −y#2 −y#1 Figure 6: Illustration, for m = 2, of the parasitic maximizer Y # of expvarUP proj. The polar decomposition associates to components Y # = y# 1 , y# 2 orthonormal vectors X = x1, x2 such that ∥y′ 1 #∥ = ∥y′ 2 #∥. The red points on the Cartan circle correspond to the parasitic maximizers Y # = ±y# 1 , ±y# 2 , the blue points to the SVD solution Y ∗ = ±y∗ 1, ±y∗ 2 = ±σ1u1, ±σ2u2 6.3 Maximizers of expvaropt proj(Y ) According to point 2 of Lemma 6.1, the optimal projected explained variance expvaropt proj(AZ) attains its maximum σ2 1 + ··· + σ2 m if and only if : span{Z} = span{Vm} and the loadings Z are (AT mAm)−1– orthogonal . (43) This situation is similar to the maximization (1) of∥AZ∥2 under orthonormality constraints ZT Z = Im, were the maximum is attained for all orthonormal Z which span Vm. But the difference is that the orthogonality condition (43)-right is not a constraint for the maximization of expvar opt proj(AZ), it just happens to be satisfied by the maximizer ! 196.4 Weighted optimal projected explained variance In order to select the SVD solutionZ = Vm among the maximizers of expvaropt proj(AZ), we introduce weights µj such that : µ1 ≥ µ2 ≥ . . . µm > 0 , (44) and define a weighted optimal projected variance by : expvaropt proj,µ(AZ) = max XT X=Im X j=1...m µ2 j⟨Azj , xj⟩2 , which coincides with expvar opt proj(AZ) when µj = 1 for all j. This leads to the weighted optimal projected explained variance block PCA formulation : max ∥zj∥ = 1 j = 1. . . m expvaropt proj,µ(AZ) = max ∥zj∥ = 1 j = 1. . . m max XT X=Im X j=1...m µ2 j⟨Azj , xj⟩2 = X j=1...m µ2 jσ2 j . (45) The nice properties of this formulation are recalled in the next proposition : Proposition 6.3. Let the singular values of A satisfy : σ1 > σ2 > ··· > σm > 0 , and the weights µj satisfy (44). Then the PCA loadings Z = Vm and nor- malized components X = Um defined in (5) are one solution of the block PCA formulation (45) when the weights µj are constant, and its unique solution (up to a multiplication by ±1 of each column of course) when the weights µj are strictly decreasing, in which case the maximizers Z∗ and X∗ are independent of the weights µj. The (unweighted) optimal projected variance explained by Y ∗ = AZ∗ is : expvaropt proj (Y ∗) = σ2 1 + ··· + σ2 m ≤ ∥A∥2 F . Proof: Exchanging the order of maximization in the center term of (45) solving analytically the maximization with respect to Z gives : max ∥zj∥ = 1 j = 1. . . m expvaropt proj,µ(AZ) = max XT X=Im X j=1...m µ2 j∥AT xj∥2 . (46) The last term in resp. (46) is the maximization of a weighted Rayleigh quotient for AT , which is known to be equivalent to a PCA problem forAT , and hence for A (see for example Absil et al. [2008], recalled as Theorem 8.1 in the Appendix for the case of constant weights, and Brockett [1991] for the case of decreasing weights). Of course, formulation (45) is of little interest for PCA itself, as there exists plenty of other efficient solution methods. But it will provide the starting point for the design of a sparse PCA algorithm, to be developped in the companion paper Chavent and Chavent [2023]. 207 Conclusion We have investigated the problem of defining the part of the variance of a data matrix explained by correlated components, such as those which arise when sparse loadings are searched for. We have established three compatibility conditions to be satisfied by any such explained variance definition in order to be compatible with the Principal Component Analysis (Condition 1), and to ensure a loss in explained variance when the components are correlated (Conditions 2 and 3). We have proved that the two existing and the four new definitions : - Subspace (total variance of Shen and Huang [2008]), - QR normalized, - UP or Polar normalized, - QR projected (adjusted variance of Zou et al. [2006]), - UP or Polar projected, - Optimal projected all satisfy the two first compatibility conditions, but that only the three pro- jected explained variance satisfy also the third one and provide proper explained variance definitions. Numerical experiments have shown that the choice of a specific definition for the ranking of correlated components by explained variance is not critical. But we have shown that only the QR and the (weighted) optimal projected explained variance definitions admit the SVD solution as unique maximizer, and hence provide new explained variance block PCA formulations rid of orthogonality constraints on loadings. Their use for the construction of a group sparse PCA algorithm is the subject of a second paper Chavent and Chavent [2023]. 8 Appendix 8.1 Generalized Rayleigh quotient This is a classical result, see for example Absil et al. [2008] and Brockett [1991] : Theorem 8.1. Let the loadings Z satisfy : Z = [z1 . . . zm] ∈ I Rp×m , rank(Z) = m ≤ rank(A) def = r . Then the generalized Rayleigh quotient tr{(ZT AT AZ)(ZT Z)−1} satisfies : tr{(ZT AT AZ)(ZT Z)−1} ≤σ2 1 + ··· + σ2 m ≤ ∥A∥2 F , and the left inequality becomes an equality if and only if : span{Z} = span{v1 . . . vm} , where v1, . . . vm are the m first right singular vectors of A. 218.2 Proof of Lemma 3.1 Definition (12) of the subspace explained variance and the properties of the Rayleigh quotient tr {ZT AT AZ(ZT Z)−1)} recalled in Theorem 8.1 show that (9) and (10), and hence Properties 1 and 2, hold as well as (13) and (15). It remains to prove (14) which shows that Condition 3 does not hold. So let Y = AZ be orthogonal components : ⟨yj, yk⟩ = 0 , j, k= 1 . . . m, j̸= k corresponding to unit norm loadings : ∥zj∥ = 1 j = 1 . . . m , and define X, Tby : xj = yj/∥yj∥ , t j = zj/∥yj∥ , j = 1 . . . m , so that : XT X = Im . Then on one side one has : ∥Y ∥2 F = X j=1...m ∥yj∥2 = X j=1...m 1/∥tj∥2 = tr{diag−1(TT T)} , (47) and on the other side, as Y and X span the same subspace : expvarsubsp (Y ) = expvarsubsp (X) = tr{(XT X)(TT T)−1} = tr{(TT T)−1} (48) Formula (14) will be proved if we show that : tr{diag−1(TT T)} ≤tr{(TT T)−1} . (49) We use for that an idea taken from Miller [1969], and perform a QR-decomposition of T. By construction, the diagonal elements of R satisfy : 0 < ri,i ≤ ∥ti∥ . Then : TT T = RT QT Q R= RT R , (TT T)−1 = R−1(RT )−1 = R−1(R−1)T , where R−1 satisfies : R−1 = upper triangular matrix , [R−1]i,i = 1/ri,i . Hence the diagonal element of ( TT T)−1 are given by : : \u0002 (TT T)−1\u0003 i,i = \u0002 R−1(R−1)T \u0003 i,i = [ R−1]2 i,i + X j>i [R−1]2 i,j ≥ [R−1]2 i,i = 1/r2 i,i ≥ 1/∥ti∥2 . (50) 22which gives (49) by summation over i = 1 . . . m, and (14) is proved. We suppose now that the orthogonal components yj, j= 1 . . . msatisfy ∥Y ∥2 F = expvarsubsp (Y ). Then (47) (48) imply that equality holds in (49) and hence all inequality in (50) are equalities : 1. first inequality : [ R−1]2 i,j = 0 for all j > i ⇒ R−1 and hence R are diagonal 2. second inequality : 1 /r2 i,i = 1/∥ti∥2 ⇒ R is diagonal But R diagonal implies that the tj - and hence also the loadings zj - are orthog- onal, which together with the hypothesis of orthogonal components yj, implies that (yj/∥yj∥, zj) are pairs of singular vectors of A, which proves that Z = Vm and ends the proof of (14). 8.3 Proof of Lemma 4.2 By construction, expvarX proj(AZ) satisfies clearly conditions 1 and 3 of Section 4. We prove now that it satisfies moreover (27), and hence also condition 2. Let EX be the ellipsoid of span(X) = span(Y ) image by A of the unit sphere of span(Z). By construction one has : yj ∈ EX , j = 1 . . . m , and the modified components Y ′ defined by projection satisfy, c.f. (26) : ∥y′ j∥ = |⟨yj, xj⟩| ≤νj def = max y ∈ EX ⟨y, xj⟩ , j = 1 . . . m , (51) so that : expvarX projY def = ∥Y ′∥2 F ≤ ν2 1 + ··· + ν2 m . (52) We can now “box” the ellipsoid EX in the parallelotope PX of span{X} defined by : PX = \b y ∈ span{X} | −νj ≤ ⟨y, xj⟩ ≤+νj , j = 1 . . . m \t , (see figure 7). By construction, one can draw from each of the 2 m vertices of PX m orthogonal hyperplanes tangent to the ellipsoid EX, which implies that they are all on the orthoptic or Cartan sphere of the ellipsoid, whose radius is known to be the sum of the squares of the half principal axes σX j , j= 1 . . . mof EX (see for example the textbook Tauvel [2000]). Hence : ν2 1 + ··· + ν2 m = (σX 1 )2 + ··· + (σX m)2 . (53) Let then yX 1 . . . yX m be vectors whose extremity are points of EX located on its principal axes, so that : ∥yX j ∥ = σX j , j = 1 . . . m ,⟨yX i , yX j ⟩ = 0 , i, j= 1 . . . m, i̸= j . Property (14) of Lemma 3.1 applied to the orthogonal components Y = Y X gives: : (σX 1 )2 + ··· + (σX m)2 = ∥Y X∥2 ≤ expvarsubsp Y X ≤ σ2 1 + ··· + σ2 m . (54) Combining inequalities (52) (53) (54) proves the inlem 4-2equality (27). 23Figure 7: Illustration of the upper bound to ∥Y ′∥2 F in span {Y } when Y ′ is defined by projection. 8.4 Proof of Lemma 6.1 (notations of Section 8.3) We prove first point 1 of the Lemma. Maximization of the convex function X ⇝ expvarprojY X under the constraint XT X = Im by algorithm (31) and passing to the limit proves (36). We prove now the “only if” part of point 2. Let X be given such that expvarX projY = σ2 1 + ··· + σ2 m. Then necessarily : • equality holds in (54), and property (13) of subspace variance implies that the loadings Z span the subspace Vm of the m first right singular vectors, which proves (37). • equality holds in (51), which implies that for j ̸= k the normals to EX at yj and yk are orthogonal (see Figure 7). The restriction Am of A to span{Vm} is an isomorphism from span {Vm} to span{Um}, hence : EX = {y ∈ span{Um} | ∥A−1 m y∥2 = 1} . A normal n(y) to EX at y is then : n(y) = 1 2∇y \u0000 ∥A−1 m y∥2 − 1 \u0001 = (A−1 m )T A−1 m y = (A−1 m )T z , 24and the orthogonality of n(yj) and n(yk) shows that : ⟨n(yj), n(yk)⟩ = ⟨(A−1 m )T z1, (A−1 m )T z2⟩ = ⟨z1, (AT mAm)−1z2⟩ = 0 , (55) which proves (38). When vectors and matrices are written on the singular bases Um and Vm, one has ( AT mAm)−1 = diag{ 1 σ2 1 . . .1 σ2m }. We finally prove the “if” part of point 2. So let (37) (38) (39) hold. Property (37) implies that the half axes of EX are σ1 . . . σm, and (38) that the normal nj to EX at yj, j= 1 . . . mare orthogonal. So one can box EX with a parallelotope PX with axes parallel to the normals nj, and define X as the orthonormal basis along its axes. Then the same reasonning as above for the proof of (27) shows that expvarprojY = σ2 1 + ··· + σ2 m, which ends the proof of the lemma. 8.5 Proof of Property (42) Let the loadings Z be such that the UP-projected explained variance of the components Y = AZ satisfies expvarUP proj(Y ) = σ2 1 +··· +σ2 m. Then by definition of expvarUP proj one has : expvarUP proj(Y ) = expvarX proj(Y ) with Y = UP, UT U = Im , P = PT , P≥ 0 (56) But expvarX proj(Y ) = σ2 1 + ··· + σ2 m, and point 1 of Lemma 6.1 implies the existence of P′ such that : Y diag(XT Y ) = XP ′ with P′T = P′ and P′ ≥ 0 (57) Comparison of the two last properties shows that : Y = UP = XP ′diag(XT Y )−1 , (58) and uniqueness of the polar decomposition implies that P′diag(XT Y )−1 = P and hence is symmetrical, which can happen in only two cases : 1. either diag( XT Y ) = λIm for some λ, which gives the parasitic solution Z = Z#, 2. or P′ - and hence also P itself - is diagonal, which implies that the com- ponents Y = UP are orthogonal. But point 2 of Lemma 6.1 implies that they are also ( AT mAm)−1– orthogonal, which is possible only if the com- ponents Y are proportional to the left singular basis Um, which gives the SVD solution Z perm = Vm. References Pierre-Antoine Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization Algorithms on Matrix Manifolds , volume 78. 12 2008. ISBN 978-0-691-13298- 3. doi: 10.1515/9781400830244. 25RW Brockett. Dynamical systems that sort lists, diagonalize matrices, and solve linear programming problems. Linear algebra and its applications, 146:79–91, 1991. Guy Chavent. Nonlinear Least Squares for Inverse Problems: Theoretical Foun- dations and Step-by-Step Guide for Applications . 01 2010. ISBN 978-90-481- 2784-9. doi: 10.1007/978-90-481-2785-6. Marie Chavent and Guy Chavent. A group sparse explained variance block pca. Submitted, 2023. Michel Journ´ ee, Yurii Nesterov, Peter Richt´ arik, and Rodolphe Sepulchre. Gen- eralized power method for sparse principal component analysis. Journal of Machine Learning Research, 11(Feb):517–553, 2010. G Miller. Closed-form inversion of the gram matrix arising in certain least- squares problems. IEEE Transactions on Circuit Theory, 16(2):237–240, 1969. R Core Team. R: A Language and Environment for Statistical Computing . R Foundation for Statistical Computing, Vienna, Austria, 2021. URL https: //www.R-project.org/. Haipeng Shen and Jianhua Z Huang. Sparse principal component analysis via regularized low rank matrix approximation. Journal of multivariate analysis , 99(6):1015–1034, 2008. Patrice Tauvel. Cours de g´ eom´ etrie: agr´ egation de math´ ematiques. Dunod, Paris, 2000. Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis. Journal of computational and graphical statistics , 15(2):265–286, 2006. 26",
      "references": [
        "Optimization Algorithms on Matrix Manifolds",
        "Dynamical systems that sort lists, diagonalize matrices, and solve linear programming problems.",
        "Nonlinear Least Squares for Inverse Problems: Theoretical Foun- dations and Step-by-Step Guide for Applications",
        "A group sparse explained variance block pca.",
        "Gen- eralized power method for sparse principal component analysis.",
        "Closed-form inversion of the gram matrix arising in certain least- squares problems.",
        "R: A Language and Environment for Statistical Computing",
        "Sparse principal component analysis via regularized low rank matrix approximation.",
        "Cours de g´ eom´ etrie: agr´ egation de math´ ematiques.",
        "Sparse principal component analysis."
      ],
      "meta_data": {
        "arxiv_id": "2402.04692v1",
        "authors": [
          "Marie Chavent",
          "Guy Chavent"
        ],
        "published_date": "2024-02-07T09:32:32Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses how to define and optimize “explained variance” when PCA components/loadings are correlated (e.g., in sparse PCA) so that orthogonality constraints on loadings can be removed. Introduces three compatibility conditions any explained-variance measure should satisfy (match PCA/SVD in the orthogonal case; be bounded by PCA optimum for m components; penalize correlation so it does not exceed total variance of Y except when Y is orthogonal). Studies two existing measures—Shen & Huang’s total/subspace variance and Zou et al.’s adjusted variance—and proposes four new ones (two normalized and two projected variants plus an optimal projected form). Proves only the projected measures satisfy all three conditions, and analyzes ordering/ranking behavior among six candidates. Shows which measures yield a block-PCA objective without orthogonality constraints whose unique maximizer is the SVD solution; identifies QR-projected (adjusted variance) and a weighted optimal projected variance as suitable, with the latter providing differentiability/easy computation and unique SVD maximizer under decreasing weights.",
        "methodology": "Defines multiple explained-variance functionals for Y=AZ with unit-norm, possibly non-orthogonal loadings Z: (1) subspace variance ||A P_Z||_F^2 (Rayleigh quotient) using projection onto span(Z); (2) normalized variances built by mapping correlated Y to an orthonormal basis X in span(Y) via QR or polar (UP) decomposition and then “normalizing” to adjusted components Y' that correspond to unit-norm adjusted loadings; (3) projected variances computed by projecting each y_j onto its associated orthonormal direction x_j and summing squared inner products. For projected variances, proposes three rules for selecting X: QR basis, polar(UP) basis, and an optimal X maximizing the projected objective. Develops theoretical properties via SVD geometry, generalized Rayleigh quotient bounds, ellipsoid/Cartan (orthoptic) sphere arguments, and characterization of maximizers. Provides an iterative algorithm (generalized power method / Journée et al. 2010) to compute the optimal X for optprojVar and introduces a weighted optprojVar to enforce uniqueness of the SVD maximizer.",
        "experimental_setup": "Numerical comparison uses simulated centered data matrices A (n×p with p=20) generated from a scheme (from companion work) with m=4 underlying loading vectors and either “close” or “different” first eigenvalues. Draws 100 matrices for each eigenvalue regime. Runs 3 sparse PCA algorithms (referenced from companion paper) over a grid of 101 sparsity parameters λ∈[0,1], producing 30,300 solutions per regime (100×3×101). Evaluates six explained-variance definitions via proportion of explained variance pev=expvar(Y)/||A||_F^2, plots mean pev vs λ, reports standard deviations at λ=0.3, and assesses ranking agreement by pairwise comparisons across ~112.5M component-set pairs (15,000 choose 2) under distinguishability thresholds ε∈{0,10^-3,10^-2}. Also provides an R implementation in package sparsePCA (GitHub).",
        "limitations": "Paper focuses on variance-definition properties rather than end-to-end sparse PCA performance on real datasets; experiments are limited to simulated data with p=20, m=4 and specific eigenvalue regimes. Some candidate objectives (e.g., UP-projected) have non-unique/‘parasitic’ maximizers, limiting their use for block-PCA formulations. QR-projected objective yields unique maximizer but depends on component ordering and requires differentiating through QR (noted as feasible but potentially cumbersome). Optimal projected variance without weights has a manifold of maximizers (subspace + weighted-orthogonality) so does not uniquely recover SVD unless weights are introduced and singular values are distinct. Ranking-consistency conclusions are empirical and may not generalize; UP-normalized variance shows unstable dispersion for different-eigenvalue simulations. Assumes rank(Y)=m≤rank(A) and unit-norm loadings; relies on centered data and Frobenius-norm variance notion.",
        "future_research_directions": "Develop and evaluate sparse/group-sparse block PCA algorithms using the proposed (weighted) optimal projected explained variance as a differentiable objective without orthogonality constraints (as planned in companion paper). Extend theory to handle equal/clustered singular values and study weight-selection strategies (data-driven or adaptive) for uniqueness/stability. Investigate scalable optimization and automatic differentiation for QR-based objectives and compare computational trade-offs with optprojVar. Validate ranking and variance-measure behavior on real high-dimensional datasets and across broader simulation regimes (larger p, varying n, noisy/heteroscedastic settings). Explore extensions to other matrix-factorization settings (PLS/CCA) and to structured sparsity penalties beyond ℓ1 (group, fused, graph-based) under the new variance objectives.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Prompt-Driven Referring Image Segmentation with Instance Contrasting",
      "full_text": "© 2020 IOP Publishing Personal use of this material is permitted. Permission from IOP Publishing must be obtained for all other uses, in any current or future media, including reprinting or republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. DOI: 10.1088/1748-3190/abc6b3 arXiv:2303.02989v1  [cs.RO]  6 Mar 2023Bio-Inspired Compact Swarms of Unmanned Aerial Vehicles without Communication and External Localization Pavel Petr´ aˇ cek1 , Viktor Walter , Tom´ aˇ s B´ aˇ ca, Martin Saska Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague, 166 36, Prague 6, Czech Republic 1Author to whom any correspondence should be addressed. E-mail: pavel.petracek@fel.cvut.cz, viktor.walter@fel.cvut.cz, tomas.baca@fel.cvut.cz, martin.saska@fel.cvut.cz August 2020 Abstract. This article presents a unique framework for deploying decentralized and infrastructure- independent swarms of homogeneous aerial vehicles in the real world without explicit communication. This is a requirement in swarm research, which anticipates that global knowledge and communication will not scale well with the number of robots. The system architecture proposed in this article employs the UltraViolet Direction And Ranging (UVDAR) technique to directly perceive the local neighborhood for direct mutual localization of swarm members. The technique allows for decentralization and high scalability of swarm systems, such as can be observed in ﬁsh schools, bird ﬂocks, or cattle herds. The bio-inspired swarming model that has been developed is suited for real-world deployment of large particle groups in outdoor and indoor environments with obstacles. The collective behavior of the model emerges from a set of local rules based on direct observation of the neighborhood using onboard sensors only. The model is scalable, requires only local perception of agents and the environment, and requires no communication among the agents. Apart from simulated scenarios, the performance and usability of the entire framework is analyzed in several real-world experiments with a fully-decentralized swarm of unmanned aerial vehicles (UAVs) deployed in outdoor conditions. To the best of our knowledge, these experiments are the ﬁrst deployment of decentralized bio-inspired compact swarms of UAVs without the use of a communication network or shared absolute localization. The entire system is available as open-source at https://github.com/ctu-mrs. Keywords: Swarm Robotics, Relative Localization, Distributed Control, Unmanned Aerial Vehicle Submitted to: Bioinspir. Biomim.©IOP Publishing, 2020. DOI: 10.1088/1748-3190/abc6b3 1. Introduction Use of a team instead of a single robot may yield several general advantages in tasks that either beneﬁt from the multi-robot conﬁguration or are altogether unsolvable by a single robot. The main advantages of robot teams are reduced task execution time, improved robustness, redundancy, fault tolerance, and convenience of cooperative abilities, such as increased precision of measurements with a stochastic element (e.g., localizing ionizing radiation sources [1]), distributing the application payload, and dynamic collaboration (e.g., cooperative object transport [2]). Deployment of a single UAV requires a complex system composed of several intricate subsystems han- dling the vehicle control, environment perception, ab- solute or relative localization, mapping, navigation, and communication. A system scaled to a set of tightly cooperating UAVs must additionally introduce decen- tralized behavior generation, fault detection, informa- tion sharing in an often low-to-none bandwidth com- munication network, and detection and localization of inter-swarm members. Furthermore, the characteris- tic environments in the context of aerial swarms suited for real-world challenges may be unknown in advance, they incorporate high density of complex obstacles, they provide none-to-low access to mutual intercom- munication between the team agents, and they allow either no access or unreliable access to a global naviga- tion satellite system (GNSS). Each of these concepts is a complex challenge on its own. However, overcom- ing all the challenges opens the way to applications requiring distributed sensing and acting, such as coop- erative area coverage for search & rescue, exploration, or surveillance tasks. In this article, we present a complete swarm system framework, which respects the swarm and environment characteristics. The properties of the framework presented here correspond closely with the deﬁnition of autonomous swarms, as listed in [3]. The properties are: scalability for large groups, high redundancy and fault tolerance, usability in tasks unsolvable by a single robot, and locally limited sensing and communication abilities. Inspired by the self- organizing behavior of large swarms of homogeneous units with limited local information that is found among biological systems, our framework goes even further beyond the swarm requirements from [3] by dealing with all centralized and decentralized communication with the use of the UVDAR local perception method. The elimination of communication is particularly important in dense swarms of fast- moving aerial vehicles, where time-based delays in mutual localization might disturb the collective behavior of swarms and thus may induce mutual collisions. The independence from communication makes the system also applicable as a backup solution for swarm stabilization in scenarios where communication is required, but suﬀers from outages. This allows us to employ a fully decentralized sys- tem architecture not limited by scalability constraints. This decentralization is advantageously robust towards a single-point of failure, reduces the hardware demands for individuals, and distributes the sensing and act- ing properties. We have been inspired mainly by or- dinary representatives of biological systems: common starlings sturnus vulgaris , which exhibit a remarkable ability to maintain cohesion as a group in highly un- certain environments and with limited, noisy informa- tion [4]. Similarly to starlings (and numerous other biological species), the proposed swarming system re- lies on sensing organs that look on two sides (cameras in our case), observing close-proximity neighbors only and responding to these sensory inputs by a local be- havior which together forms a swarm intelligence that reaches beyond the abilities of a single particle. The UVDAR method tackles the problem of mutual perception of swarm particles by localizing the bearing and the relative 3D position of their artiﬁcial ultraviolet (UV) light emission in time, using passive UV-sensitive cameras. The method is deployable in indoor and outdoor environments with no need for mutual communication or for a heavy-weight sensory setup. In addition, it is real-time, low-cost, scalable, and easy to plug into existing swarm systems. To verify the feasibility of the UVDAR technique in an aerial communication-less swarm system, we employed UVDAR to generate a decentralized bio-inspired swarming behavior employing local information about neighboring agents and close-proximity obstacles in real-world conditions. As veriﬁed in real- world experiments, the proposed system for relative localization is accurate, robust, and reliable for use in decentralized local-information based swarming models. 2©IOP Publishing, 2020. DOI: 10.1088/1748-3190/abc6b3 Figure 1: A compact aerial swarm of 3 UAVs in a controlled outdoor environment ﬁlled with artiﬁcial obstacles, as viewed by an outside observer. The decentralized approach, described in detail in section 4, applies a set of local rules contributing to safe navigation and self- organization of the swarm structure among obstacles. The UAVs are homogeneous units with solely local sensing. Figure 2: Onboard detection of 3 UAVs in the UV spectrum using UVDAR in a member of the aerial swarm. The method directly localizes the neighbors within a swarm in indoor and outdoor environments. Here, the method detects neighbors in an outdoor environment aﬀected by a powerful source of ambient UV radiation. The processing is possible due to periodic blinking of the members with a speciﬁc frequency, here with 6 Hz, 15 Hz and 30 Hz. 1.1. Related Work 1.1.1. Relative Localization In most recent work concerning swarms and formation ﬂight [5], the proposed algorithms have only been validated either in simulation or in laboratory-like conditions with the presence of absolute localization. This was merely converted to relative measurements virtually, using systems such as real-time kinematic (RTK)-GNSS or Motion capture (mo-cap). It is well known that mo-cap is impractical for real-world deployment of mobile vehicles (either outdoors or indoors), as it requires the installation of an expensive infrastructure. These absolute localization sources can provide the full pose of tracked objects, which oversimpliﬁes the whole task with respect to the reality of practical deployment. Even if only partial information derived from absolute measurements is passed to the UAVs (e.g., distance or bearing), the continuous stream of such information is produced without realistic errors, which is unrepresentative of real-world conditions. Some more practical approaches consider infra- structure-less sensing such as ranging based on a radio signal [6]. This only allows for distance-based follow- ing, without any orientation information, and requires a speciﬁc motion for suﬃcient state observability. An- other approach [7], for the 2D case, wirelessly commu- nicates the intentions of the leader. This proves to be feasible since there are fewer degrees of freedom and there is less drift than in a general 3D case. These two approaches rely on radio transmission, which is subject to the eﬀects of network congestion and interference. For this reason, we consider vision-based approaches more suitable for multi-robot groups, especially in un- controlled outdoor environments. This approach has previously been explored by the authors’ research group, relying on true outdoor relative localization, see [8]. The source of the relative localization was an onboard vision-based system using passive circular markers, as described in [9]. There were, however, drawbacks: high sensitivity to the external lighting conditions and to partial occlusion, and substantial size for an acceptable detection range. The use of active infrared (IR) markers has also been explored (see [10–12]) for the ability to suppress backgrounds using optical ﬁltering. These methods are however suitable solely for indoor, laboratory-like conditions, since solar radiation excessively pollutes the IR spectrum, and subsequently the signal tends to deteriorate. In [12], the authors employed IR markers with blinking frequency in the kilohertz range, which required event-based cameras to detect micro-scale changes. These cameras are capable of detecting micro- scale changes. However, they typically do not provide suﬃciently high ﬁeld of view and resolution, and they are not suitable for scalable swarms due to their size and cost. The IR spectrum has also been utilized in a passive manner [13], but this approach, though simple, is even less robust to the outdoor conditions 3©IOP Publishing, 2020. DOI: 10.1088/1748-3190/abc6b3 and distances applicable to UAVs. It is also feasible to visually detect and localize un- marked UAVs using machine learning (ML) methods such as Convolutional Neural Networks (CNNs). How- ever, these approaches require meticulously annotated datasets with a speciﬁc UAV and with an environ- ment similar to the intended operational space [14,15]. The computational complexity and the dependency on satisfactory lighting conditions of such ML systems precludes their deployment onboard lightweight UAVs suitable for swarming. This motivated the develop- ment of the UVDAR system, which is more robust to real-world conditions, because it reduces the computa- tional load by optically ﬁltering out visual information that is not of interest. In contrast to [14,15], UVDAR also provides target identities. The whole sensor is small, lightweight, and does not depend on the exter- nal lighting conditions. 1.1.2. System Architecture To date, deployments of real-world aerial teams have not used any of the methodologies of direct localization described here in order to deal with the mesh- communication between the team members or with the communication link with a centralization element. The record in terms of the number of UAVs cooperating at the same time is currently held by Intel ® [16] with its ﬂeet of Shooting Star quad-rotors. Intel’s centralized solution performs spectacular artistic light shows. However in Intel’s arrangement, each team member follows a pre-programmed trajectory, relying on GNSS and a communication link with a ground station. A similar methodology is employed in [17–19], where the authors deployed swarms of UAVs in order to verify bio-inspired ﬂocking behaviors in known conﬁned environments. In comparison with [16], their methods are decentralized; however, the UAVs still communicate their global states obtained by GNSS within a radio-frequency mesh network. This is not a realistic assumption in most application scenarios. Recent successful real-world deployments are summarized in table 1. Observe that some kind of communication (either ground station to unit or unit- to-unit) is employed in most of the related work. The dependency on a communication network lowers the upper limit for swarm scalability, due to the bandwidth limitations, and signiﬁcantly reduces the fault tolerance of the entire system. The UVDAR relative visual perception system, described in detail in section 3, is designed to remove this dependency. Its use may allow working swarm systems to mimic the local behavioral mechanisms found in biological systems, ranging from general ﬂocking to leader- follower scenarios. 1.1.3. Swarm Stabilization To enable short-term stabilization of an autonomous UAV, an onboard inertial measurement unit (IMU) directly measures its linear acceleration, the attitude and the angular rate, using a combination of accelerometers, gyroscopes, and magnetometers. To obtain long-term stabilization of an UAV, however, it is not suﬃcient to use only the onboard IMU, due to the inevitable measurement noises and drifts. It is common practice to provide an additional estimate of the state vector variables (typically position or velocity), which is fused together with all the inertial measurements. The most common approach is to estimate the global position using a GNSS. However, GNSS signal availability is limited strictly to outdoor environments, and the accuracy of GNSS is aﬀected by an error of up to 5 m [27]. Although the accuracy can be improved to 2 cm with the use of RTK-GNSS, this makes aerial swarms deployable solely in controlled environments and is in contradiction with the bio- mimicking premise, since precise global localization is uncommon in biological systems. Other common methods of state estimation are local, and they typically employ onboard laser- or vision-based sensors to produce local estimates of the state variables. Vision-based methods may compute the optical ﬂow to estimate the velocity of the camera relative to the projected image plane [28], or may apply algorithms of simultaneous localization and mapping (SLAM) to visual data [29]. Laser-based sensors are mostly used to estimate the relative motion between two frames of generated point-cloud data [30]. There are structurally two approaches for stabiliz- ing a swarm in a decentralized manner. The ﬁrst group of methods distributes the state estimates determined for individual self-stabilization throughout the swarm (see table 1). In addition to restricting the communica- tion infrastructure, this methodology has a major de- pendency between the swarm density and the accuracy of the global localization (e.g., GNSS). In addition, it requires knowledge of individual transformations amid the coordination frames for distributed local state es- timation methods. The second group of methods does not adopt a communication network to distribute the state estimates, but rather estimates the states directly from the relative onboard observations. This approach makes the swarm independent from the infrastructure, but it makes direct detection, estimation, and decision making with limited information more challenging. As further shown in section 5, the developed framework is part of the second group, perceiving the local neigh- borhood with visual organs and deploying a swarm of UAVs in fully-decentralized manner. 4©IOP Publishing, 2020. DOI: 10.1088/1748-3190/abc6b3 Work Decentralized Communication Relative localization Intel® [16] No Yes ∗ Shared global position (WiFi) EHang, Inc. [20] No Yes ∗ Shared global position (WiFi) Hauert et. al [21] Yes Yes Shared global position (WiFi) B¨ urkle et. al[22] Yes Yes Shared global position (WiFi) Kushleyev et. al [23] No Yes ∗ Shared global position (ZigBee) V´ as´ arhelyi et. al[17–19] Yes Yes Shared global position (XBee) Weinstein et. al [24] No Yes ∗ Shared global position (WiFi) Stirling et. al [25] Yes Yes Infrared (IR) ranging Nguyen et. al [6] N/A Yes Ultra-Wideband ranging (UWB) N¨ ageli et. al[26] Yes Yes Visual markers This work Yes No UVDAR Table 1: A brief comparison of aerial swarm systems with successful recent deployments outside of laboratory- like conditions. Methods marked with ( ∗) employ communication with a centralized ground station. 1.1.4. Swarming without Communication Decentralized swarming models accounting for com- plete or partial absence of communication were ex- plored exclusively for 2D systems in the past (this is also implied in table 1). The majority of the state-of- the-art works within this ﬁeld are biologically-inspired and emphasize self-organizing behavior of large-scale swarms of simple units with highly limited sensory ca- pabilities. Highlighted is the Beeclust [31] approach, which uses probabilistic ﬁnite state machines and a primitive motion model to mimic the collective behav- ior of honeybees. The Beeclust can be applied to com- plex tasks where information exchange among units is not required, such as in underwater exploration us- ing a swarm of underwater robots [32]. A diﬀerent method [33] analyzes the aggregation of agents towards a common spatial goal while avoiding inter-agent col- lisions. The authors of [33] show that their method with limited sensing properties of the agents performs similarly to methods employing complete pose infor- mation. All of these decentralized algorithms require some form of mutual relative localization (even limited to binary detections), making them suitable for the use of UVDAR localization. Overall review of the 2D approaches is systematically described in [34], which further highlights the lack of research focus in the ﬁeld of aerial swarming in 3D space. 1.2. Contributions This article addresses problems of the deployment of real-world aerial swarms with no allowed communi- cation or position sharing. This potential problem is overcome with the use of the novel vision-based UVDAR system for direct mutual perception of team members. The stability of the UVDAR system for use in aerial swarming is the outcome of thorough real- world experimental veriﬁcation in an outdoor environ- ment with and without obstacles. The main features of this article are as follows: (i) It provides an enabling technology for swarm research, often bio-inspired, by introducing a system that achieves fundamental swarm properties, as deﬁned in [3]. (ii) It introduces the UVDAR system as an oﬀ-the- shelf tool for relative localization and identiﬁca- tion of teammates suited for mutual perception of agents in robotic systems, such as aerial swarms. (iii) It introduces a decentralized bio-inspired swarm- ing approach suited for obstacle-ﬁlled real-world environments, which requires only local relative information and no mutual communication. (iv) It veriﬁes the feasibility and analyses the usability of aerial ﬂocking relying on direct localization, which is the most frequent mechanism in biological systems. (v) It is based on several real-world deployments of aerial swarms. (vi) It presents, to the best of our knowledge, the ﬁrst autonomous deployments of aerial swarms with no centralized element and no mutual communication. (vii) It discloses the entire system as open source at https://github.com/ctu-mrs. 2. Motivation The lack of a communication-independent approach has put a constraint on much of the work done until now in the ﬁeld of deploying teams of unmanned vehicles in challenging environments. Our work here is motivated by the need for a communication- independent approach, and presents solutions that we have developed. The insights into the development 5©IOP Publishing, 2020. DOI: 10.1088/1748-3190/abc6b3 of the real-world deployments presented here tackle the motivations and constraints of the vast majority of related work restrained by the heretofore lack of communication-independent approaches. Focusing on dense swarms of UAVs with short mutual distances, most of the swarming approaches reported in the literature have not been tested in real- world conditions. Theoretical derivations, software simulations, and occasional experiments in laboratory conditions have formed the target for most of the related literature, as analyzed in [5] and [35]. However, this research milestone is far away from a meaningful real-world veriﬁcation needed for an applicability of aerial swarms. Real world interference cannot be neglected, as the integration of a swarming intelligence onto a multi-robot system yields constraints that need to be characterized directly in models of swarming behavior. Instigated by biologically-inspired swarming mod- els [35, 36] capable of achieving complex tasks (e.g., navigation, cohesion, food scouting, nest guarding, and predator avoidance) with a team of simple units, our aim was to imitate these models with the use of local information, as is widely observed in nature. To allow the deployment of an infrastructure-independent (com- munication, environment) model, we had identiﬁed the most crucial factor impeding this type of deployment of a decentralized architecture – the mutual relative localization between team members, which is also the most crucial information for animals in ﬂocks in nature. This motivated the development of the UVDAR system (see section 3), designed as a light-weight oﬀ-the-shelf plugin providing the local localization of neighboring swarm particles. The usability of UVDAR in dense swarms is analyzed in detail in section 6. 3. UVDAR Inspired by our extensive prior experimental experience with vision-based relative localization of UAVs (see [9, 37]), we developed a novel relative localization sensor that tackles various limitations of previous solutions, namely the unpredictability of outdoor lighting and limits on the size and weight of onboard equipment. The sensor, named UVDAR, is a UV vision-based system comprising a UV-sensitive camera and active UV LED markers. These lightweight, unobtrusive markers, attached to extreme points of a target UAV, are seen as unique bright points in the UV camera image (see ﬁgure 3). This allows computationally simple detection [38] and yields directly the relative bearing information of each marker from the perspective of the camera. The ﬁsh-eye lenses that are used with the UV camera provide a 180° horizontal overview of the surroundings. Known camera calibration, together with the geometrical layout of the markers on the target, allows us also to retrieve an estimate of the distance (see [38, 39] for details). In order to provide speciﬁc markers that would be distinguishable from others, and also to provide a further increase in robustness with respect to outliers, we set the markers to blink with a speciﬁc sequence. Using our specialized implementation of the 3D time- position Hough transform (see [38] for details), we can retrieve this signal for each observed marker, giving them identities. In this project, we use these IDs to simplify the separation of multiple observed neighbor UAVs, but they can also be used to retrieve the relative orientation of the neighbors [39]. In addition to the swarming application described in this paper, UVDAR may be used for e.g., a directed leader-follower ﬂight [39], where the use of the retrieved orientation is essential. In addition, the neighbors’ orientation estimate can be exploited for automatic generation of a dataset for training ML vision for UAV detection, as applied in [40], where UVDAR was used for annotating color camera images. In swarms and in multi-UAV systems in general, the blinking frequency of the onboard LEDs can be conﬁgured to encode information for optical data transmission between swarm units, in addition to using LED blinking directly for relative localization. An example of such an application is in exploration, where a scouting unit can indicate the presence and the relative position of a discovered target to other units by combining various blinking signals and the unit’s own orientation. A further use is in cooperative voting in a group, where each unit expresses the current selection with blinking signals, and adjusts its vote on the basis of observing the selections of others. In this paper, we go beyond our preliminary works with UVDAR [38–40], and also beyond other state-of-the-art literature, by incorporating direct mutual localization of UAVs into the position control feedback loop of a fully-decentralized swarming system without any kind of communication and external localization. To the best of our knowledge, this paper presents the ﬁrst real-world deployments of fully-decentralized bio-inspired swarms of UAVs using direct local localization for collective navigation in an uncontrolled environment. This is what UVDAR was intended for. 3.1. Safety The use of UV radiation in the system has understand- ably raised some health concerns in the past. We have veriﬁed the safety of this application by consulting the International Commission on Non-Ionizing Radiation Protection (ICNIRP) ”Guidelines on limits of exposure 6©IOP Publishing, 2020. DOI: 10.1088/1748-3190/abc6b3 Figure 3: An example of the unprocessed view from the UV-sensitive camera as a part of UVDAR in a member of an aerial swarm. Note the extreme contrast of the LED markers in comparison to the background. A combination of the speciﬁc blinking frequency of the LED markers and the high contrast makes them simple to extract from background for processing. to ultraviolet radiation of wavelengths between 180 nm and 400 nm” [41]. According to these guidelines, the exposure to UV radiation (both to the eyes and to the skin) should not exceed 30 J m −2 weighted by the relative spectral eﬀectiveness (unitless wavelength- speciﬁc factor). In our case of 395 nm radiation, this factor equals to 0.000036, making the actual limit 8.3 ×105 J m−2. This means that our LEDs, produc- ing 230 mW of total radiated power [42] at the given driving current, can be safely viewed from the distance of 1 m from the frontal direction (with the highest in- tensity in its Lambertian radiation pattern) for over 3000 h, making it eﬀectively harmless. 3.2. Scalability In the context of a robotic swarm, scalability of the whole system is an important factor. Using a communication network in large groups of robots limits the scalability by an upper bound deﬁned by the total bandwidth, by the number of available channels, by the network architecture, or by the required data ﬂow. Employing a local perception method such as UVDAR, the state of swarm particles (team members, swarm units) is shared via direct observations, as is common in swarms in nature. This system therefore does not need an explicit radio communication network. As a vision-based method, UVDAR suﬀers from natural restrictions, namely visual occlusions, camera resolution, and the detection, separation, and identiﬁcation of image objects. The upper scalability bound is determined by the ability to ﬁlter out the UV markers belonging to a given swarm agent. If the markers of all UAVs in the swarm are set to blink with the same frequency, individual agents have to be distinguished by separating their positions in the UV image and in the constellations that they form. In this case, we estimate that each agent should be capable of distinguishing up to 30 neighboring agents within the range of the UVDAR system, bounded by the computational limitations. This is however not the ideal mode of operation, as it becomes problematic when there are occlusions between agents, or when the agents are in close proximity in the observed image. To tackle this challenge, we apply diﬀerent blinking frequencies to diﬀerent agents. The UVDAR system in its current conﬁguration can accommodate up to 6 diﬀerent frequencies of blinking that can be reliably distinguished from each other. This allows us to mitigate the issue of overlapping agents - indeed, even agents that are directly behind each other can often be separated, if extreme markers of the further agent protrude into the image. However, since the number of usable blinking frequencies is limited, we need to devise a method for spreading them evenly in the swarm, such that the likelihood of image separation of overlapping agents based on diﬀerent frequencies between them is maximized for the whole swarm. This has to be done in a decentralized manner, in order not to violate the swarming paradigm. 7©IOP Publishing, 2020. DOI: 10.1088/1748-3190/abc6b3 One way to solve this for dense UAV swarms is to have each agent dynamically re-assign its blinking frequency to diﬀer as much as possible from the neighbors that it observes. This challenge deﬁnition can be likewise deﬁned as the constraint satisfaction problem solved within a decentralized swarm of UAVs using direct observations only. The idea of this method is to maximize the local frequency diversity and additionally to allow all of the agents to initiate with the same ID (encoded by the blinking frequency of onboard markers). This opposes the current methodology of manually pre-setting the frequencies before deployment (see section 6). The analysis and the theoretical limits on the convergence of such an approach towards a stable ﬁnal state maximizing the scalability bound is still underway. Another approach to increase the scalability bound, while carrying the identical ID on all the agents, lies in the design of UVDAR itself. It is possible to introduce an additional omnidirectional UV source on top of each agent. This additional source is called a beacon and it blinks with a speciﬁc frequency unique to the rest of the onboard markers on an agent. This allows for the separation of pixels in the image stream based on their image distance as well as their association with the singular beacon marker. The presence of at least two beacons in one region of the observer’s image clearly implies a partial mutual occlusion. The use of beacons hence provides a limited ability to separate even agents in partial mutual occlusion relative to an observer if the beacons of both agents are visible. The maximum range of detection should be taken into account for scalability in the geometrical sense. With the current UVDAR setup, detection is possible for targets up to 15 m away from the sensor. However, for improved reliability and robustness, a maximum range of 10 m is recommended. For determining the theoretical accuracy and range limitations, see [38]. For a quantitative analysis on real-world accuracy, see section 6.3. Filtering out distant targets, the limited detection range makes the method suitable for dense swarms, which place emphasis on a number of entities in a local neighborhood rather than on the swarm as a whole. In biological systems, this perception characteristic allows for swarms of utmost magnitude, such as ﬁsh schools [43] with thousands of entities. 4. Swarming Intelligence In this article, we follow the swarm concept deﬁned in section 1, in which the group is composed of swarm units with limited computational power and a short- term memory. The concept is decentralized and uses autonomous self-organizing groups of homogeneous aerial vehicles operating in a 3D space. The proposed ﬂocking approach works entirely with local information, with no requirement for any form of radio communication between the homoge- neous swarm particles, and in an environment with convex obstacles. The approach is inspired by bio- logical systems, where global cooperative behavior can be found to emerge from elementary local interactions. We will show that this phenomenon of cooperative be- havior may yield collision-free stabilization in cluttered environments, self-organization of the swarm structure, and an ability to navigate in tasks suited for real UAVs. The proposed swarming framework is founded on pre- viously developed models [44,45], which have been en- hanced to suit the demands of real-world interference by extending them with concepts of obstacle avoidance, perception, and navigation. The introduction of such extension concepts is highly important as the assump- tions of dimensionless particles and an ideal world as in [44, 45] do not apply in the real world. The main idea of the swarming behavior presented here is to verify the feasibility, to perform an analysis, and to derive the properties of the UVDAR system for use in swarm systems. Bear in mind that UVDAR is a general system and any swarming model [17, 33, 46], formation control approach [47], or obstacle/predator avoidance method [48] utilizing local relative informa- tion can be employed to generate intelligent behavior when employing the UVDAR system. 4.1. Behavior Generation The behavioral model used throughout this article is deﬁned in discrete time step k for a homogeneous swarm unit i with an observation radius Ri n ∈R>0, an obstacle detection radius Ri o ∈R>0, a swarming velocity vi [k] ∈ R3×1, and a set of locally detected neighbors Ni [k] within the observation radius Ri n, as follows. Bear in mind that all the relative observations in particle i are given in the body frame of particle i at time step k. The individual detected neighbor particlesj∈Ni [k] are represented by vectors of relative position xij [k] ∈ R3×1 and relative velocity vij [k] ∈ R3×1, ∀j ∈ {1, ..., |Ni [k]|}, deﬁned as xij [k] = [ xij [k],yij [k],zij [k] ]T , (1) vij [k] = 1 ∆tij [k] ( xij [k] −xij [k−1] ) −vi [k−1], (2) where xij [k],yij [k],zij [k] are Cartesian coordinates of a neighbor particle j represented in the body frame of agent i in time step k, ∆tij [k] = tij [k] −tij [k−1] is the time elapsed since the last direct detection of neighbor j, 8©IOP Publishing, 2020. DOI: 10.1088/1748-3190/abc6b3 and vi [k=0] = vij [k=0] = 0. The swarming model is then deﬁned as a sum of elementary forces fi [k] ( Ni [k],Oi [k] ) = fb,i [k] ( Ni [k] ) + fn,i [k] ( Ni [k],Oi [k] ) , (3) where fb,i [k] (·) ∈R3×1 embodies the baseline forces as an interpretation of the Boids model [44] ﬂocking rules cohesion, alignment, and separation, modiﬁed for real UAVs as fb,i [k] ( Ni [k] ) = 1⏐⏐⏐Ni [k] ⏐⏐⏐ ⏐⏐⏐Ni [k] ⏐⏐⏐ ∑ j=1 [ xij [k] + vij [k] λ −κ ( xij [k],Ri n ) xij [k] ] . (4) The scalar λ [Hz] is the update rate of direct localization (camera rate) and the weighting function κ(x,r) = max ( 0; √ ∥x∥2 ∥x∥2 − √r r ) (5) represents a nonlinear weight coeﬃcient scaling the repulsion behavior by the mutual distance between two neighbors. As the original model [44] was designed for swarms of dimensionless particles, function κ(·) is particularly important for a swarm of real UAVs, in order to prevent mutual collisions while maintaining ﬂexibility of the swarm as a whole. The force fn,i [k] (Ni [k],Oi [k]) ∈ R3×1 in (3) is an extension to the simple model [44] in the form of an additional navigation rule in an environment composed of Ni [k] and a set of obstaclesOi [k] detected within the detection radius Ri o. The navigation rule can exploit any local multi- robot planning method [49–51] in order to optimize the swarm motion parameters and to prevent a deadlock situation, or can include an obstacle avoidance mechanism and a navigation mechanism by introducing them as additional simplistic rules. To provide an example of the system performance, we introduce a simple attraction force vn,i [k] ∈ R3×1 towards a speciﬁed goal, together with a local reactive obstacle avoidance rule. To represent the obstacles, we introduce the concept of a virtual swarm particle , which eﬃciently replaces a general geometric obstacle by a virtual entity. This dimensionless particle is represented by a state comprised of a position and velocity relative to particle i, similarly as deﬁned in (1) and (2). The methodology for ﬁnding the state of a virtual swarm particle is derived in the following section. The navigation rule is then derived as fn,i [k] ( Oi [k] ) = 1⏐⏐⏐Oi [k] ⏐⏐⏐ ⏐⏐⏐Oi [k] ⏐⏐⏐ ∑ v=1 [ viv [k] λ −κ ( xiv [k],Ri o ) xiv [k] ] + vn,i [k] λ , (6) where the vectors of the relative position xiv [k] ∈R3×1 and the relative velocity viv [k] ∈ R3×1 constitute the state of a v-th virtual swarm particle. The swarming model deﬁned in (3) represents the steering force of a particle i, which is used to compute the swarming velocity of particle i as vi [k] = γ ( fi [k] ( Ni [k],Oi [k] )) fi [k] ( Ni [k],Oi [k] ) fi [k] ( Ni [k],Oi [k] ) 2 , (7) where γ(f) = min {vm; λ∥f∥2} (8) bounds the magnitude of the velocity below the maximum allowed speed vm[m s−1]. The swarming velocity is then used in real-world applications to compute the desired position setpoint as rd,i [k] = vi [k] λ (9) represented in the body frame of UAV i. 4.2. Obstacle Detection To achieve ﬂocking in the targeted environment (e.g., a forest environment and an indoor environment), the obstacles in the local neighborhood are generalized into two geometrical classes (circles and lines), based on their cross-sections with the horizontal plane of a particle, as portrayed in ﬁgure 4. This assumption allows us to model more complex settings (e.g., a forest or an oﬃce-like environment) on the grounds of these two geometrical classes, while it throttles down the perception and the computational complexity onboard a lightweight UAV. Detection of these obstacles is assumed to be provided for a particleifrom any kind of an onboard sensor with an obstacle detection distance Ri o. Having in time step k a detected circular obstacle v with a radius rv [k] ∈ R>0 and a center at civ [k] ∈ R3×1 referenced in the body frame of particle i, the state of a v-th virtual swarm particle is derived as xiv [k] = ( 1 − rv [k] ∥civ [k]∥2 ) civ [k], (10) viv [k] = rv [k] ∥civ [k]∥2 ( I−µiv [k] ( µiv [k] )T) vi [k], (11) where ∥·∥2 is the L 2 norm, I ∈R3×3 is an identity matrix, and µiv = civ ∥civ∥2 . By analogy, the virtual swarm agent state can be derived for a linear obstacle deﬁned by its normal vector niv [k] ∈R3×1 and a set of 9©IOP Publishing, 2020. DOI: 10.1088/1748-3190/abc6b3 k= 0.2s⏐⏐⏐Oi [k] ⏐⏐⏐= 2 i fi [k]xi1 [k] xi2 [k] vi1 [k] vi2 [k] k= 3.0s⏐⏐⏐Oi [k] ⏐⏐⏐= 3 i fi [k] xi1 [k] xi2 [k] xi3 [k] vi1 [k] vi2 [k] vi3 [k] k= 5.0s⏐⏐⏐Oi [k] ⏐⏐⏐= 2 i fi [k] xi1 [k] xi2 [k] vi1 [k] vi2 [k] Figure 4: An autonomous UAV navigating among artiﬁcial obstacles according to the swarming model described in section 4. The UAV ﬂies in the Gazebo robotic simulator (upper row), while it continuously detects geometrical obstacles represented as circles and lines in the onboard 2D laser-scanner data with a limited obstacle detection radius (gray circle). The states of virtual particles, consisting of position xiv [k] (red dots) and velocity viv [k] (blue arrows) relative to UAV i, are visualized in the bottom image row. The steering force fi [k] (red arrow) of the swarming model represents the desired velocity. observed points Piv [k] as xiv [k] = (I−Piv [k]) ˆpiv [k] (12) viv [k] = 1 ∥ˆpiv [k]∥2 Piv [k] vi [k], (13) where Piv [k] = I−niv [k] ( niv [k] )T , (14) ˆpiv [k] = arg min p∈Piv [k] {∥p∥2}. (15) The state of a virtual swarm particle for both geometrical classes is visualized in ﬁgure 4, where an autonomous UAV navigates among artiﬁcial obstacles within an environment of the Gazebo robotic simulator. 5. System Architecture In addition to the method for direct onboard local- ization presented in section 3 and the decentralized swarming approach presented in section 4, we will now present here system architecture of the entire UAV sys- tem, supplemented by the concepts of UAV stabiliza- tion, control, and state estimation. These concepts are based on our previous research (see [1, 37, 52]) fo- cused on cooperation among autonomous aerial vehi- cles. They have been adapted for swarming research described in this article. The control pipeline, suited for stabilizing and controlling UAV swarms using lin- ear model predictive control (MPC) and the non-linear SO(3) state feedback controller [53], is depicted in the high-level scheme in ﬁgure 5. The stabilization and control pipeline is based entirely on [52]. In addition, a decentralized collision avoidance system [55] is adapted in the proposed system for safe research on compact aerial swarms. A long prediction horizon of linear MPC is used to detect collisions among trajectories of robots. The known collision trajectories are then altered prior their execution. This allows us to implement the collision avoidance system in a decentralized manner. Decentralized collision avoidance is necessary for safe veriﬁcation of bio-inspired swarming models in the real world. Although the use of mutual communication for collision avoidance is in contradiction with the system architecture presented in this article, it can be used as a low-level safety supervisor with no direct dependency on the architecture of the tested swarming model. This may prevent inadmissible collisions when there is undesired demeanor of dense swarm members, and therefore protect the hardware during the initial phases 10©IOP Publishing, 2020. DOI: 10.1088/1748-3190/abc6b3 MPC Tracker & Controller Acceleration Controller Attitude Rate Controller Actuators State Estimation Data Processing UV-emission diodes UV-sensitive camera UV-sensitive camera Overlap FusionImage Processing Sensory data UV emission UV irradiationBehavior Generator Swarm Navigation Obstacle Detection + ¨rd, ¨ψd ωd Td xr[k−1] ˙r,R,ω y rd [k] r[k] Stabilization & Control Autopilot UVDARSwarming Intelligence Figure 5: The high-level system pipeline (the schematic is based on the system pipeline diagram published in [54]) of a single homogeneous UAV swarm unit i in time step k. The stabilization & control pipeline [52] takes reference position setpoint r[k] for the MPC in the MPC tracker, which outputs a command ¨rd, ¨ψd ( ¨ψ is the heading acceleration) for the acceleration tracking SO(3) controller [53]. The acceleration controller produces the desired angular rate ωd and thrust reference Td for the embedded attitude rate controller. A state estimation pipeline outputs the current state estimate x based on the sensory data y and the onboard measurements of linear velocity ˙r, angular rate ω, and attitude R. Note that the time indices of the stabilization & control and the state estimation pipelines are omitted in the diagram, since their timeline matches the rate of the inertial measurements (typically 100 Hz), which diﬀers from the timeline of the detection cameras (10–20 Hz). Local perception of neighboring units using the UVDAR sensor is described in detail in section 3, while the decentralized swarming approach is described thoroughly in section 4. of experimental swarm deployment. However, the use of collision avoidance is not mandatory and its use is appropriate only during the initial testing phase. To stabilize UAVs using the system in ﬁgure 5, the individual UAVs estimate their state vector x= [r, ˙r, ¨r, R, ω]T , (16) where R∈SO(3) is the attitude and r= [xw,yw,zw]T is the position in the world coordinate frame. The vector ˙r ∈ R3×1 is the linear velocity, ¨r ∈ R3×1 is the linear acceleration, and ω ∈ R3×1 is the angular rate with respect to the UAV body coordinate frame. The PixHawk autopilot [56] is embedded to handle the low-level attitude rate and actuator control, and an IMU is used to directly measure the linear acceleration ¨r, the attitude R, and the angular rate ω, using a combination of accelerometers, gyroscopes, and magnetometers. The embedded autopilot integrates the measurements of ¨rto ˙rand employs the Extended Kalman Filter (EKF) to produce optimal estimates of the speciﬁc state variables with respect to the measurement noise. To self-localize an individual UAV, its global position measured by GNSS is fused together with the inertial measurements in order to stabilize the ﬂight of this dynamically unstable system. However, the global state is not shared to other swarm agents throughout our ﬁnal experimental analysis presented in section 6. Instead, the framework uses UVDAR to directly observe the relative position and the relative velocity (see (1) and (2)) of particles in the local neighborhood, and it generates a navigation decision based on the set of simple rules described in section 4. Although the use of GNSS for self- localization limits the system exclusively to outdoor environments, this dependency can be replaced by any local state estimation method with respect to the desired application and environment – e.g., the deployment of our decentralized system in a real-world forest, which was highlighted by the IEEE Spectrum ‡. 5.1. Properties The combination of the system decentralization and the local perception of individual agents makes the system as a whole robust towards failures of individuals. In the swarming model (see section 4), each agent decides on its actions in real time only from current observations or a short-past history of observations. This makes the system robust towards a single-point of failure, such as a failure of some centralized control element or the communication infrastructure. Unless the employed local perception method generates false negative detections, the swarming model (see section 4) ensures no mutual collisions between the agents. The rate of false negative detections in UVDAR is minimal as there are no objects blinking at speciﬁc rates in the given near- visible UV spectrum. In case of a hardware failure of an aerial agent (e.g., the agent lands unexpectedly), the ‡ https://spectrum.ieee.org/automaton/robotics/drones/ video-friday-dji-mavic-mini-palm-sized-foldable-drone 11©IOP Publishing, 2020. DOI: 10.1088/1748-3190/abc6b3 agent disappears from the visibility ﬁeld of other units resulting in emergent self-organization of the collective conﬁguration. As UVDAR is a vision-based system, it naturally suﬀers from visual occlusions generating blind spots in overcrowded situations. As discussed in section 3.2, the number of visual occlusions in UVDAR is mitigated with the use of diﬀerent blinking frequencies of overlapping UAVs. As the neighborhood for perception is also locally limited in the swarming model (see section 4), the distant blind spots are ﬁltered out in principle. The remaining occluded agents are neglected. This is feasible in the employed model, as the information about the units’ presence is propagated through direct observations of the motion of the middle agents (i.e., the agents causing the occlusions). Based on our empirical experience, this does not destabilize the swarm, but rather rearranges the agents to positions where the number of visual occlusions is reduced. The navigational features of the system as a whole are controlled in a decentralized manner. A decentralized navigation is possible with a swarming model capable of navigational decision making using only the perceived data onboard the units. This is the case of our swarming model (see section 4), which employs a simple steering towards a pre-speciﬁed set of global positions, hence eliminating the need for navigation managed by a centralized controller. Although our later experiments (see section 6) navigate each UAV individually, the model may navigate only a single unit with the rest of the swarm naturally following the leader – a behavior emerging from the cohesion and the alignment premises. 5.2. Hardware Platform The use of UVDAR is not dependent on the dimensions or the conﬁguration of a multi-rotor platform. The payload (onboard equipment) requirements of a single- UAV unit employing UVDAR are: an autopilot, a self- localization source (e.g., a GNSS receiver), 1-2 UV- sensitive cameras, computational power to control the ﬂight and to process the data (one camera at 20 Hz requires approximately a 30 % single-thread load on Intel-Core i7 7567U, 3.5 GHz), and a set of UV LED markers placed at known extreme points of the UAV. To verify this statement, an axiomatic function- ality validation of UVDAR was performed on two in- dependent multi-rotor platforms as shown in ﬁgure 6. The general hardware conﬁguration of UAVs exhibited in the ﬁgure consists of • the Pixhawk 4 autopilot, • onboard computer Intel NUC i7 7567U, • ProLight Opto PM2B-1LLE near-UV LEDs radi- ating at 390–410 nm wavelength [42], • mvBlueFOX-MLC cameras with – a MidOpt BP365 near-UV band-pass ﬁlter and – Sunnex DSL215 ﬁsh-eye lenses, • a GNSS receiver (the hexa-rotor platform only), and • the Slamtec RPLiDAR-A3 laser scanner (the quad-rotor platform only). The weight of this hardware conﬁguration is 370 g (or 540 g with the laser scanner required either for an obstacle detection or for a local localization replacing the GNSS dependency). The onboard Intel NUC computer weighing 225 g provides exaggerated processing power useful particularly in our case for general research purposes. For use in highly specialized applications, a feasible replacement of this payload with a microprocessor technology would allow for even further minimization of the aerial platform dimensions and cost expenses. Further miniaturization of infrastructure-indepen- dent UAVs is limited by current technology required for local self-localization. Vision-based algorithms em- ploy lightweight cameras minimizing the weight; how- ever, it comes at the cost of high processing power and thus increased weight of the processing unit. On the other hand, laser-based localization generally requires less processing power, but the sensors are heavier than cameras – approximately 170 g for planar scanners and 475 g for 3D LiDARs. 6. Experimental Analysis The primary aim of the experimental analysis is to verify the general functionality and to evaluate the performance of the entire framework exploiting direct localization rather than communication. The objec- tives of the experiments are focused primarily on de- termining the accuracy of the UVDAR direct localiza- tion, and on the stabilization and spatial navigation of an aerial swarm in real-world environments with and without obstacles. The entire experimental analysis is supported by multimedia materials available at http: //mrs.felk.cvut.cz/research/swarm-robotics. 6.1. Swarming Model Analysis To rule out the inﬂuence of UVDAR in a position control feedback loop of an aerial swarm, the Boids- based swarming intelligence (see section 4) is analyzed independently from the direct localization. For this purpose, the UAVs replace direct visual localization by sharing their global GNSS positions in an ad-hoc network in order to determine the relative arrangement 12©IOP Publishing, 2020. DOI: 10.1088/1748-3190/abc6b3 Figure 6: Two distinct multi-rotor (hexa- and quad-rotor) UAV platforms, here equipped with UV-sensitive cameras (red) and with active UV markers (green), comprising the hardware components of the UVDAR system for relative localization of neighboring UAVs. The diagonal dimension (without propellers) of the platforms are 550 mm (left) and 450 mm (right). The hexa-rotor platform was used throughout our experimental veriﬁcation presented in section 6. in the local neighborhood. This conﬁguration was necessary in order to deploy UAVs without direct localization using UVDAR, as discussed in section 1.1. The analysis showcases the usability of the proposed fully-decentralized swarming framework both in simulations and in real-world scenarios, and in environments with and without obstacles. The global positions of the obstacles are apriori available to the UAVs. First, the collective dynamics of the swarming model are analyzed thoroughly in the Gazebo robotic simulator [57], shown in ﬁgure 4, coupled with the Robot Operating System (ROS) [58]. This simulation environment emulates real-world physics, and allows us to use identical low-level controllers and state estimation methods (see section 5) for the real UAVs and also for the simulated UAVs, without simplifying assumptions. This makes the conﬁguration ideal for eﬀortless deployment of theoretical bio-inspired swarming approaches onto a group of real-world robots. Simulation deployment of a swarm of homogeneous units in a 3D environment with obstacles (see ﬁgure 7) veriﬁes the qualitative performance of the reactive obstacle avoidance methodology presented in section 4. The emerging collective dynamics show the properties of the 3D shape ﬂexibility during navigation through a narrow passage and in collision-free bypassing of static obstacles. The properties of safe navigation and high ﬂexibility are also showcased during the simulation deployment of a compact swarm of 9 homogeneous units in a dense 3D forest-like environment, according to ﬁgure 8. Second, an aerial swarm of 3 UAVs was experimentally deployed in a real-world forest-like environment similar to ﬁgure 8, in order to verify the abilities of the fully-decentralized swarming model to stabilize a set of UAVs in a decentralized manner, provide self-organizing behavior, and to navigate through an obstacle-ﬁlled environment. As explicitly shown in ﬁgure 9, even such a simplistic swarming model with only local information yields collision-free navigation (the minimum distance to an obstacle or to another UAV was 2.2 m) throughout the environment, and self-organizing compactness of the whole swarm during the entire ﬂight. The experiment likewise shows the ability of the model to divide the group when overcoming an obstacle and to unite back again afterwards. This level of ﬂexibility is important for fast and safe navigation within more complex environments in order to maximize the motion eﬀectiveness. The ﬂexibility is highlighted by dotted triangles, which represent the geometric conﬁguration of the swarm in time. Let us call this ﬂock geometry an α- lattice according to [45] and use it to represent a self-organizing structure, where individual inter- particle distances converge to a common value. This geometric conﬁguration allows for small deviations from the expected structure (especially for particles in an environment with obstacles), which can be further quantiﬁed by deviation energy and can be used to evaluate the swarming model convergence. The deviation energy is derived in [45] and represents a non- smooth potential function of a set of particles, where the α-lattice conﬁguration lies at its global minimum. 6.2. UVDAR in Control Feedback To verify the feasibility of the complete system deﬁned in ﬁgure 5, UVDAR vision-based mutual relative 13©IOP Publishing, 2020. DOI: 10.1088/1748-3190/abc6b3 −4 −2 0 2 4 y (m) 5 10 15 20 25 30 0 2 4 6 8 x (m) z (m) start goal obstacle (a) Flexible and eﬀective navigation of a decentralized swarm of 5 UAVs through a 4 m wide narrow passage. −5 0 5 y (m) -20 -10 0 10 20 0 5 10 x (m) z (m) start goal obstacle (b) Fast and eﬃcient maneuvering of 6 UAVs emerging solely from local interactions during avoidance of a static obstacle. Figure 7: A fully-decentralized swarm of homogeneous units in a simulated 3D environment with static obstacles. The swarming model yields enough ﬂexibility for the compact team to deviate from its aggregated structure in order to pass safely through a narrow gap (a) or to avoid an obstacle in an eﬃcient and fast manner (b). localization is deployed in the position control feedback loop of each homogeneous swarm agent. Throughout the experiment, the individual UAVs employ GNSS for self-state estimation. This is required to stabilize the ﬂight of each dynamically unstable UAV mid- ﬂight in a large open-space, where the swarm was deployed. However, the agents do not share any information through a communication network and instead they directly perceive the neighboring particles using UVDAR. The blinking frequencies of the UAVs (IDs) within the experiment were static and unique. This improves the performance of the UVDAR localization as unique IDs in the image stream help to separate occluded detections and track the units in time. To the best of our knowledge, this is the ﬁrst deployment of a fully decentralized aerial swarming system in a real environment (outside laboratory- like conditions) with direct localization and with no communication or position sharing allowed. As explicitly shown in ﬁgure 10, use of a −10 −5 0 5 10y (m) -20 -10 0 10 20 0 5 10 x (m) z (m) start goal obstacle (a) Orthogonal views on the trajectories of the particles. 50 100 150 200 250 0 5 10 time (s) distance (m) (b) Average distance amid the UAVs. 50 100 150 200 250 0 2 4 6 time (s) distance (m) (c) Minimal distance amid the UAVs. 50 100 150 200 250 0 1 2 4 6 time (s) distance (m) (d) Minimal distance to the closest obstacle. Figure 8: Navigation of a decentralized swarm of 9 homogeneous UAVs in a forest-like environment with a high density of circular obstacles – tree trunks (a). The experiment showcases the cooperative steering within the environment and the emerging properties of mutual long- term cohesion (b), safe mutual separation (c), and reliable obstacle avoidance (d). local sensing method maintains the abilities of the bio-inspired swarming model, namely self-organizing behavior, together with collision-free and cohered navigation. The swarm is capable of navigation throughout the environment in a compact structural constellation without any external interference to a sequence of global navigation goals. The ﬁgure shows the ability to preserve a compact structure emerging from local UVDAR-based perception (ﬁgure 2 and 14©IOP Publishing, 2020. DOI: 10.1088/1748-3190/abc6b3 (a) Swarm of 3 UAVs navigating through an artiﬁcial forest. (b) Onboard RGB view from one of the homogeneous units. −50 −40 −30 −20 −10 0 10 20 10 20 x (m) y (m) start goal obstacle (c) Trajectories of individual UAVs (coded by color). The dotted triangles represent the swarm constellation (α-lattices) at a given time, which highlights the compactness and the ﬂexibility of the swarm navigating amidst obstacles. 0 20 40 60 80 100 2.2 5 10 15 time (s) distance (m) UAV obstacle (d) Euclidean distance to the nearest UAV and obstacle for each swarm agent (coded by color). The minimum distance reached is 2.18 m. Figure 9: Aerial swarm of 3 homogeneous UAVs in a real- world forest-like environment ﬁlled with artiﬁcial obstacles. ﬁgure 3 show the perceived data of a single swarm agent in this particular experiment) and the elementary rules presented in section 4, while the homogeneous units do not share any information among themselves. 6.3. Analysis on Direct Observation Accuracy In real-world conditions, all estimation subsystems are incorporated with various measurements containing a stochastic noise element. The origin of this stochastic part is of numerous types (e.g., vibrations, discretization, approximations, sensor non-linearity, time desynchronization, lack of motion compensation, or optical discrepancies) and most of these inaccuracies need to be accounted for. For example, the stabilization and control system of UAVs requires a continuous stream of inertial measurements to cope with hardware-based and synchronization inaccuracies, in order to stabilize the dynamically unstable system in mid-ﬂight. The inﬂuence of these inaccuracies needs to t= 0s t= 30s t= 150s (a) Aerial view on the decentralized swarm of 4 UAVs (red) and a static reference to assist with the scale perception (blue). 4 6 8 avg. dist. (m) 0 20 40 60 80 100 120 140 160 0 2 4 6 time (s) min. dist. (m) (b) Average and minimal Euclidean distance among the homogeneous agents. The minimum distance reached is 2.04 m. Figure 10: A fully-decentralized swarm of 4 homogeneous UAVs navigating through an obstacle-less environment with UVDAR integrated into the position control feedback, as outlined by the scheme in ﬁgure 5. be carefully analyzed, and the results of the analyses must be incorporated into the design of a swarming model in order to compensate for the uncertainties of real-world systems. As discussed in the review of the related literature (see section 1.1), dense robotic swarms candidly communicate either external positioning estimates or individual global state estimates amid the swarm units. In addition to the requirements of the communication infrastructure, this methodology imitates the bio- inspired design of mutual localization by establishing the relative relations from the global data. This incorporates the global self-localization error, and can lead to dangerous decision making, and also to communication-based failures. However, our approach imitates biological systems by relying solely on direct localization without the need for known global states of the neighbors or of the unit itself. This bounds the overall performance of the system solely to the accuracy of the direct localization. It entirely 15©IOP Publishing, 2020. DOI: 10.1088/1748-3190/abc6b3 0 1 2 3 4 0 1 2 3 4 mutual distance (m) σr (m) Minimal Average 0.04 0.10 0.16 0.22 0.28 σΨ (rad) Figure 11: Dependency of the direct localization accuracy on the stability of an aerial swarm. The plot shows exponential decline of the minimum distance amid the swarm units with a growing degree of the localization error. The localization error is modeled as a multivariate normal distribution with uncorrelated zero-mean variables: the radial distance (standard deviation σr) and the relative azimuth (standard deviation σΨ). removes the need for a communication infrastructure, and allows for full decentralization of the system architecture. To analyze the impact of direct localization accuracy on the overall performance of our swarming framework, we present two inquiries: the inﬂuence of the error degree on the stability of a decentralized swarm, and the data-based accuracy of UVDAR in real-world conditions. As our focus applies to vision- based direct localization, the error of 3D relative localization can be expressed in spherical coordinates – radial distance, azimuth, and elevation – separately. Bear in mind that due to the vision-based nature of UVDAR discussed in section 3, the statistical characteristics of the elevation error are assumed to be identical with the azimuthal error. To maintain simplicity, the elevation error is therefore omitted from the presentation of the results. The impact of a direct localization error on the stability of a swarm was analyzed on a set of computational simulations. A decentralized swarm of UAVs with simulated dynamics, control & state estimation disturbances, and sensory inaccuracies, was deployed in scenarios with various degrees of the direct localization error according to ﬁgure 11. Although the data show the minimum inﬂuence of the error on the average distance among the swarm units, the stochastic element induces oscillations of the mutual distances. These deviations from a consensual mutual distance arise directly from the inaccuracy of direct localization and from time-based and dynamics-based delays. This has a negative impact on the stability properties of the entire swarm, as shown by the exponential decline of the minimal distance amid the swarm units with the increasing degree of the radial distance and the relative azimuth error in ﬁgure 11. In real-world systems, a −4 −2 0 2 4 0 2 4 6 ·10−2 distance error (m) meas. density (-) RMSE = 1.16 m N(0.09 m,1.16 m) −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 0 2 4 6 ·10−2 azimuth error (rad) meas. density (-) RMSE = 0.17 rad N(0.01 rad,0.17 rad) 0 1 2 3 4 5 0 2 4 6 ·10−2 localization error (m) meas. density (-) RMSE = 1.70 m N(1.53 m,0.75 m) Figure 12: Quantitative accuracy of UVDAR direct localization with respect to GNSS positioning in real-world conditions. The ﬁgure shows error histograms and their normalized normal distribution N(µ,σ) approximations of the directly estimated relative distance, the relative azimuth, and the global 3D position. suitable swarm density must be thoroughly considered with respect to the accuracy and the reliability of the direct localization in order to prevent undesired collisions. The accuracy of UVDAR in real-world conditions during the deployment of the decentralized swarm of 4 UAV units in an open environment (see ﬁgure 10) is expressed by the error histograms in ﬁgure 12. During this experiment, the self-localization of the individual UAVs was arranged by GNSS. The statistical analysis uses global positioning for a quantitative evaluation of the direct localization accuracy. Although global positioning yields a relatively high error, the state estimation module (see section 5) fuses this global state estimate with inertial measurements, which makes the output estimate robust towards sudden short-term changes. The positioning is still prone to long-term drift, which is minimal in terms of GNSS and therefore does not signiﬁcantly impact the evaluation of the direct localization within a dense swarm. The fused global estimate is therefore used as ground truth data for the quantitative evaluation in ﬁgure 12. This evaluation on real-world data shows the ability of UVDAR to estimate the relative 16©IOP Publishing, 2020. DOI: 10.1088/1748-3190/abc6b3 5 10range (m) RTK UVDAR -2 -1 azimuth (rad) 0 100 200 300 400 500 600 −1 0 1 time (s) elevation (rad) (a) Relative localization represented by the spherical coordinates (expressed in the origin of the camera) 0 0.2 0.4 0.6 0.8 1 0 1 2 3 n-th quantile (-) loc. error (m) (b) Quantiles of the absolute 3D localization error Figure 13: Real-world accuracy of UVDAR direct localization in a controlled environment — tracking of a single mid-ﬂight UAV relative to a static ground UV camera. The UVDAR localization is compared to ground-truth data obtained with the use of RTK-GNSS. The absolute RMSE of the relative 3D localization in this experiment reached 1.11 m (the median is 0.81 m). distance with 1.16 m root mean square error (RMSE) and the relative azimuth with RMSE of 0.17 rad. These separated errors then combine together with the elevation estimate to anticipate the relative 3D position of the neighboring particles within a moving aerial swarm with RMSE of 1.7 m. The accuracy of UVDAR in real-world conditions is further analyzed in a controlled outdoor environ- ment. During an independent experiment, a position of a single mid-air UAV was tracked in data from a static ground camera equipped with UVDAR and was com- pared to a precise RTK-GNSS (2 cm accuracy) serving as a ground-truth. The comparison of the relative lo- calization with the ground-truth data is shown in ﬁg- ure 13. The data show the property of UVDAR to localize an aerial unit with RMSE of 1.11 m. The concluded accuracy is particularly important for the design of bio-inspired systems employing the UVDAR sensor as a source of direct localization of neighboring units. The quantitative results of this analysis allow for appropriate compensation of the inaccuracies and credible veriﬁcation of swarming models in a simulator, which necessarily precede real- world applications. 7. Conclusion This article has presented a framework for deploying fully-decentralized aerial swarms in real-world condi- tions with the use of vision-based UV mutual relative localization of neighboring swarm units. The frame- work architecture, as well as the oﬀ-the-shelf UVDAR system for direct localization within an aerial swarm, has been thoroughly discussed, has been deployed on a decentralized swarm of UAVs in real-world environ- ments, and its performance has been analyzed. The experimental analysis veriﬁed the stability of UVDAR as an input into a fully-decentralized swarming archi- tecture, which embodies the communication-free and local-information swarming models that are commonly found among biological systems. The set of real-world experiments is, to the best of our knowledge, the ﬁrst deployment of a decentralized swarm of UAVs with no use of a communication network or of external local- ization. The system is provided as open source, and is designed for simple integration and veriﬁcation of ﬂocking techniques (often bio-inspired), respecting the requirements of the swarming paradigm. Acknowledgments This work was supported by the Czech Science Foundation (GA ˇCR) under research project No. 20- 10280S, by CTU grant no SGS20/174/OHK3/3T/13, by funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No. 871479, and by OP VVV project CZ.02.1.01/0.0/0.0/16 019/0000765 ”Research Center for Informatics”. The authors thank Daniel Heˇ rt for preparing all the necessary equipment required for the experimental analysis. Supplementary Materials The multimedia materials supporting the article are available at http://mrs.felk.cvut.cz/research/ swarm-robotics. The entire system is also available as open source at https://github.com/ctu-mrs. ORCID Pavel Petr´ aˇ cek: 0000-0002-0887-9430 Viktor Walter : 0000-0001-8693-6261 Tom´ aˇ s B´ aˇ ca: 0000-0001-9649-8277 Martin Saska : 0000-0001-7106-3816 17©IOP Publishing, 2020. DOI: 10.1088/1748-3190/abc6b3 References [1] ˇStibinger P, B´ aˇ ca T and Saska M 2020 Localization of Ionizing Radiation Sources by Cooperating Micro Aerial Vehicles with Pixel Detectors in Real-Time IEEE Robot. Autom. Lett.5 3634–3641 [2] Gassner M, Cieslewski T and Scaramuzza D 2017 Dynamic collaboration without communication: Vision-based cable-suspended load transport with two quadrotors Int. Conf. on Robotics and Automationpp 5196–5202 [3] Trianni V 2008 Evolutionary Swarm Robotics: Evolving Self-Organising Behaviours in Groups of Autonomous Robots (Studies in Computational Intelligence) 1st ed (Springer Publishing Company, Incorporated) [4] Young G F, Scardovi L, Cavagna A, Giardina I and Leonard N E 2013 Starling Flock Networks Manage Uncertainty in Consensus at Low Cost PLOS Computational Biology 9 1–7 [5] Chung S, Paranjape A A, Dames P, Shen S and Kumar V 2018 A Survey on Aerial Swarm Robotics IEEE Trans. Robot. 34 837–855 [6] Nguyen T, Qiu Z, Nguyen T H, Cao M and Xie L 2019 Distance-Based Cooperative Relative Localization for Leader-Following Control of MAVsIEEE Robot. Autom. Lett. 4 3641–3648 [7] Bhavana T, Nithya M and Rajesh M 2017 Leader-follower co-ordination of multiple robots with obstacle avoidance SmartTechCon [8] Saska M, B´ aˇ ca T, Thomas J, Chudoba J, Pˇ reuˇ cil L, Krajn´ ık T, Faigl J, Loianno G and Kumar V 2017 System for deployment of groups of unmanned micro aerial vehicles in GPS-denied environments using onboard visual relative localization Auton. Robot.41 919–944 [9] Faigl J, Krajn´ ık T, Chudoba J, Pˇ reuˇ cil L and Saska M 2013 Low-cost embedded system for relative localization in robotic swarms Int. Conf. on Robotics and Automation pp 993–998 [10] De Silva O, Mann G K I and Gosine R G 2015 An Ultrasonic and Vision-Based Relative Positioning Sensor for Multirobot Localization IEEE Sensors J. 15 1716– 1726 [11] Yan X, Deng H and Quan Q 2019 Active Infrared Coded Target Design and Pose Estimation for Multiple Objects Int. Conf. on Intelligent Robots and Systemspp 6885– 6890 [12] Censi A, Strubel J, Brandli C, Delbruck T and Scaramuzza D 2013 Low-latency localization by active LED markers tracking using a dynamic vision sensor Int. Conf. on Intelligent Robots and Systemspp 891–898 [13] Park H, Choi I, Park S and Choi J 2013 Leader-follower formation control using infrared camera with reﬂective tag 10th Int. Conf. on Ubiquitous Robots and Ambient Intelligence pp 321–324 [14] Chaudhary K, Zhao M, Shi F, Chen X, Okada K and Inaba M 2017 Robust real-time visual tracking using dual-frame deep comparison network integrated with correlation ﬁlters Int. Conf. on Intelligent Robots and Systems pp 6837–6842 [15] Carrio A, Tordesillas J, Vemprala S, Saripalli S, Campoy P and How J P 2020 Onboard Detection and Localization of Drones Using Depth Maps IEEE Access 8 30480– 30490 [16] Intel ® 2019 Drones Light Up The Sky URL intel. com/content/www/us/en/technology-innovation/ aerial-technology-light-show.html [17] Vir´ agh C, V´ as´ arhelyi G, Tarcai N, Sz¨ or´ enyi T, Somorjai G, Nepusz T and Vicsek T 2014 Flocking algorithm for autonomous ﬂying robots Bioinspir. Biomim. 9 025012 [18] V´ as´ arhelyi G, Vir´ agh C, Somorjai G, Tarcai N, Sz¨ or´ enyi T, Nepusz T and Vicsek T 2014 Outdoor ﬂocking and formation ﬂight with autonomous aerial robotsInt. Conf. on Intelligent Robots and Systemspp 3866–3873 [19] V´ as´ arhelyi G, Vir´ agh C, Somorjai G, Nepusz T, Eiben A E and Vicsek T 2018 Optimized ﬂocking of autonomous drones in conﬁned environments Sci. Robot.3 [20] EHang 2019 EHang Drone Formation Flight URL ehang. com/formation [21] Hauert S, Leven S, Varga M, Ruini F, Cangelosi A, Zuﬀerey J and Floreano D 2011 Reynolds ﬂocking in reality with ﬁxed-wing robots: Communication range vs. maximum turning rate Int. Conf. on Intelligent Robots and Systems pp 5015–5020 [22] B¨ urkle A, Segor F and Kollmann M 2011 Towards Autonomous Micro UAV Swarms J. Intell. Robot. Syst. 61 339–353 [23] Kushleyev A, Mellinger D, Powers C and Kumar V 2013 Towards A Swarm of Agile Micro Quadrotors Auton. Robot. 35 287–300 [24] Weinstein A, Cho A, Loianno G and Kumar V 2018 Visual Inertial Odometry Swarm: An Autonomous Swarm of Vision-Based Quadrotors IEEE Trans. Robot. Autom.3 1801–1807 [25] Stirling T, Roberts J, Zuﬀerey J C and Floreano D 2012 Indoor navigation with a swarm of ﬂying robots Int. Conf. on Robotics and Automation4641–4647 [26] N¨ ageli T, Conte C, Domahidi A, Morari M and Hilliges O 2014 Environment-independent formation ﬂight for micro aerial vehicles Int. Conf. on Intelligent Robots and Systems pp 1141–1146 [27] van Diggelen F and Enge P K 2015 The World’s ﬁrst GPS MOOC and Worldwide Laboratory using Smartphones ION GNSS+ pp 361–369 [28] Garcia Carrillo L R, Fantoni I, Rondon E and Dzul A 2015 Three-dimensional position and velocity regulation of a quad-rotorcraft using optical ﬂow IEEE Trans. Aerosp. Electron. Syst.51 358–371 [29] Schmid K, Lutz P, Tomi´ c T, Mair E and Hirschm¨ uller H 2014 Autonomous Vision-based Micro Air Vehicle for Indoor and Outdoor Navigation J. Field Robot.31 537– 570 [30] Kohlbrecher S, Meyer J, von Stryk O and Klingauf U 2011 A Flexible and Scalable SLAM System with Full 3D Motion Estimation Int. Symp. on Safety, Security and Rescue Robotics pp 155–160 [31] Schmickl T and Hamann H 2016 BEECLUST: A swarm algorithm derived from honeybees: Derivation of the algorithm, analysis by mathematical models, and implementation on a robot swarmpp 95–137 [32] Bodi M, M¨ oslinger C, Thenius R and Schmickl” T 2015 BEECLUST used for exploration tasks in Autonomous Underwater Vehicles 8th Int. Conf. on Mathematical Modelling pp 819–824 [33] Shah D and Vachhani L 2019 Swarm Aggregation Without Communication and Global Positioning IEEE Trans. Robot. Autom.4 886–893 [34] Olaronke, Iroju and Ikono, Rhoda and Gambo, Ishaya and Ojerinde, Oluwaseun 2020 A Systematic Review of Swarm Robots Current Journal of Applied Science and Technology 39 79–97 [35] Oh H, Shirazi A R, Sun C and Jin Y 2017 Bio-inspired self- organising multi-robot pattern formation: A review Rob. Auton. Syst.91 83–100 [36] Smith N M, Dickerson A K and Murphy D 2019 Organismal aggregations exhibit ﬂuidic behaviors: a review Bioinspir. Biomim. 14 031001 [37] Saska M, Vakula J and Pˇ reuˇ cil L 2014 Swarms of Micro Aerial Vehicles Stabilized Under a Visual Relative Localization Int. Conf. on Robotics and Automationpp 3570–3575 [38] Walter V, Staub N, Saska M and Franchi A 2018 Mutual 18©IOP Publishing, 2020. DOI: 10.1088/1748-3190/abc6b3 Localization of UAVs based on Blinking Ultraviolet Markers and 3D Time-Position Hough Transform 14th Int. Conf. on Automation Science and Engineeringpp 298–303 [39] Walter V, Staub N, Franchi A and Saska M 2019 UVDAR System for Visual Relative Localization With Application to Leader–Follower Formations of Multirotor UAVs IEEE Trans. Robot. Autom.4 2637–2644 [40] Walter V, Vrba M and Saska M 2020 On training datasets for machine learning-based visual relative localization of micro-scale UAVs Int. Conf. on Robotics and Automation Accepted [41] International Commission on Non-Ionizing Radiation Protection and others 2004 Guidelines on limits of exposure to ultraviolet radiation of wavelengths between 180 nm and 400 nm (incoherent optical radiation) Health Physics 87 171–186 [42] ProLight Opto Technology Corporation 2013 ProLight PM2B-1LLE 1W UV Power LED Technical Datasheet [43] Calovi D S, Lopez U, Ngo S, Sire C, Chat´ e H and Theraulaz G 2014 Swarming, schooling, milling: phase diagram of a data-driven ﬁsh school model New J. Phys.16 015026 [44] Reynolds C W 1987 Flocks, Herds and Schools: A Distributed Behavioral Model 14th Ann. Conf. on Computer Graphics and Interactive Techniquespp 25– 34 [45] Olfati-Saber R 2006 Flocking for multi-agent dynamic systems: algorithms and theory IEEE Trans. Autom. Control 51 401–420 [46] Zhu H, Juhl J, Ferranti L and Alonso-Mora J 2019 Dis- tributed Multi-Robot Formation Splitting and Merging in Dynamic Environments Int. Conf. on Robotics and Automation 9080–9086 [47] Erunsal I K, Ventura R and Martinoli A 2019 Nonlinear Model Predictive Control for 3D Formation of Multirotor Micro Aerial Vehicles with Relative Sensing in Local Coordinates arXiv:1904.03742 [48] Curiac D I and Volosencu C 2015 Imparting protean behavior to mobile robots accomplishing patrolling tasks in the presence of adversaries Bioinspir. Biomim. 10 056017 [49] Elamvazhuthi K and Berman S 2019 Mean-ﬁeld models in swarm robotics: a survey Bioinspir. Biomim.15 015001 [50] Alonso-Mora J 2014 Collaborative motion planning for multi-agent systems Ph.D. thesis (Autonomous Systems Lab, ETH-Z¨ urich) [51] Mohamed E F, El-Metwally K and Hanafy A R 2011 An improved Tangent Bug method integrated with artiﬁcial potential ﬁeld for multi-robot path planning Int. Symp. on Innovations in Intelligent Systems and Applications pp 555–559 [52] B´ aˇ ca T, Petrl´ ık M, Vrba M, Spurn´ y V, Pˇ eniˇ cka R, Heˇ rt D and Saska M 2020 The MRS UAV System: Pushing the Frontiers of Reproducible Research, Real- world Deployment, and Education with Autonomous Unmanned Aerial Vehicles arXiv:2008.08050 [53] Lee T, Leok M and McClamroch N H 2010 Geometric tracking control of a quadrotor UAV on SE(3)49th Conf. on Decision and Controlpp 5420–5425 [54] Petr´ aˇ cek P, Kr´ atk´ y V and Saska M 2020 Dronument: System for Reliable Deployment of Micro Aerial Vehicles in Dark Areas of Large Historical Monuments IEEE Trans. Robot. Autom.5 2078–2085 [55] B´ aˇ ca T, Hert D, Loianno G, Saska M and Kumar V 2018 Model Predictive Trajectory Tracking and Collision Avoidance for Reliable Outdoor Deployment of Unmanned Aerial Vehicles Int. Conf. on Intelligent Robots and Systemspp 6753–6760 [56] Meier L, Tanskanen P, Heng L, Lee G H, Fraundorfer F and Pollefeys M 2012 PIXHAWK: A Micro Aerial Vehicle Design for Autonomous Flight Using Onboard Computer Vision Auton. Robots33 21–39 [57] Koenig N and Howard A 2004 Design and use paradigms for Gazebo, an open-source multi-robot simulator Int. Conf. on Intelligent Robots and Systemsvol 3 pp 2149–2154 [58] Stanford Artiﬁcial Intelligence Laboratory et al Robot Operating System URL ros.org 19",
      "references": [
        "Localization of Ionizing Radiation Sources by Cooperating Micro Aerial Vehicles with Pixel Detectors in Real-Time",
        "Dynamic collaboration without communication: Vision-based cable-suspended load transport with two quadrotors",
        "Evolutionary Swarm Robotics: Evolving Self-Organising Behaviours in Groups of Autonomous Robots (Studies in Computational Intelligence)",
        "Starling Flock Networks Manage Uncertainty in Consensus at Low Cost",
        "A Survey on Aerial Swarm Robotics",
        "Distance-Based Cooperative Relative Localization for Leader-Following Control of MAVs",
        "Leader-follower co-ordination of multiple robots with obstacle avoidance",
        "System for deployment of groups of unmanned micro aerial vehicles in GPS-denied environments using onboard visual relative localization",
        "Low-cost embedded system for relative localization in robotic swarms",
        "An Ultrasonic and Vision-Based Relative Positioning Sensor for Multirobot Localization",
        "Active Infrared Coded Target Design and Pose Estimation for Multiple Objects",
        "Low-latency localization by active LED markers tracking using a dynamic vision sensor",
        "Leader-follower formation control using infrared camera with reﬂective tag",
        "Robust real-time visual tracking using dual-frame deep comparison network integrated with correlation ﬁlters",
        "Onboard Detection and Localization of Drones Using Depth Maps",
        "Drones Light Up The Sky",
        "Flocking algorithm for autonomous ﬂying robots",
        "Outdoor ﬂocking and formation ﬂight with autonomous aerial robots",
        "Optimized ﬂocking of autonomous drones in conﬁned environments",
        "EHang Drone Formation Flight",
        "Reynolds ﬂocking in reality with ﬁxed-wing robots: Communication range vs. maximum turning rate",
        "Towards Autonomous Micro UAV Swarms",
        "Towards A Swarm of Agile Micro Quadrotors",
        "Visual Inertial Odometry Swarm: An Autonomous Swarm of Vision-Based Quadrotors",
        "Indoor navigation with a swarm of ﬂying robots",
        "Environment-independent formation ﬂight for micro aerial vehicles",
        "The World’s ﬁrst GPS MOOC and Worldwide Laboratory using Smartphones",
        "Three-dimensional position and velocity regulation of a quad-rotorcraft using optical ﬂow",
        "Autonomous Vision-based Micro Air Vehicle for Indoor and Outdoor Navigation",
        "A Flexible and Scalable SLAM System with Full 3D Motion Estimation",
        "BEECLUST: A swarm algorithm derived from honeybees: Derivation of the algorithm, analysis by mathematical models, and implementation on a robot swarm",
        "BEECLUST used for exploration tasks in Autonomous Underwater Vehicles",
        "Swarm Aggregation Without Communication and Global Positioning",
        "A Systematic Review of Swarm Robots",
        "Bio-inspired self- organising multi-robot pattern formation: A review",
        "Organismal aggregations exhibit ﬂuidic behaviors: a review",
        "Swarms of Micro Aerial Vehicles Stabilized Under a Visual Relative Localization",
        "Mutual Localization of UAVs based on Blinking Ultraviolet Markers and 3D Time-Position Hough Transform",
        "UVDAR System for Visual Relative Localization With Application to Leader–Follower Formations of Multirotor UAVs",
        "On training datasets for machine learning-based visual relative localization of micro-scale UAVs",
        "Guidelines on limits of exposure to ultraviolet radiation of wavelengths between 180 nm and 400 nm (incoherent optical radiation)",
        "ProLight PM2B-1LLE 1W UV Power LED Technical Datasheet",
        "Swarming, schooling, milling: phase diagram of a data-driven ﬁsh school model",
        "Flocks, Herds and Schools: A Distributed Behavioral Model",
        "Flocking for multi-agent dynamic systems: algorithms and theory",
        "Dis- tributed Multi-Robot Formation Splitting and Merging in Dynamic Environments",
        "Nonlinear Model Predictive Control for 3D Formation of Multirotor Micro Aerial Vehicles with Relative Sensing in Local Coordinates",
        "Imparting protean behavior to mobile robots accomplishing patrolling tasks in the presence of adversaries",
        "Mean-ﬁeld models in swarm robotics: a survey",
        "Collaborative motion planning for multi-agent systems",
        "An improved Tangent Bug method integrated with artiﬁcial potential ﬁeld for multi-robot path planning",
        "The MRS UAV System: Pushing the Frontiers of Reproducible Research, Real- world Deployment, and Education with Autonomous Unmanned Aerial Vehicles",
        "Geometric tracking control of a quadrotor UAV on SE(3)",
        "Dronument: System for Reliable Deployment of Micro Aerial Vehicles in Dark Areas of Large Historical Monuments",
        "Model Predictive Trajectory Tracking and Collision Avoidance for Reliable Outdoor Deployment of Unmanned Aerial Vehicles",
        "PIXHAWK: A Micro Aerial Vehicle Design for Autonomous Flight Using Onboard Computer Vision",
        "Design and use paradigms for Gazebo, an open-source multi-robot simulator",
        "Robot Operating System"
      ],
      "meta_data": {
        "arxiv_id": "2303.02989v1",
        "doi": "10.1088/1748-3190/abc6b3",
        "authors": [
          "Pavel Petracek",
          "Viktor Walter",
          "Tomas Baca",
          "Martin Saska"
        ],
        "published_date": "2023-03-06T09:33:21Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses the challenge of deploying compact UAV swarms in real-world environments without inter-robot communication and without shared absolute localization. Introduces and validates an infrastructure-independent framework built around UVDAR (UltraViolet Direction And Ranging), a UV-camera + active UV-LED marker system enabling onboard, direct mutual relative localization and identification of neighbors. Demonstrates that bio-inspired, purely local-rule flocking (cohesion/alignment/separation plus goal attraction and obstacle avoidance) can be closed in the control loop using only onboard sensing, and reports (to the authors’ knowledge) the first outdoor deployments of decentralized UAV swarms without a communication network or shared absolute positioning. Releases the full system as open source.",
        "methodology": "Uses UVDAR for neighbor perception: UV-sensitive fisheye cameras with band-pass filters detect blinking UV LEDs mounted at known extremal points on each UAV; marker identity is recovered via a specialized 3D time-position Hough transform over image-time data, enabling multi-target separation and (optionally) relative orientation estimation. Relative distance is inferred from calibrated camera geometry and known marker layout. Swarm behavior is generated by a discrete-time, Boids-inspired force model: baseline force averages neighbor relative positions and velocities with a nonlinear distance-dependent repulsion term κ(x,r) for collision avoidance; navigation adds goal attraction and reactive obstacle avoidance. Obstacles sensed locally are abstracted into ‘virtual particles’ (for circular trunks/columns and line-like walls) with derived relative position/velocity used in the same force framework. Control stack: onboard state estimation (IMU + GNSS or alternative), linear MPC tracking, and nonlinear SO(3) acceleration tracking; optional decentralized MPC-based collision-avoidance supervisor for safety.",
        "experimental_setup": "Validation spans simulation and real-world flights. (1) Gazebo+ROS simulations using the same low-level controllers/state estimation as real UAVs: swarms of 5–9 UAVs navigate through narrow passages and dense forest-like fields of obstacles; metrics include average/min inter-UAV distance and min distance to obstacles. (2) Real-world ‘forest-like’ outdoor experiments with 3 UAVs flying among artificial obstacles; initial swarm-model analysis uses GNSS-based relative positions shared over an ad-hoc network to isolate the behavior model. (3) Key no-communication experiments: 4-UAV outdoor flight where UVDAR supplies relative measurements in the feedback loop and UAVs share no states; unique static blinking frequencies (IDs) per UAV. (4) UVDAR accuracy evaluation: (a) in-swarm comparison against GNSS/EKF-fused global estimates (treated as ground truth) to produce histograms/RMSE of distance (~1.16 m), azimuth (~0.17 rad) and combined 3D relative position (~1.7 m); (b) controlled outdoor tracking of a UAV by a static ground UVDAR camera compared to RTK-GNSS ground truth (RMSE ~1.11 m, median ~0.81 m). Also includes simulation study injecting localization noise to analyze stability vs error (min distance declines exponentially with increasing noise).",
        "limitations": "UVDAR requires active UV markers on each UAV (not markerless) and line-of-sight; performance is affected by occlusions, camera resolution, and overlap in dense swarms. Limited number of reliably distinguishable blinking frequencies (~6) constrains straightforward scaling and currently requires manual pre-configuration of IDs (dynamic reassignment not yet solved). Practical detection range is ~10 m recommended (15 m max), biasing the approach toward dense/local-neighborhood swarms. Relative localization accuracy is on the order of ~1 m, which limits admissible swarm density and necessitates careful safety margins; evaluations partly rely on GNSS/EKF as ‘ground truth’, which itself has errors/drift. Real deployments still use external self-localization (GNSS) for individual flight stabilization; thus the presented full system is not yet fully independent of external localization in all environments (indoor requires alternative SLAM/LiDAR). Obstacle handling assumes obstacles can be reduced to line/circle cross-sections and focuses on convex obstacles; perception of obstacles is assumed available from an onboard sensor. A communication-based collision-avoidance supervisor is used as an optional safety layer during testing, which deviates from the strict no-communication premise.",
        "future_research_directions": "Develop decentralized, online blinking-frequency/ID allocation (constraint-satisfaction) to maximize local frequency diversity and scale to larger swarms; formalize convergence and stability. Extend UVDAR with beacon markers or richer temporal codes to improve separation under occlusion and increase the number of uniquely identifiable agents; explore using LEDs for low-rate optical communication (voting, target signaling) while retaining radio silence. Improve relative pose estimation (including reliable relative orientation) via better marker layouts, multi-camera fusion, motion compensation, synchronization, and filtering; quantify accuracy/latency trade-offs and safety bounds for tighter swarms. Replace GNSS self-localization with onboard SLAM/visual-inertial/lidar localization to enable fully infrastructure-free indoor/urban/forest operations; integrate obstacle perception more tightly with UVDAR. Evaluate on larger N, higher speeds, and more complex, dynamic obstacles; add guarantees (e.g., formal safety/collision-avoidance without any communication) and compare with alternative local planners beyond reactive virtual-particle avoidance.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "On Prompt-Driven Safeguarding for Large Language Models",
      "full_text": "Quantum Correlation Sharing: A Review On Recent Progress From Nonlocality To Other Non-Classical Correlations Zinuo Cai and Changliang Ren∗ Key Laboratory of Low-Dimension Quantum Structures and Quantum Control of Ministry of Education, Synergetic Innovation Center for Quantum Effects and Applications, Xiangjiang-Laboratory and Department of Physics, Hunan Normal University, Changsha 410081, China Tianfeng Feng QICI Quantum Information and Computation Initiative, Department of Computer Science, The University of Hong Kong, Pokfulam Road, Hong Kong Xiaoqi Zhou State Key Laboratory of Optoelectronic Materials and Technologies and School of Physics, Sun Yat-sen University, 510006, Guangzhou, China Jingling Chen Theoretical Physics Division of Chern Institute of Mathematics, Nankai University, Tianjin 300071, China (Dated: February 8, 2024) This review offers a comprehensive exploration and synthesis of recent advancements in the domain of quantum correlation sharing facilitated through sequential measurements. We initiate our inquiry by delving into the interpretation of the joint probability, laying the foundation for an examination of quantum correlations within the context of specific measurement methods. The subsequent section meticulously explores nonlocal sharing under diverse measurement strategies and scenarios, with a specific focus on investigat- ing the impact of these strategies on the dissemination of quantum nonlocality. Key perspectives such as “asymmetry” and “weak value” are scrutinized through detailed analyses across various scenarios, allowing us to evaluate the potential of nonlocality sharing. We also provide a retrospective overview of experimental endeavors associated with this phenomenon. The third part of our exploration presents research findings on steering sharing, offering clarity on the feasibility of steering sharing and summarizing the distinctive properties of quantum steering sharing in different scenarios. Continuing our journey, the fourth section delves into discussions on the sharing of diverse quantum correlations, encompassing network nonlocality, quantum entanglement, and quantum contextuality. Moving forward, the fifth section conducts a comprehensive review of the progress in the application of quantum correlation sharing, specifically based on se- quential measurement strategies. Applications such as quantum random access coding, random number generation, and self-testing tasks are highlighted. Finally, we discuss and list some of the key unresolved issues in this research field, and conclude the entire article. CONTENTS I. Introduction 1 A. Quantum Correlations Vs Joint Probability From Sequential Measurements 3 B. The Definition And Simple Criterion Of EPR Quantum Correlation 4 ∗ Corresponding author: renchangliang@hunnu.edu.cn C. Von Neumann Measurement, Weak Measurement, POVM 6 II. Nonlocality Sharing 8 A. The Nonlocality Sharing Via Weak Measurement 9 B. Passive And Active Nonlocality Sharing 11 C. Asymmetric POVM For Boundless Nonlocality Sharing 12 D. Nonlocality Sharing Through Projective Measurements 13 E. Nonlocality Sharing In Multilateral And Multi-Qubit arXiv:2402.04652v1  [quant-ph]  7 Feb 20241 Cases 14 1. 2-Qubit System 14 2. Nonlocality Shairng In 3-Qubit System 15 F. Experimental Implementation Of Nonlocality Sharing 16 1. Double Violations Of CHSH Inequality 17 2. Observation Of Active Nonlocal Sharing 18 3. Certification Of Sustained Entanglement And Nonlocality 19 4. Realization Of Double Violation By Projective Measurements Strategy 20 III. Steering Sharing 21 A. Sequential Steering By Multiple Observers 21 B. Alice Steers Two Bobs 22 C. Steering Sharing In The Multipartite Systems And Multilateral Cases 23 1. Genuine Tripartite EPR Steering 23 2. EPR Steering In The Two-sided Sequential Measurement Scenario 25 3. Steering Sharing Using Nonlocal Measurements 25 D. Experiments Of Steering Sharing 26 1. Detection Steering Sharing 26 2. Witness Multi-observer Steering 27 IV. Sharing Of Other Kinds Of Quantum Correlations 28 A. The Sharing Of Network Nonlocality 28 1. The Extended Bilocal Scenario 28 2. The Extended Star Network Scenario 29 B. The Sharing Of Entanglement 30 C. The Sharing Of Nontrivial Preparation Contextuality 31 V. Applications Based On Quantum Correlation Sharing 33 A. Quantum Random Access Codes 33 B. Random Number Generation 34 C. Self-Testing 35 1. The Self-testing Of QRACs 35 2. Self-testing Of Unsharp Measurements 35 VI. Discussions And Conclusions 36 VII. Acknowledgment 38 References 38 I. INTRODUCTION Quantum mechanics (QM) stands as one of the most groundbreaking developments in twentieth-century physics, providing access to the underlying laws gov- erning the microscopic realm. Despite its significance, the interpretation of QM has been a persistent source of controversy, epitomized by the Einstein-Podolsky-Rosen (EPR) Paradox presented in 1935 (Einstein et al., 1935). This paradox challenges the completeness of QM, posit- ing that quantum entanglement results in instantaneous effects at a distance. The characterization of nonclassical correlations inherent in quantum entanglement has been a long-standing challenge, with questions about the exis- tence of physical systems possessing such properties and the lack of effective differentiation methods. Over a decade of investigation has revealed that align- ing quantum correlation predictions with a specific lo- cal hidden variables theory can resolve the controversy. John Bell’s introduction of the “Bell inequality” (Bell, 1964) in 1964 established a constant boundary that lo- cal hidden variables theories must adhere to, but QM predictions may exceed this boundary in certain cases. (Clauser et al., 1969) proposed a more experimentally accessible inequality, initiating a series of experiments to test the existence of quantum correlation phenomena. In 1982, Aspect’s experiments provided the first detection of Bell inequality violation, signifying the presence of Bell nonlocality (Aspect et al., 1982). While Bell nonlocality and quantum entanglement were initially considered synonymous, Werner’s work in 1989 demonstrated that quantum entanglement in mixed states is a weaker quantum correlation than Bell non- locality (Werner, 1989). In 2007, Wiseman et al. in- troduced another type of nonclassical quantum correla- tion derived from the EPR paradox, known as quantum steering (Wiseman et al., 2007), which relates quantum entanglement to Bell nonlocality. The understanding of quantum correlation has steadily evolved over more than 70 years. Experimental verification of various quantum correla- tions has flourished over the past four decades (Weihs et al., 1998; Rowe et al., 2001; Hofmann et al., 2012; Giustina et al., 2013; Christensen et al., 2013), with mile- stone experiments such as the vulnerability-free verifica- tion of Bell nonlocality in 2015 (Giustina et al., 2015; Hensen et al., 2015; Shalm et al., 2015). Several system- atic review articles cover quantum entanglement, quan- tum steering, and Bell nonlocality (Horodecki et al., 2009a; Brunner et al., 2014; Cavalcanti and Skrzypczyk, 2016; Uola et al., 2020). Ongoing research efforts focus on characterizing quantum correlations in more intricate networks (Pozas-Kerstjens et al., 2022; Tavakoli et al., 2022a) and temporal correlations (Ku et al., 2018; Costa et al., 2018; Zhao et al., 2018; Zhang et al., 2020). Initially a fundamental topic in QM, the exploration of quantum correlation has paved the way for the emergence of Quantum Information Science (Bennett and Brassard, 1984; Bennett and DiVincenzo, 2000; Bouwmeester and Zeilinger, 2000). This interdisciplinary field aims to de- velop practical applications based on advancements in quantum correlation, encompassing quantum communi- cation (Bouwmeester et al., 1997; Gisin and Thew, 2007; Ewert et al., 2016; Ren et al., 2017; Shi et al., 2020; Hu et al., 2021), quantum computing (Wang et al., 2017; Xin et al., 2018; Fitzsimons et al., 2018; Arute et al., 2019; Zurel et al., 2020; Chabaud and Walschaers, 2023), quantum precision measurement (Yamamotoet al., 1990; Braunstein, 1992; Giovannettiet al., 2006; Thomas-Peter et al., 2011), and other related topics. Although there have been many reviews about different quantum correlations (Horodecki et al., 2009a; Brunner et al., 2014; Cavalcanti and Skrzypczyk, 2016; Uolaet al., 2020), we will introduce and review the recent progress in studying quantum correlations based on sequential mea-2 surements from a different research perspective, which has gained increasing attention in recent years. The in- vestigation into this research theme is closely tied to the repetitive utilization of quantum resources. Indeed, there have been some studies on quantum correlations based on temporal measurements before (Nogues et al., 1999; Gudder and Nagy, 2001; Pryde et al., 2004; Sciarrino et al., 2006; Filip, 2011; Nagali et al., 2012; Burgarth et al., 2015; Westerbaan and Westerbaan, 2016). How- ever, Silva et al. approached this issue from a fresh per- spective in 2015. In a sense, this serves as a seed work for this review. They reported a counterintuitive result, namely that weak measurements can be used to share nonlocality between two or more observers. The scenario involves allocating a pair of a maximally entangled states to three observers: Alice, Bob 1, and Bob 2. Alice ac- cesses one qubit, while the two Bobs access the other. Alice performs a projective measurement on her qubit, and Bob 1 performs a weak measurement on his qubit, then transfers it to Bob2, who eventually performs a pro- jective measurement. The measurement results indicate that both Bob 1 and Bob 2 may simultaneously violate the CHSH inequality with Alice. So far, along this path, a series of effective theoretical studies about nonlocal- ity sharing have been investigated (Gallego et al., 2014; Mal et al., 2016; Schiavon et al., 2017a; Curchod et al., 2018; Tavakoli and Cabello, 2018; Li et al., 2018b; Datta and Majumdar, 2018; Das et al., 2019; Saha et al., 2019; Ren et al., 2019; Brown and Colbeck, 2020; Cheng et al., 2021; Zhang and Fei, 2021; Cabello, 2021; Ren et al., 2022; Steffinlongo and Tavakoli, 2022; Zhu et al., 2022; Hu et al., 2022; Zhang et al., 2022; Cheng et al., 2022; Xi and Fu, 2022; Xi et al., 2023; Sasmal et al., 2023), and various experimental demonstrations have also been carried out (Schiavon et al., 2017b; Hu et al., 2018; Feng et al., 2020; Foletto et al., 2020b; Foletto et al., 2020a; Xiao et al., 2022). These findings not only offer fresh insights into the interplay between nonlocality and quantum measure- ment, particularly the manifestation of nonlocality shar- ing through weak measurements, but also stimulate fur- ther investigation into analogous concepts such as quan- tum entanglement, quantum coherence, quantum dis- cord, and quantum steering. Moreover, researchers have delved into the broader concept of sequential measure- ments as an alternative to weak measurements, prompt- ing explorations into the properties of quantum corre- lation within a comprehensive framework of sequential measurement. This has led to the discovery of various applications of sequential measurements in understand- ing quantum correlation, such as, the steering sharing in theoretical (Sasmal et al., 2018; Yao and Ren, 2021; Gupta et al., 2021; Zhu et al., 2022; Han et al., 2023) and experimental investigation (Xiang et al., 2019; Shenoy H. et al., 2019; Choi et al., 2020; Paul and Mukherjee, 2020; Han et al., 2021; Liu et al., 2021; Liu et al., 2022; Han et al., 2022; Li et al., 2022; Lv et al., 2023; Chen et al., 2023), network nonlocality sharing (Hou et al., 2022; Wang et al., 2022; Halder et al., 2022; Mahato and Pan, 2022; Zhang et al., 2023; Kumar and Pan, 2023; Mao et al., 2023), entanglement sharing (Bera et al., 2018; Maity et al., 2020; Foletto et al., 2020b; Srivastava et al., 2021a; Pandit et al., 2022; Srivastava et al., 2022a; Das et al., 2022; Srivastava et al., 2022b; Hu and Fan, 2023; Li and Wang, 2023), contextuality sharing (Kumari and Pan, 2019; Anwer et al., 2021; Chaturvedi et al., 2021; Kumari and Pan, 2023). Furthermore, it has been observed that these research outcomes are closely connected to numerous quantum in- formation tasks, leading to a gradual exploration of is- sues related to sequential measurements, including, self- test (Mohan et al., 2019; Srivastava et al., 2021b; Miklin et al., 2020; Tavakoli et al., 2020; Mukherjee and Pan, 2021; Pan, 2021; Roy and Pan, 2023;), random num- ber generation (Curchod et al., 2017; Coyle et al., 2018; An et al., 2018; Bowles et al., 2020; Pan, 2021; Foletto et al., 2021), and quantum random access codes (Liet al., 2018a; Mohan et al., 2019; Anwer et al., 2020; Foletto et al., 2020c; Wei et al., 2021; Das et al., 2021; Xiao et al., 2021; Xiao et al., 2023) etc. The structure of this article is outlined as follows: in the subsequent part of the introduction, we delve into the nature of EPR-quantum correlations, elucidating key definitions and elementary criteria from the perspective of joint probabilities. Fundamental concepts are intro- duced, including a brief comparison of von Neumann measurements, weak measurements, and POVM, as these form the core of subsequent discussions. Section II is dedicated to a comprehensive exploration of nonlocality sharing under various measurement strategies and sce- narios. Specifically, we investigate the impact of mea- surement strategies on quantum nonlocality sharing, ex- amining perspectives such as “asymmetry” and “weak value”. Through detailed analyses in diverse scenarios, we assess the capability of nonlocal sharing, accompa- nied by a retrospective overview of experimental efforts related to nonlocal sharing. Section III introduces steer- ing sharing, elucidating the shareability of steering, with a focus on directional aspects in sequential scenarios. Various measurement strategies for steering sharing are demonstrated. The sharing of other types of quantum correlations, including network nonlocality, quantum en- tanglement, and quantum contextuality, are discussed in Section IV. It is noteworthy that these extended con- siderations offer new perspectives for sharing quantum physical resources in the realm of quantum physics. In Section V, we review the sharing of quantum correla- tions under quantum sequential measurement strategies in applications such as quantum random access coding, random number generation, and self-testing tasks. Fur- thermore, it provides updates on the latest developments. Finally, Section VI presents our conclusions, along with3 prospects and outlooks for the field of quantum resource sharing derived from sequential measurements. A. Quantum Correlations Vs Joint Probability From Sequential Measurements The research on different quantum correlations has developed for a long time, and a multitude of differ- ent correlation criteria have been proposed, such as, for Bell nonlocality (Bell, 1964; Clauser et al., 1969; Mer- min, 1990; Ardehali, 1992; Belinski˘ ı and Klyshko, 1993; Collins et al., 2002; ˙Zukowski and ˇCaslav Brukner, 2002; Brukner et al., 2002; Brunner et al., 2014), quantum en- tanglement (Horodecki et al., 1996; Terhal, 2000; Bruß et al., 2002; G¨ uhne et al., 2003; Uffink and Seevinck, 2008; Lougovski and van Enk, 2009; Horodecki et al., 2009b; G¨ uhne and T´ oth, 2009; Chru´ sci´ nski and Sarbicki, 2014), and quantum steering (Cavalcanti et al., 2009; Reid, 2013; Cavalcanti et al., 2015; Costa and Angelo, 2016). All of these criteria can be expressed in terms of joint probability distribution. Hence, the EPR quantum correlation problem can be summarized as “a thought experiment based on real joint probability”. This per- spective allows us to explain almost all of the criteria and experimental demonstrations of quantum correlations to date. This review aims to comprehensively summarize the recent research on the nature of quantum correla- tions in the context of sequential measurements. We start by elucidating the inherent reasons for quantum correla- tion sharing through sequential measurements, focusing on the change of conditions of joint probability. For a quantum state system comprising N particles, there are N observers, each assigned one particle, and each observer has the flexibility to employ multiple mea- surement methods. However, during each measurement cycle, each observer will only perform a single measure- ment on the particle they hold. This cyclic process gener- ates a joint probability distribution based on N measure- ments. Subsequently, combinations of these joint proba- bilities, or derived physical quantities (such as averages), are utilized to showcase conflicts with classical models. For instance, in a 2-qubit system, the joint probability in- volves two measurements denoted as P(a, b|A, B, ρ). It’s important to note that the quantum state measurements exhibit the characteristic that each observer measures their particle only once. Nevertheless, this characteristic should not be regarded as an indispensable criterion. In- stead, this joint probability may originate from a higher dimensional joint probability based on a more extensive set of measurements (Masanes et al., 2006; Toner, 2009). Considering a straightforward example involving a 2-qubit system, if each qubit is sequentially measured by two observers in their possession during each round—using measurement operators A 1 and A2 for one qubit, and B 1 and B2 for the other—one arrives at the joint probability P(a1, a2, b1, b2|A1, A2, B1, B2, ρ). Describing the correlation information of a 2-qubit quantum state through joint probability provides a more comprehensive perspective. This is due to its ability to consistently revert to the previous joint probability, as illustrated by the equation, P(a1, b1|A1, B1, ρ) = X a2, b2P(a1, a2, b1, b2|A1, A2, B1, B2, ρ), (1) where ρ is the density matrix of the quantum state. The previous research overlooked the above-discussed approach mainly due to the measurements. Currently, the most frequently used quantum correlation criteria approach is where Alice and Bob use strong measurements for their first measurements. And if the observer makes a subsequent second measurement, the joint probability can always be expressed as, P(a1, a2, b1, b2|A1, A2, B1, B2, ρ) = P(a1, b1|A1, B1, ρ)P(a2|A2, Πa1 A1 )P(b2|B2, Πb1 B1 ), (2) where Π a1 A1 and Π b1 B1 represent the eigenstates corresponding to the first measurement operator. In this case, any subsequent measurements hold no significance as they do not provide any correlation information about the initial state, due to P(a2, b2|A2, B2) = P(a2|A2, Πa1 A1 )P(b2|B2, Πb1 B1 ). However, if the decomposition of the joint probability mentioned above is not satisfied, the joint probability P(a2, b2|A2, B2) cannot be separated into the product of two probability densities. At this stage, subsequent measurements cease to be ordinary. The non-classical correlations originally present in the density matrix ρ of a composite quantum system may still be concealed within the outcomes of this joint probability. From a physical perspective, meeting the above re- quirements involves minimizing the disruption of the initial state’s correlation information during the for- ward measurements. With the development of quantum non-destructive measurement theory and technology, the mentioned process is achievable, such as weak measure- ment processes (Aharonov et al., 1964; Aharonov et al., 1988; Aharonov and Daniel, 2005; Gisin, 2016). Weak measurements do not fully disturb or only partially dis- turb the state of the system, where the trade-off between4 information and disturbance has been studied (Fuchs and Peres, 1996; Sciarrino et al., 2006; Banaszek, 2006; Buscemi and Horodecki, 2009; Nagali et al., 2012; Spara- ciari and Paris, 2014). Besides, the generalized Positive Operator-Valued Measurement (POVM) (Busch, 1986; Busch et al., 1997; Yuan, 2022) may also satisfy the above conditions. Quantum correlation sharing is related to the strategy of measurements, which we will unfold this re- view around. B. The Definition And Simple Criterion Of EPR Quantum Correlation Here, we will revisit the definitions of quantum cor- relations, particularly quantum entanglement, quantum steering, and Bell non-locality, derived from the EPR paradox. Additionally, we will introduce the definition of network quantum correlations which is a recent and emerging topic. The distinctions among these definitions will be elucidated within a straightforward scenario. Considered a scenario involving two observers, Alice and Bob, who jointly possess the state ρAB of a 2- qubit quantum system. Alice performs a measurement ˆA, yielding the result ‘a’. Similarly, Bob measures ˆB with the result ‘b’. Without loss of generality, we obtain the joint probability distribution P(a, b| ˆA, ˆB, ρAB) in the real-world context. For this joint probability to accurately align with the predictions of quantum mechanics, it must adhere to the following criterion, P(a, b| ˆA, ˆB, ρAB) = Tr \u0002\u0000 ˆEa A ⊗ ˆEb B \u0001 ρAB \u0003 , (3) where ˆEa A represents the density matrix associated with the eigenstate of the operator ˆA and eigenvalue ‘a’, and { ˆEb B} represents the density matrix associated with the eigenstate of the operator ˆB and eigenvalue ‘b’. For any arbitrary measurements, if the above joint probability satisfies the following decomposition, P(a, b| ˆA, ˆB, ρAB) = Z P(a| ˆA, ξ)P(b| ˆB, ξ)Pξdξ, (4) which indicates the existence of a local hidden variables (LHV) model (Brunner et al., 2014) capable of explaining the given joint probability. Here, ξ represents the hidden variable, Pξ is the distribution of the hidden variable, and P(a|A, ξ) denotes the probability of measuring ˆA and ob- taining result ‘a’ under the specific hidden variableξ, and a similar definition for P(b| ˆB, ξ) as well. If Eq. (4) lacks a solution, it signifies that no LHV model can elucidate the joint probability distribution in the described sce- nario, indicating the presence of Bell nonlocality in the system. For arbitrary measurements, if the joint probability and marginal probability satisfy(Wiseman et al., 2007), P(a, b| ˆA, ˆB, ρAB) = R P(a| ˆA, ξ)PQ(b| ˆB, ξ)Pξdξ, (5) PQ(b| ˆB, ξ) = Tr[ ˆEb BρB ξ ], (6) which indicates the presence of a local hidden state (LHS) model interpretation for the joint probability. Here, ρB ξ represents the density matrix of the subspace controlled by Bob, satisfying R PξρB ξ dξ = TrA[ρAB]. PQ(b| ˆB, ξ) de- notes the probability of obtaining the result ‘ b’ when a true quantum state ρB ξ measured by ˆB. Eq. (5) implies that Alice does not influence Bob’s state. If no solution exists, it implies that no LHS model can elucidate the joint probability distribution in the scenario, indicating the presence of steering properties: Alice can steer Bob. For any measurement, if the joint probability and the marginal probability adhere to the conditions, P(a, b| ˆA, ˆB, ρAB) = R PQ(a| ˆA, ξ)PQ(b| ˆB, ξ)Pξdξ, (7) PQ(a| ˆA, ξ) = Tr[ ˆEa AρA ξ ], (8) PQ(b| ˆB, ξ) = Tr[ ˆEb BρB ξ ], (9) it signifies the presence of a Separable Model (SM) in- terpretation for the joint probability (Horodecki et al., 2009a). Here, ρA ξ represents the density matrix of the subspace controlled by Alice, satisfying R PξρA ξ dξ = TrB[ρAB]. Eq. (7) is a formulation that satisfies the relevant probabilities of separable states in quantum me- chanics. In the absence of a solution, it implies that the Separable Model (SM) fails to explicate the joint proba- bilities of the scenario, signifying the presence of entan- glement in the given scenario. According to the provided definition, excluding Lo- cal Hidden Variables (LHV) models, Local Hidden State (LHS) models, or Separable Model (SM) interpretations represents a significant challenge. In response, Bell’s in- novative approach is to outline the constraints of classi- cal models rather than delving into their specific forms. When the predictions of quantum mechanics surpass the constraints imposed by classical models, the existence of non-classical correlations is affirmed. Bell intuitively pro- posed a method, known as Bell’s inequality (Bell, 1964), which will be always obeyed by any classical correlations. Since then, researchers have extensively explored vari- ous methods to elucidate quantum correlations. Actu- ally, all of these criteria can be expressed in terms of the joint probability distribution. Without loss of generality, we review and introduce the simplest and arguably the most renowned inequality criterion for assessing Einstein- Podolsky-Rosen (EPR) quantum correlations. In a 2-qubit system, Alice and Bob can randomly select one from m possible operators to measure their received qubits, respectively. Alice’s measurements are labeled as { ˆAx} = ˆnx · ˆσ, for Bob as { ˆBy} = ˆny · ˆσ. where x ∈ {0, 1..m − 1}, y ∈ {0, 1..m − 1}, ˆσ = ( σx, σy, σz)5 are three Pauli matrices. Through multiple repeated ex- periments, one can obtain the joint probability, denoted as P(a, b|x, y). Further, the average value of the com- bination of any arbitrary operators can be given from the joint probability, ⟨ ˆAx ˆBy⟩ = P a,b(−1)a+bP(a, b|x, y). All Bell inequalities can be constructed by the average values, such as Clauser-Horne-Shimony-Holt (CHSH) in- equality (Clauser et al., 1969), S = ⟨ ˆA0 ˆB0⟩ + ⟨ ˆA0 ˆB1⟩ + ⟨ ˆA1 ˆB0⟩ − ⟨ˆA1 ˆB1⟩ ≤2. (10) This inequality can be violated under specific conditions, thereby illustrating the non-locality inherent in quan- tum systems. For instance, when the initial state is the maximally entangled state, |ψ⟩ = 1√ 2 (|00⟩ + |11⟩), Alice chooses ˆA0 = σz and ˆA1 = σx, Bob chooses ˆB0 = σz+σx 2 and ˆB1 = σz−σx 2 , the resulting CHSH value is given by S = 2 √ 2 ≥ 2, representing the quantum-theoretical up- per bound of the CHSH inequality. Similar to the determination of Bell nonlocality, if the joint probability distribution in the given scenario defies characterization by a Local Hidden State (LHS) model, Bob must concede the possibility that Alice might steer her state through certain “remote actions” within her subspace. In such circumstances, Bob’s state is consid- ered steerable. It is important to note that “steerable” is asymmetric. Such as Alice can steer Bob in certain states, but the reverse may not hold. Similarly, we can utilize the joint probability distribution of the scenario to derive inequalities for assessing steering. A classical ex- emplary illustration is the linear steering inequality (Cav- alcanti et al., 2009), Sm = mX i=1 | D ˆAi ⊗ ˆBi E | ≤ Bm, (11) Bm = max {ai=±1} ( 1√mλmax  mX i=1 ai ˆBi !) , where Bm represents the classical bound, λmax(ˆX) de- notes the maximum eigenvalue of ˆX. The parameter m signifies the number of measurement choices. Quan- tum steering can be determined by Sm > Bm. No- tably, Bm varies with alterations in the measurement settings, for instance, B2 = 1√ 2 ≈ 0.7071, B3 = B4 = 1√ 3 ≈ 0.5773, B6 ≈ 0.5393. Specifically, for m = 2 , 3, the linear steering inequality can be expressed as Sm = 1√m Pm i=1 |⟨Ai ⊗ Bi⟩| ≤1. In the determination of entanglement, a commonly em- ployed method is the entanglement witness (Horodecki et al., 2009a). This approach relies on the expected val- ues of specific Hermitian operators to discern entangled states from separable ones. This method of entanglement detection leverages the Hahn-Banach theorem, asserting that for any element outside a closed convex set in a normed linear space, there always exists a functional on that space that “separates” the element from the closed convex set. The entanglement witness is represented by the operator W (Terhal, 2000), where ⟨W⟩ρ denotes the expected value of the Hermitian operator W concerning the state ρ. For instance, for all separable states ρo, it holds that ⟨W⟩ρo ≥ 0. Conversely, there exists at least one entangled state ρe for which ⟨W⟩ρe < 0. With the deepening exploration of quantum correla- tions, quantum phenomena within networks have grad- ually become a focal point of attention (Tavakoli et al., 2022a). The network correlation manifests in scenarios where there are multiple independent sources, represent- ing a form of non-locality that transcends Bell’s theorem ( ˙Zukowski et al., 1993; Zukowski et al., 1995; Pan et al., 1998; Bose et al., 1999; Jennewein et al., 2001; Bran- ciard et al., 2010; Fritz, 2012; Branciard et al., 2012; Gisin et al., 2017; Andreoli et al., 2017; Tavakoli et al., 2021). Numerous studies have delved into the inves- tigation of these phenomena, expanding their scope to more intricate network structures (Tavakoli et al., 2014; Mukherjee et al., 2015; Fritz, 2016; Mukherjee et al., 2017; Fraser and Wolfe, 2018; Wolfe et al., 2019; Renou et al., 2019; Mukherjee et al., 2020; Renou et al., 2021; Yang et al., 2021; Pozas-Kerstjens et al., 2022; Renou and Beigi, 2022; Munshi and Pan, 2022; Tavakoli et al., 2022b; ˇSupi´ cet al., 2022). Considered a simple network scenario with two sources, each associated with a hidden variable ξi, i ∈ {1, 2}. The two sources are shared between Alice and Bob, as well as between Bob and Charlie respectively. The ini- tial state can be given by ρABC = ρAB ⊗ ρBC . Al- ice performs a measurement ˆA in her subspace, corre- sponding to the outputs a, and Charlie performs a mea- surement ˆC in his subspace, corresponding to the out- puts c. Bob carries put a Bell state measurement ˆB on the received two particles with the outcome b = b0b1. The joint probability obtained in this scenario can be expressed as P(a, b, c| ˆA, ˆB, ˆC, ρABC ). For any measure- ment, if the joint probability can be factorized (Branciard et al., 2010), P(a, b, c| ˆA, ˆB, ˆC, ρABC ) = Z ξ1 dξ1 Z ξ2 dξ2P1(ξ1)P2(ξ2) ×P(a| ˆA, ξ1)P(b| ˆB, ξ1, ξ2)P(c| ˆC, ξ2), (12) which signifies that the joint probability is explicable through a bilocal hidden variable (BLHV) model. Here, P1(ξ1) represents the hidden variable distribution of the first source, and similarly, P2(ξ2) pertains to the second source. The term P(b| ˆB, ξ1, ξ2) denotes the probabil- ity distribution linked to a measurement ˆB yielding the outcome b, conditioned on the determined hidden vari- ables ξ1 and ξ2. Furthermore, P(a| ˆA, ξ1) characterizes the probability distribution associated with a measure-6 ment ˆA resulting in the outcome ‘ a’ under the influence of a determined hidden variable ξ1. Analogously, the term P(c| ˆC, ξ2) represents the probability distribution for a measurement ˆC resulting in the outcome c under a specified hidden variable ξ2. Eq. (12) delineates the local properties within the network scenario. If no solu- tion exists, it implies that the joint probability for such a network scenario cannot be described by the BLHV model. This substantiates the nonlocal correlation within the network scenario. Assuming that Alice and Charlie randomly select one of the m possible measurements to perform on the re- ceived particle, with Alice’s measurement denoted as ˆAx and Charlie’s measurement as ˆCz, where x, z∈ {0, 1..m}. The joint probability P(a, b, c|x, z) can be obtained. Consequently, its average value can be expressed as ⟨AxBbk Cz⟩ = P a,b0b1,c(−1)a+bk+cP(a, b0b1, c|x, z). For the case of m = 2, the joint probability can construct an inequality serving as a criterion for network nonlocal- ity, such as the Branciard-Rosset-Gisin-Peronio (BRGP) inequality (Branciard et al., 2010), p | I | + p | J | ⩽ 1, (13) where I = 1 4 X x,z=0,1 ⟨AxBb0 Cz⟩, J = 1 4 X x,z=0,1 (−1)x+z⟨AxBb1 Cz⟩. The violation of this equality transcends the constraints of locality in network scenarios, showcasing a nonlocal property inherent in the network. The above description pertains to the most typical criteria concerning Bell nonlocality, steering, and net- work nonlocality. Discussions regarding these criteria take place within the framework of the simplest Bell sce- narios or bilocal scenarios. For more complex scenarios, corresponding criteria can be constructed by extending classical standards, utilizing joint probabilities obtained from the measurement process. Further elaboration on this topic is presented in Sections II, III, and IV. C. Von Neumann Measurement, Weak Measurement, POVM The exploration of quantum correlations intricately in- tertwines with quantum measurements (Aharonov et al., 1988; Aharonov and Daniel, 2005). Among the piv- otal quantum measurements, notable ones include the von Neumann measure (Busch et al., 1996; Nielsen and Chuang, 2010; Wheeler and Zurek, 2014) and the positive-operator valued measure (POVM) (Davies and Lewis, 1970; Busch, 1986; Yuan, 2022), among others. The most basic form of quantum measurement is the von Neumann measure, also referred to as the projection- valued measure (PVM) (Nielsen and Chuang, 2010). It is defined through a Hermitian operator that can be decom- posed as ˆM = P i λi|i⟩⟨i| = P i λi ˆPi. Here, λi denotes the eigenvalue of the operator, |i⟩ represents an eigen- vector with an eigenvalue of λi, and ˆPi = |i⟩⟨i| is desig- nated as a von Neumann measurement operator within the Hilbert space H. The constraints associated with this measurement are as follows, (i). ˆPi = ˆP† i ; ( ii). ˆPi ≥ 0; ( iii). ˆP2 i = ˆPi; (iv). ˆPi ˆPj = δij, i̸= j; ( v). P i∈M ˆPi = IH. (14) In a von Neumann measurement, when the measure- ment yields the result λi, the quantum state of the sys- tem undergoes collapse to the eigenstate corresponding to ˆPi. Consequently, the post-measurement state be- comes: |ψ(i) post⟩ = ˆPi|ψ⟩√ ⟨ψ| ˆPi|ψ⟩ , which requires normaliza- tion to ensure a total probability of 1. The Von Neu- mann measurements constitute a probabilistic process, and the outcomes are not deterministic but are contin- gent upon the probability distribution inherent in quan- tum states. This distribution is dictated by the inner product of quantum states and measurement operators: p(i|ψ⟩) = ⟨ψ| ˆPi|ψ⟩. The Von Neumann measurements form the foundational basis for the quantum mechanics measurement theory, providing a mathematical frame- work elucidating the measurement of quantum states and their evolution post-measurement. POVM (Positive Operator-Valued Measure) is a gener- alized form of quantum measurements, which extends the principles of von Neumann measurements. It allows for a broader range of measurement methods, encompassing not only standard projective measurements but also non- standard ones. Within the von Neumann measurement framework, POVM measurements relax the constraints on the measurement operator regarding the idempotence condition (Eq. (14. iii)) and orthogonality constraint (Eq. (14. iv)), denoted as { ˆQi, i∈ M}. The associated constraints are, (i). ˆQi = ˆQ† i ; ( ii). ˆQi ≥ 0; ( iii) P i∈M ˆQi = IH. (15) POVM measurements employ a set of positive operator-valued operators to delineate the measurement process, rather than solely relying on standard projec- tive operators. These operators are Hermitian but not necessarily orthogonal. Diverging from von Neumann measurements, POVM measurements enable a broader and more generalized probability distribution. In POVM measurements, the probability of obtaining a measure- ment result λi is determined by the inner product be- tween the POVM elements and the quantum state ρ be- fore measurement, expressed as p(i|ρ⟩) = Tr( ˆQiρ). The7 significance of POVM measurements lies in their provi- sion of a more flexible approach to measurements, en- abling researchers to design various types of measure- ments, including those accommodating a certain degree of uncertainty or noise. This flexibility proves highly valuable in applications such as quantum information processing, quantum communication, and quantum mea- surements. Furthermore, POVM measurements play a pivotal role in the study of quantum measurement the- ory, furnishing a robust tool for describing and analyzing intricate measurement processes. It is worth noting that, due to the absence of or- thogonal constraints, directly determining the post- measurement state in POVM measurements is not fea- sible. In this context, the Kraus operators associated with the ˆQi, denoted as { ˆAi, i∈ M}, play a crucial role, enabling the implementation of a general measurement approach(Nielsen and Chuang, 2010). The Kraus decom- position for each i ∈ M can be expressed as ˆQi = ˆA† i ˆAi. However, the Kraus decomposition of the measurement operator ˆQi lacks uniqueness, as exists any unitary op- erator ˆU, ˆU ˆAi still belongs to the Kraus operator. Em- ploying the Kraus operator ˆAi = q ˆQi offers a method to describe the post-measurement state, expressed as ρ(i) post = ˆAiρ ˆAi Tr( ˆA† i ˆAiρ) . Building upon the standard von Neumann measure- ment model, Aharonov et al. have conducted research from the perspective of measurement coupling, introduc- ing another measurement form, known as weak measure- ment theory. This theory is commonly employed to in- vestigate the properties of quantum systems, particularly the relationships between states before and after mea- surement(Aharonov et al., 1988). In the aforementioned von Neumann measurements, the outcomes are denoted as λi. Consequently, the quan- tum state of the system undergoes an inevitable collapse to the associated eigenstate of ˆPi. The collapse states cor- responding to distinct measurement outcomes are non- degenerate, expressed as ˆPi ˆPj = δij. At this juncture, the position of the reading pointer yields complete infor- mation about the measured physical quantity. Von Neumann designed a model for implementing pro- jective measurements, conceptualizing measurement as the interaction between a system and measurement ap- paratus. In essence, he interconnected the quantum mea- surement uncertainty with the macroscopic uncertainty of the measuring device (Von Neumann, 1955; Home et al., 2007; Von Neumann, 2018). The Aharonov model extends this concept, differing from von Neumann’s mea- surement model in two key aspects (Aharonov et al., 1988; Aharonov and Vaidman, 1990; Aharonov and Daniel, 2005). Firstly, in weak measurements, the in- teraction between the measuring device and the sys- tem is typically weak, and the collapsed states corre- sponding to different measurement outcomes are degen- erate, minimizing significant interference with the quan- tum state of the system. This weak coupling property allows for a more nuanced exploration of the relation- ship between states before and after measurement. Sec- ondly, Aharonov requires weak measurements to project selectively onto final states, preserving the time-reversal symmetry of the measurement process. Weak values are commonly used to represent such measurement scenarios. Typically, the weak value denotes measurements on an ensemble of preselected and postselected systems (Aharonov et al., 1964). The weak value is defined as Mw = ⟨ψf | ˆM|ψi⟩ ⟨ψf |ψi⟩ (Aharonov et al., 1988), where |ψi⟩ and |ψf ⟩ represent the initial state and the postselected state of the measurement, andM is the measured observ- able. In essence, weak measurement serves as a means for an observer to extract partial information from a sys- tem while causing minimal disturbance. There exists a delicate trade-off between the amount of information ob- tained from the system and the level of interference in- troduced to it (Sciarrino et al., 2006; Banaszek, 2006; Nagali et al., 2012; Gisin, 2016): on one hand, weaker measurements extract less information but cause less dis- turbance to the system; on the other hand, stronger mea- surements provide more information but result in greater system disturbance. We project the measurements onto the measurement operator’s eigenstate space, which re- quires weak coupling of the system to the measuring de- vice (often called the auxiliary device). The acquired information will be available through the measurement of the auxiliary device. In the overall system, it is assumed that the initial quantum state of the system is|ψ⟩. The quantum state of the measuring device is |D⟩. The state of the measuring device on the projection of its pointer is represented by the probability density of its pointer reading, which has many possible distributions. The initial state of the entire system is expressed as |Ψint⟩ = |ψ⟩ ⊗ |D(q0)⟩. In the standard von Neumann measurement model, the interaction between the measur- ing device and the quantum system is H = Hint = ˆM ⊗ ˆp. Where ˆp is the degree of freedom coupled with the pointer. The pointer reading q falls on the eigenvalue λi with probability |⟨i|ψ⟩|2. By projecting the pointer state onto λi, the final state of the whole system can be given as, |Ψfin⟩ = |ψ⟩ ⊗ |D(q)⟩ = X i ⟨i|ψ⟩|D(q − λi)⟩|i⟩. (16) During the measurement process, the movement of the pointer is proportional to the eigenvalue of the measure- ment. For von Neumann measurements, the initial state of the pointer is narrower than the distance between the eigenvalues, ⟨D(q − λi)|D(q − λj)⟩ = δi,j, hence reading the position of the pointer provides all information about the measured physical quantity. For weak measurements,8 their weak coupling properties result in the initial state of the pointer being wider than the eigenvalues, reading the position of the pointer cannot provide all informa- tion. If the pointer is very large and covers the entire eigenvalue spectrum, reading the position of the pointer provides no information due to⟨D(q−λi)|D(q−λj)⟩ ≈1. The system will be not affected by interference, |ψfin⟩q0 = X i ⟨i|ψ⟩⟨q0|D(q − λi)⟩|i⟩ ≈ ⟨q0|D(q)⟩ X i ⟨i|ψ⟩|i⟩ = ⟨q0|D(q)⟩|ψ⟩ = |ψ⟩. (17) This is an extremely weak measurement. Taking spin-1 2 particles as an example, the initial state is |ψ⟩ = a|0⟩ + b|1⟩, where ( a2 + b2 = 1). After the in- teraction between the pointer and the system, the entire system state changes to a|0⟩⊗ D(q −1) +b|1⟩⊗ D(q + 1). The weak measurement can be characterized by two pa- rameters, namely the quality F and the precision factor G. The quality factor can be given as F = R∞ −∞⟨D(q + 1)|D(q −1)⟩dq. It quantifies the degree to which the sys- tem is not disturbed after measurement. The precision factor can be given as G = R+1 −1 D2(q)dq. It quantifies the information about the state obtained by the mea- surement. The greater the value of G, the smaller the corresponding F, indicating greater information extrac- tion ability of the measurement and more disturbances in the system. Strong measurement performance is achieved when G = 1 and F = 0. The trade-off between two fac- tors is defined as a pointer. Through tracking the state of the pointer, the system can be represented as, ρ′ = F ρ+ (1 − F)(Π+ρΠ+ + Π−ρΠ−), (18) where ρ = |ψ⟩⟨ψ|, and Π± is the projection of the results |0⟩ and |1⟩. The probability distribution can be given as p(±) = G⟨ψ|Π±|ψ⟩+(1−G)1 2 . Silva et al. found that the square pointer, where G + F = 1, is not optimal. It can not provide the optimal trade-off for the measurement. Specifically, for a given quality factor F, it does not of- fer the maximum accuracy G. Subsequently, Silva et al. investigated the optimal trade-off condition,G2+F2 = 1. Weak measurements are a classification of POVM that can be expressed as a set of positive operators, denoted as ˆQ ≡ {ˆQi|P i ˆQi = I, 0 < ˆQi ≤ I}. They can also be represented as a projective measurement with white noise, given by ˆQγ i = γ ˆPi + (1 − γ) I d , where γ is the unsharpness parameter in the range of [0 , 1]. It is pos- sible to construct the Kraus operator, ˆAγ i = q ˆQγ i , to describe the post-measurement state, ρ′ = P i ˆAγ i ρ ˆAγ i . Continuing with the example of a spin- 1 2 system, let’s define i ∈ {+, −}. The measurement operator can be given as ˆQγ ± = γ ˆP± + (1 − γ) I 2 = 1 2 (I ± γ⃗ n.ˆσ), and the corresponding Kraus operator can be formed as ˆAγ ± = q ˆQγ ± = 1√ 2 (√1 + γ ˆP+ + √1 − γ ˆP−). The post- measurement state can be represented as ρ′ = ˆAγ +ρ ˆAγ + + ˆAγ −ρ ˆAγ −, extending as, ρ′ = p 1 − γ2ρ + (1 − p 1 − γ2)( ˆP+ρ ˆP+ + ˆP−ρ ˆP−). (19) The probability distribution remain as p(±) = Tr[ ˆQγ ±ρ] = γTr[ˆP±ρ] + 1−γ 2 . Compared with the pre- vious form of weak measurement, it can be found that γ corresponds to the precision factor G, and p 1 − γ2 corresponds to the quality parameter F under Kraus de- composition. It is worth noting that constructing a gen- eral operator under the Kraus decomposition inherently results in the current POVM being an optimal weak mea- surement that satisfies the optimal trade-off,G2+F2 = 1. This provides additional support for the optimality of the pointer proposed by (Silva et al., 2015). II. NONLOCALITY SHARING In quantum theory, quantum correlation obeys the principle of monogamy (Masanes et al., 2006; Toner, 2009). It refers to the restriction on the distribution of quantum correlations between multiple particles. For ex- ample, if particle A is highly entangled with particle B (i.e. violate Bell inequality), then particle A cannot be entangled with particle C to the same degree. In other words, quantum correlation, including quantum entan- glement, quantum steering, and Bell nonlocality, cannot be freely shared or distributed among all particles in a system. In 2015, Silva et al. achieved the sharing of nonlo- cal correlations among multiple observers through weak measurements. Using the CHSH inequality criterion, they demonstrated that at most two Bobs can share non- locality with Alice, challenging the inherent exclusivity of one observer per measurement pair (Mal et al., 2016). This conclusion challenges intuition, as it disrupts the inherent monogamy observed in quantum correlations. Immediately, nonlocal sharing has garnered considerable attention. Subsequently, Ren et al.(2019) and Feng et al.(2020) analyzed the “weak” nature of the “measurements” in- volved in nonlocal sharing, proposing the viewpoint that the motivation behind the measurements performed by the intermediate observer, Bob, in weak measurements influences the sharing of nonlocality. Based on this moti- vation, nonlocal sharing in the CHSH scenario was de- fined as active nonlocal sharing and passive nonlocal sharing. Brown and Colbeck (2020) delved into the understand- ing of “measurements” in the context of Bell nonlocality sharing, attempting to increase the number of Bobs capa- ble of sequentially sharing nonlocality with Alice. They9 employed an asymmetric POVM taken by an intermedi- ate observer between Bobs to prove that any amount of independent Bobs can violate the CHSH inequality with a single Alice, thereby establishing the possibility of mul- tiple rounds of nonlocal sharing. Against this backdrop, Steffinlongo and Tavakoli (2022) explored the sharing of nonlocality without resort- ing to weak measurements, instead opting for projective measurements only. Furthermore, a series of studies have investigated nonlocal sharing in bilateral and trilateral scenarios, with some conclusions already experimentally validated. Furthermore, several studies have investigated non- locality sharing in bilateral (Cheng et al., 2021; Zhu et al., 2022) and trilateral scenarios (Ren et al., 2022) and some of these schemes have been experimentally val- idated (Schiavonet al., 2017b; Hu et al., 2018; Fenget al., 2020; Foletto et al., 2020b; Xiao et al., 2022). A. The Nonlocality Sharing Via Weak Measurement Here we provide an overview of the study conducted by Silva et al. focused on achieving nonlocality shar- ing through optimal weak measurements. Silva et al. initially investigated the delicate balance between infor- mation gain and the degree of perturbation in the mea- surements of spin particles. This exploration led to the derivation of pointer states that effectively reached the optimal trade-off. The findings were then applied to the nonlocality-sharing of a single pair of maximally entan- gled states, illustrating that nonlocality can be observed among multiple observers. The nonlocality sharing scenario is illustrated in Fig. 1. The initial state of the entire system can be expressed as |ψ⟩ = 1√ 2 |↑↓⟩ − |↓↑⟩. This state is distributed among multiple observers, including Alice and sequential ob- servers known as Bobs. Alice selects a measurement from a set of m possible measurements, denoted by dichotomic strong measurements ˆAx, where x ∈ 0, ..., m. The out- comes of Alice’s measurements are marked as a = 0, 1. On the Bob side, there are multiple observers labeled as Bobk, each implementing dichotomic measurements ˆByk corresponding to m inputs yk ∈ 0, ..., m. The resulting outputs are also marked as bk = 0, 1. Importantly, these observers operate independently, particularly in scenar- ios where measurement choices remain unbiased. In order to realize nonlocality sharing, the observer Bobs (where s ≤ k − 1) may conduct weak measure- ments on the received qubit, while only Bob k performs strong measurements. The trade-off between information gain and the degree of disturbance is evaluated through quality factor Fs and precision factor Gs. Fs signifies the undisturbed degree of the quantum system after measure- ment, and Gs quantifies the information gain as measured by ˆBys. Alice ࣋YU[XIK Bob1 Bobk xሽ y1ሽ ykሽ a={0, 1} b 1={0, 1} b k={0, 1} Ș Fig. 1: The scenario of nonlocality sharing: the source ρsource distributes to distant observers, namely Alice and Bob1, after which Bob 1 conveys it to subsequent observers Bobs. Upon receiving the quantum state, each observer is equipped with measurement inputs, denoted asx, y1, . . . , yk ∈ 0, . . . , m, and produces measurement outcomes a, b1, . . . , bk. Now let’s review the nonlocality sharing between Alice and two sequential Bobs, i.e. Bob 1 and Bob2. As shown in Fig. 1, Alice randomly performs the measurements of ˆA0 = σz or ˆA1 = σx on one of qubit of ρsource. The post-measurement state with the measurement outcome of Alice a = {0, 1} is described as, ρa ˆAx = (Ua ˆAx )ρsource(Ua ˆAx )†, (20) where Ua ˆAx = I+(−1)a ˆAx 2 ⊗ I. Subsequently, Bob 1 and Bob2 measures the another qubit of ρsource sequentially. Their measurement observables can be given as, ˆB0 = −(σz+σx)√ 2 , ˆB1 = (−σz+σx)√ 2 . Here Bob 1 chooses the opti- mal pointer to implement weak measurement, satisfying F2 + G2 = 1. The post-measurement state of Bob 1 with the outcome b1 = {0, 1} is given as, ρb1 ˆBy1 = F 2 ρa ˆAx + 1 + (−1)b1 G − F 2 \u0014 U1 ˆBy1 ρa ˆAx \u0010 U1 ˆBy1 \u0011†\u0015 +1 − (−1)b1 G − F 2 \u0014 U0 ˆBy1 ρa ˆAx \u0010 U0 ˆBy1 \u0011†\u0015 , (21) where Ubn ˆByn = (I ⊗ I+(−1)bn ˆByn 2 ). The post-measurement state of Bob2 with the output b2 ∈ {0, 1} becomes, ρb2 ˆBy2 = Ub2 ˆBy2 ρb1 ˆBy1 \u0010 Ub2 ˆBy2 \u0011† . (22) Ultimately, one may derive the joint probability distri- bution from ρb2 ˆBy2 , P(a, b1, b2|x, y1, y2) = Tr(ρb2 ˆBy2 ). (23) Specifically, this joint probability is P(a, b1, b2|x, y1, y2) = b1G 4 (a ˆAx· ˆBy1 +b2 ˆBy1 · ˆBy2 2 ) +F 4 (1+ab2 ˆAx· ˆBy2 2 ) + (1−F 4 )(1+ab2 ˆAx· ˆBy1· ˆBy2 2 ). (24)10 The marginal probability of the combination of Alice- Bobk(k ∈ {1, 2}) can be obtained from the whole joint probability distribution, P(a, bk|x, yk) = k̸=k′ X bk′ P(a, b1, b2|x, y1, y2), (25) and its average value is Ek xyk = X a,bk=0,1 (−1)a+bk [P(a, bk|x, yk)]. (26) One may calculate the quantum correlation between Alice-Bob1 and Alice-Bob2, I(1) CHSH = 2 √ 2G and I(2) CHSH =√ 2(1 + F) respectively using CHSH inequality, I(n) CHSH = En 00 + En 01 + En 10 − En 11. (27) Clearly, the quantum correlation between Alice and Bob2 is contingent upon both the precision and quantity of measurements carried out by Bob 1. As depicted in Fig. 2, the distinct measured strengths of Bob 1 play a piv- otal role in determining both the quantum correlation of Alice with itself and the quantum correlation between Alice and Bob 2. In the scenario of an optimal pointer for weak measurements (F2 + G2 = 1), the precision fac- tor G within the range of ( 1√ 2 , q 2( √ 2 − 1)) contributes to the violation of the two CHSH criteria. Notably, a double violation of CHSH inequalities with the same correlation is achievable when G = 0 .8, resulting in I(1) CHSH = I(2) CHSH = 8 √ 2 5 ≈ 2.26. These results under- score that both Alice-Bob 1 and Alice-Bob2 can simulta- neously reveal Bell nonlocality. Likewise, when exam- ining other types of weak measurement cases, such as square pointer states, the observation of double CHSH violations remains possible, albeit with a reduced degree of violation compared to the optimal case at the same measurement strength. In a more general scenario, the presence of multiple ob- servers on the Bob side is considered, denoted as Bob 1, Bob2,..., Bobk, as illustrated in Fig. 1. Silva et al. ad- dressed this configuration in their original work, specif- ically examining the case of biased measurements by Bobs. In this scenario, Bob 1 randomly selects two mea- surement settings and subsequently transmits the mea- sured quantum states to Bob 2 with different probabili- ties. Additionally, now the measurement operators for Bob are defined as ˆBk 0 = σz, ˆBk 1 = cos θkσz + sin θkσx. Building upon the aforementioned analysis, it is crucial to recognize that the post-measurement state of Alice and Bobk depends on the inputs from all preceding observers on Bob’s side. This dependency is denoted as, ρyk−1 = Fk−1ρyk−2 + (1 − Fk−1)DByk−1 (ρyk−2 ), (28) where Dv(ρ) = π+ v ρπ+ v + π− v ρπ− v , and π± v represents the projection in the direction ±v. Given the independence 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.5 1.0 1.5 2.0 2.5 G ICHSH Bell ICHSH (1) Bell ICHSH (2) Active ICHSH (1) Active ICHSH (2) 2 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 Fig. 2: Plot of the CHSH value as a function of the precision factor G of Bob1 for optimal pointer types in the sequential CHSH scenario, with Bell-I(k) CHSH for sharing in the bell test, and Active- I(k) CHSH for active sharing type. This figure is adapted from (Ren et al., 2019) . of each Bob from the previous one, it becomes necessary to consider the summation over all possible choices of inputs and their corresponding probabilities. Thus, the post-measurement state of Alice and Bob k is expressed as, ρk = k−1X i=1 ρyi k−1Y i=1 P(yi), (29) where P(yi) represents the probability of input yi re- ceived by Bob i. This allows us to derive the complete joint probability distribution P(a, b1, ...bn|x, y1, ....bk). Likewise, the average valueEk xyk can be determined using Eq. (25) and Eq. (26), enabling the calculation of the quantum correlation I(k) CHSH. Silva et al. established the presence of arbitrarily longer sequences of CHSH viola- tions between Alice and Bobs in the biased case, consid- ering two measurement settings, as the number of Bobs approaches infinity (k → ∞, k ∈ N). Subsequently, in the unbiased case, Mal et al. (2016) concluded that achieving a violation of the Clauser- Horne-Shimony-Holt (CHSH) inequality by more than two Bobs using two unbiased input settings with Alice is impossible. Schiavon et al. (2017a) experimentally demonstrate the violation of double CHSH inequalities by exploiting a two-photon polarisation maximally en- tangled state. Additionally, Hu et al.(2018) reported the observation of double CHSH inequality violations using a continuous-tunable optimal weak measurement strength in a photonic system. Feng et al. (2020) introduced a balanced type of nonlocality sharing that enables the ob- servation of double CHSH violations using no-so-weak measurements. Moreover, Das et al.(2019) explored the sequential demonstration of bipartite nonlocality with a single Alice, investigating the number of Bobs that can exhibit nonlocality as the number of measurement set- tings per observer increased. Various applications of se-11 quential scenarios have been explored in studies such as (Curchod et al., 2017; Mohan et al., 2019). Further- more, experimental demonstrations have been conducted in (Schiavon et al., 2017b; Hu et al., 2018). These inves- tigations contribute to the understanding and practical implementation of sequential quantum scenarios. B. Passive And Active Nonlocality Sharing In the earlier discussion, the attainment of a double CHSH violation for Alice-Bob 1 and Alice-Bob2 through Bob1’s optimal weak measurements in the sequential CHSH scenario showcased the nonlocality sharing. How- ever, this phenomenon was not fully comprehended at that time. In 2019, Ren et al. (2019) classified this phe- nomenon into two types—active nonlocality sharing and passive nonlocality sharing, depending on Bob 1’s moti- vation for measurement. They demonstrated that it is still possible to observe nonlocality sharing as long as the intermediate observers refrain from making ideal strong measurements. This phenomenon is counter-intuitive since it allows for Bob 1 to conduct measurements with intensities close to 1, while enabling nonlocality shar- ing—the situations that were not permitted in the origi- nal version (Silva et al., 2015). In the sequential CHSH scenario (refer to Fig. 1), the nature of the double violation observed in Alice-Bob 1 and Alice-Bob2 depends on Bob1’s intentions. If Bob 1 is solely focused on achieving a maximum violation for him- self without considering nonlocality sharing with Bob 2, this double violation is identified as passive nonlocality sharing. On the other hand, if Bob 1 is motivated to as- sist Bob2 in achieving the maximum CHSH violation, the observed double violation in Alice-Bob 1 and Alice-Bob2 is categorized as active nonlocality sharing. As we all know, an arbitrary 2-qubit state ρ can be decomposed as, ρ = 1 4   I ⊗ I + 3X i λiσi ⊗ I + 3X j ηjI ⊗ σj + 3X k τkσk ⊗ σk ! . Here, σi represents the Pauli matrix, where i, j, k∈ {1, 2, 3}, and |τ1| ≥ |τ2| ≥ |τ3|. Assuming each observer employs binary inputs (with m = 2) denoted as n · ˆσ, and Alice’s strong measurement is denoted as ˆωx, while the measurements executed by Bob 1 (Bob2) are labeled as ˆµy1 (ˆνy2 ), where x, y1, y2 ∈ {1, 2}. The CHSH values of Alice-Bob1 and Alice-Bob2 can be expressed as, I(1) CHSH = 1X i,j=0 (−1)ij⟨ωi ⊗ µj⟩, (30) I(2) CHSH = 1X i,j=0 (−1)ij⟨ωi ⊗ νj⟩. (31) Indeed, the CHSH value of Alice-Bob 2 is influenced by several factors, including the motivation of Bob 1 (whether active or passive), the probability of his mea- surement choice (whether biased or unbiased), and the pointer distribution of the weak measurement. These el- ements collectively contribute to shaping the quantum correlations observed in the sequential CHSH scenario. In the scenario of passive nonlocality sharing where Bob1 aims to achieve a maximum violation for himself in a biased case, the expressions for the maximum CHSH values of Alice-Bob 1 and Alice-Bob2 are given by (Ren et al., 2019), I(1) CHSH = 2 p τ2 1 + τ2 2 G, I(2) CHSH = p τ2 1 + τ2 2 + q τ4 1 +2(−1+2F2)τ2 1 τ2 2 +τ4 2 τ2 1 +τ2 2 . (32) While the maximum CHSH values for the unbiased case can be expressed as, I(1) CHSH = 2 p τ2 1 + τ2 2 G, I(2) CHSH = 2 q τ4 1 +[−1+F(2+F)]τ2 1 τ2 2 +τ4 2 τ2 1 +τ2 2 . (33) Since the Bell state satisfies τ1 = 1, τ2 = −1, and τ3 = 1, the maximal CHSH values in both the bi- ased and unbiased cases are exactly the same, charac- terized by I(1) CHSH = 2 √ 2G and I(2) CHSH = √ 2(1 + F) which is coincident with the result in (Silva et al., 2015). Hence, the original instance of nonlocality sharing can be categorized as a distinctive illustration of passive non- locality sharing in Bell states. It is important to note that when Bob 1 employs a weak measurement with the square pointer, leading to the condition F + G = 1, a clear outcome emerges: both I(1) CHSH and I(2) CHSH cannot simultaneously exceed 2. However, if Bob 1 opts for a weak measurement using the optimal pointer, satisfying F2 + G2 = 1, the observation of nonlocality sharing is possible. Specifically, as shown in Fig. 2, within the pa- rameter range of G ∈ [ 1√ 2 , q 2( √ 2 − 1)], it is found that both I(1) CHSH and I(2) CHSH can exceed two simultaneously. This result indicates the presence of passive nonlocality sharing in the system. In the context of active nonlocality sharing, where Bob1 assists Bob2 in achieving the maximum CHSH vio- lation under biased conditions, the maximum CHSH val- ues can be expressed as follows, I(1) CHSH = G( 2Fτ 2 1 +τ2 2√ F2τ12+τ22 + τ2 2√ τ2 1 +F2τ2 2 ), I(2) CHSH = 2F2τ2 1 +τ2 2√ F2τ2 1 +τ2 2 + τ2 2√ τ2 1 +F2τ2 2 . (34) Active nonlocality sharing can be observed with appro- priate measurement settings. For a Bell state, τ1 = 1, τ2 = −1, τ3 = 1, the CHSH values are turn to, I(1) CHSH = 2G(1+F)√ 1+F2 , I(2) CHSH = 2 √ 1 + F2. (35)12 When Bob 1 performs weak measurement with the op- timal pointer, I(1) CHSH and I(2) CHSH can exceed two simul- taneously in the range of G ∈ (0.718, 1). Even as G approaches one, double CHSH violations still can be ob- served. On the other hand, when Bob 1 performs weak measurement with the square pointer, the CHSH value turns to I(1) CHSH = (1 + √ 2)G, I(2) CHSH = 2 √ 1 + F2 under simple measurement settings (Ren et al., 2019). Cer- tainly, the presence of double CHSH violation becomes evident in the range of G ∈ h 2 1+ √ 2 , 1 i . This phenomenon is unattainable in the case of passive nonlocality sharing. While for the unbiased case of active nonlocality shar- ing, an analytical result for the Bell state can be achieved. when G ∈ (0, 0.8), the CHSH values attain their optimum as follows: I(1) CHSH = 2 √ 2G and I(2) CHSH = √ 2(1 + F). When G ∈ (0.8, 1) the optimal CHSH values transform into, I(1) CHSH = (−3+F)JL 3 √ 6(−1+F)F , I(2) CHSH = JL√ 6(−1+F)F , (36) where J = q 3 + (−3+F)(−1+F)2(−3+5F) (−1+F)2 , L = 3 + (−4 + F)F − p (−3 + F)(−1 + F)2(−3 + 5F). When Bob 1 utilizes weak measurement with the opti- mal pointer, it is notable that I(1) CHSH and I(2) CHSH can si- multaneously surpass two within the parameter range of G ∈ \u0010 1√ 2 , 1 \u0011 , see Fig. 2. However, in the case where Bob1 opts for weak measurement with the square pointer, achieving double CHSH violation is impossible. These results suggest that implementing nonlocality sharing becomes more feasible when Bob is cooperative in assisting subsequent observers to attain the maximum CHSH violation. This observation was experimentally validated by Feng et al.(2020). Subsequently, Hou et al. (2022) delved into network nonlocality sharing within the extended bilocal scenario through the lens of weak measurements. Their study revealed that network non- locality sharing can be dissected into passive and active network nonlocality sharing by considering the divergent motivations of intermediate observers. Notably, these motivations lack counterparts in standard Bell scenar- ios. In 2022, Cai and Ren (2022) extensively explored two types of potential full network nonlocality sharing (FNN)——passive FNN sharing and active FNN shar- ing. The findings underscored that passive FNN shar- ing is unattainable, while active FNN sharing can be ac- complished through suitable measurements. This indi- cates that achieving FNN sharing in this scenario neces- sitates greater cooperation from intermediate observers compared to Bell nonlocality sharing and network non- locality sharing. C. Asymmetric POVM For Boundless Nonlocality Sharing The earlier research on nonlocality sharing by Silva et al.(2015) established a significant result. Specifically, it was found that, under the assumption of unbiased bi- nary choices and the condition that all observers are in- dependent, no more than two observers on Bob’s side can simultaneously violate the CHSH inequalities with Alice. Subsequently, Mal et al. (2016) and Zhu et al. (2022) further proved this result. However, the question of whether nonlocality can be shared among more ob- servers on Bob’s side remains an open and unresolved inquiry. In 2020, Brown and Colbeck (2020) delved into this question and provided a positive answer. Brown and Colbeck proposed a scheme where the observers in the middle of sequential Bobs perform measurements with unequal sharpness. In this scheme, Bob s (1 ≤ s ≤ k − 1) independently selects two different dichotomic measure- ments: one being a strong measurement and the other a generalized POVM. This configuration enables an ar- bitrarily longer sequence of Bobs to simultaneously vio- late the CHSH inequality with a single Alice, showcasing boundless nonlocality sharing. In the scenario of boundless nonlocality sharing, Alice still performs strong measurements in the x-z plane of the Bloch sphere, defined as ˆAx = n · ˆσ. For Bob’s side, one measurement of Bob k can be defined as ˆB(k) 0 = σz; the other is generalized POVM, which can be defined as a pair of positive semidefinite matrices ( ˆB(k) 01 , ˆB(k) 11 ) with binary outcomes, ˆB(k) 01 + ˆB(k) 11 = I. Without loss of generality, ˆB(k) 01 = 1 2 (I + γkσz), where γk represents the sharpness parameters of measurements. Obviously, B(k) 01 returned to strong measurement when γk = 1 and the measurement disregards the state and is equivalent to an unbiased coin toss when γk = 0. If the initial state is a singlet state, the CHSH value corresponding to Alice and Bobk can be expressed as (Brown and Colbeck, 2020), I(k) CHSH = 22−k(γk sin θ + cosθ k−1Y j=1 (1 + q 1 − γ2 j )). (37) Through Eq. (37), one may identify suitable values for γk and measurement directions that lead to multiple in- stances where I(k) CHSH can surpass the classical bound of the CHSH inequality simultaneously. A result is clearly demonstrated in the following. If γk satisfies the condi- tions provided as, γk > 2k−1 cos θQk−1 j=1 (1 + q 1 − γ2 j ) sin θ , (38) arbitrary violations of I(k) CHSH can occur, implying that any number of independent Bobs can simultaneously vi- olate the CHSH inequality with a single Alice. Further- more, Brown and Colbeck introduced a function of γ113 with θ, γ1(θ) = (1 + ε)1−cos θ sin θ , where ε is an infinitesi- mal quantity greater than zero. Subsequently, γk(θ) is defined recursively, γk(θ) = ( (1 + ε)2k−1−cos θPk sin θ , if γk−1(θ) ∈ (0, 1), ∞, otherwise, (39) where Pk = Qk−1 j=1 (1 + q 1 − γ2 j (θ)). They demon- strated the existence of {γk(θ)}, which always let {I(1) CHSH, ..., I(k) CHSH} exceed 2 for k → ∞, k ∈ N. This result unveils the phenomenon of boundless nonlocality sharing. Furthermore, Roger discovered the maximum limit of violation size through further analysis of this strategy. As the number of observers on Bob’s side decreases, the violation is reduced exponentially at a double rate. Ad- ditionally, this strategy can achieve an unlimited num- ber of violations in the same process as described above. However, this measurement strategy is not appropriate for device-independent tasks, such as randomness expan- sion, where the number of provable randomness increases with the size of CHSH violations. The above results arise from a comprehensive under- standing of “measurement” in the Bell nonlocality test, contributing valuable insights to unresolved inquiries within the field. Specifically, the question arises as to whether a set of 2-qubit states exists that can be suffi- ciently characterized to permit an arbitrary number of CHSH violations. Although Brown and Colbeck (2020) proposed a condition deemed sufficient for achieving ar- bitrary violations in a 2-qubit state, the necessity of this condition remains uncertain. Moreover, the diminishing exponential trend in CHSH inequality violation as the number of independent Bobs increases poses limitations on its future applicability. The intriguing prospect of achieving a larger violation still raises a significant ques- tion. The investigation of related extensions is ongoing. In (Zhang and Fei, 2021), it has been constructively demon- strated that a class of pure entangled states allows an arbitrary number of independent Bobs can share nonlo- cality with a single Alice. Building on this insight, Cheng et al. (2022) derive corresponding one-sided monogamy relations that rule out two-sided nonlocality sharing for a wide range of parameters, based on a general trade-off relation between the strengths and maximum reversibil- ities of qubit measurements. In another study, Srivas- tava et al. (2021a) delve into sequential measurement- device-independent entanglement detection by multiple observers, exploring scenarios with both equal and un- equal sharpness parameters. Additionally, Lv et al. (2023) investigate the bilateral sharing of EPR steering through weak measurement with unequal sharpness pa- rameters in a generalized case. The findings demonstrate that an unbounded number of sequential Alice-Bob pairs can share EPR steering. D. Nonlocality Sharing Through Projective Measurements As established in the aforementioned studies, breaking the symmetry of weak measurements allows for two or more independent Bobs, leading to CHSH inequality vi- olations by Alice. In a recent development, Steffinlongo and Tavakoli (2022) explored a strategy that achieves nonlocality sharing exclusively through the implementa- tion of projective measurements in the case of a single Al- ice and two Bobs. In this specific scenario, all observers execute strong measurements, followed by Bobs perform- ing arbitrary unitaries based on the input and output of the measurements. A Bell test was then conducted using three distinct projective measurement strategies, determined by Bob’s two measurement choices: (i) both measurements are projective, (ii) both measurements are identity, (iii) one measurement is projective while the other is identity. These three strategies are labeled as λ ∈ {1, 2, 3}. Note that all Bob k’s measurements are in- dependent, and the state shared by Alice and Bob k can be expressed as, ρ(k) = 1 2 X bk,yk=k=0,1 \u0010 I ⊗ ˆKk bk|yk \u0011 .ρ(k−1). \u0010 I ⊗ ˆKk bk|yk \u0011† . (40) where ˆKk bk|yk the Kraus operators. ˆBk yk are Bob k’s projective measurements, which can be decomposed as ( ˆKk bk|yk )†( ˆKk bk|yk ). The Kraus operators can be given as, ˆKk bk|yk = q ˆBkyk = ˆUk bk|yk ˆBk yk , where ˆUk bk|yk are arbitrary unitary. Suppose Alice, Bob1, and Bob2 carry out independent measurements ˆA(λ) x , ˆB(λ) y and ˆC(λ) z respectively. The CHSH value for Alice and Bob 1 can be achieved as, S(λ) AB = X x,y (−1)x+yTr[ \u0010 ˆA(λ) x ⊗ ˆB(λ) y \u0011 ρ(1)], (41) and for Alice and Bob 2 can be given as, S(λ) AC = X x,z (−1)x+zTr[ \u0010 ˆA(λ) x ⊗ ˆC(λ) z \u0011 ρ(2)]. (42) The optimal measurement settings for the three measure- ment strategies are as follows. When λ = 1: Bob 1’s two measurement choices are projective, represented as ˆB(1) 0 = cos ϕσx + sin ϕσz and ˆB(1) 1 = sin ϕσx + cosϕσz. The corresponding unitary op- erators for Bob 1 are defined as ˆU(1) 0 = I and ˆU(1) 1 = ei(ϕ−π 4 )σy . For Alice’s measurements, the choices are ˆA(1) 0 = σx+σz√ 2 and ˆA(1) 1 = σx−σz√ 2 . On the other hand, for Bob2, both ˆC(1) 0 and ˆC(1) 1 are given by cos ϕσx + sinϕσz.14 These choices lead to the following CHSH values: S(1) AB = 2 √ 2 cosϕ and S(1) AC = √ 2(cos ϕ + sinϕ). In this case, the trade-off of Alice-Bob1 and Alice-Bob2 becomes, S(1) AC = 1 2 \u0012 S(1) AB + r 8 − \u0010 S(1) AB \u00112\u0013 . (43) When λ = 2: Bob 1 has two measurement choices, both being identity operators, i.e., ˆB(2) 0 = ˆB(2) 1 = I. The unitary operator for Bob 1 is also identity, denoted as ˆU(2) by = I. The operators for Alice are given by ˆA(2) 0 = cos θσx + sinθσz, ˆA(2) 1 = cos θσx − sin θσz. While for Bob2, ˆC(2) 0 = σx, ˆC(2) 1 = σz. The CHSH values are im- mediately determined as S(2) AB = 0, while S(2) AC can reach a maximum value of 2 √ 2. When λ = 3: Bob 1 has two measurement choices: a projective measurement denoted as ˆB(3) 0 = I and an identity measurement as ˆB(3) 1 = σz. The unitary op- erator for Bob1 is also the identity, ˆU(3) by = I. Alice’s measurements are set as ˆA(3) 0 = cos θσx + sin θσz and ˆA(3) 1 = cos θσx − sin θσz. For Bob 2, ˆC(3) 0 = σx and ˆC(3) 1 = σz. The CHSH values are given by S(3) AB = 2 sinθ and S(3) AC = cos θ + 2 sinθ. Their trade-off can be given as, S(3) AC = S(3) AB + 1 2 r 4 − \u0010 S(3) AB \u00112 . (44) The above results reveal that the violation occurs ex- clusively between Alice and Bob1 when λ = 1 and exclu- sively between Alice and Bob 2 when λ = 2 and λ = 3. Achieving a double violation necessitates employing a random combination of these three cases. The bound- ary for S(λ) AB, S(λ) AC is divided into four segments: (a) a combination of the cases λ = 2 and λ = 3; (b) the case λ = 3; (c) a combination of the cases λ = 1 and λ = 3; and (d) the case λ = 1. Certainly, the maximal CHSH values, SAB = SAC = 2 √ 10 3 = 2 .108, can be achieved. The optimal trade-off is piecewise illustrated with solid lines in Fig. 3. The aforementioned analysis pertains to the scenario where a maximally entangled state is shared. However, if the shared state is only partially entangled, denoted as |ψ⟩ = cos α |00⟩ + sinα |11⟩ with α ∈ [0, π 4 ], notewor- thy observations emerge. A crucial point to highlight is that, in the conventional CHSH scenario, achieving the most robust quantum nonlocality necessitates the shar- ing of maximally entangled states. Conversely, in this se- quential CHSH scenario, superior double violations can be attained by sharing partially entangled states rather than maximally entangled ones. For further details, re- fer to (Steffinlongo and Tavakoli, 2022). Subsequently, Xiao et al.(2022) experimentally demonstrated the non- locality sharing with projective measurements, extending 0 0.5 1 1.5 2 2.5 3 SAB 1.4 1.6 1.8 2 2.2 2.4 2.6 2.8 3 SAC SAC (1) SAC (2) SAC (3) Fig. 3: Plot of the optimal trade-off between SAB and SAC for a maximally entangled qubit pair under projec- tive measurements. This figure is adapted from (Steffin- longo and Tavakoli, 2022). beyond unsharp measurement scenarios. Similarly, Dong et al. [****] extended this method to the steering shar- ing. Recently, Sasmal et al. (2023) introduced a novel lo- cal randomness-assisted projective measurement proto- col (PPM). This protocol enables an unrestricted number of sequential observers (referred to as Bobs) to establish nonlocality with a single Alice simultaneously. The au- thors endeavor to devise an elegant set of Kraus operators designed to minimally disturb the system. In essence, this strategy circumvents the limitations encountered in the earlier work (Steffinlongo and Tavakoli, 2022) mark- ing a notable advancement in the field. E. Nonlocality Sharing In Multilateral And Multi-Qubit Cases 1. 2-Qubit System To date, the majority of research has focused on the unilateral scenario of nonlocality sharing, involving the sharing of an entangled state with Alice and multiple Bobs. Drawing on previous work, the possibility of gen- erating nonlocality sharing in the bilateral scenario (re- fer to Fig. 4) has attracted attention. Zhu et al. (2022) investigated this matter, determining that two CHSH in- equality violations cannot be attained simultaneously in the bilateral scenario. In the bilateral scenario, involving multiple Alices and Bobs, we assume the shared state to be the singlet state |ψ⟩ = 1√ 2 (|01⟩ − |10⟩). Alice 1 performs the optimal weak measurement with her two unbiased measurement choices being σx and σz. Subsequent Alices perform the same measurement as Alice 1 until the last one performs15 Alice1 ࣋YU[XIK Bob1 Bobk y1ሽ ykሽ b1={0, 1} b k={0, 1} Ș x1ሽ a1={0, 1}an={0, 1} xnሽ Alicen Ș Fig. 4: The bilateral sequential scenario of nonlocality sharing. The source ρsource distributes to distant observers, namely Alice1 and Bob1, which transmit the received particle to the following observers after measuring. Alices and Bobs have m measurement choices, denoted as xn = {0, ..., m} and yk = {0, ..., m} respectively. an = {0, 1}, bk = {0, 1} repre- sent measurement outcomes of Alices and Bobs. . a strong measurement. Similarly, Bob 1 performs the optimal weak measurement with unbiased measurement choices set to σx−σz√ 2 and −σz−σx√ 2 . Subsequent Bobs per- form the same measurement until the last one performs a strong measurement. The two parameters for Alice k’s weak measurement are defined as GAk and FAk , and for Bobk are GBk and FBk . Consider the scenario with two Alices and two Bobs, the CHSH values can be presented as, IA1B1 = 2 √ 2GA1 GB1 , IA1−B2 = √ 2GA1 (1 + FB1 )GB2 , IA2−B1 = √ 2GB1 (1 + FA1 )GA2 , IA2B2 = √ 2 2 (1 + FA1 )GA2 (1 + FB1 )GB2 . (45) When both GA2 and GB2 are set to 1, Alice 2 and Bob2 perform a strong measurement. In the event that FA1 = 1 and GA1 = 0, this scenario transforms into the sequen- tial CHSH scenario (refer to Fig. 1). From Eq. (45), it becomes evident that Alice 1-Bob1 and Alice2-Bob2 can- not simultaneously violate the CHSH inequality. For the bilateral scenario with n Alices and k Bobs, the CHSH value of Alice r-Bobs satisfies, IAr−Bs = 2 √ 2 2r−1 × 2s−1 (1 + FA1 ) ··· (1 + FAr−1 )GAr ×(1 + FB1 ) ··· (1 + FBs−1 )GBs, where r ∈ {1, .., n}, s ∈ {1, .., k}, and Alice n and Bobk perform a strong measurement with GAn = GBk = 1. Thus, it remains evident that the bilateral scenario can- not simultaneously achieve two CHSH inequality viola- tions. As a result, sequential observers from both sides are unable to share nonlocality. Nonlocality sharing is only demonstrable between any number of Bobs and a single Alice. Along with that, Cheng et al. (2021) proposed a con- jecture asserting that in scenarios where observers are independent with unbiased case, nonlocality sharing is feasible in one side extended of the bell scenario. How- ever, this conjecture does not apply to scenarios with multiple observers on each side, which aligns with (Zhu et al., 2022). Cheng et al. (2021) considered a scenario where the shared state is partially entangled, and all measurement operators are expressed as POVM. In an effort to val- idate the conjecture in the bilateral scenario, Cheng et al. systematically analyzed 16 parameter sets of mea- surement directions and one parameter set for the initial state. This analysis encompassed CHSH values, specif- ically S(A 1, B1), S(A 1, B2), S(A 2, B1), and S(A 2, B2). Numerical evidence was derived from the optimization of S(A 2, B2) and S(A 2, B1) under the constraint that S(A1, B1) and S(A 1, B2) reach minimum fixed values. Employing a constrained differential evolution algorithm implemented in the SCIPY library, the maximal CHSH values within the 17-dimensional parameter space were calculated. The optimization process revealed that both CHSH values remain below 2, signifying the absence of double violations of the CHSH inequality. This result furnishes robust analytical and numerical evidence sup- porting the conjecture that double CHSH inequality vi- olations do not occur in the bilateral scenario. Cabello (2021) has shown that it is possible to have arbitrarily long sequences of Alices and Bobs such that every (Alice, Bob) pair violates a Bell inequality in a four-dimensional quantum system. However, the accu- racy of this result needs further verification. Subse- quently, Zhang et al. (2022) investigated the nonlocality sharing under bilateral measurements in such a scenario, and found Bell nonlocality cannot be shared for a lim- ited number of times under a specific class of projection operators. Such a result can be generalized to higher- dimensional cases. Cheng et al. (2022) generalized a scheme for gener- ating Bell nonlocality between arbitrarily many indepen- dent observers on each side, via the two-sided recycling of multiqubit states. Subsequently, a series of works (Han et al., 2022; Li et al., 2022; Lv et al., 2023) present a steering scenario where both subsystems are accessible by multiple observers. Liu et al. (2022) observe multi- observer steering on both sides simultaneously via weak measurements. Hou et al. (2022) observed network non- locality sharing in the extended bilocal scenario. Addi- tionally, works by Hu et al. (Pandit et al., 2022; Hu and Fan, 2023) considered scenarios involving two-sided sequential measurements where the entangled pair is dis- tributed to multiple Alices and Bobs for entanglement sharing. 2. Nonlocality Shairng In 3-Qubit System Compared with the case of two qubits, nonlocality sharing in three qubits systems exhibits richer proper-16 ties. Ren et al.(2022) have extensively explored nonlocality sharing in a 3-qubit system by leveraging multiple viola- tions of the Mermin-Ardehali-Belinskii-Klyshko (MABK) inequality. Their investigation reveals complete nonlo- cality sharing in the context of trilateral sequential sce- narios, particularly when employing Greenberger-Horne- Zerlinger (GHZ) states, as illustrated in Fig. 5. In com- parison to 2-qubit scenarios, the phenomenon of nonlo- cality sharing in 3-qubit systems exhibits more intricate and diverse properties. Alice1 ࣋YU[XIK Charlie1 Bob1 Bobk y1ሽ ykሽ b1={0, 1} b k={0, 1} Ș x1ሽ a1={0, 1}an={0, 1} xnሽ Alicen z1ሽ ztሽ c1={0, 1} ct={0, 1} Ș Ș Charliet Fig. 5: The trilateral scenario of nonlocality sharing. The source ρsource distributes to distant observers, namely Alice1, Bob1 Charlie1, which transmits the received particle to the following observers after measuring. The observer has mea- surement inputs xn, yk, zt ∈ {0, ..., m} respectively. Measure- ment outputs after receiving the quantum state are labeled as an, bk, ct ∈ {0, 1} respectively. For the trilateral sequential scenario considered, they specifically set n = k = t ∈ {1, 2}, where each ob- server has two measurement directions to choose from denoted as n · ˆσ. The GHZ state is represented as |ψ⟩ = 1√ 2 (|000⟩ + |111⟩). In this framework, Alice n’s i- th measurement choices are labeled as ˆAn,i, Bobk’s j-th measurement choices are denoted as ˆBk,j, and Charliet’s t-th measurement choices are represented as ˆCt,l. The MABK inequality for the scenario can be expressed as, B = | −Ma + Mb + Mc + Md| ≤2. (46) Where Ma = −E (An,1, Bk,1, Ct,1) , M b = E (An,2, Bk,1, Ct,2) , Mc = E (An,2, Bk,2, Ct,1) , M d = E (An,1, Bk,2, Ct,2) . The list of quantum correlations of Alicen-Bobk-Charliet corresponds to eight MABK inequalities. The measure- ment direction of each observer is always in the X-Y plane. Let Alice 1, Bob 1, and Charlie 1 perform a weak measurement with the same parameter, G = G1 = G2 = G3. In this scenario, the MABK values of Alice 1-Bob1- Charlie1 and Alice 2-Bob2-Charlie2 can be expressed as B1 = 4 G3 1; B8 = 1 2 (1 + √ 1 − G2)3. It is easy to find that B1 and B8 can exceed 2 simultaneously when G ∈ ( q 2(2 2 3 − 2−1 3 ), 2 1 3 ). For the other list of Alice n-Bobk- Charliet, the corresponding MABK value, B2 − B7, can also exceed 2 simultaneously when G ∈ ( q√ 5−1 2 , 0.839). Certainly, for the whole list of Alice n-Bobk-Charliet, the MABK value B1 −B8 can exceed 2 simultaneously when G ∈ ( q 2(2 2 3 − 2−1 3 ), 2 1 3 ), and they can achieve the max- imal violation when G = 0.8. The findings indicate that trilateral sequential scenarios can exhibit complete non- locality sharing. Besides, Saha et al.(2019) and Xi and Fu (2022) have explored the feasibility of tripartite nonlocality sharing involving more than three observers. Gupta et al.(2021) delved into the potential for multiple uses of a single copy of a 3-qubit state to detect genuine tripartite EPR steer- ing. Wang et al. (2022) observed nonlocality sharing in a star network scenario, demonstrating its applicability with any number of branches. Additionally, Maity et al. (2020) investigated the sequential detection of genuinely multipartite entanglement in quantum systems, consider- ing an arbitrary number of qubits. The sequential detec- tion of genuinely multipartite entanglement in quantum systems with an arbitrary number of qubits has also been explored in the work by (Srivastava et al., 2022b). F. Experimental Implementation Of Nonlocality Sharing In this section, we review the recent advancements in experimental studies on nonlocality sharing. The first experimental demonstration of nonlocality sharing is presented by Schiavon et al. (2017b). Specif- ically, Schiavon et al. observed double violations of the CHSH inequality among three observers who shared a 2-particle maximally entangled state through weak mea- surements. At the same time, Hu et al. (2018) delved deeper into the continuously tunable optimal weak mea- surements for nonlocality sharing. Subsequently, Feng et al.(2020) maximized the violation of Bell’s inequality under optimal weak measurements of varying strengths and developed an optimal protocol for nonlocality shar- ing among three observers. Prior to this advancement, nonlocality sharing in experiments was constrained to three observers, until Foletto et al. (2020b,c) observed triple violations of the CHSH inequality among four ob- servers, thereby further validating the multiple sharing of nonlocality in the experiment. Additionally, Xiao et al. (2022) diverged from a weak measurement strategy in the nonlocal sharing experiment, opting for projective measurements to share nonlocality. They observed that three independent observers could effectively share Bell nonlocality.17 1. Double Violations Of CHSH Inequality Schiavon et al. (2017b) demonstrated experimentally that nonlocality sharing can be observed between three observers Silva et al. (2015). As depicted in Fig. 7, the nonlocality-sharing scenario involves three observers: Al- ice, Bob1, and Bob2. They share pairs of a 2-particle en- tangled state, denoted as |Ψ−⟩ = 1√ 2 (|H⟩|V ⟩ − |V ⟩|H⟩), and choose the number of measurement options asm = 2. Alice performs a strong measurement on her system us- ing the basis|ux⟩, \f\fu⊥ x \u000b . Subsequent to her measurement, the state on Bob’s side is projected onto \f\f\fu(−a) x E , where |u+ x ⟩ ≡ |ux⟩ and |u− x ⟩ ≡ \f\fu⊥ x \u000b . Specifically, Bob1 employs a weak measurement utiliz- ing a controlled phase gate, denoted as CP ϵ= |H⟩⟨H|⊗ I +|V ⟩⟨V |⊗ eiϵσz , to establish entanglement between an auxiliary qubit and the system. The ancillary qubit is initialized in the state |+⟩ and subsequently measured in the basis |+⟩, |−⟩. By adjusting the rotation angle in the controlled phase gate, the strength of the measurement can be modulated. To extend the measurement to an arbitrary basis |ωy1 ⟩, \f\fω⊥ y1 \u000b , the state requires rotation using a rotation matrix ˆRy1 . This rotation ensures that ˆRy1 |ωy1 ⟩ = |H⟩ and ˆRy1 \f\fω⊥ y1 \u000b = |V ⟩, thereby generaliz- ing the measurement to accommodate different bases. Fig. 6: Measurement of I(1) CHSH (squares) and (diamonds) for several values of ϵ. This figure is adapted from (Schiavon et al., 2017b) The qubit transmitted to Bob 1 system \f\fψa|x \u000b =\f\f\fu(−a) x E can be rewritten in his measurement basis as α |ωy1 ⟩ + β \f\fω⊥ y1 \u000b , where α = D ωy1 |u(−a) x E , β =D ω⊥ y1 |u(−a) x E . Inside Bob 1’s measurement device, the joint state \f\fψa|x \u000b ⊗ |+⟩ becomes ( α |H⟩ + β |V ⟩) |+⟩ by the rotation. The controlled phase gate applies the uni- tary operator eiϵσz to the ancilla qubit, subject to the condition that the system qubit is in the state |V ⟩. This transforms the state into α |H⟩|+⟩ + β |V ⟩(cos ϵ |+⟩ + i sin ϵ |−⟩). After the inverse rotation ˆR† y1 , the joint state changes to \f\fψa|xy1 \u000b = α |ωy1 ⟩|+⟩ + β \f\fω⊥ y1 \u000b (cos ϵ |+⟩ + i sin ϵ |−⟩). Bob1 measures the ancillary qubit in the basis {|+⟩, |−⟩}, while Bob 2 measures the polarization qubit in the basis {|vy2 ⟩, \f\fv⊥ y2 \u000b }. The whole joint probability distribution corresponding to Alice-Bob1 and Alice-Bob2 can be given as, p (a, b1|x, y1) = Tr h\u0010 I ⊗ ˆΠb1 \u0011 ρa|xy1 i p (a|x) , p (a, b2|x, y2) = X y1 p (y1) T r h\u0010 ˆΠy2 b1 ⊗ I \u0011 ρa|xy1 i p (a|x) , where ˆΠb1, ˆΠy2 b2 are the projectors for Bob 1 and Bob2 re- spectively. If Alice randomly chooses two measurement settings −σz+σx√ 2 and −σz+σx√ 2 while Bobs randomly choose two measurement settings σz and σx, the CHSH values of Alice-Bob1 and Alice-Bob2 are I(1) CHSH = 2 √ 2 sin2 ϵ and I(2) CHSH = √ 2 (1 + cosϵ) respectively. In the experimental setup, Alice utilizes two measurement bases determined by the orientation of a half-wave plate set at angles of 11.25◦ and 33.75◦. On the other hand, Bob’s measure- ment bases are defined by rotations of 0 ◦ and 22.5◦. The experimental results of double violations of CHSH inequality are shown in Fig. 6. For ϵ = 0, there is no interaction between the polarization and the ancil- lary state, I(1) CHSH approaches 0, whereas I(2) CHSH is close to the Tsirelson’s bound. With an increase in ϵ, the ex- perimental findings revealed a rise in I(1) CHSH and a con- current decrease in I(2) CHSH. To achieve the double vi- olation, researchers conducted sequential measurements using two distinct values of ϵ. The results of 8 sequn- tial measurements with ϵ = 1.049 ±0.002 clearly demon- strate that both I(1) CHSH and I(2) CHSH surpass the classical bound, registering values of I(1) CHSH = 2.125 ± 0.003 and I(2) CHSH = 2.096 ± 0.003, respectively. Similarly, in a sec- ond series of trials with ϵ = 1.053 ± 0.002, both I(1) CHSH and I(2) CHSH exceed the classical bound, maintaining val- ues of I(1) CHSH = 2.114 ±0.003 and I(2) CHSH = 2.064 ±0.003. The experimental results demonstrate the possibility of achieving double violations of CHSH inequalities, i.e. nonlocaltiy sharing, even if weak measurements are not optimal. Almost the same period, Hu et al. (2018) reported the experimental observation of the double violation of CHSH-Bell inequality for a single pair of entangled pho- tons, utilizing a photonic system with continuous-tunable optimal weak measurement. In their experiments, the pointer states are labeled as |ϕH⟩(|ϕV ⟩). The states {|ϕ+1⟩, |ϕ−1⟩} chosen as reading states in the implemen- tation represent two separate paths. These paths are denoted by |0⟩ and |1⟩. By introducing a rotation of the half-wave plate, the pointer states transform into |ϕH⟩ = cos θ |0⟩ + sinθ |1⟩ and |ϕV ⟩ = sin θ |0⟩ + cosθ |1⟩,18 Fig. 7: Scheme of the experimental setup. This figure is adapted from (Schiavon et al., 2017b) where 0 ≤ θ ≤ π 2 . In this context, the quality fac- tor and information gain are expressed as F = sin 2θ and G = cos 2 θ, respectively. It is noteworthy that the condition for optimal weak measurement, given by F2 + G2 = 1, is satisfied in this scenario. To attain the maximal CHSH bound of 2 √ 2, Al- ice’s measurement directions are chosen as σz, σx, while Bob’s measurement directions are chosen as −σz+σx 2 , −(σz+σx) 2 . In the experiment, the angles for Alice’s half-wave plate can be set to either (0 ◦, 45◦) or (22 .5◦, 67.5◦), and similarly, for Bob 1 and Bob 2 to (−11.25◦, 33.75◦) or (11 .25◦, 56.25◦). Five different an- gles θ = 4◦, 16.4◦, 18.4◦, 20.5◦, 28◦ were selected, among which θ = 16.4◦, 18.4◦, 20.5◦ lie in the region where dou- ble violations are predicted to be observed. The theoret- ical case of F = 0.6, corresponding to θ = 18.4◦, yields a balanced double violation with I(1) CHSH = I(2) CHSH = 2.26 under optimal weak measurements. The experimental results, depicted in Fig. 8, show a double violation at θ = 16 .4◦, 18.4◦, 20.5◦ with I(1) CHSH = 2 .20 ± 0.02 and I(2) CHSH = 2.17 ± 0.02. This observed double violation at θ = 16 .4◦, 18.4◦, 20.5◦ is significant, with a confidence level of 10 standard deviations. 2. Observation Of Active Nonlocal Sharing While Silva et al. (2015) successfully demonstrated nonlocality sharing among three observers, the optimal protocol for achieving this remains unclear. The exist- ing demonstration establishes the feasibility of nonlocal- ity sharing, yet a systematic and optimized protocol for implementing this phenomenon has yet to be fully elu- cidated. In 2020, Feng et al. (2020) solved this prob- lem and developed an optimal protocol for nonlocality sharing among three observers, allowing for Bob 1’s weak Fig. 8: Experimental results. This figure is adapted from (Hu et al., 2018). measurement strength to be nearly maximized. Counter- intuitively, the nonlocality sharing in this area is impos- sible in the previous scheme. Feng et al. (2020) improved the measurement strat- egy based on the original work of Silva et al. (2015). In the original work, Bob 1’s observables only consider the maximum value of the CHSH value I(1) CHSH corre- sponding to him and Alice. This inevitably lowers the upper bound for the CHSH value I(2) CHSH corresponding to Alice-Bob2. To maximize the simultaneous violation range of both I(1) CHSH, Feng et al. opted for a suitable measurement strategy to maximize min {I(1) CHSH, I(2) CHSH}, as shown in Fig. 9. They determined Alice’s observ- ables as σx and σz, with the choices of observables for both Bob1 and Bob2 contingent upon the precision fac- tor G of Bob1’s measurement. When G <0.8, I(1) CHSH is always less than or equal to I(2) CHSH. Consequently, Bob 119 Fig. 9: The measurement setting and the corresponding CHSH values. This figure is adapted from (Feng et al., 2020) and Bob2 can employ Silva’s original scheme to maximize min{I(1) CHSH, I(2) CHSH}. When G >0.8, I(2) CHSH may not nec- essarily exceed I(1) CHSH. It can be observed that, under a given value of G, the smaller the difference between the two observables for Bob1, the greater the quantum corre- lation between Alice and Bob2. Therefore, enhancing the value of min{I(1) CHSH, I(2) CHSH} can be achieved by adjusting the similarity between the two observables for Bob 1, ˆµ = cos γX + sinγZ; ˆ µ′ = cos γX − sin γZ; ˆv = cos δX + sinδZ; ˆ v′ = cos δX − sin δZ; (47) where γ, δ∈ {0, π 4 }. For any given G greater than 0.8, they can always find the values of γ and δ to achieve I(1) CHSH = I(2) CHSH, thereby obtaining the maximum value of min{I(1) CHSH, I(2) CHSH}. This measurement strategy further extends the scheme proposed by Silva et al., enabling the attainment of nonlocality sharing when 0 .910 < G <1, which is unattainable in the original scheme. Similar to the setup proposed by Huet al.(2018), Feng et al. (2020) made use of an ancillary qubit to realize a weak measurement with tunable strength. Specifically, defining any single qubit state as |ψ⟩ = α|0⟩ + β|1⟩,\u0000 α2 + β2 = 1 \u0001 , which will be conducted as a weak mea- surement. The qubit |ψ⟩ is coupled to an ancillary qubit prepared in the state |0⟩ by a 2-qubit unitary U. The resulting 2-qubit state is expressed as α |0⟩ ⊗ (cos θ |0⟩ + sinθ |1⟩) + β |1⟩ ⊗(sin θ |0⟩ + cosθ |1⟩). Then a projective measurement on the 0 /1 basis is conducted on the ancillary qubit, effectively resulting in a weak mea- surement on |ψ⟩. The quality and precision factors sat- isfy the optimal pointer of the weak measurement since G = cos 2θ and F = sin 2θ. In the experiment, it is assumed that the qubit is|ψ⟩ = α |H⟩ + β |V ⟩, exploiting the spatial degree of freedom as the ancillary qubit. The state outgoing will become α |H⟩⊗(cos θ |l⟩ + sinθ |u⟩)+ β |V ⟩⊗(sin θ |l⟩ + cosθ |u⟩). Where |l⟩ and |u⟩ denote the lower and upper spa- tial modes, respectively. The initial state is the max- imum entangled state of two photons, which is shared with observers on both sides. They selected 9 differ- ent values of G in the experiment and set the measure- ment directions. Theoretically, the maximum value of min \u0010 I(1) CHSH, I(2) CHSH \u0011 can reach 2.263 when G = 0.8. The experimental value is I(1) CHSH = 2 .214 ± 0.011, I(2) CHSH = 2.168 ± 0.007. Both values exceed the classical bound by 10 standard deviations. Even if Bob 1’s measurement strength reaches a very high level, such as G = 0 .96, the double violation of CHSH inequality can still be ob- served. Specifically, I(1) CHSH = 2 .028 ± 0.024, I(2) CHSH = 2.047 ± 0.020, refer to Fig. 9. Overall, the authors devised an optimal approach for achieving nonlocality sharing among three observers and demonstrated in experiments that it uses weak measure- ments for a broad range of strengths. Previous exper- iments revealed that double violation of the CHSH in- equality could only be observed using weak measure- ments of moderate strength. Feng et al. (2020) exper- imentally proved that double violation of CHSH inequal- ity can also be achieved using not-so-weak measurements. 3. Certification Of Sustained Entanglement And Nonlocality In the work by Foletto et al. (2020a,b), it is observed that pairs of entangled photons in polarization main- tain their entanglement even when one particle under- goes three sequential measurements. Remarkably, each of these measurements has the potential to violate a CHSH inequality. The study specifically explores a sequential scenario, as illustrated in Figure 1, where k is defined as 3, and the number of measurement choices is set to 2. In their experiments, Alice and Bob share a 2-qubit maximally entangled state |ψ1⟩ = 1√ 2 (|00⟩ + |11⟩). Bobs perform sequential measurements, { ˆB0, ˆB1} = {σz, σx}, on his photon. At the generic step k, Bob k ap- plies ˆU† B,k to his photon ( k ≥ 2); he performs ei- ther measurement ˆB0, ˆB1 with strength parameter µk and the state takes the form of |ψk⟩ = ˆUA,k ⊗ ˆUB,k [cos (ηk) |00⟩ + sin (ηk) |11⟩]. For the step k + 1, if Bobs choose µj > 0, ∀j ≤ k, then ηk+1 > 0, it means that20 |ψk+1⟩ is still entangled. And if Bobs perform measure- ment ˆB0 with strength parameterµk > arctan \u0002 tan2 (ηk) \u0003 and the outcome is −1, then the new entanglement pa- rameter is ηk+1 = arctan h tan(µk) tan(ηk) i > ηk, which am- plifies the entanglement. Alice’s side must apply ˆU† A,k, and her projective measurement can be given as ˆA0,k = cos (θk) σx +sin (θk) σz, ˆA1,k = −cos (θk) σx +sin (θk) σz, where θk = arccot [sin (2ηk)]. The CHSH value corre- sponding to Alice-Bob k can be obtained as ICHSH,k = 2 cos (2µk) q 1 + sin2 (2ηk). For any choice of k, if Bobs choose that µk < µk,max = 1 2 arctan [sin (2ηk)], the CHSH value can exceed 2. Foletto et al. (2020a) verified that Bob’s side makes at most three sequential measurements and the protocol can be stopped at steps 1, 2, or 3. They choose that µ1 = 0 .34, µ2 = 0 .19, µ3 = 0. Overall, they measured a total of 9 independent ICHSH (one stopped in the first step of the protocol, four stopped in the second step, and four stopped in the third step). Foletto et al. en- code two qubits in the polarization degree of freedom of two separated photons. Polarization-entangled photon pairs are generated by a custom-built source based on a Sagnac interferometer. For each step of Bobs’s mea- surement setup in the experiment: two half-wave plates represent the application ˆU† B, the other half-wave plate represents the measurement choices between ˆB0 and ˆB1, a polarization-based Mach-Zehnder interferometer (MZI) implements the unsharp measurement. It entangles the polarization with the path degree of freedom, while the sharpness parameter is set by the angles {−µ 2 , π 4 − µ 2 } of the internal half-wave plate. The experimental results reveal that all nine indepen- dent ICHSHs values surpass 2, providing evidence that sequential measurements do not disrupt entanglement. Notably, the ICHSH value obtained in step 3 exceeds those in steps 1 and 2. This observation aligns with ex- pectations based on the sharpness parameters employed in the experiment, illustrating the feasibility of apply- ing the protocol for entanglement amplification. It’s im- portant to note that this amplification effect is observed only for a subset of measurement choices and outcomes. The reported results demonstrate robust violations of the CHSH inequality, supported by a statistical significance exceeding 10 standard deviations, even at the third step of the sequential process. 4. Realization Of Double Violation By Projective Measurements Strategy Recent studies by Steffinlongo and Tavakoli (2022) have demonstrated that projective measurements alone are sufficient for recycling nonlocality. Additionally, the work of Xiao et al. (2022) has employed projective mea- surements and validated the CHSH inequality to show that, for initial states that are either maximally entan- gled or partially entangled, it is experimentally observ- able that three independent observers can share the Bell nonlocality of binary qubit states. In their experiment, the initial state is chosen as a two- photon entangled state given by |ψφ⟩ = cos φ |HH ⟩ + sin φ |V V⟩, where φ ∈ 0◦, 45◦. The parameter φ is ma- nipulated using a half-wave plate. One of the entangled photons is directly transmitted to Alice, who performs a projective polarization measurement using a half-wave plate and a polarization beam splitter. Simultaneously, the second photon undergoes a distinct path, travers- ing an unbalanced interferometer before reaching Bob and Charlie for their respective projective measurements. Bob employs linear polarizers, while Charlie utilizes half- wave plates and polarization beam splitters. The unitary operation setup involves a sequence of optical elements, including a quarter-wave plate, a half-wave plate, and another quarter-wave plate on both paths. This config- uration allows Bob to implement arbitrary single-qubit unitary transformations. Furthermore, variable neutral density filters, easily rotatable, provide control over the relative probability of combining the two paths in the interferometer setup. Xiao et al.(2022) first investigate the Bell nonlocality recycling for a maximally entangled state. In order to obtain the optimal double violation, the first and second cases are randomly combined (Steffinlongo and Tavakoli, 2022), and the optimal settings for the first and second cases are, ϕ = 75 ◦, χ= 45 ◦. As p increases, IAB in- creases and IAC decreases, where p is the probability of the first scenario. The double violation of CHSH inequal- ity can be observed in the range of p ∈ h 2√ 6 , 4−2 √ 2 3− √ 3 i (see Fig. 10). In theory, IAB and IAC reach their maximum value when p = 6−2 √ 3 3 . In the experiment, IAB = 2 .0451 ± 0.0015, IAC = 2 .0676 ± 0.0014 can be obtained when p = 0.83207 ± 0.0015, both of which are 30 standard deviations higher than the classical bound. Furthermore, the first and third cases are randomly combined (Steffinlongo and Tavakoli, 2022), with the optimal measurement setting being ϕ = 71 .57◦ and θ = 18 .43◦. The maximum values of the two CHSH expressions are IAB = 2 .1056 ± 0.0013, and IAC = 2.0927 ± 0.0013, respectively, when p = 0.3256 ± 0.0012. Both values are 71 standard deviations higher than the classical bound. Obviously, these results are stronger than the combination of the first and second cases. They further studied the Bell nonlocality of partially entangled states. By varying the probability p of the first scenario, they discovered that if the proper ϕ is chosen, partially entangled states can achieve a stronger double violation than the maximum entangled state, regardless of whether it is a blend of the first and second cases or the first and third cases. When examining the second case, where ϕ21 2.2 1.0 1.5 2.0 2.5 0.0 1.0 2.0 3.0 Sλ ΑΒ SΑ λ C 2.00 2.10 2.15 2.05 2.0 2.1 2.3 1.0 2.0 3.0 1.5 2.0 2.5 3.0 SΑΒ Bound (d) (a) Bound λ=1 λ= 3 λ=2 and 3 λ=1 λ=1 and 3 0.0 3.0 SΑC λ=3λ=2 0.50 1.000.75 p 0.0 1.0 2.0 3.0 2.04 2.12 2.08 0.84 0.86 (b)SΑC(S    ΑΒ ) 0.00 0.25 Bound SΑC SΑΒ λ=1 and 2 0.50 1.000.75 1.75 2.00 2.25 2.50 p Bound SΑC SΑΒ λ=1 and 3 0.00 0.25 SΑC ( ΑS    Β ) (c) Exp Fig. 10:Experimental results of double violation by projec- tive measurements. This figure is adapted from (Xiao et al., 2022). takes on values of {45◦, 39.23◦, 34.08◦, 28.32◦, 21.77◦}, all results exceed the bound established by the maximum entangled state. The average fidelity of the state is ap- proximately 0.9853 ± 0.0012. In conclusion, the researchers successfully conducted an experimental demonstration of nonlocality sharing involving a 2-qubit entangled state among three ob- servers through the utilization of projective measure- ments. Through the random combination of basis projec- tive and identity measurements, they observed a double violation of the CHSH inequality for the maximally en- tangled state. Interestingly, the violations became more pronounced when employing random combinations of ba- sis projective and mixed measurements. Notably, certain partially entangled states exhibited even stronger double violations, with the maximum violation surpassing that achievable with the maximally entangled state by up to 11 standard deviations. III. STEERING SHARING Einstein-Podolaky-Rosen(EPR) steering is a unique quantum correlation of two-particle quantum states be- tween Bell nonlocality and entanglement (Einstein et al., 1935). The steerable states are a subset of entangled states and a superset of Bell states. Especially, steer- ability is an asymmetrical correlation resulting in a more intricate monogamous relationship than bell nonlocality (Bell, 1964; Werner, 1989; Wiseman et al., 2007; Jones et al., 2007; Cavalcanti and Skrzypczyk, 2016; Uolaet al., 2020). Reid (2013) established monogamy constraints contingent and demonstrated that the steerable states have a directional property. Subsequently, Bowles et al. (2014) also demonstrated such directional property, i.e., one can steer the other but not the other way around (Adesso et al., 2016; Lami et al., 2016; Mal et al., 2017). Inspired by the investigation of nonlocality sharing, it is straightforward to extend to quantum steering. Sas- mal et al. (2018) investigated the maximum number of multiple observers that could steer a single observer in the sequential scenario. Conversely, Yao and Ren (2021) examined the maximum number of multiple observers that a single observer could steer. Subsequently, a se- ries of works concentrated on steering sharing in the one- sided sequential case has been explored (Sasmal et al., 2018; Shenoy H. et al., 2019; Choi et al., 2020; Gupta et al., 2021; Yao and Ren, 2021; Han et al., 2023; Chen et al., 2023). Zhu et al. (2022) further explored steer- ing sharing in the two-sided case and observed double EPR steering. In addition, in a tripartite sequential sce- nario (where sequential measurements performs only on one side), Gupta et al. (2021) investigated all types of (1→2) and (2 →1) genuine tripartite EPR steering. Re- cently, steering sharing with unequal sharpness measure- ment has been investigated in (Han et al., 2023; Lv et al., 2023). A. Sequential Steering By Multiple Observers The phenomenon of sharing nonlocality offers a dif- ferent perspective for delving into quantum correlations. Subsequent studies have expanded this concept to en- compass the sharing of steering phenomena through se- quential measurements. Sasmal et al. (2018) explored a limited number of Bobs that Alice can steer in the se- quential scenario. They choose the ρsource with the sin- glet state. To explore the sequential steering by multiple observers, they employ CHSH-like inequality as criteria, which is also known as Cavalcanti Foster Fuwa Wise- man(CFFW). Suppose that each observer has a choice between two dichotomic measurements, the CHSH-like inequality remains, SBA = q ⟨(B1 + B2)A1⟩2 + ⟨(B1 + B2)A2⟩2 + q ⟨(B1 − B2)A1⟩2 + ⟨(B1 − B2)A2⟩2 ≤ 2. (48) And when each observer has a choice between m di- chotomic measurements, they employ linear steering in- equality (see Eq. (11)) as criteria. In the sequential scenario, both Alice and Bob k have two independent dichotomic observables, ˆxj = ˆyi k = ⃗ n.ˆσ, i, j∈ {0, 1}. Since all Bobs steer Alice’s system, the measurement settings of Alice are mutually unbiased and satisfy ˆx0.ˆx1 = 0. The intermediate Bob k carry out weak measurements with precise parameter λk. While the last observer on Bob’s side will perform strong mea- surements (λk = 1). They discussed the simplest case,22 k = 2, it is shown that the double violation of the CHSH-like inequalities can be observed in the range of λ1 ∈ (0.71, 0.91). So Bob 1 and Bob2 can steer Alice si- multaneously. Besides, they determined that no more than two Bobs can steer a single Alice based on the CHSH-like inequality criterion. Subsequently, they increased the number of measure- ment settings of all the observers in the sequential sce- nario. Assuming that Alice chooses between three binary measurements ˆxi, and Bob k chooses between three bi- nary measurements ˆyi k, where i ∈ {0, 1, 2}. The steering verification that Bobk steers Alice’s system can be deter- mined by the violation of the three-setting linear inequal- ity, which can expressed as F3 m = 1√ 3 P3 i=1 ¯Cii m. Since all Bobs steer Alice’s system, the measurement settings of Alice are orthogonal to each other, ˆ x0.ˆx1 = 0 , ˆx0.ˆx2 = 0, ˆx1.ˆx2 = 0. In this scenario, they show that no more than three Bobs simultaneously can steer Alice based on the violation of the three-setting linear inequality. Since then, a logical proposition has emerged, suggest- ing that the capacity for multiple Bobs to steer Alice esca- lates with the increasing number of measurement choices each observer possesses. The hypothesis suspected that an ensemble of n-Bobs can steer the Alice system when probing steering through the n-settings CJWR inequal- ity, which remains unproven or unrefuted. The other related work includes, Shenoy H. et al. (2019) has found that with isotropic entangled states of local dimension d, the number of Bobs that can steer Alice is found to be NBob ∼ d logd thus leading to an arbitrarily large number of successive instances of steering with independently chosen and unbiased inputs. Choi et al. (2020) experimentally demonstrate multiple- observer quantum steering by exploiting sequential weak measurements. Han et al.(2023) have proposed an effec- tive method for constructing optimal nonlocal measure- ments using quantum ellipsoids to share quantum steer- ing in a 3-qubit systems. It has further investigated the (n→1) steering sharing in 3-qubit system (Gupta et al., 2021; Chen et al., 2023). B. Alice Steers Two Bobs Conversely, Yao and Ren (2021) investigated the case where a single Alice steers multiple Bobs by sharing an arbitrary 2-qubit state in the sequential scenario. The arbitrary 2-qubit state can be expressed as ρAB = 1 4 (I ⊗ I + P3 i λiσi ⊗ I + P3 j ηjI ⊗ σj + P3 k γkσk ⊗ σk), where i, j, k∈ {1, 2, 3}, |γ1| ≤ |γ2| ≤ |γ3|. Alice per- forms a strong measurement ˆ σwx = P3 i=1 wi xσi, where ⃗ wx = {w1 x, w2 x, w3 x} represents the measurement direc- tions. Bob 1 performs weak measurement on the received qubit along the ⃗ µy1 , the remaining observers on Bob’s side can be defined similarly. To explore the phenomenon of steering sharing, they have chosen the CHSH-like inequality and the three- setting linear steering inequality as the benchmarks for evaluating steering. Under the CHSH-like inequality, each observer is provided with two measurement options. While, in the case of the three-setting linear inequality, each observer has three measurement choices. They started by exploring whether Alice can steer two Bobs. In the scenario, Bob 2 performs a strong measurement, where F2 = 0 and G2 = 1. Under the optimal measurement setting, the CHSH-like aver- age value corresponding Alice-Bob n (n=1,2) can be ex- pressed as I(1) CHSH−like = 2 p γ2 2 + γ2 3G1, I(2) CHSH−like =p 2(1 + F2 1 )(γ2 2 + γ2 3). Defined |γi| = 1, the CHSH- like average value was given as I(1) CHSH−like = 2 √ 2G1, I(2) CHSH−like = 2 p 1 + F2 1 . No matter whether the pointer distribution of Bob 1’s weak measurement is square or optimal, it is evident that a double violation of two in- equalities exists within the appropriate range. Hence, it demonstrates Alice can steer two Bobs. Furthermore, they show that three Bobs can be steered by a single Alice when the triple violation of the three- setting linear steering inequality is observed. In this case, each observer has three measurement choices ( m = 3). According to (Costa and Angelo, 2016), the measure- ment settings of Bobs also need to meet the following two conditions, X y1 (µ1 y1 )2 = X y1 (µ2 y1 )2 = X y1 (µ3 y1 )2 = 1, X y1 µ1 y1 µ2 y1 = X y1 µ1 y1 µ3 y1 = X y1 µ2 y1 µ3 y1 = 0. The three-setting linear inequalities corresponding to Alice-Bobn can be simplified as I(1) linear = 3 G1, I(2) linear = 1 + 2F1 under the optimal measurement settings. It is evident that achieving double violations of three-setting linear inequalities remains easily attainable, signifying Alice’s capability to steer two Bobs. While the more interesting question is how many mul- tiple observers can share the correlation in this scenario. Assuming the observer has three inputs x, y1, y2 ∈ {0, 1}, and employing the linear steering inequality, when the measurement settings of Bob 2 and Bob 3 match those of Bob 1, the average value of linear steering inequal- ity I(n) linear, (n ∈ {1, 2, 3}) can attain its maximum value. Under the optimal measurement settings, three linear inequalities can be given as, I(1) linear = 3 G1, I(2) linear = 3γ1G2, I(3) linear = 3 γ1γ2. Obviously, the triple violation of the linear inequalities can be obtained under the op- timal pointer of weak measurement, but not under the square pointer. This result proves that Alice can steer three Bobs. Subsequently, they investigated the potential for quadruple violations, encompassing Alice and four Bobs. In cases where the measurement configurations of Bob 2,23 Bob3, and Bob4 align with those of Bob1 in the preceding measurement settings, the linear inequalities can reach their utmost value. However, irrespective of whether the distribution of pointer measurements is optimal or square, no observable quadruple violation occurs. Thus, in this sequential scenario, the number of Bobs steered by Alice aligns with the number of measurement settings for each observer. Currently, the ongoing debate regard- ing multiple violations of inequality revolves around the prioritization of maximized violation by Alice and Bob 1. It remains an open question whether there exist multiple violations simultaneously when the violation of the in- equality associated with Alice and Bob 1 is not maximal. In addition, it has further investigated (1 → n) steering sharing in a 3-qubit system (Gupta et al., 2021; Chen et al., 2023). C. Steering Sharing In The Multipartite Systems And Multilateral Cases 1. Genuine Tripartite EPR Steering Multipartite quantum steering has demonstrated its importance as a vital resource for diverse quantum pro- tocols, including quantum key distribution and quan- tum teleportation. Delving into the subtleties of mul- tipartite quantum steering offers valuable insights into the quantum correlations present in intricate quantum systems with more than two parties. Specifically, com- prehending the effective distribution and utilization of multipartite quantum steering among diverse parties rep- resents a noteworthy focus. This exploration stands to contribute significantly to the advancement of sophisti- cated quantum communication protocols and technolo- gies. Gupta et al. (2021) explored the sharing of gen- uine tripartite EPR steering among multiple sequential observers. Within the trilateral sequential scenario, a pure 3-qubit state of either the GHZ type or W type is distributed among three observers (Alice, Bob, Char- lie). On one side, arbitrary sequential observers conduct unsharp measurements. Due to the inherent asymmetry of EPR steering, two distinct types of genuine tripar- tite EPR steering scenarios: one where Alice endeavors to genuinely steer the particles of both Bob and Charlie (1→2), and another where Alice and Bob collaborate to genuinely steer Charlie’s particles (2 →1), as illustrated in Fig. 11 and Fig. 12. In (1→2) case, Supposed that Alice’s measurement op- erator is ˆAa|x which is the projective measurement along the ˆA-direction with measurement outcome a. The un- normalized conditional states on Bob-Charlie’s side can be expressed as n σBC a|x o a|x , where every element can be given by as σBC a|x = Tr[( ˆAa|x ⊗ IB ⊗ IC)ρ]. When the ini- tial state ρ is not genuinely entangled but separable, it Alice1 ࣋YU[XIK Charlie1 Bob1 Bobk y1ሽ ykሽ b1={0, 1} b k={0, 1} Ș x1ሽ a1={0, 1}an={0, 1} xnሽ Alicen z1ሽ c1={0, 1} Ș Fig. 11: A schematic diagram (1 →2): Alice attempting to genuine steer Bob and Charlie’s particles. ࣋YU[XIK Bob1 Bobk y1ሽ ykሽ b1={0, 1} b k={0, 1} Ș x1ሽ a1={0, 1} Ș Alice1 Charlie1 Charliet z1ሽ c1={0, 1} ztሽ ct={0, 1} Fig. 12: A schematic diagram (2 →1): Alice and Bob at- tempting to genuine steer Charlie’s particles. always can be decomposed as, ρ = X λ pA:BC λ ρA λ ⊗ ρBC λ + pB:AC µ ρB µ ⊗ ρAC µ + pAB:C v ρAB v ⊗ ρC v . (49) Where pA:BC λ , pB:AC µ , pAB:C v are probability distributions. A : BC, B : AC, and AB : C represent the different types of bi-partitions. It is worth noting that, there is no dif- ference between AB : C and C : AB. Once each element of n σBC a|x o a|x cannot be described as, σBC a|x = X λ pA:BC λ pλ(a|x)ρBC λ + X µ pB:AC µ ρB µ ⊗ σC a|xµ + X v pAB:C v σB a|xv ⊗ ρC v , ∀a, x, (50) which means the genuine tripartite EPR steering from Alice to Bob-Charlie. σC a|xµ is the unnormalized condi- tional state on Charlie’s side when Alice performs the measurement x with the outcome ‘ a’. Similarly, σB a|xv is the unnormalized conditional state on Bob’s side when Alice performs the measurement x with the outcome ‘a’. In the (2→1) case, assuming that Alice’s measurement operator is represented by ˆAa|x and Bob’s measurement operator is represented by ˆBb|y, the unnormalized con- ditional state prepared on Charlie’s side can be given as σC ab|xy = Tr[( ˆAa|x ⊗ ˆBb|y ⊗ IC)ρ]. Similarly, once each24 element of n σC ab|xy o a,b,x,y cannot be written as, σC ab|xy = X λ pA:BC λ pλ(a|x)σC b|yλ + X µ pB:AC µ pµ(b|y)σC a|xµ + X v pAB:C v pv(ab|xy)ρC v , ∀a, b, x, y, (51) which means the genuine tripartite EPR steer from Alice- Bob to Charlie. Unfortunately, the above definition is not practical, and few criteria for determining genuine multipartite quantum steering were constructed. Nevertheless, Cav- alcanti et al. designed several inequalities to detect the genuine quantum steering of GHZ and W states in these two cases, which can be expressed as,    G1 =1 + gα⟨ZBZC⟩ −1 3(⟨A3ZB⟩ + ⟨A3ZC⟩ + ⟨A1XBXC⟩ − ⟨A1YBYC⟩ − ⟨A2XBYC⟩ − ⟨A2YBXC⟩) ≥ 0, G2 =1 − α(⟨A3B3⟩ + ⟨A3Z⟩ + ⟨B3Z⟩) − β(⟨A1B1X⟩ − ⟨A1B2Y ⟩ − ⟨A2B1Y ⟩ − ⟨A2B2X⟩) ≥ 0, W1 =1 + wα(⟨ZB⟩ + ⟨ZC⟩) − wβ⟨ZBZC⟩ −wγ(⟨XBXC⟩ + ⟨YBYC⟩ + ⟨A3XBXC⟩ + ⟨A3YBYC⟩) + wδ(⟨A3⟩ + ⟨A3ZBZC⟩) + wϵ(⟨A3ZB⟩ + ⟨A3ZC⟩) − wϕ(⟨A1XB⟩ + ⟨A1XC⟩ + ⟨A2YB⟩ + ⟨A2YC⟩ + ⟨A1XBZC⟩ + ⟨A1ZBXC⟩) + ⟨A2YBZC⟩ + ⟨A2ZBYC⟩) ≥ 0, W2 =1 + wκ(⟨A3⟩ + ⟨B3⟩) + wλ⟨Z⟩ −wη(⟨A1X⟩ + ⟨A2Y ⟩ + ⟨B1X⟩ + ⟨B2Y ⟩) + wµ(⟨A3Z⟩ + ⟨B3Z⟩) − wν(⟨A1B1⟩ + ⟨A2B2⟩) + wω⟨A3B3⟩ −wπ(⟨A1B1Z⟩ + ⟨A2B2Z⟩) + wθ⟨A3B3Z⟩ − wξ(⟨A1B3X⟩ + ⟨A2B3Y ⟩ + ⟨A3B1X⟩ + ⟨A3B2Y ⟩) ≥ 0, (52) where the two inequalities G1 and W1 are used to deter- mine EPR steering from Alice to Bob-Charlie, while G2 and W2 are used to determine EPR steering from Alice- Bob to Charlie. Obviously, the genuine tripartite EPR steering sharing can be investigated in two trilateral se- quential scenarios based on these four inequalities. In the first scenario, where multiple Alices (Alice 1, Alice2,..., Alicen) perform sequential measurements, ex- ploring the amount of Alices capable of effectively steer- ing Bob-Charlie, as well as the amount of Alices capable of effectively steering Charlie when cooperated with a single Bob, are interesting. Without loss of generality, supposed that Alice n’s weak measurement is ˆxn i , while the Bob and Charlie’s sharp measurement are ˆyj and ˆzk respectively, where i, j, k∈ {0, 1, 2}, which can be ex- pressed as, ˆyj = sin θy j cos ϕy j ˆX + sinθy j sin ϕy j ˆY + cosθy j ˆZ, ˆzk = sin θz k cos ϕz k ˆX + sinθz k sin ϕz k ˆY + cosθz k ˆZ, ˆxn i = sin θxn i cos ϕxn i ˆX + sinθxn i sin ϕz k ˆY + cosθxn i ˆZ. When the shared state is the GHZ state, either in Alice n genuine tripartite steer Bob-Charlie(1 →2) or in Alice n- Bob genuine tripartite steer Charlie(2→1), under the op- timal measurement setting for all observers, the maxi- mum amount of Alices is 3 by analyzing the multiple vi- olations of G1 and G2. Especially, the allowable range of sharpness parameters is relatively large under 1 →2 EPR steering case. When the shared state is the W state, the maximum amount of Alices is 2 in both two EPR steer- ing cases according to the existence of the multiple vio- lations of W1 and W2. Similarly, the allowable range of sharpness parameters is relatively large under 1 →2 EPR steering case. In the other scenario, where multiple Charlies (Charlie1, Charlie 2, ..., Charlie n) perform sequential measurements, they also explored the amount of Alices capable of effectively steering Bob-Charlie, as well as the amount of Alices capable of effectively steering Charlie when cooperating with a single Bob. Similar discussions with the above, when the shared state is GHZ state, it is shown that up to six sequential Charlies and a single Bob can be genuinely steered by a single Alice (1 →2) by analyzing multiple violations of the G1 inequalities. In addition, up to three Charlie can be genuinely steered by Alice-Bob (2→1). Compared to the 2 →1 steering case, the amount of sequential observers and range of sharp- ness parameters in the 1 →2 steering case are higher. when the shared state is W state, at most four Char- lies and Bob can genuinely be steered by Alice (1 →2) based on the multiple violations of W1 inequalities. On the other hand, Alice-Bob can truly steer up to three Charlies. The amount of sequential observers and the sharpness range for the 1→2 steering case are also higher than those for the 2 →1 steering case. The result shows that the GHZ state is more power-25 ful than the W state, allowing for more observers in the sequential steering scenario. Additionally, the 1 →2 EPR steering case is more effective across a wider range of al- lowable sharpness parameters than that of the 2→1 EPR steering case. They also explain that these steering shar- ing phenomena can be verified in quantum secret shar- ing protocol, and determining the precise relationship between the security of such sequential quantum secret- sharing protocols and genuine tripartite EPR-steering se- quential detection is worthy of further investigation. Subsequently, Chen et al. (2023) also investigated steering sharing in three-qubit systems. Han et al.(2023) has found that each pair of Alice 1-Bob1 remotely steers the quantum state of Charlie simultaneously and inde- pendently. 2. EPR Steering In The Two-sided Sequential Measurement Scenario The above discussion is the simplest one-sided sequen- tial measurement scenario. In a more general case, it is interesting to extend it to the bilateral scenario. As in- troduced in Sec. II.E.1, Zhu et al. (2022) indicated that nonlocal sharing of 2-qubit systems in the bilateral sce- nario is impossible. While it is possible to observe the EPR steering sharing in the two-sided sequential mea- surement scenario. In their work, they investigated whether Alice 1-Bob1 and Alice 2-Bob2 can observe EPR steering simultane- ously in the simplest bilateral scenario with two Alices and two Bobs. Assumed that the entangled pair in a singlet state, are assigned to multiple Alices and Bobs on both sides. The measurement operators of Alice i and Bobj are de- fined by ˆxi and ˆyj with the binary outcomes ai and bj respectively. The linear steering inequality (11), Sm, was used as the criteria for verifying quantum steer- ing, where m is the amount of measurement settings for each observer. The average value SA1B1 for Alice1- Bob1 is only determined by GA1B1 , and the average value SA2B2 for Alice2-Bob2 is only determined by FA1B1 when the measurement settings are determined. When the amount of measurement settings for each observers m = 3, where the measurement settings are chosen as {σx, σy, σz}, and the quality factor G is same for Alice 1 and Bob1, the maximal average values can be given as SA1B1 3 = G2, SA2B2 3 = 1−2G2 3 . It is shown that the EPR steering sharing can be observed between Alice 1-Bob1 and Alice2-Bob2 when G ∈ (0.7598, 0.796). In practical terms, obtaining a higher quantity of mea- surement settings (m) is essential to observe a greater number of instances of violation. Notably, in a unilat- eral sequential scenario where multiple Bobs aim to steer a single Alice, all Bobs must select the same measure- ment to steer Alice synchronously. Conversely, in a bilat- eral sequential scenario, each Bob endeavors to indepen- dently steer several Alices. Consequently, the measure- ment choices of different Bobs are independent of each other. It would be intriguing to explore the potential for multiple pairs of Alice i-Bobj to simultaneously demon- strate EPR steering in a bilateral sequential scenario. Similarly, the steering sharing in the two-sided sequen- tial measurement scenario has also been investigated in (Liu et al., 2021; Han et al., 2021; Li et al., 2022; Liu et al., 2022; Han et al., 2022; Lv et al., 2023). Espe- cially, Han et al. (2022) showed that how to manipulate that steering direction among multiple Alices and Bobs through sequential unsharp measurement. It is indicated that the change of steering direction depends on the num- ber of measurement settings and the sharpness of mea- surements. Liu et al. (2022) demonstrated this steering scenario experimentally. 3. Steering Sharing Using Nonlocal Measurements The investigation of quantum steering sharing among multiple observers is presently constrained to imprecise local measurements. Recently, Han et al. (2023) showcases that imprecise nonlocal product measurements can trigger steering shar- ing, surpassing the efficacy of local measurements. Fur- thermore, the optimal nonlocal measurements can be ob- tained through the use of steering ellipses. Experimental findings indicate that, in the sharing of GHZ states with two measurement settings, the activation of steering shar- ing is more pronounced when employing imprecise non- local product measurements compared to imprecise local measurements. Notably, the activation of steering shar- ing through nonlocal measurements, particularly those with unequal strength measurements, exceeds that ob- served with equal strength measurements. As illustrated in Fig. 11, assumed that three-qubit state ρABC initially shared between the spatially sepa- rated Alice1, Bob1, and Charlie. Alice1 and Bob1’s qubits will be sequentially handed down. It is worth exploring whether each pair of Ai and Bi can steer Charlie’s quan- tum state simultaneously and independently. Different from the previous discussion, they relax the measure- ment restrictions, Alice 1 and Bob 1 can adopt nonlocal measurements. The k-th nonlocal measurement of A i-Bi is set to ˆΠ(i) k , which strength is labeled as λ(i) k . And k-th local sharpness measurement of Charlie is represented as ˆΛk. They adopted n-setting Linear Steering inequality to verify steering. For each group of observers, the post-measurement state shared between Ai-Bi and Charlie can be expressed26 as, ρ(i) ABC = 1 n nX k=1 X ok=−1,1 [(K(i) ok ⊗ IC)ρ(i−1) ABC (K(i)† ok ⊗ IC)], (53) where K(i) ok K(i) ok † = ˆΠ(i) k , ok ∈ {+1, −1} is of ˆOi. The av- erage value is given as ⟨ˆΠ(i) k ⊗ ˆΛk⟩ = Tr[ˆΠ(i) k ⊗ ˆΛkρ(i) ABC ]. Through the steering ellipsoids, the optimal nonlocal measurements can be obtained as ˆΠ(i) 1 = λ(i) 1 (σy ⊗ σy), ˆΛ1 = σx, ˆΠ(i) 2 = λ(i) 2 (σy ⊗ σx), ˆΛx = σy. So the average value of the corresponding linear steer inequality can be given as, S(i) 2 = 1 2i h λ(i) 2 Y 1≤j≤i−1 ( 1+Fλ(j) 1 ) + λ(i) 1 Y 1≤j≤i−1 ( 1+Fλ(j) 2 ) i ≤C2, (54) where Fλ(j) 1 = q 1 − (λ(j) 1 )2, Fλ(j) 2 = q 1 − (λ(j) 2 )2, C2 = 1√ 2 . While if the measurements are constrained local, then the optimal measurement is,{ ˆMAi 1 = η(i) 1 σy, ˆMBi 1 = γ(i) 1 σy, ˆMC 1 = σx}, { ˆMAi 2 = η(i) 2 σy, ˆMBi 2 = γ(i) 2 σx, ˆMC 2 = σy}. The average value of the corresponding linear steer inequality can be given as, eS(i) 2 = 1 2i h λ(i) 2 Y 1≤j≤i−1 (1+ Fγ(j) 1 ) +λ(i) 1 Y 1≤j≤i−1 ( 1+Fγ(j) 2 ) i ≤C2, (55) where Fγ(j) 1 = q 1 − (γ(j) 1 )2, Fγ(j) 2 = q 1 − (γ(j) 2 )2. The parameters of the weak measurement should satisfy λ(i) k = η(i) k ∗ γ(i) k , k∈ {1, 2}. Assumed that the strengths of two-setting measure- ments used by each observer are equal, which is defined as λ(i) = λ(i) 1 = λ(i) 2 , η(i) = η(i) 1 = η(i) 2 , γ(i) = γ(i) 1 = γ(i) 2 . The criterion inequalities in both cases can be given as S(i) 2 = 1 2i−1 \u0014 λ(i) Q 1≤j≤i−1(1 + p 1 − (λ(j))2) \u0015 , eS(i) 2 = 1 2i−1 \u0014 λ(i) Q 1≤j≤i−1(1 + p 1 − (γ(j))2) \u0015 . Due to the re- quirement λ(i) = η(i) k ∗ γ(i) k , which means λ(i) < γ(i) k , hence S(i) 2 > ˜S(i) 2 . This result indicated that using the nonlocal product measurement can activate steer- ing sharing where it is unattainable through local mea- surement. Additionally, it can be observed that a max- imum of two pairs of Ai and Bi can steer Charlie si- multaneously. When the strength of two-setting mea- surements used by A i and B i are unequal, λ(i) 1 ̸= λ(i) 2 , η(i) 1 ̸= η(i) 2 , γ(i) 1 ̸= γ(i) 2 , η(i) k = γ(i) k , k∈ {1, 2} = q λ(i) k , the maximum number of Ai-Bi pairs that can simultaneously steer Charlie does not increase. Fig. 13: Experimental setup and schematic of the weak measurement with variable strength λ. This figure is adapted from (Choi et al., 2020). In the case of nonlocal measurements and unequal strength of the two-settings measurements, the steering ellipsoid of Ai and Bi, as well as Charlie’s turning ellip- soid, vary with λ(i) 1 and λ(i) 2 . The volume of Charlie’s steering ellipsoids generated by the measurements of A i and Bi can be given as V (i) C = |det(T−em⃗ n⊤)|/(1−|em⃗ n|2)2 4π/3 . It is shown that the volume reduction rate of the steering ellipsoid is relatively slow compared to using measure- ments of equal intensity. Since the volume of the steer- ing ellipsoid characterizes the steering capacity, nonlocal measurements with unequal strength are relatively more favorable for activating quantum steering than with equal strength. Lv et al. (2023) also investigated the shar- ing of EPR steering via weak measurement with unequal sharpness parameters in a generalized case, showing that an unbounded number of sequential Alice-Bob pairs can share the EPR steering. D. Experiments Of Steering Sharing 1. Detection Steering Sharing Choi et al.(2020) has experimentally showcased multi- observer quantum steering by leveraging sequential weak measurements, providing direct confirmation that quan- tum steering can be shared among more than four ob- servers. In their experiment (see Fig. 13), considering a unilat- eral sequential scenario, the initial state is a singlet state |Ψ−⟩ ≡ 1√ 2 (|HV ⟩ − |V H⟩). To achieve weak measure-27 Fig. 14: Demonstrating the triple violations of the steering inequalities, indicating simultaneous quantum steering by three observers. This figure is adapted from (Choi et al., 2020). ment, it is necessary to suppose an arbitrary pure state |ψ⟩in = α |ϕ⟩+ β |ϕ⊥⟩ with ⟨ϕ|ϕ⊥⟩ = 0 given as input. A quarter-wave plate and a half-wave plate rotate the polar- ization state in such a way thatˆR+ |ψ⟩in → α |H⟩+β |V ⟩. The photon is then entered into the interferometer with polarizing beam displacers. Inside the interferometer, both the horizontal and vertical polarization components at each path evolve to sin 2θλ |H⟩+cos 2θλ |V ⟩. Then, the unnormalized output state after polarizing beam displac- ers is given as α cos 2θλ |H⟩ + β sin 2θλ |V ⟩. Finally, ˆR−1 + operation is applied by using a quarter-wave plate and a half-wave plate, giving the final (unnormalized) out- put state as |ψ⟩out = α cos 2θλ |ϕ⟩+ β sin 2θλ |ϕ⊥⟩, which has the same effect as the evolution of the Kraus op- erator. The measurement strength λ can be expressed as λ ≡ 2 cos2(2θλ) − 1, where θλ ∈ \u0002 0, π 8 \u0003 . The half- wave plate at θλ determines the measured strength. This completes the weak measurement process. The mea- surement strength of Bob 1 and Bob2 are labeled as λ1 and λ2. Finally, Alice and Bob 3 choose their mea- surement directions with a combination of wave plates and a polarizing beam splitter and have two detectors (D1, D2) and (D3 , D4). Four case of coincidence de- tection (D1 − D3), (D1 − D4), (D2 − D3), (D2 − D4) are observed. Each event corresponds to four possible out- comes for Alice and Bob 3. For a given measurement di- rection ⃗ αk, ⃗βl,⃗ γm,⃗δn. The measurement results are la- beled as {a, b, c, d}. The joint probability is obtained as p(a, b, c, d|⃗ αk, ⃗βl,⃗ γm,⃗δn). The value of linear steering inequality corresponds to Alice-Bob 1, Alice-Bob2, Alice- Bob3 can be given as Sn = √nSn. To obtain the maximum value of S1, the measurement direction can be setted as ⃗ α1 = ⃗β1 = ⃗ x, ⃗ α2 = ⃗β2 = ⃗ y, ⃗ α3 = ⃗β3 = ⃗ z. Similar for Bob 2 and Bob3 as ⃗ γ1 = ⃗δ1 = ⃗ x, ⃗ γ2 = ⃗δ2 = ⃗ y, ⃗ γ3 = ⃗δ3 = ⃗ z.While ⃗ γm and ⃗δn have no lim- itation. Optimization is only possible when Bob 2 knows the value of λ1 and Bob3 knows λ1 and λ2. The max- imum steering value is expected to be √ 3 in our three measurement cases, which can be obtained at λ1 → 1 for S1, λ1 → 0, λ2 → 1 for S2 and λ1 = λ2 → 0, λ3 → 1 for S3. As the measurement strength λ1 becomes weaker, the steering value S1 decreases, while S2 and S3 both increase. For a fixed value λ1, there exists a trade-off be- tween S2 and S3 depending on λ2. When λ1 = 0.64 and λ2 = 0.76, the theoretical results for {S1, S2, S3} can be given as {1.113, 1.107, 1.124} respectively, and the cor- responding experimental results as {1.047, 1.070, 1.078}. These results clearly demonstrate the tripe violation of the linear steering inequality, with more than 40 standard deviations (see Fig. 14). The research demonstrated that quantum steering can be distributed among more than four observers using merely three measurement settings. Augmenting the number of measurement settings holds the potential to involve a greater number of parties concurrently sharing quantum steering. These findings offer insights into the fundamental properties of quantum correlation, paving the way for a comprehensive utilization of entangled sys- tems across diverse applications. 2. Witness Multi-observer Steering Subsequently, Liu et al. (2022) conducted an exper- imental demonstration of the steering scenario. They observe quartic EPR-steerings of two-side sequential ob-28 /s48/s46/s48 /s48/s46/s50 /s48/s46/s52 /s48/s46/s54 /s48/s46/s56 /s49/s46/s48 /s48/s46/s48 /s48/s46/s50 /s48/s46/s52 /s48/s46/s54 /s48/s46/s56 /s49/s46/s48 /s48/s46/s53/s48 /s48/s46/s53/s50 /s48/s46/s53/s52 /s48/s46/s53/s54 /s48/s46/s53/s56 /s48/s46/s54/s48 /s48/s46/s55/s53 /s48/s46/s55/s55 /s48/s46/s55/s57/s48/s46/s55/s54 /s48/s46/s55/s56 /s48/s46/s56/s48 /s83 /s49/s49 /s40/s98 /s41 /s40/s97/s41 /s76 /s72/s83 /s67 /s54 /s67 /s51/s44/s52 /s67 /s49/s48 /s67 /s50 /s83 /s50/s50 /s83 /s49/s50 /s47 /s83 /s50/s49 /s76 /s72/s83 Fig. 15: The experimental results of quartic violation of the steering inequalities, showcasing the steering on both sides simultaneously. This figure is adapted from (Liu et al., 2022). servers between each side of the entangled pair with a state fidelity of 97.6%, compared to a maximally- entangled state. Considering a bilateral sequential sce- nario, the initial state is prepared inρ = |ψ−⟩⟨ψ−|, where ψ = 1√ 2 (|01⟩ − |10⟩). They experimentally verified four EPR steering, with Alice1 and Bob 1 performing weak measurements and Alice2 and Bob 2 performing projective measurements. If Alice 1 and Bob 1 use the same sharpness parameter η = ηA = ηB with six independent measurements, it can be obtained that S(12) = S(21) due to the symmetry be- tween Alices and Bobs. The steerabilities for the observer-pair (Alice 1, Bob1) and (Alice 2, Bob 2) is a trade-off with η, and their steerabilities reach the peak around 0.56826 with η ∼ 0.81. Around η ∼ 0.76, it is found that four S(ij) can violate the linear steering inequality for the case of n = 6. This means that each observer con- vinces others in the remote subsystem that their pho- tons can be steered to the corresponding state without the need for any LHS model. Specifically, when η = 0.766, 0.788, the steering value {S(11), S(12), S(21), S(22)} can be given as {0.55481, 0.54416, 0.5378, 0.53096} and {0.58282, 0.55052, 0.54034, 0.51196} respectively, whose error are generally below 0.00276. Although matched well with the theoretical curves, the LHS values when η = 0.766 succeed in violating the C10 bound but partly fail to violate the C6 bound because of the imperfections of the BD interferometers (see Fig. 15). The experimental results showed that four EPR steer- ing: (Alice 1,Bob1), (Alice 2,Bob2), (Alice 1,Bob2), and (Alice2,Bob1). In this scenario, the operator also used weak measurements efficiently to conserve quantum re- sources. The simultaneous occurrence of the four EPR steering events is crucial for testing the stability of the entire network system and eliminating the possibility of a fifth observer in the network. IV. SHARING OF OTHER KINDS OF QUANTUM CORRELATIONS The series of studies on nonlocality sharing and steer- ing sharing have also triggered similar research on other non-classical correlations, such as network nonlocality (Hou et al., 2022; Wang et al., 2022; Halder et al., 2022; Mahato and Pan, 2022; Zhang et al., 2023; Kumar and Pan, 2023; Mao et al., 2023), quantum entanglement (Bera et al., 2018; Maityet al., 2020; Folettoet al., 2020b; Srivastava et al., 2021a; Pandit et al., 2022; Srivastava et al., 2022a; Das et al., 2022; Srivastava et al., 2022b; Hu and Fan, 2023; Li and Wang, 2023), quantum con- textuality (Kumari and Pan, 2019; Anwer et al., 2021; Chaturvedi et al., 2021; Kumari and Pan, 2023), among others. These results provide new insights into the prob- lem of reusing relevant quantum physical resources. A. The Sharing Of Network Nonlocality 1. The Extended Bilocal Scenario Standard Bell nonlocality inspired a type of network nonlocality, which refers to a concept of nonlocality ob- served in the well-known entanglement-swapping sce- nario. A series of works have found that network nonlo- cality has stronger sharing properties compared to Bell nonlocality. For example, in bilateral scenarios, Zhuet al. found that Bell nonlocality sharing does not exist in the case of two-sided sequential measurements. Subse- quently, Hou et al. (2022) further investigated whether network nonlocality can be shared through weak mea- surements, and discovered the existence of network non- locality through bilateral sequence measurements. This indicates the difference between Bell nonlocality sharing and network nonlocality sharing. In the extended bilocal scenario Fig. 16, there are two sources, S1 and S2, each of which generates a 2-29 qubit state. One source is shared between Alice 1 and Bob, while the other is shared between Charlie1 and Bob. Alice1 and Alice2, as well as Charlie1 and Charlie2 on the other side, will measure their subsystems sequentially. Supposed that the two shared states emitted by the two sources are defined as ρAB and ρBC respectively, ρAB = ρBC = |ψ⟩⟨ψ|, and |ψ⟩ = 1√ 2 (|00⟩ + |11⟩). The whole initial state of the system can be described by ρABC = ρAB ⊗ ρBC . Bob performs Bell state measurements on his re- ceived qubits, with four possible outputs b = b0b1 = {00, 01, 10, 11}, which corresponding four bell state, {|ϕ+⟩, |ϕ−⟩, |ψ+⟩, |ψ−⟩, }, where |ϕ±⟩ = 1√ 2 (|00⟩±| 11⟩), |ψ±⟩ = 1√ 2 (|01⟩ ± |10⟩). Alice n chooses two different di- chotomic observables independently, which are defined as ˆAn,i = cos θn,iσz − (−1)i sin θn,iσx, where ˆAn,i rep- resents the (i)th measurement choices of Alice n. Sim- ilarly to ˆCt,j for Charlie t, and ˆCt,j = cos θt,jσz + (−1)j sin θt,jσx. After completing the measurements of all observers, the whole joint probability distribution P(a1, a2, b, c1, c2| ˆX1, ˆX2, ˆY ,ˆZ1, ˆZ2) can be fully obtained. By marginalizing the joint probability distribution, the marginal probability P(an, b, cm| ˆXn, ˆY ,ˆZt) of any com- bination of Alice n-Bob-Charliet can be obtained. As is known that the network nonlocality can be determined by the violation of BRGP inequality (Branciardet al., 2012). The network nonlocality sharing can be judged when the BRGP inequalities corresponding arbitrary combination of Alicen-Bob-Charliet can be violated. Similar to Bell nonlocality sharing, network nonlo- cality sharing can be also divided into two categories: passive and active network nonlocality sharing. In these two different types of network nonlocality shar- ing, Alice 1 and Charlie 1 will choose different unsharp measurements based on different motivations. For pas- sive network nonlocality sharing, the maximal BRGP value can obtained under the optimal measurement set- tings, where B11 = √2G1G2, B12 = p (1 + F2)G1, B21 = p (1 + F1)G2, B22 = q (1+F1)(1+F2) 2 . It is evident that under the optimal pointer of weak measurement, the four BRGP values can exceed 1 simultaneously within G1 = G2 = G ∈ ( 1√ 2 , q 2( √ 2 − 1)), indicating the pas- sive network nonlocality sharing. For active network non- locality sharing, Hou, et.al give a suboptimal solution when G1 = G2 = G. When G ∈ [0, 0.8], the maximal BRGP values can be given as, B11 = √ 2G, B22 = (1+F)√ 2 . When G ∈ (0.8.1], the suboptimal values are B11 = (F+ √ 2−F(2+F))G√2−2F , B22 = q 1 + F3 + 1 2 F4 ≥ 1. B11 and B22 can exceed 1 in the range of G ∈ ( 1√ 2 , 1) under the optimal pointer of weak measurements. The comparative analysis reveals that active network nonlocality sharing exhibits a wider spectrum of dual vio- lations compared to passive network nonlocality sharing. y b Bob ࣋('࣋ )( Charlie1Alice1 Alicen Charlien a1={0, 1} c1={0, 1} ȘȘ x1ሽ z1ሽ xnሽ an={0, 1} znሽ cn={0, 1} Fig. 16: The extended bilocal scenario. These theoretical findings offer fresh perspectives for the exploration of network nonlocality. Subsequently, Mahato and Pan (2022) demonstrated that, in the symmetric case when the sharing is consid- ered for both the edge parties, the nonlocality can be shared by, at most, two sequential observers per edge party. Das et al. (2022) has explored the resource- theoretic efficacy of the single copy of a 2-qubit entangled state in a sequential network. Halder et al. (2022) has considered an alternative way of sharing network non- locality when two observers initially share a maximally entangled, a nonmaximally entangled and a noisy entan- gled state. Cai and Ren (2022) has explored the recycling of full network nonlocality as quantum resources. 2. The Extended Star Network Scenario Shortly after the proposal of network nonlocality shar- ing (Hou et al., 2022), Wang et al. (2022) investigated the recycling of network nonlocality in the generalized star network scenario with any number of unbiased bi- nary inputs via weak measurements. In this scenario, n independent sources s1, s2...sn distribute a 2-qubit state to Alice 11... Alice n1 and Bob respectively. The cor- responding measurement is set as ˆAxij(i) iji ( the jth ob- server in branch i, i ∈ {1...n}, j ∈ {1...m}) and ˆBl, and ˆA0 iji = ˆAk iji. If the n-partite distribution can not be writ- ten as, P(a1j(1) , . . . , anj(n) , b| x1j(1) , . . . , xnj(n) , y) = Z \" nY i=1 dλiqi(λi)p(aij(i) | xij(0) , λi) # × p(b|y, λ1, . . . , λn), (56) then the network n-nonlocality exists, where qi(λi) is the distribution of the hidden variableλi and the n sets of dis- tributions of hidden variableλ1...λn related to the source,30 λ = λ1..λn. The local response function of Alice ij only depends on j(i) corresponding λi, and for Bob depends on λ. The star network n-local scenario acknowledges the following nonlinear chain n-locality Bell inequality, S(n,m,k) j = kX l=1 |Ilj| 1 n ⩽ k − 1, (57) where Ilj = 1 2n lX x1j(1) ,...,xnj(n) =l−1 ⟨ ˆA x1j(1) 1j(1) . . .ˆA xnj(n) nj(n) ˆBl−1⟩. (58) This inequality is a generalization of the original star network n-locality inequality, the bilocal inequality, and the chain CHSH inequality. And its upper bound only depends on the number of inputs k. Suppose that a singlet state Ψ = 1√ 2 (|01⟩ − |10⟩) is shared in each branch. The measurement set for Aliceij is ˆA xij(i) ij(i) = sin \u0012 xij(i) π k \u0013 σx +cos \u0012 xij(i) π k \u0013 σz. The quantum upper bound of the network inequality with any combi- nation of observers can be given as follows, S(n,m,k) j = Ck   nY i=1 j(i) Y o=1 Tio   1 n , (59) where Tio =    1, o = j(i) = m, Gio, o = j(i) < m, 1+Fio 2 , o < j(i), which depends only on the number of inputs k and the weak measurement parameters of the in- termediate Alice involved. In the generalized case (n, m, k), S1...1 = Ck(G11...Gn1) 1 n , Sm...m = Ck(1+F11 2 ...1+Fn1 2 ... 1+F1(m−1) 2 |... 1+Fn(m−1) 2 ) 1 n . When Gij(i) and Fij(i) are symmetrical, then S1...1 and Sm...m can simplified as CkG1 and Ck(1+F1 2 ...1+Fm−1 2 ). In the (n,2,k) case, choosing G 1=G, F1=F, the quan- tum upper bound under the optimal pointer of weak measurement is Sj = Ck[Gn1( 1+F 2 )n2 ] 1 n . Here n1 refers to the first observer in j, while n2 refers to the sec- ond, n1 + n2 = n. A total of 2 n inequalities can be analyzed—in particular, the violation interval of G is only determined by S1...1 = CkG and S2...2 = Ck 1+F 2 . When k=2, these 2 n inequalities can simul- taneously exceed the classical bounds in the range of G ∈ \u001a 1√ 2 , q 2( √ 2 − 1) \u001b . When k=3, the quantum vi- olation obtained can be achieved in a relatively narrow range that G ∈ \u001a 4 3 √ 3 , 4 3 q 1 3 (3 √ 3 − 4) \u001b . When k ≥ 3, all inequalities in (n, 2, k) will not be violated simultane- ously, as the maximum achievable value in this scenario is 4Ck 5 , which is lower than the classical bound (k-1) of k¿3. When m=3, it can be immediately concluded that there is no nonlocality sharing among all observers in the generalized star network. Therefore, network nonlocality sharing exists only when m=2 and the input number is either k=2 or k=3, for any n branches where all 2n star network inequalities concerning Alice1j1 -Alicenjn-Bob can be violated simul- taneously. Subsequently, Zhang et al. (2023) presented for the first time comprehensive results on the nonlocality shar- ing properties of quantum network states, for which the quantum nonlocality of the network quantum states has a stronger sharing ability than the Bell nonlocality. Mao et al. (2023) experimentally demonstrated nonlocality sharing in a photonic quantum three-branch star net- work. Mahato and Pan (2022) investigated a two-input n-local scenario in the star-network scenario and demon- strated that the network nonlocality can be shared by an unbounded number of sequential observers across one edge party for a suitably large value of n in the asymmet- ric case. Later, Kumar and Pan (2023) further demon- strated the sharing of nonlocality in the star network scenario. B. The Sharing Of Entanglement Quantum entanglement stands as the pivotal resource that empowers both parties to conduct intriguing non- locality protocols, conventionally recognized as an insep- arable property. If the quantum state ρAB cannot be separated into product states ρAB = P λ p(λ)ρA λ ⊗ ρB λ , it means that the subsystem A and B are entangled. Srivas- tava et al. (2022a) has proved that entanglement “non- classicality” can be witnessed sequentially and indepen- dently by an arbitrarily large number of observers at one end of the shared state with the single observer at the other end. Furthermore, Pandit et al. (2022) unveiled the sharing of entanglement correlation among 2-qubit quantum states through a sequential detection process conducted an arbitrary number of times. This study il- lustrates the distinction between the recycling of entan- glement and Bell nonlocality. Quantum entanglement can be verified by the entan- glement witness. This method relies on the Hahn-Banach theorem, which guarantees that there is always a func- tional on a normed linear space that “separates” any el- ement that falls outside a closed convex set from the set itself. The entanglement witness operator is labeled as W, and the expected value of the Hermitian operator W concerning the state ρ is denoted by ⟨W⟩ρ. Notably, all separable states satisfy ⟨W⟩ρo ≥ 0, whereas there exists at least one entangled state for which ⟨W⟩ρe < 0.31 The entanglement scenario is a generalized version of the sequential scenario proposed in Fig. 1. Suppose that a bipartite entangled state is shared between a pair of ob- servers, and the ultimate goal is to maximize the number of pairs that can independently witness entanglement. Aussmed that the k-th Alice-Bob ( ABk) pair shares the state ρABk , the witness operator used by this pair is as follows, Wk = 1 4 [I4+σz ⊗σx−λkσx⊗λkσx−λkσy ⊗λkσy], where λk is the sharpness parameter. They choose that each observer has three measurement settings, σ = {σx, σy, σz}, which are labelled as { ˆAλi k 0 , ˆAλi k 1 } for Alices, and { ˆBλi k 0 , ˆBλi k 1 } for Bobs. λi k is the i-th weak measure- ment parameter in the k-th observer, i ∈ {1, 2, 3}, and λ(3) k = 1, λ(1) k = λ(2) k = λk. If the measurement choices for all observers are unbiased, the post-measurement state of the k-th observer can be given as, ρABk = 1 9 3X i,j=1 1X a,b=0 q ˆA λ(i) k−1 a ⊗ r ˆB λ(j) k−1 b ρABk−1 q ˆA λ(i) k−1 a ⊗ r ˆB λ(j) k−1 b = 1 36 3X i,j=1 \u0002\u0000 1 + Λi k−1 \u0001\u0000 1 + Λj k−1 \u0001 ρABk−1 + \u0000 1 + Λi k−1 \u0001\u0000 1 − Λj k−1 \u0001 I2 ⊗ σjρABk−1 I2 ⊗ σj + (1 − Λi k−1)(1 + Λj k−1)σi ⊗ I2ρABk−1 σi ⊗ I2 + (1 − Λi k−1)(1 − Λj k−1)σi ⊗ σjρABk−1 σi ⊗ σj], where Λ i k = q 1 − λ(i)2 k . The expected values of each term present in the witness operator W − k with respect to ρABk can be expressed by their expected value with respect to the state ρAB1 , Tr[σz ⊗ σzρABk ] = Tr[σz ⊗ σzρAB1 ]Πk−1 l=1 (1 + 2Λl)2 9 , Tr[σx ⊗ σxρABk ] = Tr[σx ⊗ σxρAB1 ]Πk−1 l=1 (1 + Λl)2 9 . And note that Tr[σx⊗σxρABk ] = Tr[σy ⊗σyρABk ], k ∈ N. Pandit et al. (2022) proved that an arbitrary number of such pairs of observers can witness the entanglement, which can result from the shared entanglement state, such as all pure entangled states, a class of CHSH Bell- nonlocal mixed entangled states, and a class of CHSH- Bell local entangled states. These results show a clear dis- tinction in the sequential witnessing of paired observers between “entanglement” and “Bell nonlocality”. Some other related work includes, Bera et al. (2018) explored the sharing of bipartite entanglement in a se- quential scenario. (Maity et al., 2020) presented the possibility of sharing genuine tripartite entanglement by considering a scenario consisting of three spin 1 2 particles shared between Alice, Bob, and multiple Charlies. Sri- vastava et al. (2021a) employed a measurement-device- independent entanglement witness to detect the sharing of entanglement with two sequential observers. Srivas- tava et al.(2022b) has investigated the sequential detec- tion of genuinely multipartite entanglement of quantum systems composed of an arbitrary number of qubits. Hu and Fan (2023) has related entanglement sharing to the entropic uncertainty relation in a (d×d)-dimensional sys- tem via weak measurements with different pointers, and consider both the scenarios of one-sided sequential mea- surements in which the entangled pair is distributed to multiple Alices and one Bob and two-sided sequential measurements in which the entangled pair is distributed to multiple Alices and Bobs. Li and Wang (2023) in- vestigated measurement strategies in a scenario where multiple pairs of Alices and Bobs independently and se- quentially observe entangled states, and maximize the number of observer pairs that can witness entanglement. Das et al. (2022) investigated the quantitative advan- tages, within the resource-theoretic framework, of reusing a single instance of a 2-qubit entangled state for informa- tion processing. C. The Sharing Of Nontrivial Preparation Contextuality Nontrivial preparation contextuality represents a cor- relation of lesser strength compared to nonlocality. This concept highlights that the outcome of a quantum mea- surement depends not only on the physical quantity being measured but also on the manner in which the measure- ment is conducted. In their 2019 study, Kumari and Pan (2019) explored the sharing of nonlocality and nontrivial preparation contextuality by investigating the quantum violation of a family of Bell’s inequalities. The study es- tablishes that nontrivial preparation contextuality can be shared among observers in any sequence within unbiased scenarios. Utilizing a communication game and Bell’s local real- ist inequalities as a foundation, the application of parity- oblivious conditions results in equivalent representations within the ontological model. These conditions are then regarded as assumptions of nontrivial preparation non- contextuality. Consequently, the local realist inequali- ties transform into nontrivial, noncontextual inequalities.32 As a consequence, the local realist inequalities transform into nontrivial, noncontextual inequalities. Describing this scenario, Alice possesses an n-bit input xδ, with δ ∈ 1, 2, ...,2n randomly and evenly chosen from 0 , 1n. Bob can select any bit y ∈ 1, 2, ..., nand retrieve bits xδ y with a certain probability. The task’s condition stipu- lates that Bob’s output must be b = xδ y, and the y-th bit of Alice’s input string x is subject to a constraint that prevents any parity information about x from be- ing transmitted to Bob. When the above property is not satisfied, Alice’s observables are limited by a non- trivial constraint, P2n−1 i=1 (−1)s.xi ˆAn,i = 0. The total number of nontrivial constraints on Alice’s observables is ˆCn = 2 n−1 − n. For y ∈ {1, 2, ...n}, Bob performs a binary measurement ˆMn,y with the outputs b. The measurements are given as, ˆMn,y = ( ˆMi n,y, when b = xi y, ˆMl n,y, when b = xl y, ˆMi(l) n,y = ( ˆPBn,y , when xi(l) y = 0. I − ˆPBn,y , when xi(l) y = 1. The family of Bell expressions is formulated as fol- lows: Bn = Pn y=1 P2n−1 i=1 (−1)xi y ˆAn,i ⊗ ˆBn,y. Utilizing the sum of square decomposition (SOS) on the maxi- mum quantum values, it can be deduced that ⟨Bn⟩Q ≤ 2n−1√n. Achieving the optimal quantum value requires measurements that satisfy 2n−1 √n P2n−1 k=1 (−1)xi y ˆAn,i ⊗ I = ˆBn,y ⊗ I. This necessitates a maximum entangled state with a dimension of 2 n 2 , represented as |ϕ⟩AB = 1√ 2⌊n/2⌋ P2⌊n/2⌋ k=1 |k⟩A|k⟩B. The local bound of Bn remains (B)local ≤ n \u0012n − 1 [n−1 2 ] \u0013 . Considering that 2 n−1 − n non- trivial constraints are satisfied, a set of nontrivial prepa- ration noncontextual inequalities can be established as (Bn)pnc ≤ 2n−1. Notably, the nontrivial preparation noncontextual bound is lower than the local bound for n ≥ 2. They considered a scenario with a single Alice and k Bobs. Alice performs a dichotomic measurement with 2n−1 choices, and each Bob has n choices. The sharp- ness and biasedness parameters for the jth Bob are de- noted as ηn,j(0 < ηn,j ≤ 1) and αn,j(0 ≤ |αn,j| ≤ 1), subject to the conditions |αn,j| + ηn,j ≤ 1, j = 1, ..., k. Here, the sharpness and biasedness parame- ters for all measurements of Bobs are assumed to be the same. The Bell expression for the kth Bob can be expressed as, \u0000 Bk n \u0001 Q = 2n−1√nηn,k Qk−1 j=1 γn,j, where γn,j = [1+( n−1)ξn,j]/n, ξ n,j = [ q (1 + αn,j)2 − η2 n,j + q (1 − αn,j)2 − η2 n,j]/2. If ( Bk n)Q > (Bn)local and (Bk n)Q > (Bn)pnc, then nonlocality and nontrivial pre- pared contextuality can be shared between Alice and the kth Bob. On this basis, a specific result can be derived, indicating that the sharpness parameters must satisfy ηn,k > 1√n Qk−1 j=1 γn,j for the n-bit case. Additionally, the sharpness parameter needs to satisfy ηn,1 ≥ 1√n for the first Bob. For n = 2, no nontrivial constraint exists on Alice’s measurement and ( B2)local = (Bn)pnc. The dis- cussion then commences from n ≥ 3. In the context of one-parameter POVMs, the maxi- mum number of Bobs sharing nontrivial preparation non- contextuality with Alice is 3 for n=3. Similarly, for n=4, this maximum number does not exceed 4. In the case of an n-bit scenario, if the k-th Bob shares nontrivial prepa- ration noncontextuality with Alice, the unsharpness pa- rameter must satisfy ηn,k ⩾ 1√nγn,k−1 Qk−2 j=1 γn,j . Substi- tuting the critical value of the (k-1)-th Bob’s unsharpness parameter into it yields ηn,k ≥ ηn,k−1 γn,k−1 . Through γn,k−1 >q 1 − η2 n,k−1, it follows that ηn,k−1√ 1−η2 n,k−1 > ηn,k−1 γn,k−1 . There- fore, it can be stipulated that ηn,k ≥ ηn,k−1√ 1−η2 n,k−1 . Ulti- mately, ηn,k ≥ 1√n−k+1 . For a given k, one can identify a condition n ≡ n(k) ≥ k − 1 + 1 η2 n,k under which the prepared contextuality can be shared by the k-th Bob. The final Bob (k-th) performs a sharp measurement with ηn,k = 1, ensuring n(k) ≥ k. Therefore, for any arbi- trary number of Bobs (k), there exists an n(k) equal to or greater than k that can share nontrivial preparation contextuality. For two-parameter positive operator-valued measures (POVMs) that satisfy |αn,j|+ηn,j ≤ 1, the value of |αn,j| is determined by ηn,j when |αn,j| + ηn,j = 1. Numerical evidence suggests that for n = 100, up to 18 Bobs can share nontrivial preparation noncontextuality. If |αn,j| + ηn,j < 1, increasing the lower value of |αn,j| can enhance the maximum number of Bobs that share nontrivial preparation contextuality. Numeric tests have demonstrated that an arbitrary number of Bobs can share nontrivial preparation contextuality for a specific selection of two-parameter POVM. It is intuitive to observe that one-parameter POVMs outperform two-parameter POVMs in this scenario. Ad- ditionally, it has been observed that nontrivial prepara- tion contextuality, compared to nonlocality, allows for a larger number of Bobs to share. (Anwer et al., 2021) demonstrated that any number of independent observers can share the preparation of contextual outcome statistics enabled by state ensembles in quantum theory. (Chaturvedi et al., 2021) showcased the monogamy of preparation contextuality in a tripar- tite setting. (Kumari and Pan, 2023) study the sharing of nonlocality and preparation contextuality based on a bipartite Bell inequality, involving arbitrary n measure- ments by one party and 2 n−1 measurements by another party.33 V. APPLICATIONS BASED ON QUANTUM CORRELATION SHARING The sharing of quantum correlations is not only closely related to various important and interesting quantum in- formation applications, but also touches upon the recy- cling of quantum resources (Das et al., 2022). Here we will revisit several applications in typical tasks, such as quantum random access codes (Li et al., 2018a; Mohan et al., 2019; Anwer et al., 2020; Foletto et al., 2020c; Wei et al., 2021; Das et al., 2021; Xiao et al., 2021; Xiao et al., 2023), random number generation (Curchod et al., 2017; Coyle et al., 2018; An et al., 2018; Bowles et al., 2020; Pan, 2021; Foletto et al., 2021), and self-test (Mo- han et al., 2019; Miklin et al., 2020; Tavakoliet al., 2020; Mukherjee and Pan, 2021; Pan, 2021; Srivastava et al., 2021b; Roy and Pan, 2023). A. Quantum Random Access Codes Random Access Codes (RACs) are a typical and widely used communication protocol (Ambainis et al., 2002; Wiesner, 1983). In the RAC model, the sender (Alice) prepares a set of random data, while the receiver (Bob) wishes to determine and decrypt Alice’s data through communicating with her. In an n → m RAC, Alice holds n number of bits, while Bob receives m bits. Ide- ally, when Alice is permitted to send unlimited informa- tion, which is n = m, Bob can easily decrypt Alice’s data. While, when m < n, Alice cannot encode all the data. Specifically, taking n → 1 RAC as an example, it can be achieved through a prepare-and-measure sce- nario, where Alice encodes a string of bits{x0, x1..xn−1} with xi ∈ {0, 1} and transmits it to Bob, who ran- domly decodes one of the n bits, which defined as xy, y ∈ {0, 1, .., n− 1}. Adopting appropriate strategies will enhance the applicability of RAC and be limited by the best classical strategies. Interestingly, it can be surpassed by quantum strategies (Ambainis et al., 2009; Tavakoli et al., 2015), the probability of Bob attaining the manda- tory information can be increased. The QRAC has been introduced and developed for qubit systems and even high-dimensional quantum systems. They serve as prim- itives for network coding (Hayashi et al., 2007), random number generation (Li et al., 2011), and quantum key distribution (Paw lowski and Brunner, 2011), and are es- sential tools for numerous protocols in quantum informa- tion theory (Wehner et al., 2008; Paw lowskiet al., 2009; Tavakoliet al., 2016; Hameedi et al., 2017; Tavakoliet al., 2018; Farkas and Kaniewski, 2019; Tavakoli et al., 2020 ). While the initial series of QRAC protocols are one-to- one encoding-decoding models. Due to the decoding op- erators for the QRAC protocol being projective measure- ments, the original coding state will be erased entirely, it is reasonable to only investigate one-to-one encoding- decoding models. Inspired by the work of the non-locality sharing via sequential measurements (Silva et al., 2015), the constraint of projective measurements in the QRAC model can be discarded and explore more general sequen- tial QRACs ( Li et al., 2018a; Mohan et al., 2019; Anwer et al., 2020; Foletto et al., 2020c; Wei et al., 2021; Das et al., 2021; Xiao et al., 2021; Xiao et al., 2023). Mohan et al. (2019) explored QRAC in a three-party-prepare- transform-measure scenario, where a pair of the simplest QRAC (2 → 1) was achieved sequentially between Alice- Bob and Alice-Charlie. It is shown that a trade-off exists between these two QRACs. In the three-party-prepare-transform-measure scenario (Mohan et al., 2019), Alice prepared an arbitrary qubit state based on a random four-valued inputx = (x0, x1) ∈ {0, 1}2. Bob will receive the states from Alice and mea- sure it based on a random binary input y ∈ {0, 1}. Fi- nally, Charlie will receive and measure the delivered state according to a random binary input z ∈ {0, 1}. The joint probability distribution p(b, c|x, y, z) can be completely obtained. The RAC is initially considered between Alice and Bob (Charlie), where the task will succeed only when Bob (Charlie) can guess the yth bit ( x0, x1) of Alice’s in- put. The average success probability is bounded WAB = 1 8 P x,y p(b = xy|x, y) ≤ 3 4 for any random classical strategies. While this bound will be surpassed by using quantum strategies. The optimal QRAC can achieved as WAB = 1 8 P x,y Tr[ρxMxy|y] = 1 2 (1+ 1√ 2 ) ≈ 0.854 (Mohan et al., 2019). The four preparations are given as ρ00 = 1 2 (I + σx+σz√ 2 ), ρ11 = 1 2 (I − σx+σz√ 2 ), ρ01 = 1 2 (I + σx−σz√ 2 ), ρ10 = 1 2 (I− σx−σz√ 2 ). This QRAC shown above is indepen- dent of Charlie. Similarly, for the QRAC between Alice and Charlie, the corresponding witness of this QRAC is WAC = 1 8 P x,z p(c = xz|x, z) ≤ 3 4 , and the QRAC is not independent of Bob. For sequential QRACs exhibiting their optimal trade- off, the range of interest available to QRAC for (WAB, WAC) is limited in [ 1 2 , (2+ √ 2) 4 ]. In this range, ei- ther witness being 1 2 − ϵ is equivalent to a witness value of 1 2 + ϵ by classically bit-flipping the outcomes. The trade-off between WAB and WAC can be formulated as: for a given WAB value (expressed as α), find the maxi- mum WAC. The optimal QRAC can be achieved when Bob performs weak measurements with the unsharpened parameter η ∈ [0, 1], and Charlie performs strong mea- surements, M0 = σx, M1 = σz. The witness can be given as, WAB = 1 4 (2 + η √ 2), WAC = 1 8 (4 + √ 2 + p s − sη2), whose optimal trade-off corresponds to Wα AC = 1 8 (4 + √ 2 + √ 16α − 16α2 − 2), α ∈ [1 2 , 1+ 1√ 2 2 ]. This character- izes the nontrivial boundary of the quantum set in the space of witness pairs. In a general scenario with multiple observers, the RACs between Alice and a sequence of Bobs have been34 investigated. It is shown that the longest sequence of QRAC is limited to 2,WAB = WAC = 5+2 √ 2 10 ≈ 0.7828 > 3 4 . This result is consistent with (Silva et al., 2015) where the sequential violations of the CHSH inequality. Subsequently, Das et al. (2021) considered an unbi- ased scenario involving multiple independent pairs of ob- servers performing sequential measurement on a singlet state, and addressed whether more than one pair of ob- servers can demonstrate quantum advantage in some spe- cific n → 1 RAC, (n ∈ {2, 3}). The efficacy of the RAC protocol will quantified by minimum success probability, Pn→1 Min = minx0,x1...xn−1,y P(b = xy), which is always less than or equal to 1 2 for classic RAC tasks. Moreover, these findings persist even if all observers make appropriate projective measurements and the ini- tially shared state is separable. Subsequently, Foletto et al. (2020c) used single photons to experimentally demonstrate the feasibility of weak measurements and the achievability of nonclassical success probabilities with two decoders. Li et al.(2018a) demonstrated that double classical dimension witness violation is achievable if they choose appropriate weak measurement parameters by ap- plying the dimension witness inequality based on the quantum random access code and the nonlinear determi- nant value. Anwer et al.(2020) reported an experimental implementation of unsharp qubit measurements in a se- quential communication protocol, based on a QRAC. Wei et al.(2021) conducted a three-party prepare-transform- measure experiment with unsharp measurements based on 3 → 1 sequential RAC, and derived optimal trade-off between the two correlation witnesses in 3→ 1 sequential QRACs. Xiao et al. (2021) proposed an entanglement- assisted sequential QRAC protocol which can enable device-independent tasks. Xiao et al.(2023) investigated the sequential 3 → 1 parity-oblivious QRAC using dif- ferent sharpness parameters for different measurement settings. B. Random Number Generation Randomness refers to the uncertainty exhibited by each event in a set of events with a certain probability. The generation of randomness has a wide range of appli- cations, such as in cryptography, simulation algorithms, and so on. The use of quantum correlation to gener- ate and verify true randomness has received widespread attention and research (Popescu and Rohrlich, 1994; Masanes et al., 2006; D’Ariano et al., 2005; Pironio et al., 2010; Ac´ ınet al., 2016). The quantum correlation shar- ing model can be used to further investigate the prop- erties of quantum random number generation (QRNG). Curchod et al. (2017) proposed a method for generating random numbers using sequential measurements, demon- strating that an infinite number of random bits can be produced from a pair of entangled qubits. In this work, they analyzed and quantified the ran- domness generation by non-local guessing games in a se- quential scenario (Ac´ ınet al., 2012; de la Torre et al., 2015). Supposed that an additional adversary Eve (E) having a quantum system which may be related to one of A and B, E prepares a state ρABE and distributes ρABE and distributes ρAB = TrE[ρABE ] to A and B re- spectively. The purpose of E is to attempt to guess the outcomes of Bob by performing measurements on ρABE . The other observers treat their respective devices as black boxes, and the probability distribution Pobs(a,⃗b|x, ⃗ y) is obtained from their inputs and outputs. This probability can be obtained from the extreme points, which isPobs =P ext qextPext, where P ext qext = 1 and qext ≥ 0. Once E knows the probability distribution at the extreme points, she can fully understand how to limit her predictive ability through decomposing Pobs into extreme points. The predictive power is quantified through the device- independent guessing probability (DIGP), G(⃗ y0, Pobs). where the input string y0 1, y0 2..y0 n ≡ ⃗ y0, and E should guess the output ⃗b. The problem changes to the opti- mal solution of such optimization problems, G(⃗y0, Pobs) = max {qext,Pext} X ext qext max ⃗b pext(⃗b|⃗y0), (60) where pext(⃗b|⃗ y0) = P a pext(a,⃗b|x, ⃗ y0), ∀x; Pobs =P ext qextPext, Pext ∈ Q. It is clearly shown that m ran- dom bits are returned once G(⃗ y0, Pobs) = 2 −m. Subse- quently, with the condition that Pobs = P ext qextPext is dissatisfied and only mandatory observed statistical Pobs give specific Bell inequality violations, the optimal so- lution can be given as the upper bound of the optimal solution of the equation Pobs = P ext qextPext. In this case, Alice and Bobs share a pure two- qubit state and use the CHSH inequality as a cri- terion, Iθ = β⟨B0⟩ + ⟨A0B0⟩ + ⟨A1B0⟩ + ⟨A0B1⟩ − ⟨A1B1⟩. The maximal violation can be reached, Imax = 2 √ 2 p 2 + β2/4, when Alice’s two measurement choices are ˆA0 = cos µσz + sin µσx, ˆA1 = cos µσz − sin µσx, and for Bob as ˆB0 = σz, ˆB1 = σx. When the max- imum violation proves that Bob’s second measurement choice is y0 = 1 : G(⃗ y0 = 1 , Pobs) = 1 2 , and then Iθ → Imax θ ⇒ G(y0 = 1 , Pobs) → 1 2 . Supposed that each Bobs also has two measurement choices with yi = 0 and yi = 1, corresponding σz and ˆσx(ξi) respectively, where ξi is the sharpness parameter. If the inequality Iθ⃗bi−1 between Bobi and Alice can approach to the max- imum value infinitely, which ensures that the guessing probability G(⃗ y0 = 1, Pobs) can be arbitrarily close to 1 2 , thus verifying approximately one random bit. When ξ approaching 0, DIGP(G) can be arbitrarily close to 2 −n. Then all Bobs can produce m random bits by a suitable sequence ˆσx(ξi), i ∈ {1, 2...n} measurements, where n¿m. This certification only requires that each Bob occasion- ally select the projective measurement σz to obtain the35 whole statistics. Bob can choose σz with probability γi and ˆσx(ξi) with probability of 1 − γi. By setting γi as close to zero as possible, Bob has complete freedom to choose. This allows the demonstration of the random- ness of each Bob’s measurements in the sequence, at the expense of Alice’s increased measurement choices. Their results show that an infinite number of random bits can be derived from a pair of entangled quantum bits when one of them undergoes a series of measurements, which requires an exponential increase in the measure- ment choices of Alice, Pn i=1 2i measurement choices for n measurements in the sequence. It is valuable to investi- gate the design of device-independent protocols for gener- ating randomness that involves sequential measurement scenarios. It is worth pointing out that, the sequential measurement scenario is a more significant challenge in practical experiments. And the efficiency of the sequen- tial protocol compared to the standard Bell test remains unknown. Other related developments include, Coyleet al.(2018) presented a new protocol that allows an unbounded amount of randomness to be certified as being legiti- mately the consequence of a measurement on a quantum state. An et al.(2018) proposed and realized a quantum random number generator among three observers based on the weak measurement technique. Bowleset al.(2020) considered a more general scenario in which one performs sequences of local measurements on an entangled state of arbitrary dimension with the methods of NPA hier- archy. Foletto et al. (2021) showed that the weak mea- surements are realizable through experimental tests, but can improve the performance of randomness generation only in close-to-ideal conditions. Pan (2021) provided an interesting two-party parity-oblivious communication game whose success probability is solely determined by the Bell expression. The parity-oblivious condition in an operational quantum theory implies the preparation of noncontextuality in an ontological model of it. C. Self-Testing Self-testing is a fundamental physics method for infer- ring quantum experiments in black box scenarios, rep- resenting the most powerful form of authentication for quantum systems (Mayers and Yao, 2004; McKague, 2016; McKague, 2017; ˇSupi´ c and Bowles, 2020; ˇSupi´ c et al., 2021). At present, the investigations of self- testing are often closely related to the Bell test (Yang and Navascu´ es, 2013; Bancalet al., 2015; ˇSupi´ c and Hoban, 2016; Coladangelo et al., 2017; Coopmans et al., 2019). Taking a simple self-testing scheme as an example, a source generating a quantum system and distributing it between two laboratories, the observers in these two laboratories can perform different local experiments by changing the device settings. In device-independent con- straint, each laboratory can be considered as a black box, which selects labels ( x, y) corresponding to the specific settings of the experiment as input and outputs corre- sponding results ( a, b). Repeating performing local ex- periments, an estimated probability P (a, b — x, y) can be obtained. The purpose of self-testing of the quan- tum state and measurement is to infer the source and measurement of the laboratories only from probability P(a, b|x, y) in such a black box scenario (Mayers and Yao, 2004; McKague, 2016; McKague, 2017). 1. The Self-testing Of QRACs Inspired by the nonlocality sharing via sequential mea- surements, Mohan et al. (2019) constructed semi-device independent self-tests of quantum instruments. They showed that self-testing is possible with the optimal trade-off between two sequential QRACs. In a three-party prepare-transform-measure scenario, the investigation of the ability to execute QRAC for Bob and Charlie, is equivalent to exploring the optimal trade- off of (WAB, WAC). For self-testing of QRACs, it is nec- essary to prove that the optimal QRAC pairs can only be achieved through unique preparations, instruments, and measurements (until a collective unitary transformation). Based on the optimal sequence QRAC (WAB, WAC) = (α, Wα AC) (as mentioned above in Sec. V.A), such self- testing argument can be established as follows (Mohan et al., 2019): (i) Alice’s states are pure, pairwise an- tipodal on the Bloch sphere, and they form a square on the Bloch sphere. (ii) Bob’s instruments are Kraus operators ˆKb|y = ˆUyb q ˆMb|y, corresponds to unsharpen measurements along the diagonals of Alice’s square of preparations, until its collective unitary. Specifically, ∀y, b : ˆUyb = U, ˆM0 = ησx and ˆM1 = ησz, where σ = √ 2(2WAB − 1). (iii) Charlie’s measurement is the first order projection along the diagonal of Alice’s square of preparations, until Bob’s unitary, ˆC0 = ˆUσx ˆU†, ˆC1 = ˆUσz ˆU†. This self-testing remains valid until a collective reference frame is chosen. 2. Self-testing Of Unsharp Measurements Subsequently, Roy and Pan (2023) provided device- independent (DI) self-testing of the unsharp instrument through the quantum violation of two Bell inequalities. In a sequential CHSH scenario, Alice performs local measurements ˆA1 and ˆA2 on her subsystem upon re- ceiving inputs x. For Bob receives inputs yk ∈ {1, 2}, whose measurement operator is represented by ˆK±|yk = αkI ± βk ˆByk , where αk = 1 2 ( q 1+λk 2 + q 1−λk 2 ), βk = 1 2 ( q 1+λk 2 − q 1−λk 2 ), and α2 k + β2 k = 1 2 , αk ≥ βk. Sup- posed that each Bob measures the same set of measure-36 ments ˆB1 and ˆB2, the effective observables for Bob (un- normalized) can be written as ˜B1 = (2 α2 1 + β2 1) ˆB1 + β2 1 ˆB2 ˆB1 ˆB2 and ˜B2 = (2α2 1 + β2 1) ˆB2 + β2 1 ˆB1 ˆB2 ˆB1. The CHSH value of Alice-Bob 1 is given as ( B1)Q = λ1Bopt Q , while for Alice-Bob 2 as (B2) = Tr[ρAB1 (( ˆA1 + ˆA2) ˜B1 + ( ˆA1 − ˆA2) ˜B2)]. Here, we have ( ˜B1)2 ̸= I,( ˜B2)2 ̸= I and ω1 = || ˜B1)||, ω2 = || ˜B2)||. By utilizing the SOS method, we can determine its maximal value with (B2)Q = max(ω1 ˜ω1 + ω2 ˜ω2). For Bob’s (unnormalized) measurements require that { ˜B1, ˜B2} = 4 α2 1(α2 1 + 2β2 1){ ˆB1, ˆB2} + β4 1{ ˆB1, ˆB2}3 = 0. As α1 > 0, β1 ≥ 0, it imply that { ˆB0, ˆB1} = 0. Hence, in order to achieve the maximum aver- age value of the Bell expression, Bob’s measurement of Bob2 needs to be anticommuting. This yields ˜ ω1 =q (2α2 1)2 + (2α2 1 + β2 1)β2 1{ ˆB1, ˆB2}2 = 2 α2 1. The CHSH value for Alice-Bob 2 reduces to ( B2)Q = 2α2 1 max(ω1 + ω2) = 2 α2 1(B)opt Q , when substituted into the value of α1 becomes (B2)Q = 1 2 (1 + p 1 − λ2 1)(B)opt Q . The maximum average value of the Bell expression for two sequences of Bobs is, (B2)Q = √ 2(1 + s 1 − ( (B1)Q (B)opt Q )2). (61) Consequently, there exists a trade-off between ( B1)Q and ( B2)Q, resulting in an optimal pair that demon- strates the certification of the unsharpness parameter. The suboptimal value (B1)Q and (B2)Q forms an optimal pair {(B1)Q, (B2)Q}, which uniquely proves the shared state, the measurement set, and the unsharp parameter λ1 between Alice and Bob1. The DI self-testing of this protocol is as follows (Roy and Pan, 2023): (i) Alice makes sharp measurements of two anticommuting observable values on her local subsys- tem in any arbitrary local dimension. (ii) Bob 1 performs unsharp measurements with two possible measurements, which are also anticommuting in any arbitrary local di- mension. The measurement sets of Bob 1 and Bob2 are the same. (iii) Alice and Bob1 share the maximum entan- gled state in any arbitrary dimension. (iv) The optimal pair {(B1)Q, (B2)Q} self-tests unsharp parameters, which in turn proves the shared entanglement state between Al- ice, Bob1 and Bob2. Additionally, they broaden it to encompass the elegant Bell inequality. Suppose that it possesses two classical bounds—the local bound and the non-trivial preparation non-contextual bound, the latter being lower than the local bound. Utilizing the shared preparation contextu- ality among three independent sequential observers, the DI self-testing of two unsharpness parameters has been demonstrated. The robustness of their certification has been discussed (Roy and Pan, 2023). The other relative work includes: Miklin et al. (2020) proposed a scheme for semi-device-independent self- testing of unsharp measurements and showed that all two-outcome qubit measurements can be characterized in a robust way. Tavakoli et al. (2020) investigated self- testing of nonprojective quantum measurements theoret- ically and experimentally. That is, how can one cer- tify, from observed data only, that an uncharacterized measurement device implements a desired nonprojec- tive positive-operator-valued measure (POVM). Mukher- jee and Pan (2021) provided semi-device-independent self-testing protocols in the prepare-measure scenario to certify multiple unsharpness parameters along with the states and the measurement settings. Pan (2021) demonstrated device-independent self-testing of projec- tive and non-projective measurements. Additionally, Sri- vastava et al. (2021b) employed a measurement-device- independent entanglement witness to detect entangle- ment in a scenario where half of an entangled pair is possessed by a single observer while the other half is with multiple observers performing unsharp measurements, sequentially, independently, and preserving entanglement as much as possible. VI. DISCUSSIONS AND CONCLUSIONS Discussions- In the field of quantum phenomena, non- local sharing is a fascinating frontier that allows us to wit- ness the intrinsic nature of particle entanglement, chal- lenging classical intuition. As we delve into the com- plexity of quantum correlations, the study of nonlocal sharing opens up another avenue for exploration. This not only provides richer perspectives on our understand- ing of quantum mechanics and its practical implications but also holds exciting possibilities for the development of Quantum Information Science. However, current re- search on quantum correlation sharing and even the reuse of quantum resources, whether in terms of models or theoretical structures, remains relatively limited. In the following discussion, we summarize some open problems that are yet to be solved and require overcoming. In- depth research on these issues may lead to new break- throughs in our understanding of quantum correlation sharing. Essentially, the issue of sharing quantum correlations is closely linked to the trade-off between information gain and disturbance of the system. In a broader context, the optimal measurement choices for quantum correla- tion sharing remain unclear (Silvaet al., 2015), especially for more complex high-dimensional and multi-body sys- tems. Hence, there is an interesting question, what is the best form of pointer when making measurements in high-dimensional systems? Currently, sporadic research findings suggest that high- dimensional quantum correlation systems seem to be more easily able to achieve correlation sharing in various scenarios. For instance, a 2-qutrit system can achieve37 nonlocal sharing among more observers in bilateral se- quential measurements (Cabello, 2021), which is impos- sible for the original 2-qubit system (Mal et al., 2016). These conclusions align with our intuitive judgment. However, we still need more evidence to understand the nonclassical correlation sharing in high-dimensional sys- tems. In high-dimensional or quantum networks, the sharing involves not only the strength of correlations but may also extend to sharing from global correlations to local correlations. In multi-party correlations, exploring other important features of quantum correlations, such as en- tanglement depth (Sørensen and Mølmer, 2001), entan- glement intactness (Lu et al., 2018), and characteristics based on quantum measurements, are equally intriguing. These research directions have the potential to offer us fascinating perspectives. Regarding the issue of correlation sharing, the progress of research varies for different definitions of quantum correlations. For instance, there has been considerable advancement in the study of nonlocality sharing, and some research has been conducted on quantum steer- ing. However, compared to other nonclassical correla- tions, research in these areas is relatively straightforward, with many studies directly drawing from earlier research. Therefore, further exploration is needed for research on other types of correlation sharing. From the perspective of resource theory (Chitambar and Gour, 2019), it is currently unclear whether the cor- relation sharing based on sequential measurements in- deed consumes fewer resources to handle quantum infor- mation tasks or under what scenarios it may have advan- tages. Classifying various information processing tasks involving sequential measurements based on the advan- tages of resource theory is another potentially attractive direction for future research (Das et al., 2022). Explor- ing more general resource-based comparison and analysis methods between sequential and non-sequential scenarios will contribute to the advancement of this field. The observation of quantum correlations introduced the concept of measurement, giving rise to research on quantum correlation sharing. This research began with early weak measurement models, evolved into generalized POVM models, and progressed to demonstrating quan- tum correlation sharing using combined strategies with strong measurements. It is crucial to explore whether other physical strategies that can achieve similar phe- nomena. Even with modifications to known physical models, such as investigating whether strong measure- ment combinations can achieve infinite sharing, remains a research question. Recent work has provided valu- able insights, demonstrating correlation sharing through combined projective measurements (Sasmal et al., 2023). Similarly, understanding how to achieve more general- ized sharing in bilateral or even multipartite scenarios is a topic worthy of in-depth exploration. Regarding the experimental observation of quantum correlation sharing, most current experiments are demon- strations in 2-qubit systems. Exploring more complex correlation sharing in higher-dimensional or multi-body physical systems is a topic that needs further consid- eration. Essentially, we need to design more explicit correlation-sharing schemes. Additionally, current exper- iments are limited to optical systems, and there is a need to expand experimental demonstrations to more diverse physical platforms. In the context of random code problems under sequen- tial measurements, especially when extended to higher dimensions and involving more sequences of observers, there are still some unclear but evidently crucial ques- tions. It is essential to explore the optimal trade- offs among sequential Quantum Random Access Cod- ing (QRAC) involved in the preparation scenario tests. What are the essential differences in sequential QRAC when different quantum strategies are employed? How to establish a more comprehensive set of comparison meth- ods? These are questions that require in-depth investi- gation. Exploring self-testing problems based on sequential measurements, especially in the context of noise resis- tance analysis, is undoubtedly a highly meaningful re- search avenue. Designing optimal sequential measure- ment schemes is a feasible path by considering different scenarios. This includes both the self-testing of quantum states and the self-testing process of generalized POVM measurement operators. In addition to known application models, further con- templation is needed to identify scenarios suitable for non-classical correlations. For instance, exploring the possibility of extending these scenarios to encompass a broader study of numerous quantum game models. This includes investigating the application of nonlocal sharing in conflicting nonlocal games, exploring incomplete infor- mation games and perfect information games, determin- ing the dynamics of deterministic and nondeterministic games, and analyzing symmetric and asymmetric infor- mation games, among other possibilities. Conclusions-This paper comprehensively reviews and summarizes the recent progress in quantum correlation sharing based on sequential measurements. Starting with the interpretation of joint probability, we introduce the study of quantum correlations considering real measure- ment methods. In the second part, we delve into the exploration of nonlocal sharing under different measure- ment strategies and scenarios. Specifically, we investi- gate the impact of measurement strategies on the sharing of quantum nonlocality, examining perspectives such as “asymmetry” and “weak value.” Through detailed anal- yses in diverse scenarios, we assess the potential of non- local sharing and provide a retrospective overview of ex- perimental efforts related to nonlocal sharing. The third part introduces the research findings on steering sharing,38 clarifying the feasibility of steering sharing and summa- rizing the properties of quantum steering sharing in dif- ferent scenarios. In the fourth part, we discuss the shar- ing of other types of quantum correlations, including net- work nonlocality, quantum entanglement, and quantum contextuality. In the fifth part, we review the progress of quantum correlation sharing based on quantum sequen- tial measurement strategies in applications such as quan- tum random access coding, random number generation, and self-testing tasks. Finally, we deduce and analyze im- portant open questions in this research topic and provide a summary. VII. ACKNOWLEDGMENT C.R. was supported by the National Natural Science Foundation of China (Grant No. 12075245, 12247105), the Natural Science Foundation of Hunan Province (2021JJ10033), Xiaoxiang Scholars Programme of Hu- nan Normal University, the Foundation Xiangjiang- Laboratory (XJ2302001) and Hunan provincial major sci-tech program No. (2023zk1010). REFERENCES Ac´ ın, A., S. Massar, and S. Pironio (2012), Phys. Rev. Lett. 108, 100402. Ac´ ın, A., S. Pironio, T. V´ ertesi, and P. Wittek (2016), Phys. Rev. A 93, 040102. Adesso, G., T. R. Bromley, and M. Cianciaruso (2016), J. Phys. A: Math. Theor. 49 (47), 473001. Aharonov, Y., D. Z. Albert, and L. Vaidman (1988), Phys. Rev. Lett. 60, 1351. Aharonov, Y., P. G. Bergmann, and J. L. Lebowitz (1964), Phys. Rev. 134, B1410. Aharonov, Y., and R. Daniel (2005), “Quantum measure- ments,” in Quantum Paradoxes, Chap. 7 (John Wiley and Sons, Ltd) pp. 93–103. Aharonov, Y., and L. Vaidman (1990), Phys. Rev. A 41, 11. Ambainis, A., D. Leung, L. Mancinska, and M. Ozols (2009), “Quantum random access codes with shared randomness,” arXiv:0810.2937 [quant-ph]. Ambainis, A., A. Nayak, A. Ta-Shma, and U. V. Vazirani (2002), J. ACM 49, 496. An, X.-B., H.-W. Li, Z.-Q. Yin, M.-J. Hu, W. Huang, B.-J. Xu, S. Wang, W. Chen, G.-C. Guo, and Z.-F. Han (2018), Opt. Lett. 43 (14), 3437. Andreoli, F., G. Carvacho, L. Santodonato, R. Chaves, and F. Sciarrino (2017), New J. Phys. 19 (11), 113020. Anwer, H., S. Muhammad, W. Cherifi, N. Miklin, A. Tavakoli, and M. Bourennane (2020), Phys. Rev. Lett. 125, 080403. Anwer, H., N. Wilson, R. Silva, S. Muhammad, A. Tavakoli, and M. Bourennane (2021), Quantum 5, 551. Ardehali, M. (1992), Phys. Rev. A 46, 5375. Arute, F., K. Arya, R. Babbush, D. Bacon, J. C. Bardin, R. Barends, R. Biswas, S. Boixo, F. G. Brandao, D. A. Buell, et al. (2019), Nature 574 (7779), 505. Aspect, A., J. Dalibard, and G. Roger (1982), Phys. Rev. Lett. 49, 1804. Banaszek, K. (2006), OPEN SYST INF DYN 13 (1), 1. Bancal, J.-D., M. Navascu´ es, V. Scarani, T. V´ ertesi, and T. H. Yang (2015), Phys. Rev. A 91, 022115. Belinski˘ ı, A. V., and D. N. Klyshko (1993), Phys. Usp.36 (8), 653. Bell, J. S. (1964), Physics 1, 195. Bennett, C., and G. Brassard (1984), in Proc. of IEEE Int. Conf. on Computers, Systems, and Signal Processing (Ban- galore, India, 1984) , pp. 175–179. Bennett, C. H., and D. P. DiVincenzo (2000), Nature 404 (6775), 247. Bera, A., S. Mal, A. Sen(De), and U. Sen (2018), Phys. Rev. A 98, 062304. Bose, S., V. Vedral, and P. L. Knight (1999), Phys. Rev. A 60, 194. Bouwmeester, D., J.-W. Pan, K. Mattle, M. Eibl, H. Weinfurter, and A. Zeilinger (1997), Nature(London) 390 (6660), 575. Bouwmeester, D., and A. Zeilinger (2000), in The physics of quantum information (Springer) pp. 1–14. Bowles, J., F. Baccari, and A. Salavrakos (2020), Quantum 4, 344. Bowles, J., T. V´ ertesi, M. T. Quintino, and N. Brunner (2014), Phys. Rev. Lett. 112, 200402. Branciard, C., N. Gisin, and S. Pironio (2010), Phys. Rev. Lett. 104, 170401. Branciard, C., D. Rosset, N. Gisin, and S. Pironio (2012), Phys. Rev. A 85, 032119. Braunstein, S. L. (1992), Phys. Rev. Lett. 69, 3598. Brown, P. J., and R. Colbeck (2020), Phys. Rev. Lett. 125, 090401. Brukner, ˇC., M. ˙Zukowski, and A. Zeilinger (2002), Phys. Rev. Lett. 89, 197901. Brunner, N., D. Cavalcanti, S. Pironio, V. Scarani, and S. Wehner (2014), Rev. Mod. Phys. 86, 419. Bruß, D., J. I. Cirac, P. Horodecki, F. Hulpke, B. Kraus, M. Lewenstein, and A. Sanpera (2002), J. Mod. Opt. 49 (8), 1399. Burgarth, D., V. Giovannetti, A. N. Kato, and K. Yuasa (2015), New J. Phys. 17 (11), 113055. Buscemi, F., and M. Horodecki (2009), Towards a Unified Ap- proach to Information-Disturbance Tradeoffs in Quantum Measurements. Busch, P. (1986), Phys. Rev. D 33, 2253. Busch, P., M. Grabowski, and P. J. Lahti (1997), Operational quantum physics , Vol. 31 (Springer Science and Business Media). Busch, P., P. J. Lahti, and P. Mittelstaedt (1996), The quan- tum theory of measurement (Springer). Cabello, A. (2021), “Bell nonlocality between sequential pairs of observers,” arXiv:2103.11844 [quant-ph]. Cai, Z., and C. Ren (2022), “Full network nonlocality shar- ing in extended scenario via optimal weak measurements,” arXiv:2211.08153 [quant-ph]. Cavalcanti, D., and P. Skrzypczyk (2016), Rep. Prog. Phys. 80 (2), 024001. Cavalcanti, E. G., C. J. Foster, M. Fuwa, and H. M. Wiseman (2015), J. Opt. Soc. Am. B 32 (4), A74. Cavalcanti, E. G., S. J. Jones, H. M. Wiseman, and M. D. Reid (2009), Phys. Rev. A 80, 032112. Chabaud, U., and M. Walschaers (2023), Phys. Rev. Lett. 130, 090602. Chaturvedi, A., M. Farkas, and V. J. Wright (2021), Quantum 5, 484.39 Chen, Y., F. Guo, S. Wei, and Q. Wen (2023), Chin. Phys. B 32 (4), 040309. Cheng, S., L. Liu, T. J. Baker, and M. J. W. Hall (2021), Phys. Rev. A 104, L060201. Cheng, S., L. Liu, T. J. Baker, and M. J. W. Hall (2022), Phys. Rev. A 105, 022411. Chitambar, E., and G. Gour (2019), Rev. Mod. Phys. 91, 025001. Choi, Y.-H., S. Hong, T. Pramanik, H.-T. Lim, Y.-S. Kim, H. Jung, S.-W. Han, S. Moon, and Y.-W. Cho (2020), Op- tica. 7 (6), 675. Christensen, B. G., K. T. McCusker, J. B. Altepeter, B. Calkins, and T. Gerrits (2013), Phys. Rev. Lett. 111, 130406. Chru´ sci´ nski, D., and G. Sarbicki (2014), J. Phys. A: Math. Theor 47 (48), 483001. Clauser, J. F., M. A. Horne, A. Shimony, and R. A. Holt (1969), Phys. Rev. Lett. 23, 880. Coladangelo, A., K. T. Goh, and V. Scarani (2017), Nat. Commun. 8 (1), 15485. Collins, D., N. Gisin, N. Linden, S. Massar, and S. Popescu (2002), Phys. Rev. Lett. 88, 040404. Coopmans, T., J. m. k. Kaniewski, and C. Schaffner (2019), Phys. Rev. A 99, 052123. Costa, A. C. S., and R. M. Angelo (2016), Phys. Rev. A 93, 020103. Costa, F., M. Ringbauer, M. E. Goggin, A. G. White, and A. Fedrizzi (2018), Phys. Rev. A 98, 012328. Coyle, B., M. J. Hoban, and E. Kashefi (2018), EPTCS 273, 14. Curchod, F. J., M. Johansson, R. Augusiak, M. J. Hoban, P. Wittek, and A. Ac´ ın (2017), Phys. Rev. A95, 020102. Curchod, F. J., M. Johansson, R. Augusiak, M. J. Hoban, P. Wittek, and A. Ac´ ın (2018), in 12th Conference on the Theory of Quantum Computation, Communication and Cryptography (TQC 2017). D’Ariano, G. M., P. L. Presti, and P. Perinotti (2005), J. Phys. A: Math. Theor. 38 (26), 5979. Das, A. K., D. Das, S. Mal, D. Home, and A. S. Majumdar (2022), Quantum Inf. Process. 21 (12), 381. Das, D., A. Ghosal, A. G. Maity, S. Kanjilal, and A. Roy (2021), Phys. Rev. A 104, L060602. Das, D., A. Ghosal, S. Sasmal, S. Mal, and A. S. Majumdar (2019), Phys. Rev. A 99, 022305. Datta, S., and A. S. Majumdar (2018), Phys. Rev. A 98, 042311. Davies, E. B., and J. T. Lewis (1970), Commun. Math. Phys. 17 (3), 239. Einstein, A., B. Podolsky, and N. Rosen (1935), Phys. Rev. 47, 777. Ewert, F., M. Bergmann, and P. van Loock (2016), Phys. Rev. Lett. 117, 210501. Farkas, M., and J. m. k. Kaniewski (2019), Phys. Rev. A 99, 032316. Feng, T., C. Ren, Y. Tian, M. Luo, H. Shi, J. Chen, and X. Zhou (2020), Phys. Rev. A 102, 032220. Filip, R. (2011), Phys. Rev. A 83, 032311. Fitzsimons, J. F., M. Hajduˇ sek, and T. Morimae (2018), Phys. Rev. Lett. 120, 040501. Foletto, G., L. Calderaro, A. Tavakoli, M. Schiavon, F. Pic- ciariello, A. Cabello, P. Villoresi, and G. Vallone (2020a), Phys. Rev. Appl. 13, 069902. Foletto, G., L. Calderaro, A. Tavakoli, M. Schiavon, F. Pic- ciariello, A. Cabello, P. Villoresi, and G. Vallone (2020b), Phys. Rev. Appl. 13, 044008. Foletto, G., L. Calderaro, G. Vallone, and P. Villoresi (2020c), Phys. Rev. Res. 2, 033205. Foletto, G., M. Padovan, M. Avesani, H. Tebyanian, P. Vil- loresi, and G. Vallone (2021), Phys. Rev. A 103, 062206. Fraser, T. C., and E. Wolfe (2018), Phys. Rev. A 98, 022113. Fritz, T. (2012), New J. Phys. 14 (10), 103001. Fritz, T. (2016), Commun. Math. Phys. 341, 391. Fuchs, C. A., and A. Peres (1996), Phys. Rev. A 53, 2038. Gallego, R., L. E. W¨ urflinger, R. Chaves, A. Ac´ ın, and M. Navascu´ es (2014), New J. Phys.16 (3), 033037. Giovannetti, V., S. Lloyd, and L. Maccone (2006), Phys. Rev. Lett. 96, 010401. Gisin, N. (2016), “Quantum measurement of spins and mag- nets, and the classical limit of pr-boxes,” in The Frontiers Collection (Springer International Publishing) p. 321–330. Gisin, N., Q. Mei, A. Tavakoli, M. O. Renou, and N. Brunner (2017), Phys. Rev. A 96, 020304. Gisin, N., and R. Thew (2007), Nat. Photonics 1 (3), 165. Giustina, M., A. Mech, S. Ramelow, B. Wittmann, J. Kofler, J. Beyer, and A. Lita (2013), Nature 497 (7448), 227. Giustina, M., M. A. M. Versteegh, S. Wengerowsky, J. Hand- steiner, and A. Hochrainer (2015), Phys. Rev. Lett. 115, 250401. Gudder, S., and G. Nagy (2001), J. Math. Phys. 42 (11), 5212. G¨ uhne, O., P. Hyllus, D. Bruß, A. Ekert, M. Lewenstein, C. Macchiavello, and A. Sanpera (2003), J. Mod. Opt. 50 (6-7), 1079. Gupta, S., A. G. Maity, D. Das, A. Roy, and A. S. Majumdar (2021), Phys. Rev. A 103, 022421. G¨ uhne, O., and G. T´ oth (2009), Phys. Rep.474 (1), 1. Halder, P., R. Banerjee, S. Mal, and A. Sen(De) (2022), Phys. Rev. A 106, 052413. Hameedi, A., D. Saha, P. Mironowicz, M. Paw lowski, and M. Bourennane (2017), Phys. Rev. A 95, 052345. Han, X., Y. Xiao, H. Qu, R. He, X. Fan, T. Qian, and Y. Gu (2021), Quantum Inf. Process. 20 (8), 278. Han, X.-H., T. Qian, S.-C. Dong, Y. Xiao, and Y.-J. Gu (2023), “Activation of quantum steering sharing with un- sharp nonlocal product measurements,” arXiv:2303.05954 [quant-ph]. Han, X.-H., H.-C. Qu, X. Fan, Y. Xiao, and Y.-J. Gu (2022), Phys. Rev. A 106, 042416. Hayashi, M., K. Iwama, H. Nishimura, R. Raymond, and S. Yamashita (2007), in Annual Symposium on Theoreti- cal Aspects of Computer Science (Springer) pp. 610–621. Hensen, B., H. Bernien, A. E. Dr´ eau, A. Reiserer, N. Kalb, M. S. Blok, and J. Ruitenberg (2015), Nature 526 (7575), 682. Hofmann, J., M. Krug, N. Ortegel, L. G´ erard, M. We- ber, W. Rosenfeld, and H. Weinfurter (2012), Science 337 (6090), 72. Home, D., A. K. Pan, M. M. Ali, and A. S. Majumdar (2007), J. Phys. A: Math. Theor. 40 (46), 13975. Horodecki, M., P. Horodecki, and R. Horodecki (1996), Phys. Lett. A 223 (1), 1. Horodecki, R., P. Horodecki, M. Horodecki, and K. Horodecki (2009a), Rev. Mod. Phys. 81 (2), 865. Horodecki, R., P. Horodecki, M. Horodecki, and K. Horodecki (2009b), Rev. Mod. Phys. 81, 865. Hou, W., X. Liu, and C. Ren (2022), Phys. Rev. A 105, 042436.40 Hu, M.-J., Z.-Y. Zhou, X.-M. Hu, C.-F. Li, G.-C. Guo, and Y.-S. Zhang (2018), npj Quantum Inf. 4 (1), 1. Hu, M.-L., and H. Fan (2023), Phys. Rev. A 108, 012423. Hu, M.-L., J.-R. Wang, and H. Fan (2022), Sci.Chain- Phys.Mech.Astron. 65 (6), 260312. Hu, X.-M., C.-X. Huang, Y.-B. Sheng, L. Zhou, B.-H. Liu, Y. Guo, C. Zhang, W.-B. Xing, Y.-F. Huang, C.-F. Li, and G.-C. Guo (2021), Phys. Rev. Lett. 126, 010503. Jennewein, T., G. Weihs, J.-W. Pan, and A. Zeilinger (2001), Phys. Rev. Lett. 88, 017903. Jones, S. J., H. M. Wiseman, and A. C. Doherty (2007), Phys. Rev. A 76, 052116. Ku, H.-Y., S.-L. Chen, N. Lambert, Y.-N. Chen, and F. Nori (2018), Phys. Rev. A 98, 022104. Kumar, R., and A. Pan (2023), Quantum Stud.: Math. Found. , 1. Kumari, A., and A. K. Pan (2019), Phys. Rev. A100, 062130. Kumari, A., and A. K. Pan (2023), Phys. Rev. A107, 012615. Lami, L., C. Hirche, G. Adesso, and A. Winter (2016), Phys. Rev. Lett. 117, 220502. Li, H.-W., Z.-Q. Yin, Y.-C. Wu, X.-B. Zou, S. Wang, W. Chen, G.-C. Guo, and Z.-F. Han (2011), Phys. Rev. A 84, 034301. Li, H.-W., Y.-S. Zhang, X.-B. An, Z.-F. Han, and G.-C. Guo (2018a), Commun. Phys. 1 (1), 1. Li, J., Y. Zhou, and Q. Wang (2022), Chin. Phys. Lett. 39 (11), 110301. Li, M.-S., and Y.-L. Wang (2023), “Sequentially wit- nessing entanglement by independent observer pairs,” arXiv:2311.10347 [quant-ph]. Li, X.-G., J. Zou, and B. Shao (2018b), Quantum Inf. Process. 17 (6), 1–18. Liu, K., T. Liu, W. Fang, J. Li, and Q. Wang (2021), “Both qubits of the singlet state can be steered simultaneously by multiple independent observers via sequential measure- ment,” arXiv:2102.12166 [quant-ph]. Liu, T.-J., K. Liu, W. Fang, J. Li, and Q. Wang (2022), Opt. Express 30 (23), 41196. Lougovski, P., and S. J. van Enk (2009), Phys. Rev. A 80, 034302. Lu, H., Q. Zhao, Z.-D. Li, X.-F. Yin, X. Yuan, J.-C. Hung, L.- K. Chen, L. Li, N.-L. Liu, C.-Z. Peng, Y.-C. Liang, X. Ma, Y.-A. Chen, and J.-W. Pan (2018), Phys. Rev. X8, 021072. Lv, Q.-Q., J.-M. Liang, Z.-X. Wang, and S.-M. Fei (2023), J. Phys. A Math. Theor. 56 (32), 325301. Mahato, S. S., and A. K. Pan (2022), Phys. Rev. A 106, 042218. Maity, A. G., D. Das, A. Ghosal, A. Roy, and A. S. Majumdar (2020), Phys. Rev. A 101, 042340. Mal, S., D. Das, S. Sasmal, and A. S. Majumdar (2017), “Nec- essary and sufficient state condition for two-qubit steering using two measurement settings per party and monogamy of steering,” arXiv:1711.00872 [quant-ph]. Mal, S., A. S. Majumdar, and D. Home (2016), Mathematics 4 (3), 48. Mao, Y.-L., Z.-D. Li, A. Steffinlongo, B. Guo, B. Liu, S. Xu, N. Gisin, A. Tavakoli, and J. Fan (2023), Phys. Rev. Res. 5, 013104. Masanes, L., A. Acin, and N. Gisin (2006), Phys. Rev. A 73, 012112. Mayers, D., and A. Yao (2004), Quantum Info. Comput.4 (4), 273–286. McKague, M. (2016), New J. Phys. 18 (4), 045013. McKague, M. (2017), Quantum 1, 1. Mermin, N. D. (1990), Phys. Rev. Lett. 65, 1838. Miklin, N., J. J. Borka la, and M. Paw lowski (2020), Phys. Rev. Res. 2, 033014. Mohan, K., A. Tavakoli, and N. Brunner (2019), New J. Phys. 21 (8), 083034. Mukherjee, K., B. Paul, and A. Roy (2020), Phys. Rev. A 101, 032328. Mukherjee, K., B. Paul, and D. Sarkar (2015), Quantum Inf. Process. 14 (6), 2025–2042. Mukherjee, K., B. Paul, and D. Sarkar (2017), Phys. Rev. A 96, 022103. Mukherjee, S., and A. K. Pan (2021), Phys. Rev. A 104, 062214. Munshi, S., and A. K. Pan (2022), Phys. Rev. A 105, 032216. Nagali, E., S. Felicetti, P.-L. De Assis, V. D’Ambrosio, R. Filip, and F. Sciarrino (2012), Sci. Rep. 2 (1), 443. Nielsen, M. A., and I. L. Chuang (2010), Quantum com- putation and quantum information (Cambridge university press). Nogues, G., A. Rauschenbeutel, S. Osnaghi, M. Brune, J.-M. Raimond, and S. Haroche (1999), Nature. 400 (6741), 239. Pan, A. K. (2021), Phys. Rev. A 104, 022212. Pan, J.-W., D. Bouwmeester, H. Weinfurter, and A. Zeilinger (1998), Phys. Rev. Lett. 80, 3891. Pandit, M., C. Srivastava, and U. Sen (2022), Phys. Rev. A 106, 032419. Paul, B., and K. Mukherjee (2020), Phys. Rev. A102, 052209. Paw lowski, M., and N. Brunner (2011), Phys. Rev. A 84, 010302. Paw lowski, M., T. Paterek, D. Kaszlikowski, V. Scarani, A. Winter, and M. ˙Zukowski (2009), Nature. 461 (7267), 1101. Pironio, S., A. Ac´ ın, S. Massar, A. B. de La Giroday, D. N. Matsukevich, P. Maunz, S. Olmschenk, D. Hayes, L. Luo, T. A. Manning, et al. (2010), Nature 464 (7291), 1021. Popescu, S., and D. Rohrlich (1994), Found. Phys. 24, 379. Pozas-Kerstjens, A., N. Gisin, and A. Tavakoli (2022), Phys. Rev. Lett. 128, 010403. Pryde, G. J., J. L. O’Brien, A. G. White, S. D. Bartlett, and T. C. Ralph (2004), Phys. Rev. Lett. 92, 190402. Reid, M. D. (2013), Phys. Rev. A 88, 062108. Ren, C., T. Feng, D. Yao, H. Shi, J. Chen, and X. Zhou (2019), Phys. Rev. A 100, 052121. Ren, C., X. Liu, W. Hou, T. Feng, and X. Zhou (2022), Phys. Rev. A 105, 052221. Ren, J.-G., P. Xu, H.-L. Yong, L. Zhang, S.-K. Liao, J. Yin, W.-Y. Liu, W.-Q. Cai, M. Yang, L. Li, et al. (2017), Na- ture(London) 549 (7670), 70. Renou, M.-O., E. B¨ aumer, S. Boreiri, N. Brunner, N. Gisin, and S. Beigi (2019), Phys. Rev. Lett. 123, 140401. Renou, M.-O., and S. Beigi (2022), Phys. Rev. Lett. 128, 060401. Renou, M.-O., D. Trillo, M. Weilenmann, T. P. Le, A. Tavakoli, N. Gisin, A. Ac´ ın, and M. Navascu´ es (2021), Nature 600 (7890), 625. Rowe, M. A., D. Kielpinski, V. Meyer, C. A. Sackett, W. M. Itano, C. Monroe, and D. J. Wineland (2001), Nature 409 (6822), 791. Roy, P., and A. K. Pan (2023), New J. Phys. 25 (1), 013040. Saha, S., D. Das, S. Sasmal, D. Sarkar, K. Mukherjee, A. Roy, and S. S. Bhattacharya (2019), Quantum Inf. Process. 18 (2), 42. Sasmal, S., D. Das, S. Mal, and A. S. Majumdar (2018), Phys. Rev. A 98, 012305.41 Sasmal, S., S. Kanjilal, and A. K. Pan (2023), “Unbounded sharing of nonlocality using projective measurements,” arXiv:2311.07977 [quant-ph]. Schiavon, M., L. Calderaro, M. Pittaluga, G. Vallone, and P. Villoresi (2017a), Quantum Sci. Technol. 2 (1), 015010. Schiavon, M., L. Calderaro, M. Pittaluga, G. Vallone, and P. Villoresi (2017b), Quantum Sci. Technol. 2 (1), 015010. Sciarrino, F., M. Ricci, F. De Martini, R. Filip, and L. Miˇ sta (2006), Phys. Rev. Lett. 96, 020408. Shalm, L. K., E. Meyer-Scott, B. G. Christensen, P. Bierhorst, and M. A. Wayne (2015), Phys. Rev. Lett. 115, 250402. Shenoy H., A., S. Designolle, F. Hirsch, R. Silva, N. Gisin, and N. Brunner (2019), Phys. Rev. A 99, 022317. Shi, S., L. Tian, Y. Wang, Y. Zheng, C. Xie, and K. Peng (2020), Phys. Rev. Lett. 125, 070502. Silva, R., N. Gisin, Y. Guryanova, and S. Popescu (2015), Phys. Rev. Lett. 114, 250401. Sørensen, A. S., and K. Mølmer (2001), Phys. Rev. Lett. 86, 4431. Sparaciari, C., and M. G. A. Paris (2014), INT J QUANTUM INF 12, 1461012. Srivastava, C., S. Mal, A. Sen(De), and U. Sen (2021a), Phys. Rev. A 103, 032408. Srivastava, C., S. Mal, A. Sen(De), and U. Sen (2021b), Phys. Rev. A 103, 032408. Srivastava, C., M. Pandit, and U. Sen (2022a), Phys. Rev. A 105, 062413. Srivastava, C., M. Pandit, and U. Sen (2022b), “Sequen- tial detection of genuine multipartite entanglement is un- bounded for entire hierarchy of number of qubits recycled,” arXiv:2208.08435 [quant-ph]. Steffinlongo, A., and A. Tavakoli (2022), Phys. Rev. Lett. 129, 230402. ˇSupi´ c, I., and J. Bowles (2020), Quantum4, 337. ˇSupi´ c, I., D. Cavalcanti, and J. Bowles (2021), Quantum 5, 418. Tavakoli, A., and A. Cabello (2018), Phys. Rev. A97, 032131. Tavakoli, A., N. Gisin, and C. Branciard (2021), Phys. Rev. Lett. 126, 220401. Tavakoli, A., A. Hameedi, B. Marques, and M. Bourennane (2015), Phys. Rev. Lett. 114, 170502. Tavakoli, A., J. m. k. Kaniewski, T. V´ ertesi, D. Rosset, and N. Brunner (2018), Phys. Rev. A 98, 062307. Tavakoli, A., B. Marques, M. Paw lowski, and M. Bourennane (2016), Phys. Rev. A 93, 032336. Tavakoli, A., A. Pozas-Kerstjens, M.-X. Luo, and M.-O. Re- nou (2022a), Rep. Prog. Phys. 85 (5), 056001. Tavakoli, A., A. Pozas-Kerstjens, M.-X. Luo, and M.-O. Re- nou (2022b), Rep. Prog. Phys. 85 (5), 056001. Tavakoli, A., P. Skrzypczyk, D. Cavalcanti, and A. Ac´ ın (2014), Phys. Rev. A 90, 062109. Tavakoli, A., M. Smania, T. V´ ertesi, N. Brunner, and M. Bourennane (2020), Sci. Adv. 6 (16), eaaw6664. Terhal, B. M. (2000), Phys. Lett. A 271 (5), 319. Thomas-Peter, N., B. J. Smith, A. Datta, L. Zhang, U. Dorner, and I. A. Walmsley (2011), Phys. Rev. Lett. 107, 113603. Toner, B. (2009), Proc. R. Soc. A. 465 (2101), 59. de la Torre, G., M. J. Hoban, C. Dhara, G. Prettico, and A. Ac´ ın (2015), Phys. Rev. Lett.114, 160502. Uffink, J., and M. Seevinck (2008), Phys. Lett. A 372 (8), 1205. Uola, R., A. C. S. Costa, H. C. Nguyen, and O. G¨ uhne (2020), Rev. Mod. Phys. 92, 015001. Von Neumann, J. (1955), . Von Neumann, J. (2018), Mathematical foundations of quan- tum mechanics: New edition , Vol. 53 (Princeton university press). Wang, H., Y. He, Y.-H. Li, Z.-E. Su, B. Li, H.-L. Huang, X. Ding, M.-C. Chen, C. Liu, J. Qin, et al. (2017), Nat. Photonics 11 (6), 361. Wang, J.-H., Y.-J. Wang, L.-J. Wang, and Q. Chen (2022), Phys. Rev. A 106, 052412. Wehner, S., M. Christandl, and A. C. Doherty (2008), Phys. Rev. A 78, 062112. Wei, S., F. Guo, F. Gao, and Q. Wen (2021), New J. Phys. 23 (5), 053014. Weihs, G., T. Jennewein, C. Simon, H. Weinfurter, and A. Zeilinger (1998), Phys. Rev. Lett. 81, 5039. Werner, R. F. (1989), Phys. Rev. A 40, 4277. Westerbaan, A., and B. Westerbaan (2016), J. Math. Phys. 57 (9). Wheeler, J. A., and W. H. Zurek (2014), Quantum theory and measurement, Vol. 40 (Princeton University Press). Wiesner, S. (1983), SIGACT News 15 (1), 78–88. Wiseman, H. M., S. J. Jones, and A. C. Doherty (2007), Phys. Rev. Lett. 98, 140402. Wolfe, E., R. W. Spekkens, and T. Fritz (2019), J CAUSAL INFERENCE 7 (2), 20170020. Xi, Y., and L. Fu (2022), arXiv:2207.00296 [quant-ph]. Xi, Y., M.-S. Li, L. Fu, and Z.-J. Zheng (2023), Phys. Rev. A 107, 062419. Xiang, Y., X. Su, L. Miˇ sta, G. Adesso, and Q. He (2019), Phys. Rev. A 99, 010104. Xiao, Y., F. Guo, H. Dong, and F. Gao (2023), Quantum Inf. Process. 22 (5), 195. Xiao, Y., X.-H. Han, X. Fan, H.-C. Qu, and Y.-J. Gu (2021), Phys. Rev. Res. 3, 023081. Xiao, Y., Y.-X. Rong, X.-H. Han, S. Wang, X. Fan, W.-C. Li, and Y.-J. Gu (2022), “Experimental recycling of bell non- locality with projective measurements,” arXiv:2212.03815 [quant-ph]. Xin, T., S. Huang, S. Lu, K. Li, Z. Luo, Z. Yin, J. Li, D. Lu, G. Long, and B. Zeng (2018), Sci. Bull. 63 (1), 17. Yamamoto, Y., S. Machida, S. Saito, N. Imoto, T. Yanagawa, M. Kitagawa, and G. Bj¨ ork (1990) (Elsevier) pp. 87–179. Yang, L., X. Qi, and J. Hou (2021), Phys. Rev. A104, 042405. Yang, T. H., and M. Navascu´ es (2013), Phys. Rev. A 87, 050102. Yao, D., and C. Ren (2021), Phys. Rev. A 103, 052207. Yuan, R. (2022), “A brief introduction to povm measurement in quantum communications,” arXiv:2201.07968 [quant- ph]. Zhang, T., O. Dahlsten, and V. Vedral (2020), New J. Phys. 22 (2), 023029. Zhang, T., and S.-M. Fei (2021), Phys. Rev. A 103, 032216. Zhang, T., N. Jing, and S.-M. Fei (2023), Frontiers of Physics 18 (3), 31302. Zhang, T., Q. Luo, and X. Huang (2022), Quantum Inf. Pro- cess. 21 (10), 350. Zhao, Z., R. Pisarczyk, J. Thompson, M. Gu, V. Vedral, and J. F. Fitzsimons (2018), Phys. Rev. A 98, 052312. Zhu, J., M.-J. Hu, C.-F. Li, G.-C. Guo, and Y.-S. Zhang (2022), Phys. Rev. A 105, 032211. Zukowski, M., A. Zeilinger, and H. Weinfurter (1995), New York Acad. Sci. 755 (1), 91. Zurel, M., C. Okay, and R. Raussendorf (2020), Phys. Rev. Lett. 125, 260404.42 ˇSupi´ c, I., J.-D. Bancal, Y. Cai, and N. Brunner (2022), Phys. Rev. A 105, 022206. ˇSupi´ c, I., and M. J. Hoban (2016), New J. Phys. 18 (7), 075006. ˙Zukowski, M., and ˇCaslav Brukner (2002), Phys. Rev. Lett. 88, 210401. ˙Zukowski, M., A. Zeilinger, M. A. Horne, and A. K. Ekert (1993), Phys. Rev. Lett. 71, 4287.",
      "references": [
        "Quantum measure- ments,",
        "Quantum random access codes with shared randomness,",
        "Towards a Unified Ap- proach to Information-Disturbance Tradeoffs in Quantum Measurements.",
        "Quantum measurement of spins and mag- nets, and the classical limit of pr-boxes,",
        "Bell nonlocality between sequential pairs of observers,",
        "Full network nonlocality shar- ing in extended scenario via optimal weak measurements,",
        "Nec- essary and sufficient state condition for two-qubit steering using two measurement settings per party and monogamy of steering,",
        "Both qubits of the singlet state can be steered simultaneously by multiple independent observers via sequential measure- ment,",
        "Activation of quantum steering sharing with un- sharp nonlocal product measurements,",
        "Unbounded sharing of nonlocality using projective measurements,",
        "Sequential detec- tion of genuine multipartite entanglement is un- bounded for entire hierarchy of number of qubits recycled,",
        "Sequentially wit- nessing entanglement by independent observer pairs,",
        "A brief introduction to povm measurement in quantum communications,",
        "Experimental recycling of bell non- locality with projective measurements,"
      ],
      "meta_data": {
        "arxiv_id": "2402.04652v1",
        "doi": "10.1016/j.physrep.2024.10.003",
        "authors": [
          "Zinuo Cai",
          "Changliang Ren",
          "Tianfeng Feng",
          "Xiaoqi Zhou",
          "Jingling Chen"
        ],
        "published_date": "2024-02-07T08:36:29Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Comprehensive 2024 review of “quantum correlation sharing” enabled by sequential measurements—how multiple independent observers can reuse a single entangled resource to demonstrate nonclassical correlations (Bell nonlocality, EPR steering, network nonlocality, entanglement witnessing, contextuality) despite standard monogamy constraints. Synthesizes theoretical mechanisms (information–disturbance trade-off, unsharp/weak/POVM measurements, asymmetric sharpness) and maps when/why multiple violations of correlation inequalities are possible (e.g., double/triple CHSH violations; passive vs active sharing; boundless sharing via asymmetric POVMs; projective-measurement-only recycling; multipartite/trilateral sharing with MABK). Surveys experimental progress demonstrating sustained entanglement/nonlocality under sequential probing and connects sharing to applications (sequential QRACs, device-independent/SDI randomness generation, self-testing of instruments/POVMs).",
        "methodology": "Unifying framework based on joint probabilities from sequential measurements P(a1,a2,…|A1,A2,…,ρ), emphasizing when joint distributions do not factorize after the first measurement, allowing later observers to access residual correlations. Formal definitions of correlations via LHV/LHS/separable models (Bell nonlocality, steering, entanglement) and network bilocal models; corresponding criteria via inequalities/witnesses: CHSH, linear/CFFW steering inequalities, BRGP bilocal inequality, star-network n-local inequalities, MABK for GHZ, entanglement witnesses, preparation-contextuality inequalities. Measurement modeling via von Neumann PVM, general POVMs/Kraus operators, and weak measurement (pointer coupling) characterized by precision G and quality F with optimal trade-off G^2+F^2=1; also unsharp measurements Qγ=γP+(1−γ)I/d and asymmetric POVMs with unequal sharpness across settings. Analytical derivations of sequential-update channels for states and correlators, trade-off relations between successive violations, and optimization of measurement directions/strengths; classification into passive vs active sharing depending on intermediate observers’ objectives.",
        "experimental_setup": "As a review, no new dataset; compiles benchmarks used across the literature: (1) Bell nonlocality sharing tested mainly with CHSH values S>2 in sequential Alice–Bob1–Bob2… scenarios using maximally/partially entangled two-qubit photon states; (2) steering sharing validated via CFFW/linear steering inequality violations with sequential weak measurements; (3) multipartite nonlocality sharing via MABK inequality violations using three-qubit GHZ states; (4) network nonlocality sharing via BRGP bilocal inequality and star-network inequalities with sequential unsharp measurements on edge nodes; (5) entanglement sharing via sequential entanglement-witness expectation values. Experimental platforms surveyed are predominantly photonics: polarization-entangled photon pairs, ancillary path qubits or controlled-phase gates/Mach–Zehnder interferometers to implement tunable-strength weak/unsharp measurements, waveplates and PBS for projective settings, and coincidence counting; also demonstrations of double/triple CHSH violations, “active” optimized sharing, sustained entanglement/nonlocality under three sequential measurements, and projective-measurement-based recycling protocols. Validation is via repeated trials to estimate joint/marginal probabilities, compute inequality values with error bars/standard-deviation significance, and compare to theoretical trade-off curves.",
        "limitations": "Being a synthesis article, results depend on assumptions of the reviewed models: idealized i.i.d. sources, independent observers without signaling except via state handoff, well-characterized measurement strength parameters, and often unbiased random inputs. Many strongest sharing results rely on finely tuned unsharp/weak measurements or asymmetric POVMs; boundless nonlocality sharing typically yields exponentially vanishing CHSH violations, limiting device-independent usefulness (e.g., randomness rates). Experimental evidence is still concentrated in low-dimensional (mostly qubit) and primarily photonic setups, with limited demonstrations in higher-dimensional systems, complex networks, or non-photonic platforms; scalability and noise robustness of long sequential chains remain challenging. For several settings (e.g., bilateral two-sided sequential Bell nonlocality sharing, optimal pointers in higher dimensions, maximal shareability under general POVMs), only partial/no-go results, sufficient conditions, or numerical evidence are available rather than complete necessary-and-sufficient characterizations.",
        "future_research_directions": "Derive optimal measurement strategies (including optimal pointer states) for correlation sharing in higher-dimensional and multipartite systems; characterize necessary/sufficient conditions for unbounded sharing with meaningful (non-negligible) violations. Extend and unify resource-theoretic analysis: quantify when sequential reuse truly reduces resource consumption versus parallel resources, and connect shareability to operational tasks and monotones. Explore broader correlation types and finer-grained multipartite structure under sequential probing (entanglement depth/intactness, contextuality variants, global-to-local correlation conversion in networks). Develop scalable, noise-robust, device-independent protocols leveraging sequential sharing for randomness generation, self-testing of instruments/POVMs, and communication games (QRACs), including tight trade-off bounds and robustness analyses. Expand experimental implementations beyond photonics (ions, superconducting qubits, NV centers, cavity QED), to higher dimensions and genuine network topologies, and test projective-only or randomness-assisted schemes enabling many sequential observers.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Federated Text-driven Prompt Generation for Vision-Language Models",
      "full_text": "Front. Comput. Sci., 2024, 0(0): 1–36 https://doi.org/10.1007/sxxxxx-yyy-zzzz-1 REVIEW ARTICLE The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends Mengqi Chen1, Bin Guo(B)1, Hao Wang1, Haoyu Li1, Qian Zhao1, Jingqi Liu1, Yasan Ding1, Yan Pan2, Zhiwen Yu1 1 Northwestern Polytechnical University, Xi’an, China 2 National University of Defense Technology, Changsha, China © Higher Education Press 2024 Abstract Persuasion, as one of the crucial abili- ties in human communication, has garnered exten- sive attention from researchers within the field of intelligent dialogue systems. Developing dialogue agents that can persuade others to accept certain standpoints is essential to achieving truly intelli- gent and anthropomorphic dialogue systems. Ben- efiting from the substantial progress of Large Lan- guage Models (LLMs), dialogue agents have ac- quired an exceptional capability in context under- standing and response generation. However, as a typical and complicated cognitive psychological sy- stem, persuasive dialogue agents also require knowl- edge from the domain of cognitive psychology to attain a level of human-like persuasion. Conse- quently, the cognitive strategy-enhanced persuasive dialogue agent (defined as CogAgent), which in- corporates cognitive strategies to achieve persua- sive targets through conversation, has become a pre- dominant research paradigm. To depict the research trends of CogAgent, in this paper, we first present several fundamental cognitive psychology theories and give the formalized definition of three typical cognitive strategies, including the persuasion strat- egy, the topic path planning strategy, and the argu- ment structure prediction strategy. Then we pro- pose a new system architecture by incorporating the formalized definition to lay the foundation of CogAgent. Representative works are detailed and investigated according to the combined cognitive strategy, followed by the summary of authoritative benchmarks and evaluation metrics. Finally, we summarize our insights on open issues and future directions of CogAgent for upcoming researchers. Keywords Persuasive dialogue, cognitive strat- egy, cognitive psychology, persuasion strategy Received month dd, yyyy; accepted month dd, yyyy E-mail: guob@nwpu.edu.cn arXiv:2402.04631v1  [cs.CL]  7 Feb 20242 Front. Comput. Sci., 2024, 0(0): 1–36 1 Introduction Dialog agents can engage in chitchat with humans to establish certain emotional connections or help us complete tasks through long-form conversations (e.g., restaurant reservation, travel time arrange- ment). Building intelligent human-machine dia- logue agents that can conduct natural and engag- ing conversations with humans is the long-standing goal of artificial intelligence (AI) [1, 2]. Moreover, the persuasive ability of dialogue agents has gar- nered extensive attention from researchers. Persua- sion is one of the crucial abilities in human commu- nication. The Elaboration Likelihood Model (ELM) theory [3] suggests that people tend to engage with persuasive messages when communicating with oth- ers. It is a prevalent phenomenon for individu- als to hold diverse perspectives on a given topic and endeavor to influence others in altering their viewpoints, attitudes, or behaviors through conver- sational interactions [4, 5]. There are massive per- suasive scenarios in the real world, such as bargain- ing prices of goods, debating on specific topics, and arguing in online comment sections [6, 7], and active online communities, such as Debate 1) and ChangeMyView2), for people to communicate and influence other people’s opinions by posting their views [8]. A persuasive conversion includes two distinct parties, corresponding to persuader and per- suadee, respectively [9]. The goal of the persuader is to change the persuadee’s viewpoint on a spe- cific topic by combining cognitive strategies, the personality of the persuadee, and other context fea- tures [10, 11]. The development of intelligent per- suasive dialogue agents that can persuade users to accept certain standpoints is emerging as a promis- 1)https://www.debate.org/ 2)https://www.reddit.com/r/changemyview/ ing research field [12, 13]. Modern dialogue agents have arrived at the era characterized by large language models (LLMs) [14, 15]. Driven by an immense scale of parameters and an abundance of training data, dialogue agents (e.g., ChatGPT3), LLaMA [16], Claude [17], Chat- GLM4)) have acquired an exceptional capability of context understanding and response generation [18, 19], reaching a satisfactory level of fluency, logic, emotional expression and personalization when con- versing with humans [20,21]. In addition to engag- ing in casual conversations with humans, existing dialogue agents, represented by ChatGPT, can as- sist humans in accomplishing intricate tasks, such as writing codes [22, 23], writing long and coher- ent academic papers [24, 25] and aiding in o ffice works (e.g., Microsoft Copilot5)), thereby substan- tially augmenting productivity and the quality of life. However, the persuasion process is an activity that involves human psychological cognition [10, 26, 27]. The design of persuasive dialogue agents needs to incorporate cognitive strategies to orga- nize content, logic, and presentation of dialogue re- sponse reasonably from the cognitive psychology perspective. There have been considerable works of persuasive dialogue systems, which mainly en- hance persuasiveness from three aspects, namely, integrating persuasion strategies, planning topic pa- ths, and extracting argument structures. For ex- ample, Wang et al.[11] provide an in-depth anal- ysis of the impact of persuasion strategies on the persuasive power of dialogue systems in the con- text of persuasion for donation scenarios. To en- able persuasion e fficiently, Qin et al. [28] drive a 3)https://openai.com/blog/chatgpt/ 4)https://github.com/THUDM/ChatGLM3 5)https://adoption.microsoft.com/en-us/ copilot/Mengqi Chen et al. 3 Fig. 1 The example of persuasive dialogue, where the dia- logue agent persuades the user to relieve for job crisis using various persuasion strategies. conversation towards a specified persuasive target with explicit topics /keywords planning over kno- wledge graphs. Arguments are also an important source of e ffective persuasive content. Prakken et al. [29] develop a dialogue system wherein an argu- ment graph serves as the persuasive knowledge for persuasive response generation. Different from ex- isting works that focus on the persuasiveness of di- alogue systems, in this paper, we argue that persua- sion is a cognitive psychology activity and that the persuasion strategy, the topic path planning strat- egy, and the argument structure prediction strategy can all be categorized as cognitive strategies. We define cognitive strategy-enhanced persuasive dia- logue agent as CogAgent. CogAgent aims to integrate a variety of cogni- tive strategies to ensure that the generated dialogue contents can effectively influence the persuadee in terms of their perceptions, opinions, or attitudes [11, 30]. CogAgent has great potential in many scenarios, such as counseling depressed children [31], persuasion for social good [11], winning de- bates [32], and recommending items to users [33, 34]. Fig. 1 depicts a persuasive dialogue example. The dialogue agent persuades the user to reduce anxiety from job crises using various persuasion strategies. The social and communicative dynam- ics behind persuasive dialogue contexts are com- plex. E ffective and successful persuasive dialogue does not mechanically convey target viewpoints to persuadees but rather empathetically addresses per- suadees through social and emotional communi- cations [35]. Thus, persuasive dialogues are not strictly task-oriented but are carried around tasks with additional cognitive strategies to build trust and empathy with persuadees, leading to a smooth persuasive process. As an emerging research area, an in-depth sur- vey of the existing academic e fforts is necessary. Duerr et al.[36] broadly reviews the works that use natural language generation to automatically de- tect and generate persuasive texts. Zhan et al.[37] concentrates on the negotiation dialogue system, a typical type of persuasive dialogue system, and comprehensively summarizes benchmarks, evalu- ations, and methodologies of negotiation dialogue systems. Deng et al.[38] provide an overview of the prominent problems and advanced designs in proactive dialogue systems, which treats persua- sive dialogue as the subset of the proactive dia- logue. Compared with these surveys, we provide a comprehensive review of concepts, challenges, methodologies, and applications in the field of cog- nitive strategy-enhanced persuasive dialogue. We formalize the definition of cognitive strategies ex- tended from cognitive psychology theory. Based on the formalized concept model and generic sys- tem architecture, we summarize representative re- search in the field of CogAgent from a systematic perspective. Furthermore, benchmarks, evaluation metrics, and thoughts on promising research trends are analyzed to promote the research progress. To sum up, our contributions are summarized as fol- lows.4 Front. Comput. Sci., 2024, 0(0): 1–36 • Drawing from cognitive psychology theories, we formalize the definition of cognitive strat- egies, and present the concept model and gen- eric system architecture of CogAgent, to pro- vide an overall picture for the summary of research works. • We make a profound investigation of the de- velopment in CogAgent by presenting the co- re contributions of each work, according to the addressed challenges. Besides, we also comprehensively summarize available datase- ts and evaluation metrics. • We further discuss some open issues and pro- mising research trends in CogAgent, includ- ing model adaptivity/generality of CogAgent, multi-party CogAgent, multimodal CogAge- nt, etc., to promote the development of the research community. The rest of the paper is organized as follows. In Section 2, we first summarize the typical cog- nitive psychology theories and present the defini- tion of cognitive strategies. Then we formalize the concept model of CogAgent and design a generic system architecture, followed by typical applica- tion scenarios of CogAgent. In Section 3, we first introduce the challenges faced by CogAgent and then summarize the key techniques to achieve Co- gAgent based on the user cognitive strategies. In Section 4, we summarize the available datasets and evaluation metrics, followed by open issues and promising research trends in Section 5. 2 Formalized Concept Model and Sys- tem Architecture for CogAgent In this section, we first summarise the typical cog- nitive psychology theories involved in human con- versations, as the theoretical foundation for the de- sign of CogAgent. Then we formalize the concept model for CogAgent and present the generic sys- tem architecture to visualize the overall picture in CogAgent. 2.1 The Cognitive Psychology Theory As a typical cognitive-psychological activity, the persuasion process requires the support of cogni- tive psychology theories to e ffectively model the mental changes that people experience during con- versations, thus promoting the design of CogAgent. This section summarises typical cognitive psychol- ogy theories to inspire subsequent CogAgent re- searchers. 2.1.1 Pre-suasion The concept of Pre-suasion [39], proposed by the renowned authority on persuasion, Robert Cialdini, is a prominent theory in the persuasion field. Pre- suasion means that the success rate of persuasion can be significantly enhanced by attracting the at- tention of the persuadee through appropriate choic- es of words and actions before communication or requests are conducted. Pre-suasion emphasizes that the timing of persuasion is as important as per- suasive content. When we intend to persuade oth- ers to accept our points, we need to consider others’ perspectives and organize our conversational argu- ments at the appropriate time to e ffectively com- plete the persuasion process. 2.1.2 Principle of Consistency The principle of consistency suggests that people usually try to maintain consistency based on what they have expressed and the commitments they have made in the past [40]. By planning topic paths, one can think about and define one’s opinions and arguments in advance to maintain consistency andMengqi Chen et al. 5 increase persuasiveness when communicating with others. The principle of consistency plays an im- portant role in persuading others. Through consis- tency of statements, the persuadees will recognize that the points raised are consistent with their be- liefs or opinions and will e ffectively increase the effectiveness of persuasion. 2.1.3 Theory of Mind The theory of mind (ToM) [41] suggests that effec- tive questions and answers in communications are based on a shared world of experiences and refer- ents between interlocutors. To communicate effec- tively, people model both the mental states of their listeners and the e ffects of their behavior on the world, and then react to and predict the behavior of others. This ability to understand and infer human intentions is defined as a ToM. One way to imitate ToM is to observe others’ perspectives in various situations and to derive a set of rules that a ffect their perspectives and emotions. When the same or highly similar scenarios reoccur, we can make reasonable behavioral or emotional predictions ac- cordingly. Many researchers explicitly model ToM as a concrete cognitive process to ensure that dia- logue agents can access potential human psycho- logical states and cognitive processes [42–44]. 2.1.4 Rhetoric Aristotle, one of the earliest masters of the art of persuasion, proposes three basic elements of per- suasion: ethos (credibility), pathos (emotions), and logos (logic) in his work, The Philosophy of Rheto- ric [45]. These principles serve as a guide to ef- fective persuasive communication. By establish- ing credibility, appealing to emotions, and applying logical reasoning, one can effectively persuade oth- ers to accept his propositions. Aristotle’s insights in Rhetoric remain highly influential not only in the field of persuasive dialogue but also in shaping our understanding of aesthetics and related concepts. Credibility represents the identification of per- suaders, including their identity and moral char- acter, which influences the persuasiveness of the speaker. Aristotle in his Rhetoric explains in detail the three elements that a ffect credibility, namely wisdom, virtue, and goodwill. Wisdom includes elements such as breadth of knowledge, expertise, and authority. Virtue includes elements such as fairness, honesty, and dignity. By demonstrating wisdom, virtue, and goodwill, persuaders can en- hance their persuasiveness and foster the trust and reliability of persuadees. Combining essential ele- ments of credibility can greatly enhance the e ffec- tiveness of CogAgent. Emotion refers to the ex- pression of sentiments during the persuasion pro- cess, thus lowering people’s psychological defenses in accepting persuasive content. Aristotle stated that we cannot persuade others through rationality, but can achieve it with emotion. Emotional expres- sions play an important role in changing the cog- nitive decisions of others. The use of emotionally charged content and expressions can be more ef- fective in eliciting agreement and empathy from the persuadee. Logic refers to the use of inher- ent factual logic, causality, or other rational fac- tors in expressions to gain persons’ trust and per- suade them to change their perceptions. By pre- senting coherent logical arguments, supported by factual data and authoritative sources, persuaders can establish credibility, gain persuadees’ percep- tions, and change their opinions. The cognitive psychology theories, which can be used to model the dynamics of human cogni- tive psychological status, provide a solid founda- tion for CogAgent. Under the guidance of cogni-6 Front. Comput. Sci., 2024, 0(0): 1–36 Table 1 Part of definitions and examples of persuasion strategies. Strategy Definition Example Present of Facts Using factual evidence (e.g., official news reports, statistics) and a credible reasoning process to persuade others In recent months, the demand for residential properties has become extremely high. The price of residential property has risen almost twenty percent. Challenges and Inquiries Expressing disbelief or opposition to the other side’s viewpoints and providing strong rebuttal evidence to enhance persuasiveness Really? I don’t agree. This Star Wars episode was incredible! Emotional Resonance Eliciting specific emotions to influence others’ attitudes State-of-the-art special effects are the main reason for the success of previous episodes, so audiences have high expectations for this one, and I don’t think they will be disappointed Eliciting Anger If that’s the case, there’s not much point in further discussion. We might as well call the whole deal off. Eliciting Guilt Come on, you can at least try a little, besides your cigarette. Self- modeling Indicating one’s intention to act and choosing to act as a role model for the persuadee to follow That still leaves a gap of 20 dollars to be covered. Let’s meet each other halfway once more, then the gap will be closed and our business completed. Building Trust Building rapport and psychological trust through a harmonious conversation I’m glad we’ve agreed on price. We’ll go on to the other terms and conditions at our next meeting. Courtesy Tips Expressing gratitude, approval, praise, etc. to lower the other party’s psychological defensesI know exactly what you mean. Hearing that song gives me a nostalgic feeling. Compromise Expressing concessions on time to avoid being too intense in the guidance process and causing the other party to end the conversation I think it unwise for either of us to insist on his price. How about meeting each other halfway so that business can be concluded? Attachment of Views Expressing kindness and concern through active listening and to some extent seconding the other person’s point of view Better late than never. tive psychology, we can comprehensively investi- gate and model explicit cognitive factors and strate- gies that can change users’ cognitive psychological states, such as logical expressions and emotional appeals. These cognitive strategies can facilitate CogAgent to understand the psychological state of the persuadee and enhance the persuasiveness of responses from multiple perspectives to achieve mo- re efficient persuasion processes. 2.2 Cognitive Strategy To achieve efficient persuasion, it is necessary to integrate various cognitive strategies for precisely responding to the psychological changes of the per- suadee. Evolved from cognitive psychological the- ories, we categorize cognitive strategies into three aspects, persuasion strategy, topic path planning strategy, and argument structure prediction stra- tegy, detailed as follows. 2.2.1 Persuasion Strategy The persuasion strategy aims to influence or change the perceptions, opinions, attitudes, or behaviors of persuadees from a psychological standpoint, thro- ugh the use of linguistic techniques of expression, such as logical appeal, foot-in-the-door, and self- disclosure [4, 11, 46]. Based on existing research, we construct a comprehensive and e ffective set of persuasion strategies that can achieve persuasive goals, inspired by the theory of mind, the rhetoric, and other psychology theories. We formalize the definitions and examples of expressions of persua- sion strategies, as shown in Table 1 and Table 2. Numerous studies [11,47–49] have demonstrated that persuasion strategies can e ffectively enhance the persuasiveness of the dialogue content. How to reasonably select the appropriate strategies accord- ing to the dialogue context and the perusadee’s psy- chological state to generate a persuasive dialogue response is crucial to achieving high-quality Co-Mengqi Chen et al. 7 Table 2 Part of definitions and examples of persuasion strategies. Strategy Definition Example Problem De- composition Decomposing the ultimate persuasion goal into sub-issues and stepping through the persuasion process Let me get down some information about your apartment first. what is your property’s address? Social Identity Gaining psychological support from the other person by emphasizing group and identity belonging I know. I have been a subscriber for the past two years. Herd Mentality Presenting a viewpoint that is recognized or accepted by the majority of people and persuading the other side to accept it There was always a good round of applause every time she sang. Expression of Disgust Expressing a particular point of view or emotion to emphasize the persuasive content Oh, my god! I look so old. I look as if I were 40. I think it’s time for some plastic surgeries. Expression of Empathy I know, dear. I am too. But we’ve just been too busy to look for a house. Expression of Views That means the apartment has furniture in it. Logical Appeal Enhancing the credibility of persuasive content through the logical and reasoning process It certainly is. But to tell you the truth, the room is so large that I can share it with someone else, and that will decrease the total amount of the rent. Task Inquiry Asking questions related to persuasive goals That might be going overboard a bit. How about just that scarf with a bracelet? Personal Story Using narrative examples to illustrate the positive outcomes of your actions to inspire others to follow suit Yes, I’m sure I’ve done a lot of house painting in my life. If I got even a tiny drop of paint on her furniture, she would get furious. So I learned to be very picky. Refutation of Objections Directly refuting the other side’s point of view Not necessary. If we use a realtor to find a house, it will be more expensive. Greeting Greeting at the beginning of a dialogue Hi there! How are you doing today? gAgent. 2.2.2 Topic Path Planning Strategy The topic path planning strategy aims to plan the topic transition sequence during the persuasive di- alogue process, to ensure the dialogue coherence and the progress of the dialogue towards the per- suasive target. The persuasive dialogue agent shou- ld smoothly navigate between topics to reduce ir- relevant associations of the persuadee and the dif- ficulty of the persuasion process [50,51]. The topic path planning strategy is widely employed in target- guided persuasive dialogue systems [52,53]. Start- ing from the topic of interest to the persuadee, the persuasive dialogue agent needs to gradually and smoothly transfer the conversation topic to the per- suasive target to improve the persuadee’s psycho- logical acceptability and ensure the persuasive ef- fect. How to plan the reasonable topic path and generate an in-depth multi-turn persuasive conver- sation according to the corresponding topics is to be explored. 2.2.3 Argument Structure Prediction Strategy Argument structure prediction strategy is designe- d to predict persuasive and authoritative argument surrounding the discussed topic, thereby enhancing the credibility of persuasive dialogue contents and convincing the persuadee of the plausibility of the proposed claims [54–56]. Persuasive dialogue age- nts need to be equipped with a large-scale library of arguments and counter-arguments. By predicting reasonable argument structures based on specific persuasive topics, dialogue agents can incorporate coherent argumentation skills, such as citing au- thorities and providing convincing arguments and evidence, to effectively enhance the plausibility of dialogue contents and the credibility of the persua- sion process. The argument structure prediction strategy has been extensively explored in the field of debate dialogue, where debaters often consider argument structures to express viewpoints with clar-8 Front. Comput. Sci., 2024, 0(0): 1–36 ity, logical coherence, and compelling evidence [32, 57,58]. With the argumentative structure, the whole persuasive process can be progressed incrementally, and the overall organization, logical coherence, and credibility of the persuasive process can be signifi- cantly increased. How to mine the supporting argu- ment structures based on the dialogue context and reasonably integrate the argument structures into dialogue contents to enhance the credibility of per- suasive dialogue is to be investigated. 2.3 Formalized Concept Model for CogAgent Based on the definitions of cognitive strategies, we define the dialogue system that is incorporated with cognitive strategies to accomplish persuasive tasks through smooth and accessible conversations as C- ognitive Strategy-enhanced Persuasive Dialogue (- CogAgent). We introduce the formalized concept model of CogAgent as follows. Typically, given the dialogue context sequence H = {(Q1, A1), ...,(QS −1, AS −1)}with S -1 turns, wh- ere Qi and Ai are the dialogue query and response at the i-th dialogue turn, and the current dialogue query QS = (q1, ...,qm) with m words, the objec- tive of general dialogue system is to generate the dialogue response AS = (a1, ...,an) with n words. The modern dialogue systems usually follow the encoder-decoder architecture [2,59] or decoder-on- ly architecture [16,60]. For the encoder-decoder ar- chitecture, the encoder aims to transform input text sequence into vector representations using LSTM [61], Transformer [62] or other advanced neural models, as shown in Eq. 1. QS, H = Encoder(QS , H) (1) Based on the semantic vectors of dialogue con- text and input query, the decoder generates the dia- logue response word by word in an auto-regressive manner, as shown in Eq. 2, where at is the t-th words in the response. P(A|QS, H) = nY t=1 p(at|QS, H, a<t) (2) For the decoder-only architecture, all input text sequences will concatenated into a uniform sequen- ce with special tokens, and then the decoder also generates the response in a word-by-word manner. The general dialogue system can generate smooth and fluent responses based on the dialogue context. To generate persuasive dialogue content, it is es- sential to combine three kinds of cognitive strate- gies. Based on the definitions of three cognitive strate- gies, we give the formalized definition of CogA- gent. Given the dialogue context and the current query, CogAgent needs to first predict the persua- sion strategy Per, conversation topic Top , and the argument content Arg based on the current dialogue content, as follows. Per, Top , Arg = S trPre(QS, H) (3) where S trPre refers to the cognitive strategy pre- dictor. Then the dialogue decoder generates the di- alogue response word by word conditioned on ad- ditional cognitive strategies, as shown in Eq. 4. P(A|QS, H, Per, Top , Arg) = nY t=1 p(at|QS, H, Per, Top , Arg, a<t) (4) 2.4 Generic System Architecture After the concept model of CogAgent, we present the generic system architecture of CogAgent, asMengqi Chen et al. 9 Fig. 2 The generic system architecture of CogAgent. shown in Fig 2. The overall process of CogAgent starts from the semantic understanding of dialogue context and the persuasive target, powered by LLMs (e.g., ChatGPT, LLaMa, Claude, ChatGLM). The input text will be encoded into semantic embed- dings for subsequent processes. TheCognitive Stra- tegy Mining part is responsible for mining cogni- tive strategies, including persuasion strategies, topic paths over knowledge graph, and argument struc- ture of topics. The Cognitive Strategy Prediction for Dialogue Modelling part predicts appropriate cognitive strategies based on dialogue context and enhances the linguistic expression, logical struc- ture, and other persuasive aspects of responses. The persuasion strategy mining process first min- es various kinds of persuasion strategies through crowd strategy emergence based on cognitive psy- chology theories. According to the dialogue con- text, the persuasion strategies to be used in subse- quent rounds of dialogue will be predicted. The topic graph construction process constructs topic graphs or topic paths and then plans the wander- ing paths of topics for persuasion according to the dialogue context and persuasion strategies. The ar- gument mining process first constructs a complete argument structure from credible data sources and then predicts the arguments needed for persuasion based on the above cognitive strategies. Finally, the cognitive strategies-enhanced dialogue context will be fed into LLMs to generate persuasive dialogue responses, for numerous applications, such as psy- chological counseling, bargaining, and persuasion for social good.10 Front. Comput. Sci., 2024, 0(0): 1–36 2.5 Application Scenarios Persuasive dialogue system has widespread appli- cations in daily life. It is an ongoing e ffort of the academic/industry researchers to conduct persua- sive dialogue with users to achieve persuasive tar- gets, summarized as follows. Persuasion for social good. Persuasion for so- cial good is a typical persuasive dialogue scenario where people are persuaded to donate money or goods to charities for social good purposes, such as children’s aid and natural disaster relief. Many researchers have explored how to combine persua- sion strategies to promote users’ donation behavior. For example, Wanget al.[11] provide an insightful analysis of what persuasion strategies are effective for what types of personal characteristics of users. Mishra et al.[63] propose a Reinforcement Learn- ing (RL) based persuasive dialogue system with an efficient reward function consisting of five di ffer- ent sub rewards, Persuasion, Emotion, Politeness- Strategy Consistency, Dialogue-Coherence, and N- on-repetitiveness. Chen et al.[35] produce a mod- ular persuasive dialogue system that seamlessly in- tegrates factual information and persuasive content into generated dialogue response using the condi- tional language model. Persuasion for psychological counseling. The frequent occurrence of mental diseases, such as de- pression, makes mental health gradually receive ex- tensive attention from society [64–66]. Psycho- logical counseling aims at reducing people’s emo- tional distress and helping them understand and wo- rk through the challenges that they face. Relieving the psychological pressure of the persuaded through conversation holds profound significance for the pe- rsuasive dialogue system. Extensive studies have explored the possibility of using persuasive dialogue systems to provide psychological counseling. For example, Liu et al. [46] collect the Emotion Sup- port Conversation dataset (ESConv) with well-desi- gned persuasion strategy annotation to train dia- logue system to provide emotional support through dialogue interactions. Zhou et al.[67] build a com- monsense cognition graph and an emotional con- cept graph based on commonsense knowledge from COMET [68] and concept knowledge from Con- ceptNet [69]. The two kinds of knowledge are alig- ned to generate dialogue responses for emotional support. Persuasion for negotiation. Negotiation is a common real-life persuasion scenario in which two parties negotiate through ongoing conversations to persuade the other party to accept the terms or de- mands they make to maximize their interests. Ne- gotiation is a necessary means of facilitating agree- ments among people and improving the e fficiency of society. There have been several studies using persuasive dialogue systems to achieve negotiation. For instance, Joshi et al. propose DIALOGRAPH [13], a negotiation dialogue system that explicitly incorporates dependencies between sequences of strategies into graph neural networks. Nortio et al. [70] embark on an exploration of persuasive tech- niques in international negotiations, emphasizing the significance of persuasive strategies during the negotiation process. Persuasion for debate. Debate is a professional persuasive scenario in which debaters persuade the opponent and the audience to accept their view- points by planning their arguments wisely and ar- guing their points from multiple perspectives. Many researchers have explored the automatic generation of persuasive arguments from online discussions or debate competitions [57, 71, 72]. Slonimet al.[32] introduce Project Debater, an autonomous debat-Mengqi Chen et al. 11 ing system that can engage in a competitive debate with humans. Persuasion for recommendation. Engaging in dialogue-based recommendations for movies, prod- ucts, and other such aspects proves to be a highly practical application of a persuasive dialogue sys- tem. To achieve successful recommendations, it is crucial to employ a persuasion strategy to fa- cilitate rapid user comprehension and acceptance of the recommendations. For example, Gupta et al. [73] propose to decompose the recommenda- tion response generation process into first generat- ing explicit commonsense paths between the source and persuasive target followed by generating re- sponses conditioned on the generated paths. 3 Research Challenges and Key Tech- niques Due to the complexity of modeling the psycholog- ical changes in the persuasive conversation, many critical challenges in CogAgent need to be address- ed. In this section, we first detail these challenges faced by CogAgent, and then conduct a compre- hensive investigation of representative works of Co- gAgent according to the adopted cognitive strate- gies, i.e., the persuasion strategy, the topic path planning strategy, and the argument structure pre- diction strategy. 3.1 Research Challenges in CogAgent Exhaustive mining of cognitive strategies . Ps- ychology defines human cognition as the process by which a person encounters, perceives, and un- derstands things [74, 75]. The formation and evo- lution of human cognition is an extremely complex process involving knowledge, personality, emotion, and many other aspects. E ffective persuasive di- alogue changes people’s feelings and perceptions about things through persuasive strategies that con- vince people to change their opinions and behav- iors [11, 76]. Therefore, it is a great challenge to build a complete set of cognitive strategies from the perspective of cognitive psychology by mining cognitive strategies that can effectively change the way human beings perceive and understand things. Several researchers have defined some persuasion strategies based on cognitive psychology theories (e.g., logical appealand emotion appealfrom [11], self-disclosure from [46]). However, most of these strategies are task-specific and not exhaustive enough to cope with generalized persuasion scenarios. How to construct well-defined cognitive strategies from multiple perspectives needs to be explored in depth. Modeling and selecting of cognitive strategies. In persuasive dialogues, people usually dynamicall- y choose different persuasive strategies depending on different persuasive goals and the evolving con- versational contexts. Persuasive strategies contain complex semantic patterns, rather than mere names or descriptions [77, 78]. How to model the implicit associations between strategy definitions and lin- guistic expressions, and precisely select cognitive strategies according to the dialog context to facili- tate the smooth flow of the persuasive dialog pro- cess is a serious challenge. Some researches have explored how to select appropriate cognitive strate- gies based on the dialogue context [13,49]. The ap- propriate selection of cognitive strategies is a crit- ical step for CogAgent to simulate humans in per- suasive conversations and is essential for achieving high-quality persuasive conversations. Integrating cognitive strategies into models. As defined at the cognitive psychology level, cog- nitive strategies are more abstract semantic con- cepts. Data-driven neural network models (DNNs), even LLMs, remain superficial in the understand-12 Front. Comput. Sci., 2024, 0(0): 1–36 ing of cognitive strategies. How to facilitate DNNs to learn the profound semantics of cognitive strate- gies, to rationally integrate cognitive strategies into the generation of persuasive dialogues, and to im- prove the persuasiveness of CogAgent, is quite cha- llenging. Graph-based, [13], reinforcement lear- ning-based [79] and other advanced methods are investigated to integrate cognitive strategies into per- suasive dialog generation. It is promising to inte- grate cognitive strategies with the outstanding lan- guage comprehension ability of LLMs. Absence of evaluation metrics. To improve the quality of persuasive dialogue, the performance of CogAgent needs to be evaluated accurately and co- mprehensively. However, existing evaluation met- rics for dialog systems (e.g., BLEU [80], METEOR [81], ROUGE-L [82]) are usually evaluated at the level of word similarity or semantic similarity be- tween generated responses and ground truth, with- out taking into account the effectiveness of persua- sive strategies, the rationality of persuasive path planning, and the richness of argument structure. It is a challenge to develop comprehensive and rea- sonable evaluation metrics to accurately evaluate the quality of CogAgent, incorporating the charac- teristics of persuasive dialog systems. 3.2 Persuasion Strategy-based CogAgent Incorporating persuasion strategies to enhance the persuasiveness of dialog responses is an important research direction in CogAgent. By using specific persuasive strategies, CogAgent can express the pe- rsuasive content in a way that is more acceptable to the persuadees, thus accomplishing the persua- sive goals more smoothly. As abstract psychologi- cal concepts, how to select appropriate persuasion strategies according to the dialogue context and gui- de the generation of responses is an important re- search question. In this section, we make an inves- tigation of the employment of persuasion strategies in CogAgent, summarized in Table 3. 3.2.1 Strategy Classification based on Dialogue Context A straightforward approach to fusing persuasion strategies in persuasive conversations is to predict a strategy label (e.g., Present of Facts) based on the dialogue context and feed the strategy into the decoder with the dialogue context to generate the dialogue response. For example, Wang et al. [11] propose a per- suasion strategy classifier to predict 10 persuasion strategies based on the dialogue context informa- tion and sentence-level features. The authors also analyze the impact that different people’s backgrou- nds on strategy prediction, laying the groundwork for research on personalized persuasive dialogue agents. He et al. [47] decouple strategy selection and response generation in CogAgent. The dia- logue manager predicts a persuasion strategy ba- sed on the persuasion strategies in dialogue history by a sequence-to-sequence model and the response generator produces a response conditioned on the strategy and dialogue history. 3.2.2 Persuasion Strategy Planning Persuasive dialogue is usually a process that lasts multiple turns, supported by successive strategies [87, 88]. Consequently, strategy planning within a long planning horizon in CogAgent is quite im- portant, rather than predicting a specific strategy based on the dialogue history. Several studies fo- cus on long-term planning of persuasion strategies, making CogAgent more e fficient in reaching per- suasion goals.Mengqi Chen et al. 13 Table 3 Representative works of persuasion strategy-based CogAgent. Solution Work Description Strategy classifying based on dialogue context Wang et al. [11] Proposing a classifier to predict persuasion strategies in dialogue using context and sentence features. He et al. [47] Decoupling strategy selection and response generation in CogAgent for predicting strategy and generating responses based on dialogue history. Persuasion strategy planning Cheng et al. [49] Proposing lookahead heuristics to estimate future user feedback after using the specific strategy. Yu et al. [83] Using Monte Carlo Tree Search for persuasion strategy planning without model training. Graph-based strategy incorporation Joshi et al. [48] Using GNNs to model strategies, dialogue acts, and dependencies in graph structures for response generation. Zhou et al. [84] Modeling both dialogue context semantic and persuasion strategy history with finite state transducers. Knowledge-enhanced strategy modeling Jia et al. [85] Introducing a knowledge-enriched encoder and memory-enhanced strategy module for dynamic emotion and semantic pattern modeling. Chen et al. [35] Designing RAP for dynamic factual and persuasive responses based on knowledge and individual persuasion strategies. Novel integration mechanism Mishra et al. [63] Creating an RL reward function to enhance consistency in politeness strategy, persuasiveness, and emotion acknowledgment in persuasive dialogue. Tu et al. [86] Proposing a novel model MISC, which firstly infers the user’s fine-grained emotional status, and then responds skillfully using a mixture of strategies. For instance, Cheng et al. [49] firstly adopt an A* search algorithm for persuasion strategy plan- ning. When predicting the appropriate strategy in each dialogue turn, look-ahead heuristics are pro- posed to estimate future user feedback after using the specific strategy, thus considering the long-term effect of persuasion strategies. The proposed looka- head method requires abundant annotated data, af- fecting the application to broader persuasive dia- logue scenarios. To overcome this bottleneck, Yu et al. [83] prompts LLMs to perform persuasion strategy planning by simulating future dialogue in- teractions using the Monte Carlo Tree Search (MC- TS) algorithm. This method requires no model trai- ning and can therefore be adapted to any persua- sion scenario. 3.2.3 Graph-based Strategy Incorporation Graph Neural Networks (GNNs) [89–91] can com- bine the benefits of interpretability and expressiv- ity, benefiting from encoding graph-structured data through message propagation. Due to the human brain’s reasoning process to capture semantic as- sociations, graph-based methods have been widely used in various tasks [92,93]. Numerous researche- rs have embarked on exploring the potential of gra- ph-based methods for incorporating persuasion str- ategies in CogAgent. For example, Joshi et al. [48] introduce DIAL- OGRAPH, as shown in Fig 3, a persuasive dialogue system that incorporates persuasion strategies and dialogue acts using GNNs. DIALOGRAPH mod- els persuasion strategies in multi-turn dialogue con- text and their dependencies as graph structures and incorporating strategies into response generation us- ing hierarchical graph pooling-based approaches. Zhou et al. [84] propose to model both dialogue context semantic and persuasion strategy history fi- nite state transducers (FSTs). To model the per- suasion factors affecting the persuasive content of dialogues, Liu et al.[94] present persuasion-factor graph convolutional layers to encode and learn rep- resentations of the persuasion-aware interaction data.14 Front. Comput. Sci., 2024, 0(0): 1–36 Fig. 3 Overview architecture of DIALOGRAPH which models persuasion strategies as graph structure. 3.2.4 Knowledge-enhanced Strategy Modeling As concepts in cognitive psychology, persuasion strategies encompass complex semantic informa- tion and various intricate linguistic features [88, 95]. To comprehensively represent the complex semantics embedded within persuasion strategies, researches investigate combining external knowl- edge to model and mimic the the intricate patterns in strategies. For example, Jiaet al.[85] propose a knowledge- enriched dialogue context encoder to model the dy- namic emotion state and a memory-enhanced strat- egy modeling module to model the semantic pat- terns of persuasion strategies. The same-strategy responses are stored in the memory bank to pro- vide more specific guidance for the strategy- con- strained response generation. Chen et al.[35] de- sign the Response-Agenda Pushing Framework (R- AP) to dynamically produce factual responses bas- ed on knowledge facts and persuasive responses conditioned on individual persuasion strategies. 3.2.5 Novel Integration Mechanisms In addition to the above studies to model and in- tegrate persuasion strategies, researchers propose some novel integration mechanisms to improve the performance of CogAgent, summarized as follows. Combined with RL, Yanget al.[96] propose two variants of ToM-based persuasive dialog agent, wh- ere the explicit version that outputs the opponent type as an intermediate prediction, and an implicit version that models the opponent type as a latent variable. Both models are optimized using rein- forcement learning. Similarly, Mishra et al. [63] design an e fficient reward function in RL to im- prove the politeness-strategy consistency, persua- siveness, and emotional acknowledgement in per- suasive dialogue. To increase the expressed empathy and learn the gradual transition in the long response, Tu [86] in- troduce a MIxed Srategy-aware model (MISC) in- tegrating COMET, a pre-trained generative com- monsense reasoning model, for emotional persua- sive dialogue. The COMET knowledge tuples are adopted to enhance the fine-grained emotional un- derstanding of users. Then MISC formulates per-Mengqi Chen et al. 15 Table 4 Representative works of topic path planning strategy-based CogAgent. Solution Work Description Reinforcement learning based planning Xu et al. [98] Presenting KnowHRL, a three-layer Knowledge-aware hierarchical RL-based model for coherent topic path planning and multi-turn persuasive dialogue responses. Liu et al. [99] Hierarchical RL for conversation topic path planning, using high-level strategies and low-level responses. Lei et al. [100] Introducing four persuasion-related factors in the reward function to achieve persuasive goals efficiently. Graph-based planning Zhong et al. [101] Using commonsense knowledge graphs and GNN to enhance semantic relations between topic keywords, improving keyword-augmented response retrieval. Zou et al. [102] Employing a concept graph for topic planning, utilizing an Insertion Transformer for persuasive response generation based on multi-concept paths. Wang et al. [103] Introducing a Transformer-based network for target-driven topic path planning with knowledge-target mutual attention and set-search decoding. Novel planning mechanism Tang et al. [53] Combining various planning algorithms for robust and smooth topic path planning, incorporating a sampling strategy, flow generator, and global planner. Wang et al. [104] Introducing a consistency-driven dialogue planning approach that utilizes stochastic processes to model the temporal evolution of the conversation path dynamically. suasion strategy as a probability distribution over a strategy codebook to use a mixture of strategies for persuasive response generation. To investigate the potential of LLMs in persua- sive conversations, Zheng et al.[97] first construc- t a large-scale persuasive dialogue dataset in the emotional support domain, leveraging the genera- tive capabilities of LLMs. Then several advanced tuning techniques (fine-tuning, adapter-tuning, Lo- RA-tuning) are employed to to showcase the supe- riority of LLMs in persuasive dialogue generation. 3.3 Topic Path Planning Strategy-based CogAgent In persuasive dialogues, generating engaging re- sponses through effective topic path planning is crit- ical to achieving persuasive targets. Topic path plan- ning strategy is a navigation tool that enhances the coherence of the persuasion process by continu- ously leading users to discuss di fferent points and topics until reaching persuasive targets. This sec- tion deepens into the intricate details of the topic path planning strategy, summarized in Table 4. 3.3.1 Reinforcement Learning-based Planning In the context of topic paths planning strategy, Re- inforcement Learning serves as a dynamic framew- ork for guiding persuasive dialogue systems in a goal-oriented manner. The core of RL is to learn the optimal sequence of actions according to the reward function and is therefore ideally suited for planning coherent topic paths in CogAgent. For example, to achieve coherent topic path plan- ning, Xu et al.[98] introduce a three-layer Knowl- edge aware hierarchical RL-based model (KnowH- RL). The upper layer of KnowHRL plans a high- level topic sequence to track user interests toward persuasive targets. The lower layers are responsi- ble for generating multi-turn persuasive dialogue responses. similarly, Liu et al.[99] propose a hier- archical RL method, GoChat, for topic path plan- ning, as shown in Fig 4. The high-level strategies in GoChat determine sub-goals that guide the conver- sation towards the ultimate target and the low-level strategy generates the corresponding responses to achieve those sub-goals. To plan topic paths from a global perspective, Yanget al.[105] introduce the global planning met- hod integrated with a commonsense knowledge gr-16 Front. Comput. Sci., 2024, 0(0): 1–36 Fig. 4 The overall framework of GoChat with hierarchical reinforcement learning. aph (KG). The key advancement is the introduction of a global RL framework that utilizes topic path planning on KG to guide the local response gener- ation model toward persuasive targets, resulting in more coherent conversations. To achieve persua- sive goals more e ffectively, Lei et al. [100] con- sider four factors (dialogue turn, goal completion difficulty, user satisfaction estimation, and cooper- ative degree) in the reward function. The targets of achieving persuasive targets quickly and maintain- ing the engagingness of users. 3.3.2 Graph-based Planning Knowledge is essential to the cognitive reasoning processes of human beings. We humans usually perform common reason during persuasive conver- sation to enhance the logic and persuasiveness of dialog contents. Therefore, relying on common- sense knowledge graphs for topic path planning can produce more persuasive target-related topic paths for CogAgent, thus reaching persuasive targets mo- re efficiently. Initially, the semantic knowledge relations amo- ng topic keywords are captured to perform next- turn topic prediction during conversation [28,106]. Then the predicted topic keywords are used to re- trieve appropriate candidate responses for persua- sive targets. Furthermore, Zhong et al.[101] intro- duce commonsense knowledge graphs and Graph Neural Networks (GNN) to model the semantic re- lations between topic keywords and enhance the keyword-augmented response retrieval, To plan topic paths more reasonably, Zou et al. [102] introduces a concept graph based on the dia- logue data, where the vertices represent concepts and edges are concept transitions between utter- ances. The topic sequence containing multiple con- cepts is obtained by the multi-concept planning mo- dule and an Insertion Transformer generates a per- suasive response according to the planned topic pat- hs. Wang et al.[107] propose a target-driven plan- ning network (TPNet), which models the topic path planning as a sequence generation task using Trans- former, as shown in Fig 5. A knowledge-target mu- tual attention mechanism and a set-search decod- ing (SSD) strategy are developed to generate topic paths based on the dialogue context. 3.3.3 Novel Planning Mechanism In addition to the above research to plan topic paths in CogAgent, there are some novel planning mech- anisms to be explored, summarized as follows. Combining the strengths of multiple topic plan- ning algorithms, the Tang et al. [53] propose anMengqi Chen et al. 17 Fig. 5 The overview framework of TPNet. EAGLE model for topic path planning. Compris- ing a topic path sampling strategy, topic flow gener- ator, and global planner, EAGLE achieves robust- ness to unseen target topics and smooth transitions. The model demonstrates enhanced global planning ability through its integrated approach, addressing limitations in existing topic-planning conversation models. To ensure the smooth and coherent progression toward persuasive goals across different turns, Wan- g et al. [108] introduce a consistency-driven di- alogue planning approach that utilizes stochastic processes to model the temporal evolution of the conversation path dynamically. Firstly, a latent spa- ce is defined, and Brownian bridge processes are employed to capture the continuity of goal-oriented behavior, allowing for more flexible integration of user feedback into dialogue planning, and explic- itly generating conversation paths. Ultimately, these paths are employed as natural language prompts to guide the generation of persuasive dialogue. 3.4 Argument Structure Prediction Strategy-based CogAgent CogAgent entails an ongoing conversation between a dialogue agent and a user at the cognitive level, where the dialogue agent proactively steers the con- versation. As the conversation progresses, the con- tents presented by the dialogue agent to support its perspectives undergo dynamic transformations. Consequently, the reasonable selection and appli- cation of arguments and evidence play a pivotal role in the persuasiveness of the dialogue. The uti- lization of arguments and evidence is imperative in the process of persuasion. Firstly, employing argu- ments and evidence allows for the gradual decom- position and progressive reasoning of persuasive targets, thereby facilitating a logical and sequential flow of the conversation that enhances the users’ acceptance of viewpoints [109]. Secondly, the pro- vision of factual support elevates the credibility of persuasive discourse, thereby augmenting the per- suasiveness of the conversation. In this section, we provide an investigation of the crucial techniques for argument mining and argument structure pre- diction in CogAgent, as summarized in Table 5.18 Front. Comput. Sci., 2024, 0(0): 1–36 Table 5 Representative works of argument structure prediction strategy-based cogAgent. Solution Work Description Argument mining Khatib et al. [110] Classifying and structurally modeling arguments from online debate portals based on diverse vocabulary, grammar, and metric features. Hua et al. [111] Proposing an argument generation framework with retrieval modules and a sentence-level LSTM for generating viewpoints. Srivastava et al. [112] Using attention-based link prediction and Transformer encoder to model hierarchical causal relationships and discover associations in online argument structures. Niculae et al. [113] Introducing factor graph model for argument mining, concurrently learning fundamental unit types classification and argument relationship prediction. Argument structure prediction Rach et al. [57] Proposing argument search technique using supervised learning-based relation classification to retrieve arguments for debate dialogue system Sakai et al. [71] Introducing an approach to consider the human agreement and disagreement, resulting in a persuasive argument with a hierarchical argumentation structure. Prakken et al. [29] Enhancing argument modeling with a five-layer graph, serving as a knowledge base for a chatbot to identify user focal points and select rebuttal points. Li et al. [114] Using factor graphs to extract online debate features, incorporating them into an LSTM model to predict persuasive arguments. 3.4.1 Argument Mining To integrate the argument structure into CogAgent, it is first necessary to perform argument mining ac- cording to conversation topics. Researchers em- bark on mining argumentative text from dialogues for CogAgent. Debate involves the explicit use of argumenta- tive content for dialogue expression, making it an important source of argument mining. For exam- ple, Khatib et al.[110] utilize online debate portals to acquire both controversial and non-controversial text snippets related to several contentious topics. These snippets are organized in a semi-structured format. Eventually, by employing a diverse set of vocabulary, grammar, and metric feature types, the arguments are classified and structurally modeled. Hua et al. [111] propose a framework for gener- ating arguments to opposing viewpoints. The re- trieval module of this framework comprises Query Formulation, Keyphrase Extraction, and Passage Ranking and Filtering. Subsequently, a sentence- level LSTM is trained to generate a sequence of sentences. In online discussion platforms, people also use argumentative texts to enhance their expressions. For instance, Tran et al.[115] and others [104,116, 117] employ multi-task learning to unearth argu- ments and evidence at both the micro and macro levels, enhancing persuasive power in online dis- cussions. Srivastava et al.[112] employs an attenti- on-based link prediction embedding model to mod- el the hierarchical causal relationships within com- mon argument structures in online discussions. Th- ey then utilize Transformer encoder layers to dis- cover the associations and boundaries between ar- guments. Furthermore, they employ AMPERSAN- D et al. [56] and SMOTE et al. [118] to address data imbalance issues, thereby improving model accuracy. Furthermore, Niculae et al.[113] intro- duce a factor graph model for argument mining, wherein the model concurrently learns the classifi- cation of fundamental unit types and prediction of argument relationships. Furthermore, the parame- ter structures of structured SVM and RNN can en- force structural constraints (e.g., transitivity), while also representing dependencies between adjacent relationships and propositions.Mengqi Chen et al. 19 Fig. 6 The overall framework of the model for predicting which side makes more convincing arguments [114]. 3.4.2 Argument Structure Prediction Dialogue systems of persuasive tasks commonly rely on structured knowledge concerning arguments and their relationships. Numerous researchers have demonstrated that predicting argument structures and integrating them into CogAgent can signifi- cantly enhance topic consistency, content coher- ence, and persuasiveness of persuasive dialogue co- ntents [32, 71, 72]. For example, Rach et al. [57] propose an argu- ment search technique for a debate dialogue sys- tem, which utilizes supervised learning-based re- lation classification to retrieve arguments mapped to a generic tree structure for the dialogue model. Sakai et al.[71] introduce an approach to consider human agreement and disagreement, resulting in a persuasive argument with a hierarchical argumen- tation structure. The dialogue agent selects the next action based on the user’s agreement or disagree- ment and sends the chosen action to the response generation module to generate logically consistent and persuasive dialogue. For more intensive argument modeling, Prakken et al. [29] equip dialogue agents with a five-layer argument graph, consisting of 1288 nodes, with an average of three counterarguments per node. This graph serves as the knowledge base for the pro- posed chatbot, allowing it to dynamically identify and annotate the user’s focal points on the param- eters, enabling the selection of appropriate rebuttal points. Li et al. [114] utilized factor graph mod- els to extract features of argument structures from online debate platforms. These features were then incorporated into an LSTM model to predict the most persuasive arguments, as shown in Fig 6. This study proves that the consideration of argument str- ucture plays a vital role in producing persuasive di- alogue content. 4 Datasets and Evaluation Metrics for CogAgent 4.1 Datasets for CogAgent Massive data is undeniably indispensable for train- ing high-quality CogAgent. To foster advancement in this field, numerous large-scale and high-quality datasets have been released. In this section, we cat- egorize existing datasets by application scenarios, including psychological counseling, debate, price negotiation, persuasion for donation, and product recommendation, summarized as Table 6.20 Front. Comput. Sci., 2024, 0(0): 1–36 Table 6 A review of available datasets for CogAgent. Scenario Dataset Description Psychological counseling ESConv [46] The first dataset for psychological counseling, annotated with persuasive strategies. AUGESC [119] The enhanced dataset from ESConv using LLMs with a broader range of topics. PsyQA [120] A Chinese mental health support dataset featuring annotated persuasive strategies. Debate IAC [121] Argumentative dialog dataset with curated threads, posts, and annotations. Winning Arguments [7] A metadata-rich subset of r/ChangeMyView subreddit conversations includes data on the success of user utterances in persuading the poster. DebateSum [122] A dataset for the competitive formal debate with corresponding argument and extractive summaries. Price negotiation Craigslist- Bargain [47] A human-human dialogue dataset for price negotiation where the buyer and seller are encouraged to reach an agreement to get a better deal. Negotiation- Coach [123] An additional negotiation coach based on CraigslistBargain, which monitors the exchange between two annotators and provides real-time negotiation strategy. Persuasion for donation Persuasion For Good [11] A collection of online conversations where one participant (the persuader) tries to convince the other (the persuadee) to donate to a charity. EPP4G and ETP4G [63] Datasets extending Persuasion For Good by annotating it with the emotion and politeness-strategy labels. FaceAct [124] A dataset extending Persuasion For Good by adding the utterance-level annotations that change the positive and/or the negative face of the participants in a conversation Product recommen- dation TG-ReDial [33] A dataset consisting of dialogues between a seeker and a recommender. DuRecDial [125] A human-to-human Chinese dialog dataset, which contains multiple sequential dialogues for every pair of a recommendation seeker and a recommender. INSPIRED [126] A movie recommendation dataset, consisting of human-human dialogues with an annotation scheme for persuasive strategies. 4.1.1 Datasets for Psychological Counseling Psychological counseling is a typical field of per- suasive dialogue, where CogAgent reduces users’ psychological anxiety and encourages positive emo- tions through the persuasive dialogue process. Re- searchers have released several datasets for psy- chological counseling. ESConv. ESConv6) [46] is a well-designed and rich, e ffective corpora for psychological counsel- ing, consisting of 1,053 dialogue pairs and a to- tal of 31,410 sentences. Each dialogue pair in- cludes information about the initial emotional state of the seeker, the persuasive strategies employed by the supporter during each interaction, and the con- tent of the conversation. The dataset encompasses seven distinct emotional states and eight supportive strategies, with the labeling of these strategies be- ing inspired by Hill’s Helping Skills Theory [88]. 6)https://github.com/thu-coai/ Emotional-Support-Conversation AUGESC. The limitations imposed by crowd- sourcing platforms on data themes and collection methods, along with the substantial regulatory costs, have hindered the extension of downstream dia- logue models to open-domain topics. In response, Zheng et al. augment ESConv to AUGESC7) [119] using LLMs, which comprises 65,000 dialogue ses- sions and a total of 1,738,000 utterances. It sub- stantially expands the scale of ESConv and encom- passes a broader range of topics. PsyQA. PsyQA8) [120] is a Chinese mental heal- th support dataset collected from a Chinese mental health service platform, including 22,000 questions and 56,000 lengthy, well-structured answers. In line with psychological counseling theory, PsyQA annotates some of the answer texts with persuasive strategies and further conducts in-depth analyses of the lexical features and strategic patterns within 7)https://github.com/thu-coai/AugESC 8)https://github.com/thu-coai/PsyQAMengqi Chen et al. 21 counseling responses. 4.1.2 Datasets for Debate Debates are typically persuasive scenarios in which each party of the debate organizes arguments to persuade the other party to accept his or her side’s viewpoints. Existing datasets for debate are listed as follows. Internet Argument Corpus (IAC).IAC9) [121] is a scriptless argumentative dialog dataset, com- prising 390,704 posts extracted from 11,800 dis- cussions on the online debate platform 4forums. com. Within this corpus, a manually curated subset of 2,866 threads and 130,206 posts is formed, cate- gorized based on discussion topics. Extended from IAC, IAC 210) [127] is a corpus for research in po- litical debate on Internet forums, consists of three data sets: 4forums (414K posts), ConvinceMe (65K posts), and a sample from CreateDebate (3K posts). Winning Arguments. To delve deeper into the mechanisms of changing others’ viewpoints in so- cial interactions, Tan et al. [7] introduce the Win- ing Arguments (ChangeMyView) Corpus. Wining ArgumentsCorpus is a metadata-rich subset of con- versations made in the r/ChangeMyview subreddit between 1 Jan 2013 - 7 May 2015, with informa- tion on the delta (success) of a user’s utterance in convincing the poster. There are 34911 Speakers, 293297 Utterances, and 3051 Conversations. DebateSum. DebateSum11) [122] is a dataset for the competitive formal debate, including 187,386 unique pieces of evidence with corresponding ar- gument and extractive summaries. The argument data is collected from the National Speech and De- bate Association over 7 years. 9)https://nlds.soe.ucsc.edu/iac 10)https://nlds.soe.ucsc.edu/iac2 11)https://debate.cards/ 4.1.3 Datasets for Price Negotiation Price negotiation is an everyday persuasive scenario where buyers and sellers reach their desired price through the persuasive dialog process. Datasets for price negotiation are summarized as follows. CraigslistBargain. CraigslistBargain12) [47] is a human-human dialogue dataset for price negoti- ation, which consists of 6682 dialogues, collected using Amazon Mechanical Turk (AMT) in a ne- gotiation setting where two workers were assigned the roles of buyer and seller, respectively. The buyer is additionally given a target price and both parties are encouraged to reach an agreement while each of the workers tries to get a better deal. Negotiation-Coach. Negotiation-Coach13) [123] introduce an additional negotiation coach based on CraigslistBargain, which monitors the exchange be- tween two annotators and provides real-time nego- tiation strategy recommendations to the seller for achieving better deals. 4.1.4 Datasets for Persuasion for Donation Persuasion for donation is very common in life, where the persuader persuades others to donate pro- perty or labor to charities for a public good pur- pose. Datasets for persuasion for donation are listed as follows. Persuasion for Social Good. Persuasion for So- cial Good14) [11] is a collection of online conversa- tions generated by AMT workers, where one par- ticipant (the persuader) tries to convince the other (the persuadee) to donate to a charity. This dataset 12)https://worksheets.codalab.org/worksheets/ 0x453913e76b65495d8b9730d41c7e0a0c/ 13)https://github.com/zhouyiheng11/ Negotiation-Coach 14)https://gitlab.com/ucdavisnlp/ persuasionforgood22 Front. Comput. Sci., 2024, 0(0): 1–36 contains 1017 conversations, along with demograp- hic data and responses to psychological surveys fro- m users. 300 conversations also have per-sentence human annotations of dialogue acts that pertain to the persuasion setting, and sentiment. EPP4G and ETP4G.EPP4G and ETP4G15) [63] extend Persuasion For Good by annotating it with the emotion and politeness-strategy labels. FaceAct. FaceAct16) [124] further extend Per- suasion For Good by adding the utterance-level an- notations that change the positive and /or the neg- ative face of the participants in a conversation. A face act can either raise or attack the positive face or negative face of either the speaker or the listener in the conversation. 4.1.5 Datasets for Product Recommendation Product recommendation intends to induce the rec- ommended person to accept or buy a particular pro- duct through persuasive dialogues. Datasets for product recommendation are listed as follows. TG-ReDial. TG-ReDial17) [33] consists of 10,0- 00 two-party dialogues between a seeker and a rec- ommender in the movie domain. DuRecDial. DuRecDial18) [125] is a human-to- human Chinese dialog dataset (about 10k dialogs, 156k utterances), which contains multiple sequen- tial dialogues for every pair of a recommendation seeker (user) and a recommender (bot). In each di- alogue, the recommender proactively leads a multi- type dialogue to approach recommendation targets and then makes multiple recommendations with ri- ch interaction behavior. 15)https://github.com/Mishrakshitij/PEPDS 16)https://github.com/ShoRit/face-acts 17)https://github.com/RUCAIBox/TG-ReDial 18)https://github.com/PaddlePaddle/Research/ tree/master/NLP/ACL2020-DuRecDial INSPIRED. INSPIRED19) [126] is a movie rec- ommendation dataset, consisting of 1,001 human- human dialogues with an annotation scheme for persuasive strategies based on social science the- ories. 4.2 Evaluation metrics Toward CogAgent The reasonable evaluation of the quality of CogA- gent is a challenging dilemma. Di fferent from the open-domain dialog system, the evaluation of Co- gAgent needs to be performed under di fferent per- suasion scenarios and multifaceted persuasive goal- s. This requires judging the quality of dialogue response while emphasizing the persuasive e ffects in specific persuasive contexts and assessing the adaptability and persuasiveness of the system’s cog- nitive strategies in di fferent domains. Up to now, there is no unified theory on how to effectively eval- uate CogAgent, and researchers predominantly em- ploy two kinds of evaluation methods: automatic evaluation metrics and human evaluation. We sum- marized commonly used automatic evaluation and human evaluation metrics in table 7. Notably, eval- uating CogAgent based on LLMs has also recently received significant attention. 4.2.1 Automatic Evaluation Metrics Automatic evaluation metrics evaluate the perfor- mance of CogAgent by calculating the similarity between the responses generated by CogAgent and ground truths. There are typical categories of auto- matic evaluation metrics: overlap-based methods, embedding-based methods, and learning-based tec- hniques. Overlap-based metrics. Overlap-based meth- ods measure the degree of text overlap between 19)https://github.com/sweetpeach/InspiredMengqi Chen et al. 23 Table 7 Evaluation metrics for CogAgent. Evaluation Method Category Description Metrics Automaticevaluation Overlap- based Measuring the degree of text overlap between generated responses and golden responses BLEU [80], ROUGE [82], METEOR [128], CIDEr [129] Embedding- based Evaluating the semantic similarity of embedding vectors between generated responses and reference ones Greedy Matching [130], Embedding averaging [131], Vector Extreme [132] Learning- based Employing machine learning models to predict the quality scores of generated responses, relying not only on given references ADEM [133] Human evaluation Scoring by human annotators to evaluate the quality of the generated responses with subjective judgment Fluency, Coherence, Contextualization, Emotional expression, Diversity, Persuasiveness generated responses and golden responses, with pa- rticular emphasis on the number of the same n- grams. These methods quantify the similarity of the text, especially the local structural similarity, to measure the quality of generated responses. Clas- sical Overlap-based methods include BLEU [80], ROUGE [82], METEOR [128] and CIDEr [129]. Among these, BLEU evaluates response quality by comparing the harmonic mean of n-gram overlaps between generated responses and the golden ones. BLEU is a straightforward and intuitive metric, yet it is constrained by surface features and may ex- hibit a weak capture of semantic relevance. ROUG- E calculates the length of the longest common sub- sequences between generated and golden responses and considers the precision and recall to evaluate the quality. METEOR integrates multiple aspects of information, including precision, recall, and syn- tactic structure, providing a more comprehensive evaluation. CIDEr evaluates the semantic similar- ity between generated responses and ground truths using n-gram level cosine similarity. These metrics have been widely applied in the evaluation of open- domain dialog systems, but they focus mainly on surface features of the response and may not cap- ture semantic relevance. In addition, relying solely on n-gram overlap to measure similarity may not always accurately evaluate the quality of long texts. Embedding-based metrics. Embedding-based metrics evaluate the semantic similarity of embed- ding vectors between generated responses and ref- erence ones. These methods utilize pre-trained wo- rd embedding models (e.g., BERT [134]) to map textual responses into embedding vectors, thus cap- turing the semantic relationships between the texts more accurately. Specifically, Greedy Matching [130] computes the cosine similarity of word em- beddings between each word in generated response and golden ones. Embedding averaging [131] av- erages all words in the sentence to calculate the sentence-level similarity. Vector Extrema [132] takes the most extreme value in the embedding vec- tor to represent the response to be evaluated. In essence, embedding-based metrics emphasize the semantic quality of CogAgent more than overlap- based metrics and better capture the semantic cor- relations between generated responses and refer- ences. Learning-based metrics. Learning-based met- rics employ machine learning models to predict the quality scores of generated responses, relying not only on given references but aiming to better cor- relate with human judgment. ADEM [133] is a deep model-based evaluation metric for dialogue systems. A hierarchical RNN model is trained in a semi-supervised manner to capture semantic infor- mation and contextual associations and align with the human preferences for dialogue responses.24 Front. Comput. Sci., 2024, 0(0): 1–36 In summary, automatic evaluation metrics o ffer advantages in terms of e fficiency and consistency. However, they face challenges in terms of semantic understanding, manual annotation costs, and model complexity. When selecting and applying automat- ed evaluation metrics, it is important to balance their advantages and disadvantages according to sp- ecific persuasive tasks and scenarios. 4.2.2 Human Evaluation Human evaluation involves subjective judgment an- d scoring by human annotators to evaluate the qual- ity of the generated responses. The annotators are usually domain experts and crowd workers who sub- jectively evaluate the generated responses based on specified criteria and task requirements. Compared to automatic evaluation metrics, human evaluation captures the subjectivity, emotion, and use of per- suasive strategies expressed by CogAgent. There- fore, the flexibility and highly customizable nature of human evaluation becomes a reliable means to ensure that the quality of CogAgent is robustly eval- uated. The human evaluation mainly evaluates CogA- gent in the following main aspects: fluency, coher- ence, contextualization, emotional expression, di- versity, and persuasiveness. In summary, human evaluation has advantages in terms of insightful and accurate evaluation of the quality of CogAgent. Ho- wever, it also has limitations in terms of cost and efficiency, due to the requirement of human labor and time resources. Therefore, in practical applica- tions, researchers need to strike a balance between human and automatic evaluation and choose the evaluation metrics that best suit the task require- ments. 5 Open Issues and Future Trends Though researchers have made considerable efforts to address the above challenges in CogAgent, there are still open issues to be resolved. In this section, we present some open issues and future develop- ment trends for CogAgent to promote the advance- ment of the research community. 5.1 Comprehensive Modeling of Cognitive Psy- chology Theory for CogAgent Although we have summarized some of the cogni- tive psychology theories, a comprehensive investi- gation of the cognitive mechanisms of persuasive dialogues from a cognitive psychology perspective is essential for understanding users’ cognitive weak- nesses and generating engaging persuasive dialogu- es. Many researchers have demonstrated the in- dispensability of employing specific strategies to achieve persuasive effects based on different cogni- tive psychology theories. Utilizing cognitive strate- gies, CogAgent can avoid cognitive dissonance in users and e fficiently persuade them to accept spe- cific viewpoints [11, 96, 135]. Prakken et al.[136] argue that psychological dissonance occurs when individuals are confronted with multiple conflict- ing cognitions. To alleviate this dissonance, three approaches can be used: changing cognitively rel- evant factors in the environment, introducing new cognitive elements, and changing cognitive elemen- ts in behavior. CogAgent should be aware of cogni- tive dissonance to mitigate the obstacles it creates in the persuasion process. In addition, researchers utilize the dual process theory of persuasion and guide the persuasive process with the Elaboration Likelihood Model (ELM) [27], a theory that fo- cuses on cognitive and affective appeals in persua- sion. Another noteworthy aspect is modeling theMengqi Chen et al. 25 user’s cognition. Proposing agreements or mak- ing concessions promptly facilitates the perception of the user’s cognitive state, enabling CogAgent to adapt to changes in the user’s cognition on time and avoiding the failure of the persuasive process [137]. Besides using data analysis to study the mech- anisms of persuasive dialog, we can also explore this phenomenon from the perspective of the cog- nitive functions of the human brain. Advances in neuroscience have provided valuable methods for studying the cognitive mechanisms of persuasive dialogue. As Poldrack et al.state [138], the use of electroencephalography (EEG), magnetoencephal- ography (MEG), functional magnetic resonance im- aging (fMRI), and other brain-imaging tools can deepen our understanding of how the human brain produces social behavior. Arapakis et al.[139] use brainwave recordings to measure users’ interest in news articles, and the experimental results suggest that frontal asymmetry (FFA) can objectively as- sess users’ receptive preferences for content. Ex- ploring the changes in neural signals in the brain of the persuadee during persuasive conversations to model which persuasive factors are effective in be- ing accepted by users and convincing them to adopt persuasive targets is a promising research direction. 5.2 Model Adaptivity /Generality of CogAgent Equipping CogAgent with cross-domain understan- ding and generation capabilities is a promising re- search direction. Existing CogAgent usually fo- cuses on one specific persuasion scenario, such as persuasion for social good, bargaining, and debat- ing. However, it is crucial to develop the ability of CogAgent to understand and transfer through mul- tiple domains, which enables CogAgent to dynam- ically optimize cognitive strategies based on differ- ent persuasive targets and e fficiently perform per- suasive tasks. For example, Wolfet al.[140] utilize transfer learning to jointly fine-tune multiple unsu- pervised response prediction tasks. They demon- strate the e ffectiveness of language model trans- fer learning on the PERSONA-CHAT dataset, es- pecially on the dialogue response generation task. Qian et al.[141] propose a meta-learning-based ap- proach to domain adaptive dialogue generation that learns from multiple resource-rich tasks. They uti- lize multiple resource-rich single-domain dialog dat- asets to train the dialogue system so that it can adapt to new domains with minimal training sam- ples. Therefore, improving the transferability of CogAgent across di fferent domains using transfer learning and other advanced approaches is an im- portant step towards the universal CogAgent. 5.3 Multi-party CogAgent Existing research of CogAgent has demonstrated remarkable performance in two-party conversation- al scenarios. However, in real world, multi-party conversations (MPCs) are more prevalent and re- quire CogAgent to persuade multiple participants simultaneously. Unlike existing persuasive dialog systems, multi-party dialogue scenarios require the collaboration of multiple CogAgent to e fficiently achieve persuasion targets [142–144]. Specifically, a single CogAgent is prone to be overly purpose- ful when interacting with users, which can cause the users’ resentment and resistance and hinder the realization of persuasion targets. In contrast, mul- tiple CogAgents can assume di fferent persuasive roles, cooperate, and persuade from di fferent per- spectives, thus winning users’ trust and realizing persuasion targets more effectively. Existing stud- ies have explored MPCs in open-domain dialogue systems. For instance, Ito et al. [145] construct a multi-modal and multi-party model based on GRU26 Front. Comput. Sci., 2024, 0(0): 1–36 to predict the persuasiveness of multiple members within a group during multi-party conversations, thereby providing a model paradigm for the study of multi-party dialogues. Gu et al.[146] propose a Speaker-Aware BERT (SABERT) model to select appropriate speaking targets from multiple users based on dialogue contexts. Gu et al. [147] ex- plore the problem of ”who says what to whom” in MPCs and propose a plug-and-play graphically- induced fine-tuning (GIFT) module for tuning a va- riety of PLMs for generalized multi-party conver- sation understanding. Inspired by multi-party dia- logue research, it is promising to utilize multiple CogAgents to collaborate on persuasive tasks to enhance the credibility and efficiency of the persua- sion process. Multiple CogAgents utilize persua- sive roles with complementary capabilities, strate- gies, and trust-building to enhance persuasion and effectiveness, thereby facilitating more persuasive and successful persuasion results. 5.4 Interpretability of Persuasive Process Interpretability of models can improve their credi- bility. Improving the interpretability of the persua- sion process is essential to ensure that persuasive dialogue contents produced by CogAgent are ac- cepted and adopted. In recent years, the field of Natural Language Processing (NLP) has increas- ingly focused on improving the interpretability of deep models [148, 149]. For example, Gaur et al. [150] argue that domain-specific knowledge helps to understand how deep models work. They demon- strate the utility of incorporating knowledge-infuse- d learning in knowledge graph format into complex neural networks to achieve model interpretability. Similarly, Yasunagaet al.[151] demonstrate model interpretability and structure inference by combin- ing a pre-trained language model a knowledge grap- h, and a quality assurance context into a unified graph. Currently, research on the interpretability of the persuasion process still lacks an overall frame- work. For the interpretability of the persuasion pro- cess, the e ffectiveness of the persuasion strategy can be verified from the cognitive theory, combined with the knowledge graph reasoning, and the per- suasion behavior can be analyzed interactively. 5.5 Multimodal CogAgent Multimodal perception and comprehension capa- bilities are essential for human beings in daily con- versations. By understanding the multimodal sur- roundings around them, including visual, textual, auditory, and other modal information, we humans can produce engaging dialogues to communicate messages, emotions, and attitudes with others [152, 153]. Despite the outstanding natural language un- derstanding and generation capabilities, perceiving and understanding multimodal context information is essential for natural and harmonious human-mac- hine conversation systems [154, 155]. To persuade people to change their thoughts, opinions, or at- titudes, it is crucial to understand the multimodal surroundings of users. Different environments may lead users to develop different attitudes towards thi- ngs. Combining multimodal contextual informa- tion, persuasive dialogue systems can comprehen- sively understand users’ mental states to generate more specific persuasive dialogue content. There has been extensive research on multimodal dialogue systems that enable the understanding of image or video content through dialogue [156–158]. For ex- ample, Murahari et al. adapt ViLBERT [159] to achieve multi-turn image-based dialogue, which un- derstands the image information through image-text pre-trained on multimodal datasets. Visual Chat- GPT [158] integrates ChatGPT with visual foun-Mengqi Chen et al. 27 dation models to achieve visual dialogue. Different kinds of visual information, such as images, depth images, and mask matrices, are converted into lan- guage formats based on visual foundation models and the prompt manager. Then ChatGPT takes the information from visual and textual modalities to generate dialogue responses. These e fforts have laid a solid foundation for multimodal persuasive dialogue systems. The integration of multimodal information to generate more persuasive conversa- tional content is a highly promising research direc- tion. 5.6 Data and Model Co-Optimization for CogA- gent The huge impact of LLMs (e.g., ChatGPT) in the field of dialog systems has sparked the enthusiasm of researchers and has been widely used in many domains [160–162]. For example, Lianget al.[161] rewrite the policy code for controlling a robot us- ing LLMs. The policy code can receive and un- derstand commands and then outputs the execution code to the API to achieve coherent control of the robot’s actions through the classical chain logic. Similarly, Wen et al.[162] combine the common- sense knowledge implicit in LLMs with the domai- n-specific knowledge of mobile applications to re- alize hands-free speech-based interaction between users and smartphones. LLM can be surprisingly useful in a variety of domains. To develop a high- quality CogAgent, we can utilize LLMs to gener- ate large-scale persuasive dialogue data to quickly validate the algorithm at an early stage. Since the capability of LLMs stems from massive amounts of data, retraining this data is hugely expensive. Therefore, the persuasion process also needs to be modeled to e fficiently and accurately perform the persuasion task. The combination of data-driven LLMs and model-driven persuasion process is the most efficient way to develop intelligent CogAgent. Future research directions for combining LLMs and model-driven persuasion processes include issues such as when to employ the generation abilities of LLMs, when model constraints are needed, and the rules and timing of collaboration between LLMs and the persuasion process. 5.7 Construction of standardized datasets and be- nchmarks Despite the significant progress researchers have made in CogAgent, datasets, and benchmarks for the study of CogAgent are still scarce. The rel- atively small size of many existing datasets (e.g., Persuasion for good [11]) limits the performance of the model in a wider range of applications. The limited amount of data hinders the ability to cap- ture the full complexity and diversity of persuasive dialogue. Moreover, the lack of detailed annota- tions about cognitive strategies in existing datasets creates challenges for training persuasive dialogue agents. Building large-scale, high-quality datasets of persuasive dialogues with rich cognitive strategy annotations is indispensable for the development of CogAgent. Combining the superior text genera- tion capabilities of LLMs [163, 164] is a potential way to build large-scale and high-quality datasets for CogAgent. 6 Conclusion Persuasion is an essential ability in human social communication, and people often skillfully persua- de others to accept their standpoints, views, or per- spectives for various purposes. Consequently, per- suasive dialogue systems have become an engaging research direction. In this paper, we have made a systematic survey of CogAgent. We first present28 Front. Comput. Sci., 2024, 0(0): 1–36 some representative cognitive psychology theories to guide the design of CogAgent at the principle level and formalize the necessary cognitive strate- gies for generating highly persuasive dialogue con- tents, including the persuasion strategy, the topic path planning strategy, and the argument structure prediction strategy. Based on the formalized def- inition and generic architecture of CogAgent, we comprehensively investigate representative works by categorizing cognitive strategies. The available datasets and evaluation metrics for CogAgent are also summarized. Despite significant progress, the research of CogAgent is still in the early stage and massive open issues and prospective future trends to be explored, such as model adaptivity/generality of CogAgent, multi-party CogAgent, and multimo- dal CogAgent. Acknowledgements This work was partially supported by the National Science Fund for Distinguished Young Schol- ars(62025205), and the National Natural Science Foundation of China (No. 62032020). References 1. Guo B, Wang H, Ding Y , Wu W, Hao S, Sun Y , Yu Z. Conditional text generation for harmonious human- machine interaction. ACM Transactions on Intelligent Systems and Technology (TIST), 2021, 12(2): 1–50 2. Huang M, Zhu X, Gao J. Challenges in building in- telligent open-domain dialog systems. ACM Trans- actions on Information Systems (TOIS), 2020, 38(3): 1–32 3. Petty R E, Cacioppo J T, Petty R E, Cacioppo J T. The elaboration likelihood model of persuasion. Springer, 1986 4. Fogg B J. Persuasive technology: using computers to change what we think and do. Ubiquity, 2002, 2002(December): 2 5. IJsselsteijn W, De Kort Y , Midden C, Eggen B, Van Den Hoven E. Persuasive technology for human well- being: setting the scene. In: Persuasive Technology: First International Conference on Persuasive Tech- nology for Human Well-Being, PERSUASIVE 2006, Eindhoven, The Netherlands, May 18-19, 2006. Pro- ceedings 1. 2006, 1–5 6. Fogg B J. Mass interpersonal persuasion: An early view of a new phenomenon. In: Persuasive Technol- ogy: Third International Conference, PERSUASIVE 2008, Oulu, Finland, June 4-6, 2008. Proceedings 3. 2008, 23–34 7. Tan C, Niculae V , Danescu-Niculescu-Mizil C, Lee L. Winning arguments: Interaction dynamics and persua- sion strategies in good-faith online discussions. In: Proceedings of the 25th international conference on world wide web. 2016, 613–624 8. Hidey C, Musi E, Hwang A, Muresan S, McKeown K. Analyzing the semantic types of claims and premises in an online persuasive forum. In: Proceedings of the 4th Workshop on Argument Mining. 2017, 11–21 9. Torning K, Oinas-Kukkonen H. Persuasive system de- sign: state of the art and future directions. In: Proceed- ings of the 4th international conference on persuasive technology. 2009, 1–8 10. Eagly A H, Chaiken S. Cognitive theories of persua- sion. In: Advances in experimental social psychology, volume 17, 267–359. Elsevier, 1984 11. Wang X, Shi W, Kim R, Oh Y , Yang S, Zhang J, Yu Z. Persuasion for good: Towards a personalized persua- sive dialogue system for social good. In: Proceed- ings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019, 5635–5649 12. Shi W, Wang X, Oh Y J, Zhang J, Sahay S, Yu Z. Ef- fects of persuasive dialogues: testing bot identities and inquiry strategies. In: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 2020, 1–13 13. Joshi R, Balachandran V , Vashishth S, Black A, Tsvetkov Y . Dialograph: Incorporating interpretable strategy-graph networks into negotiation dialogues. In: International Conference on Learning Represen- tations (ICLR). 2021 14. Min B, Ross H, Sulem E, Veyseh A P B, Nguyen T H, Sainz O, Agirre E, Heintz I, Roth D. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Computing Sur- veys, 2021 15. Zhao W X, Zhou K, Li J, Tang T, Wang X, Hou Y , Min Y , Zhang B, Zhang J, Dong Z, others . A survey of large language models. arXiv preprint arXiv:2303.18223, 2023 16. Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y , Bashlykov N, Batra S, Bhargava P, Bhosale S, others . Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023 17. Bai Y , Kadavath S, Kundu S, Askell A, Kernion J, Jones A, Chen A, Goldie A, Mirhoseini A, McKin-Mengqi Chen et al. 29 non C, others . Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022 18. Zhou C, Li Q, Li C, Yu J, Liu Y , Wang G, Zhang K, Ji C, Yan Q, He L, others . A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. arXiv preprint arXiv:2302.09419, 2023 19. Ray P P. Chatgpt: A comprehensive review on back- ground, applications, key challenges, bias, ethics, lim- itations and future scope. Internet of Things and Cyber-Physical Systems, 2023 20. Li J, Han D, Guo Z, Qiao B, Wu G. Generating empathetic responses through emotion tracking and constraint guidance. Frontiers of Computer Science, 2024, 18(2) 21. Wang W, Feng S, Song K, Wang D, Li S. Infor- mative and diverse emotional conversation generation with variational recurrent pointer-generator. Frontiers of Computer Science, 2022, 16: 1–3 22. Vaithilingam P, Zhang T, Glassman E L. Expectation vs. experience: Evaluating the usability of code gen- eration tools powered by large language models. In: Chi conference on human factors in computing sys- tems extended abstracts. 2022, 1–7 23. Ni A, Iyer S, Radev D, Stoyanov V , Yih W t, Wang S, Lin X V . Lever: Learning to verify language-to-code generation with execution. In: International Confer- ence on Machine Learning. 2023, 26106–26128 24. Yuan A, Coenen A, Reif E, Ippolito D. Wordcraft: story writing with large language models. In: 27th In- ternational Conference on Intelligent User Interfaces. 2022, 841–852 25. Dergaa I, Chamari K, Zmijewski P, Saad H B. From human writing to artificial intelligence generated text: examining the prospects and potential threats of chat- gpt in academic writing. Biology of Sport, 2023, 40(2): 615–622 26. Bless H, Bohner G, Schwarz N, Strack F. Mood and persuasion: A cognitive response analysis. Personality and social psychology bulletin, 1990, 16(2): 331–345 27. Petty R E, Bri ˜nol P. Emotion and persuasion: Cog- nitive and meta-cognitive processes impact attitudes. Cognition and Emotion, 2015, 29(1): 1–26 28. Qin J, Ye Z, Tang J, Liang X. Dynamic knowledge routing network for target-guided open-domain con- versation. In: Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 8657–8664 29. Prakken H, others . A persuasive chatbot using a crowd-sourced argument graph and concerns. Com- putational Models of Argument, 2020, 326: 9 30. Dijkstra A. The psychology of tailoring-ingredients in computer-tailored persuasion. Social and personality psychology compass, 2008, 2(2): 765–784 31. Kolenik T, Gams M. Intelligent cognitive assistants for attitude and behavior change support in mental health: state-of-the-art technical review. Electronics, 2021, 10(11): 1250 32. Slonim N, Bilu Y , Alzate C, Bar-Haim R, Bogin B, Bonin F, Choshen L, Cohen-Karlik E, Dankin L, Edel- stein L, others . An autonomous debating system. Na- ture, 2021, 591(7850): 379–384 33. Zhou K, Zhou Y , Zhao W X, Wang X, Wen J R. To- wards topic-guided conversational recommender sys- tem. In: Proceedings of the 28th International Confer- ence on Computational Linguistics. 2020, 4128–4139 34. Kang D, Balakrishnan A, Shah P, Crook P, Boureau Y L, Weston J. Recommendation as a communica- tion game: Self-supervised bot-play for goal-oriented dialogue. In: 2019 Conference on Empirical Meth- ods in Natural Language Processing and 9th Interna- tional Joint Conference on Natural Language Process- ing, EMNLP-IJCNLP 2019. 2020, 1951–1961 35. Chen M, Shi W, Yan F, Hou R, Zhang J, Sahay S, Yu Z. Seamlessly integrating factual information and social content with persuasive dialogue. In: Proceed- ings of the 2nd Conference of the Asia-Pacific Chap- ter of the Association for Computational Linguistics and the 12th International Joint Conference on Natu- ral Language Processing. 2022, 399–413 36. Duerr S, Gloor P A. Persuasive natural lan- guage generation–a literature review. arXiv preprint arXiv:2101.05786, 2021 37. Zhan H, Wang Y , Feng T, Hua Y , Sharma S, Li Z, Qu L, Ha ffari G. Let’s negotiate! a sur- vey of negotiation dialogue systems. arXiv preprint arXiv:2212.09072, 2022 38. Deng Y , Lei W, Lam W, Chua T S. A survey on proactive dialogue systems: Problems, methods, and prospects. arXiv preprint arXiv:2305.02750, 2023 39. Cialdini R. Pre-suasion: A revolutionary way to influ- ence and persuade. Simon and Schuster, 2016 40. Bilu Y , Gera A, Hershcovich D, Sznajder B, Lahav D, Moshkowich G, Malet A, Gavron A, Slonim N. Ar- gument invention from first principles. In: Proceed- ings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019, 1013–1026 41. Premack D, Woodru ff G. Does the chimpanzee have a theory of mind? Behavioral and brain sciences, 1978, 1(4): 515–526 42. Wu J, Chen Z, Deng J, Sabour S, Huang M. Coke: A cognitive knowledge graph for machine theory of mind. arXiv preprint arXiv:2305.05390, 2023 43. Sap M, Le Bras R, Fried D, Choi Y . Neural theory-of-30 Front. Comput. Sci., 2024, 0(0): 1–36 mind? on the limits of social intelligence in large lms. In: Proceedings of the 2022 Conference on Empiri- cal Methods in Natural Language Processing. 2022, 3762–3780 44. Roman H R, Bisk Y , Thomason J, Celikyilmaz A, Gao J. Rmm: A recursive mental model for dialogue navi- gation. In: Findings of the Association for Computa- tional Linguistics: EMNLP 2020. 2020, 1732–1745 45. Campbell G. The philosophy of rhetoric. SIU Press, 1988 46. Liu S, Zheng C, Demasi O, Sabour S, Li Y , Yu Z, Jiang Y , Huang M. Towards emotional support dialog sys- tems. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan- guage Processing (V olume 1: Long Papers). 2021, 3469–3483 47. He H, Chen D, Balakrishnan A, Liang P. Decou- pling strategy and generation in negotiation dialogues. In: 2018 Conference on Empirical Methods in Natu- ral Language Processing, EMNLP 2018. 2018, 2333– 2343 48. Joshi R, Balachandran V , Vashishth S, Black A, Tsvetkov Y . Dialograph: Incorporating interpretable strategy-graph networks into negotiation dialogues. In: International Conference on Learning Represen- tations. 2020 49. Cheng Y , Liu W, Li W, Wang J, Zhao R, Liu B, Liang X, Zheng Y . Improving multi-turn emotional support dialogue generation with lookahead strategy planning. In: Proceedings of the 2022 Conference on Empiri- cal Methods in Natural Language Processing. 2022, 3014–3026 50. Cacioppo J T, Petty R E. E ffects of message repetition and position on cognitive response, recall, and persua- sion. Journal of personality and Social Psychology, 1979, 37(1): 97 51. Cialdini R B, Cialdini R B. Influence: The psychology of persuasion. volume 55. Collins New York, 2007 52. Ni J, Pandelea V , Young T, Zhou H, Cambria E. Hitkg: Towards goal-oriented conversations via multi- hierarchy learning. In: Proceedings of the AAAI con- ference on artificial intelligence. 2022, 11112–11120 53. Tang Z H, Yeh M Y . Eagle: Enhance target-oriented dialogs by global planning and topic flow integration. In: Proceedings of the 32nd ACM International Con- ference on Information and Knowledge Management. 2023, 2402–2411 54. Petty R E, Cacioppo J T. Communication and persua- sion: Central and peripheral routes to attitude change. Springer Science & Business Media, 2012 55. Swanson R, Ecker B, Walker M. Argument mining: Extracting arguments from online dialogue. In: Pro- ceedings of the 16th annual meeting of the special in- terest group on discourse and dialogue. 2015, 217–226 56. Chakrabarty T, Hidey C, Muresan S, Mckeown K, Hwang A. Ampersand: Argument mining for persua- sive online discussions. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Lan- guage Processing and the 9th International Joint Con- ference on Natural Language Processing (EMNLP- IJCNLP). 2019, 2933–2943 57. Rach N, Schindler C, Feustel I, Daxenberger J, Minker W, Ultes S. From argument search to argumentative dialogue: A topic-independent approach to argument acquisition for dialogue systems. In: Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue. 2021, 368–379 58. Wambsganss T, Kueng T, Soellner M, Leimeister J M. Arguetutor: An adaptive dialog-based learning system for argumentation skills. In: Proceedings of the 2021 CHI conference on human factors in computing sys- tems. 2021, 1–13 59. Ni J, Young T, Pandelea V , Xue F, Cambria E. Recent advances in deep learning based dialogue systems: A systematic survey. Artificial intelligence review, 2023, 56(4): 3055–3155 60. Bubeck S, Chandrasekaran V , Eldan R, Gehrke J, Horvitz E, Kamar E, Lee P, Lee Y T, Li Y , Lund- berg S, others . Sparks of artificial general intelli- gence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023 61. Hochreiter S, Schmidhuber J. Long short-term mem- ory. Neural computation, 1997, 9(8): 1735–1780 62. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez A N, Kaiser Ł, Polosukhin I. Attention is all you need. Advances in neural information processing systems, 2017, 30 63. Mishra K, Samad A M, Totala P, Ekbal A. Pepds: A polite and empathetic persuasive dialogue system for charity donation. In: Proceedings of the 29th In- ternational Conference on Computational Linguistics. 2022, 424–440 64. Walker E R, McGee R E, Druss B G. Mortality in mental disorders and global disease burden implica- tions: a systematic review and meta-analysis. JAMA psychiatry, 2015, 72(4): 334–341 65. Xu B, Zhuang Z. Survey on psychotherapy chatbots. Concurrency and Computation: Practice and Experi- ence, 2022, 34(7): e6170 66. Liang Y , Liu L, Ji Y , Huangfu L, Zeng D D. Identify- ing emotional causes of mental disorders from socialMengqi Chen et al. 31 media for effective intervention. Information Process- ing & Management, 2023, 60(4): 103407 67. Zhou J, Zheng C, Wang B, Zhang Z, Huang M. Case: Aligning coarse-to-fine cognition and a ffection for empathetic response generation. arXiv preprint arXiv:2208.08845, 2022 68. Bosselut A, Rashkin H, Sap M, Malaviya C, Celikyil- maz A, Choi Y . Comet: Commonsense transformers for automatic knowledge graph construction. In: Pro- ceedings of the 57th Annual Meeting of the Associa- tion for Computational Linguistics. 2019 69. Speer R, Chin J, Havasi C. Conceptnet 5.5: An open multilingual graph of general knowledge. In: Proceed- ings of the AAAI conference on artificial intelligence. 2017 70. Nortio E, Jasinskaja-Lahti I, H ¨am¨al¨ainen M, Pakkasvirta J. Fear of the russian bear? negoti- ating finnish national identity online. Nations and Nationalism, 2022, 28(3): 861–876 71. Sakai K, Higashinaka R, Yoshikawa Y , Ishiguro H, Tomita J. Hierarchical argumentation structure for persuasive argumentative dialogue generation. IE- ICE TRANSACTIONS on Information and Systems, 2020, 103(2): 424–434 72. Rach N, Minker W, Ultes S. Increasing the natural- ness of an argumentative dialogue system through ar- gument chains. In: Computational Models of Argu- ment, 331–338. IOS Press, 2020 73. Gupta P, Jhamtani H, Bigham J P. Target-guided di- alogue response generation using commonsense and data augmentation. In: Findings of the Association for Computational Linguistics: NAACL 2022. 2022, 1301–1317 74. Mondal P. A unifying perspective on perception and cognition through linguistic representations of emo- tion. Frontiers in Psychology, 2022, 13: 768170 75. Shettleworth S J. Cognition, evolution, and behavior. Oxford university press, 2009 76. Nguyen H, Mastho ff J. Designing persuasive dialogue systems: Using argumentation with care. In: Per- suasive Technology: Third International Conference, PERSUASIVE 2008, Oulu, Finland, June 4-6, 2008. Proceedings 3. 2008, 201–212 77. Orji R. Why are persuasive strategies e ffective? exploring the strengths and weaknesses of socially- oriented persuasive strategies. In: Persuasive Technol- ogy: Development and Implementation of Personal- ized Technologies to Change Attitudes and Behaviors: 12th International Conference, PERSUASIVE 2017, Amsterdam, The Netherlands, April 4–6, 2017, Pro- ceedings 12. 2017, 253–266 78. Ham J, Bokhorst R, Cuijpers R, Van Der Pol D, Cabibihan J J. Making robots persuasive: the influence of combining persuasive strategies (gazing and ges- tures) by a storytelling robot on its persuasive power. In: Social Robotics: Third International Conference, ICSR 2011, Amsterdam, The Netherlands, November 24-25, 2011. Proceedings 3. 2011, 71–83 79. Samad A M, Mishra K, Firdaus M, Ekbal A. Em- pathetic persuasion: reinforcing empathy and persua- siveness in dialogue systems. In: Findings of the Association for Computational Linguistics: NAACL 2022. 2022, 844–856 80. Papineni K, Roukos S, Ward T, Zhu W J. Bleu: a method for automatic evaluation of machine transla- tion. In: Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 2002, 311–318 81. Banerjee S, Lavie A. Meteor: An automatic metric for mt evaluation with improved correlation with hu- man judgments. In: Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for ma- chine translation and/or summarization. 2005, 65–72 82. Lin C Y , Hovy E. Automatic evaluation of summaries using n-gram co-occurrence statistics. In: Proceedings of the 2003 human language technology conference of the North American chapter of the association for computational linguistics. 2003, 150–157 83. Yu X, Chen M, Yu Z. Prompt-based monte-carlo tree search for goal-oriented dialogue policy planning. arXiv preprint arXiv:2305.13660, 2023 84. Zhou Y , Tsvetkov Y , Black A W, Yu Z. Augmenting non-collaborative dialog systems with explicit seman- tic and strategic dialog history. In: International Con- ference on Learning Representations. 2019 85. Jia M, Chen Q, Jing L, Fu D, Li R. Knowledge- enhanced memory model for emotional support con- versation. arXiv preprint arXiv:2310.07700, 2023 86. Tu Q, Li Y , Cui J, Wang B, Wen J R, Yan R. Misc: A mixed strategy-aware model integrating comet for emotional support conversation. In: Proceedings of the 60th Annual Meeting of the Association for Com- putational Linguistics (V olume 1: Long Papers). 2022, 308–319 87. Greene J O, Burleson B R. Handbook of communi- cation and social interaction skills. Psychology Press, 2003 88. Hill C E. Helping skills: Facilitating, exploration, in- sight, and action. American Psychological Associa- tion, 2009 89. Kipf T N, Welling M. Semi-supervised classification with graph convolutional networks. In: International32 Front. Comput. Sci., 2024, 0(0): 1–36 Conference on Learning Representations. 2016 90. Veli ˇckovi´c P, Cucurull G, Casanova A, Romero A, Li`o P, Bengio Y . Graph attention networks. In: Interna- tional Conference on Learning Representations. 2018 91. Wu Z, Pan S, Chen F, Long G, Zhang C, Philip S Y . A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 2020, 32(1): 4–24 92. Wu L, Chen Y , Shen K, Guo X, Gao H, Li S, Pei J, Long B, others . Graph neural networks for natu- ral language processing: A survey. Foundations and Trends® in Machine Learning, 2023, 16(2): 119–328 93. Wang H, Guo B, Liu J, Ding Y , Yu Z. Towards infor- mative and diverse dialogue systems over hierarchi- cal crowd intelligence knowledge graph. ACM Trans- actions on Knowledge Discovery from Data, 2023, 17(7): 1–25 94. Liu C, Gao C, Yuan Y , Bai C, Luo L, Du X, Shi X, Luo H, Jin D, Li Y . Modeling persuasion factor of user decision for recommendation. In: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2022, 3366–3376 95. Zheng C, Liu Y , Chen W, Leng Y , Huang M. Comae: A multi-factor hierarchical framework for empathetic response generation. In: Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021, 813–824 96. Yang R, Chen J, Narasimhan K. Improving dialog sys- tems for negotiation with personality modeling. In: Proceedings of the 59th Annual Meeting of the As- sociation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (V olume 1: Long Papers). 2021, 681–693 97. Zheng Z, Liao L, Deng Y , Nie L. Building emotional support chatbots in the era of llms. arXiv preprint arXiv:2308.11584, 2023 98. Xu J, Wang H, Niu Z, Wu H, Che W. Knowledge graph grounded goal planning for open-domain conversation generation. In: Proceedings of the AAAI conference on artificial intelligence. 2020, 9338–9345 99. Liu J, Pan F, Luo L. Gochat: Goal-oriented chat- bots with hierarchical reinforcement learning. In: Pro- ceedings of the 43rd International ACM SIGIR Con- ference on Research and Development in Information Retrieval. 2020, 1793–1796 100. Lei W, Zhang Y , Song F, Liang H, Mao J, Lv J, Yang Z, Chua T S. Interacting with non-cooperative user: A new paradigm for proactive dialogue policy. In: Pro- ceedings of the 45th International ACM SIGIR Con- ference on Research and Development in Information Retrieval. 2022, 212–222 101. Zhong P, Liu Y , Wang H, Miao C. Keyword-guided neural conversational model. In: Proceedings of the AAAI Conference on Artificial Intelligence. 2021, 14568–14576 102. Zou Y , Liu Z, Hu X, Zhang Q. Thinking clearly, talk- ing fast: Concept-guided non-autoregressive genera- tion for open-domain dialogue systems. In: Proceed- ings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021, 2215–2226 103. Wang J, Lin D, Li W. Dialogue planning via brownian bridge stochastic process for goal-directed proactive dialogue. arXiv preprint arXiv:2305.05290, 2023 104. Wang S, Yin Z, Zhang W, Zheng D, Li X. Two stage learning for argument pairs extraction. In: Natural Language Processing and Chinese Computing: 10th CCF International Conference, NLPCC 2021, Qing- dao, China, October 13–17, 2021, Proceedings, Part II 10. 2021, 538–547 105. Yang Z, Wang B, Zhou J, Tan Y , Zhao D, Huang K, He R, Hou Y . Topkg: Target-oriented dialog via global planning on knowledge graph. In: Proceedings of the 29th International Conference on Computational Lin- guistics. 2022, 745–755 106. Tang J, Zhao T, Xiong C, Liang X, Xing E, Hu Z. Target-guided open-domain conversation. In: Pro- ceedings of the 57th Annual Meeting of the Associa- tion for Computational Linguistics. 2019, 5624–5634 107. Wang J, Lin D, Li W. A target-driven planning ap- proach for goal-directed dialog systems. IEEE Trans- actions on Neural Networks and Learning Systems, 2023 108. Wang J, Lin D, Li W. Dialogue planning via brownian bridge stochastic process for goal-directed proactive dialogue. 2023 109. Vecchi E M, Falk N, Jundi I, Lapesa G. Towards ar- gument mining for social good: A survey. In: Pro- ceedings of the 59th Annual Meeting of the Associa- tion for Computational Linguistics and the 11th Inter- national Joint Conference on Natural Language Pro- cessing (V olume 1: Long Papers). 2021, 1338–1352 110. Al Khatib K, Wachsmuth H, Hagen M, K ¨ohler J, Stein B. Cross-domain mining of argumentative text through distant supervision. In: Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human lan- guage technologies. 2016, 1395–1404 111. Hua X, Hu Z, Wang L. Argument generation with re- trieval, planning, and realization. In: Proceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics. 2019, 2661–2672 112. Srivastava P, Bhatnagar P, Goel A. Argument min-Mengqi Chen et al. 33 ing using bert and self-attention based embeddings. In: 2022 4th International Conference on Advances in Computing, Communication Control and Networking (ICAC3N). 2022, 1536–1540 113. Niculae V , Park J, Cardie C. Argument mining with structured svms and rnns. In: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers). 2017, 985–995 114. Li J, Durmus E, Cardie C. Exploring the role of argu- ment structure in online debate persuasion. In: Pro- ceedings of the 2020 Conference on Empirical Meth- ods in Natural Language Processing (EMNLP). 2020, 8905–8912 115. Tran N, Litman D. Multi-task learning in argument mining for persuasive online discussions. In: Proceed- ings of the 8th Workshop on Argument Mining. 2021, 148–153 116. Cheng L, Bing L, He R, Yu Q, Zhang Y , Si L. Iam: A comprehensive and large-scale dataset for integrated argument mining tasks. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers). 2022, 2277– 2287 117. Sun J, Zhu Q, Bao J, Wu J, Yang C, Wang R, Xu R. A hierarchical sequence labeling model for argument pair extraction. In: Natural Language Processing and Chinese Computing: 10th CCF International Confer- ence, NLPCC 2021, Qingdao, China, October 13–17, 2021, Proceedings, Part II 10. 2021, 472–483 118. Chawla N V , Bowyer K W, Hall L O, Kegelmeyer W P. Smote: synthetic minority over-sampling tech- nique. Journal of artificial intelligence research, 2002, 16: 321–357 119. Zheng C, Sabour S, Wen J, Zhang Z, Huang M. Augesc: Dialogue augmentation with large language models for emotional support conversation. In: Find- ings of the Association for Computational Linguistics: ACL 2023. 2023, 1552–1568 120. Sun H, Lin Z, Zheng C, Liu S, Huang M. Psyqa: A chinese dataset for generating long counseling text for mental health support. In: Findings of the Asso- ciation for Computational Linguistics: ACL-IJCNLP 2021. 2021, 1489–1503 121. Walker M, Tree J E F, Anand P, Abbott R, King J. A corpus for research on deliberation and debate. In: Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12). 2012, 812–817 122. Roush A, Balaji A. Debatesum: A large-scale argu- ment mining and summarization dataset. In: Proceed- ings of the 7th Workshop on Argument Mining. 2020, 1–7 123. Zhou Y , He H, Black A W, Tsvetkov Y . A dynamic strategy coach for effective negotiation. In: 20th An- nual Meeting of the Special Interest Group on Dis- course and Dialogue. 2019, 367 124. Dutt R, Joshi R, Rose C. Keeping up appearances: Computational modeling of face acts in persuasion oriented discussions. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP). 2020, 7473–7485 125. Liu Z, Wang H, Niu Z Y , Wu H, Che W, Liu T. To- wards conversational recommendation over multi-type dialogs. In: Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics. 2020, 1036–1049 126. Hayati S A, Kang D, Zhu Q, Shi W, Yu Z. In- spired: Toward sociable recommendation dialog sys- tems. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020, 8142–8152 127. Abbott R, Ecker B, Anand P, Walker M. Internet ar- gument corpus 2.0: An sql schema for dialogic social media and the corpora to go with it. In: Proceedings of the Tenth International Conference on Language Re- sources and Evaluation (LREC’16). 2016, 4445–4452 128. Lavie A, Agarwal A. Meteor: An automatic metric for mt evaluation with high levels of correlation with hu- man judgments. In: Proceedings of the second work- shop on statistical machine translation. 2007, 228–231 129. Vedantam R, Lawrence Zitnick C, Parikh D. Cider: Consensus-based image description evaluation. In: Proceedings of the IEEE conference on computer vi- sion and pattern recognition. 2015, 4566–4575 130. Rus V , Lintean M. An optimal assessment of natu- ral language student input using word-to-word simi- larity metrics. In: Intelligent Tutoring Systems: 11th International Conference, ITS 2012, Chania, Crete, Greece, June 14-18, 2012. Proceedings 11. 2012, 675– 676 131. Wieting J, Bansal M, Gimpel K, Livescu K. Towards universal paraphrastic sentence embeddings. arXiv preprint arXiv:1511.08198, 2015 132. Forgues G, Pineau J, Larchev ˆeque J M, Tremblay R. Bootstrapping dialog systems with word embeddings. In: Nips, modern machine learning and natural lan- guage processing workshop. 2014, 168 133. Lowe R, Noseworthy M, Serban I V , Angelard-Gontier N, Bengio Y , Pineau J. Towards an automatic turing test: Learning to evaluate dialogue responses. In: Pro- ceedings of the 55th Annual Meeting of the Associa- tion for Computational Linguistics (V olume 1: Long34 Front. Comput. Sci., 2024, 0(0): 1–36 Papers). 2017 134. Devlin J, Chang M W, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. In: Proceedings of the 2019 Conference of the North American Chapter of the As- sociation for Computational Linguistics: Human Lan- guage Technologies, V olume 1 (Long and Short Pa- pers). 2019, 4171–4186 135. Chen Y , Deng S, Kwak D H, Elnoshokaty A, Wu J. A multi-appeal model of persuasion for online petition success: A linguistic cue-based approach. Journal of the Association for Information Systems, 2019, 20(2): 105–131 136. Jing Wen T, Kim E, Wu L, Dodoo N A. Activating persuasion knowledge in native advertising: the influ- ence of cognitive load and disclosure language. Inter- national Journal of Advertising, 2020, 39(1): 74–93 137. Thimm M. Strategic argumentation in multi-agent sys- tems. KI-K ¨unstliche Intelligenz, 2014, 28: 159–168 138. Poldrack R A, Farah M J. Progress and challenges in probing the human brain. Nature, 2015, 526(7573): 371–379 139. Arapakis I, Barreda-Angeles M, Pereda-Ba ˜nos A. In- terest as a proxy of engagement in news reading: Spectral and entropy analyses of eeg activity patterns. IEEE Transactions on A ffective Computing, 2017, 10(1): 100–114 140. Wolf T, Sanh V , Chaumond J, Delangue C. Trans- fertransfo: A transfer learning approach for neural network based conversational agents. arXiv preprint arXiv:1901.08149, 2019 141. Qian K, Yu Z. Domain adaptive dialog generation via meta learning. In: Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics. 2019, 2639–2649 142. Shi Z, Huang M. A deep sequential model for dis- course parsing on multi-party dialogues. In: Pro- ceedings of the AAAI Conference on Artificial Intel- ligence. 2019, 7007–7014 143. Ju D, Feng S, Lv P, Wang D, Zhang Y . Learning to improve persona consistency in multi-party dia- logue generation via text knowledge enhancement. In: Proceedings of the 29th International Conference on Computational Linguistics. 2022, 298–309 144. Yuan L, Chen F, Zhang Z, Yu Y . Communication- robust multi-agent learning by adaptable auxiliary multi-agent adversary generation. Frontiers of Com- puter Science, 2024, 18(6): 186331 145. Ito A, Nakano Y I, Nihei F, Sakato T, Ishii R, Fukayama A, Nakamura T. Predicting persuasive- ness of participants in multiparty conversations. In: 27th International Conference on Intelligent User In- terfaces. 2022, 85–88 146. Gu J C, Li T, Liu Q, Ling Z H, Su Z, Wei S, Zhu X. Speaker-aware bert for multi-turn response selec- tion in retrieval-based chatbots. In: Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2020, 2041–2044 147. Gu J C, Ling Z H, Liu Q, Liu C, Hu G. Gift: Graph- induced fine-tuning for multi-party conversation un- derstanding. arXiv preprint arXiv:2305.09360, 2023 148. Belinkov Y , Gehrmann S, Pavlick E. Interpretability and analysis in neural nlp. In: Proceedings of the 58th annual meeting of the association for computational linguistics: tutorial abstracts. 2020, 1–5 149. Jacovi A, Goldberg Y . Towards faithfully interpretable nlp systems: How should we define and evaluate faith- fulness? In: Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics. 2020, 4198–4205 150. Gaur M, Faldu K, Sheth A. Semantics of the black- box: Can knowledge graphs help make deep learning systems more interpretable and explainable? IEEE Internet Computing, 2021, 25(1): 51–59 151. Yasunaga M, Ren H, Bosselut A, Liang P, Leskovec J. Qa-gnn: Reasoning with language models and knowl- edge graphs for question answering. In: Proceedings of the 2021 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies. 2021, 535–546 152. Quek F, McNeill D, Bryll R, Duncan S, Ma X F, Kir- bas C, McCullough K E, Ansari R. Multimodal human discourse: gesture and speech. ACM Transactions on Computer-Human Interaction (TOCHI), 2002, 9(3): 171–193 153. Turk M. Multimodal interaction: A review. Pattern recognition letters, 2014, 36: 189–195 154. Jaimes A, Sebe N. Multimodal human–computer in- teraction: A survey. Computer vision and image un- derstanding, 2007, 108(1-2): 116–134 155. Baltru ˇsaitis T, Ahuja C, Morency L P. Multimodal ma- chine learning: A survey and taxonomy. IEEE trans- actions on pattern analysis and machine intelligence, 2018, 41(2): 423–443 156. Qi J, Niu Y , Huang J, Zhang H. Two causal principles for improving visual dialog. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition. 2020, 10860–10869 157. Alamri H, Cartillier V , Das A, Wang J, Cherian A, Essa I, Batra D, Marks T K, Hori C, Anderson P, oth- ers . Audio visual scene-aware dialog. In: Proceedings of the IEEE/CVF Conference on Computer Vision andMengqi Chen et al. 35 Pattern Recognition. 2019, 7558–7567 158. Wu C, Yin S, Qi W, Wang X, Tang Z, Duan N. Vi- sual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023 159. Lu J, Batra D, Parikh D, Lee S. Vilbert: Pretrain- ing task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural infor- mation processing systems, 2019, 32 160. Pal S, Bhattacharya M, Lee S S, Chakraborty C. A domain-specific next-generation large language model (llm) or chatgpt is required for biomedical engineer- ing and research. Annals of Biomedical Engineering, 2023, 1–4 161. Liang J, Huang W, Xia F, Xu P, Hausman K, Ichter B, Florence P, Zeng A. Code as policies: Language model programs for embodied control. In: 2023 IEEE International Conference on Robotics and Automation (ICRA). 2023, 9493–9500 162. Wen H, Li Y , Liu G, Zhao S, Yu T, Li T J J, Jiang S, Liu Y , Zhang Y , Liu Y . Empowering llm to use smart- phone for intelligent task automation. arXiv preprint arXiv:2308.15272, 2023 163. Kim H, Hessel J, Jiang L, Lu X, Yu Y , Zhou P, Bras R L, Alikhani M, Kim G, Sap M, others . Soda: Million-scale dialogue distillation with so- cial commonsense contextualization. arXiv preprint arXiv:2212.10465, 2022 164. Zheng C, Sabour S, Wen J, Huang M. Augesc: Large- scale data augmentation for emotional support con- versation with pre-trained language models. arXiv preprint arXiv:2202.13047, 2022 Mengqi Chen was born in 1997. She received her master’s de- gree in digital textiles from Xi’an Polytechnical University (XPU) in 2022. She is currently working toward a Ph.D. degree at North- western Polytechnical University (NWPU). Her current research in- terests include natural language processing, dialog sys- tems, and large language models. Bin Guo was born in 1980. He is a Ph.D. professor and Ph.D. super- visor at Northwestern Polytechni- cal University (NWPU). He is a senior member of the China Com- puter Federation. His main re- search interests include ubiquitous computing, social and community intelligence, urban big data mining, mobile crowdsensing, and human- computer interaction. Hao Wang was born in 1996. He received his B.E. degree in com- puter science and technology from Northwestern Polytechnical Uni- versity (NWPU) in 2019. He is currently working toward a Ph.D. degree at NWPU. His current re- search interests include natural language processing, di- alog systems, and large language models. Haoyu Li was born in 2002. He received his B.E. degree in com- puter science and technology from Northwestern Polytechnical Uni- versity (NWPU) in 2023. He is currently working toward a mas- ter’s degree at NWPU. His cur- rent research interests include natural language process- ing, large language models, and robot dynamic obstacle avoidance.36 Front. Comput. Sci., 2024, 0(0): 1–36 Qian Zhao was born in 2001. She received her B.E. degree in In- ternet of Things engineering from Tianjin University of Technology (TUT) in 2023. She is currently working toward a master’s de- gree at Northwestern Polytechni- cal University (NWPU). Her cur- rent research interests include multimodal dialogue, large language models, and visual human-computer in- teraction. Jingqi Liu was born in 2002. She entered Northwestern Polytechni- cal University(NWPU) to study for a bachelor’s degree in infor- mation and computing science in 2020. Her current research inter- ests include natural language pro- cessing, dialogue systems, and large language models. Yasan Ding was born in 1995. He received his B.E. degree in com- puter science and technology from Northwestern Polytechnical Uni- versity (NWPU) in 2018. He is currently working toward a Ph.D. degree at NWPU. His current research interests include fake news detection and natural language processing. Yan Pan was born in 1991. He is a lecturer at the Science and Technology on Information Sys- tems Engineering Laboratory. He respectively received the B.S. de- gree in 2013 and the Ph.D. degree in 2020 from Northwestern Poly- technical University (NWPU). His research interests in- clude Big Data, Machine Learning, and Crowd Intelli- gence. Zhiwen Yu was born in 1977. He is a Ph.D. professor and Ph.D. su- pervisor. He is a senior member of the China Computer Federation. His main research interests in- clude mobile internet, ubiquitous computing, social and community intelligence, urban big data min- ing, mobile crowdsensing, and human-computer inter- action.",
      "references": [
        "Conditional text generation for harmonious human- machine interaction.",
        "Challenges in building in- telligent open-domain dialog systems.",
        "The elaboration likelihood model of persuasion.",
        "Persuasive technology: using computers to change what we think and do.",
        "Persuasive technology for human well- being: setting the scene.",
        "Mass interpersonal persuasion: An early view of a new phenomenon.",
        "Winning arguments: Interaction dynamics and persua- sion strategies in good-faith online discussions.",
        "Analyzing the semantic types of claims and premises in an online persuasive forum.",
        "Persuasive system de- sign: state of the art and future directions.",
        "Cognitive theories of persua- sion.",
        "Persuasion for good: Towards a personalized persua- sive dialogue system for social good.",
        "Ef- fects of persuasive dialogues: testing bot identities and inquiry strategies.",
        "Dialograph: Incorporating interpretable strategy-graph networks into negotiation dialogues.",
        "Recent advances in natural language processing via large pre-trained language models: A survey.",
        "A survey of large language models.",
        "Llama 2: Open foundation and fine-tuned chat models.",
        "Constitutional ai: Harmlessness from ai feedback.",
        "A comprehensive survey on pretrained foundation models: A history from bert to chatgpt.",
        "Chatgpt: A comprehensive review on back- ground, applications, key challenges, bias, ethics, lim- itations and future scope.",
        "Generating empathetic responses through emotion tracking and constraint guidance.",
        "Infor- mative and diverse emotional conversation generation with variational recurrent pointer-generator.",
        "Expectation vs. experience: Evaluating the usability of code gen- eration tools powered by large language models.",
        "Lever: Learning to verify language-to-code generation with execution.",
        "Wordcraft: story writing with large language models.",
        "From human writing to artificial intelligence generated text: examining the prospects and potential threats of chat- gpt in academic writing.",
        "Mood and persuasion: A cognitive response analysis.",
        "Emotion and persuasion: Cog- nitive and meta-cognitive processes impact attitudes.",
        "Dynamic knowledge routing network for target-guided open-domain con- versation.",
        "A persuasive chatbot using a crowd-sourced argument graph and concerns.",
        "The psychology of tailoring-ingredients in computer-tailored persuasion.",
        "Intelligent cognitive assistants for attitude and behavior change support in mental health: state-of-the-art technical review.",
        "An autonomous debating system.",
        "To- wards topic-guided conversational recommender sys- tem.",
        "Recommendation as a communica- tion game: Self-supervised bot-play for goal-oriented dialogue.",
        "Seamlessly integrating factual information and social content with persuasive dialogue.",
        "Persuasive natural lan- guage generation–a literature review.",
        "Let’s negotiate! a sur- vey of negotiation dialogue systems.",
        "A survey on proactive dialogue systems: Problems, methods, and prospects.",
        "Pre-suasion: A revolutionary way to influ- ence and persuade.",
        "Ar- gument invention from first principles.",
        "Does the chimpanzee have a theory of mind?",
        "Coke: A cognitive knowledge graph for machine theory of mind.",
        "Neural theory-of- mind? on the limits of social intelligence in large lms.",
        "Rmm: A recursive mental model for dialogue navi- gation.",
        "The philosophy of rhetoric.",
        "Towards emotional support dialog sys- tems.",
        "Decou- pling strategy and generation in negotiation dialogues.",
        "Improving multi-turn emotional support dialogue generation with lookahead strategy planning.",
        "E ffects of message repetition and position on cognitive response, recall, and persua- sion.",
        "Influence: The psychology of persuasion.",
        "Hitkg: Towards goal-oriented conversations via multi- hierarchy learning.",
        "Eagle: Enhance target-oriented dialogs by global planning and topic flow integration.",
        "Communication and persua- sion: Central and peripheral routes to attitude change.",
        "Argument mining: Extracting arguments from online dialogue.",
        "Ampersand: Argument mining for persua- sive online discussions.",
        "From argument search to argumentative dialogue: A topic-independent approach to argument acquisition for dialogue systems.",
        "Arguetutor: An adaptive dialog-based learning system for argumentation skills.",
        "Recent advances in deep learning based dialogue systems: A systematic survey.",
        "Sparks of artificial general intelli- gence: Early experiments with gpt-4.",
        "Long short-term mem- ory.",
        "Attention is all you need.",
        "Pepds: A polite and empathetic persuasive dialogue system for charity donation.",
        "Mortality in mental disorders and global disease burden implica- tions: a systematic review and meta-analysis.",
        "Survey on psychotherapy chatbots.",
        "Identify- ing emotional causes of mental disorders from social media for effective intervention.",
        "Case: Aligning coarse-to-fine cognition and a ffection for empathetic response generation.",
        "Comet: Commonsense transformers for automatic knowledge graph construction.",
        "Conceptnet 5.5: An open multilingual graph of general knowledge.",
        "Fear of the russian bear? negoti- ating finnish national identity online.",
        "Hierarchical argumentation structure for persuasive argumentative dialogue generation.",
        "Increasing the natural-ness of an argumentative dialogue system through ar- gument chains.",
        "Target-guided di- alogue response generation using commonsense and data augmentation.",
        "A unifying perspective on perception and cognition through linguistic representations of emo- tion.",
        "Cognition, evolution, and behavior.",
        "Designing persuasive dialogue systems: Using argumentation with care.",
        "Why are persuasive strategies e ffective? exploring the strengths and weaknesses of socially- oriented persuasive strategies.",
        "Making robots persuasive: the influence of combining persuasive strategies (gazing and ges- tures) by a storytelling robot on its persuasive power.",
        "Em- pathetic persuasion: reinforcing empathy and persua- siveness in dialogue systems.",
        "Bleu: a method for automatic evaluation of machine transla- tion.",
        "Meteor: An automatic metric for mt evaluation with improved correlation with hu- man judgments.",
        "Automatic evaluation of summaries using n-gram co-occurrence statistics.",
        "Prompt-based monte-carlo tree search for goal-oriented dialogue policy planning.",
        "Augmenting non-collaborative dialog systems with explicit seman- tic and strategic dialog history.",
        "Knowledge- enhanced memory model for emotional support con- versation.",
        "Misc: A mixed strategy-aware model integrating comet for emotional support conversation.",
        "Handbook of communi- cation and social interaction skills.",
        "Helping skills: Facilitating, exploration, in- sight, and action.",
        "Semi-supervised classification with graph convolutional networks.",
        "Graph attention networks.",
        "A comprehensive survey on graph neural networks.",
        "Graph neural networks for natu- ral language processing: A survey.",
        "Towards infor- mative and diverse dialogue systems over hierarchi- cal crowd intelligence knowledge graph.",
        "Modeling persuasion factor of user decision for recommendation.",
        "Comae: A multi-factor hierarchical framework for empathetic response generation.",
        "Improving dialog sys- tems for negotiation with personality modeling.",
        "Building emotional support chatbots in the era of llms.",
        "Knowledge graph grounded goal planning for open-domain conversation generation.",
        "Gochat: Goal-oriented chat- bots with hierarchical reinforcement learning.",
        "Interacting with non-cooperative user: A new paradigm for proactive dialogue policy.",
        "Keyword-guided neural conversational model.",
        "Thinking clearly, talk- ing fast: Concept-guided non-autoregressive genera- tion for open-domain dialogue systems.",
        "Dialogue planning via brownian bridge stochastic process for goal-directed proactive dialogue.",
        "Two stage learning for argument pairs extraction.",
        "Topkg: Target-oriented dialog via global planning on knowledge graph.",
        "Target-guided open-domain conversation.",
        "A target-driven planning ap- proach for goal-directed dialog systems.",
        "Towards ar- gument mining for social good: A survey.",
        "Cross-domain mining of argumentative text through distant supervision.",
        "Argument generation with re- trieval, planning, and realization.",
        "Argument min- ing using bert and self-attention based embeddings.",
        "Argument mining with structured svms and rnns.",
        "Exploring the role of argu- ment structure in online debate persuasion.",
        "Multi-task learning in argument mining for persuasive online discussions.",
        "Iam: A comprehensive and large-scale dataset for integrated argument mining tasks.",
        "A hierarchical sequence labeling model for argument pair extraction.",
        "Smote: synthetic minority over-sampling tech- nique.",
        "Augesc: Dialogue augmentation with large language models for emotional support conversation.",
        "Psyqa: A chinese dataset for generating long counseling text for mental health support.",
        "A corpus for research on deliberation and debate.",
        "Debatesum: A large-scale argu- ment mining and summarization dataset.",
        "A dynamic strategy coach for effective negotiation.",
        "Keeping up appearances: Computational modeling of face acts in persuasion oriented discussions.",
        "To- wards conversational recommendation over multi-type dialogs.",
        "In- spired: Toward sociable recommendation dialog sys- tems.",
        "Internet ar- gument corpus 2.0: An sql schema for dialogic social media and the corpora to go with it.",
        "Meteor: An automatic metric for mt evaluation with high levels of correlation with hu- man judgments.",
        "Cider: Consensus-based image description evaluation.",
        "An optimal assessment of natu- ral language student input using word-to-word simi- larity metrics.",
        "Towards universal paraphrastic sentence embeddings.",
        "Bootstrapping dialog systems with word embeddings.",
        "Towards an automatic turing test: Learning to evaluate dialogue responses.",
        "Bert: Pre-training of deep bidirectional transformers for lan- guage understanding.",
        "A multi-appeal model of persuasion for online petition success: A linguistic cue-based approach.",
        "Activating persuasion knowledge in native advertising: the influ- ence of cognitive load and disclosure language.",
        "Strategic argumentation in multi-agent sys- tems.",
        "Progress and challenges in probing the human brain.",
        "In- terest as a proxy of engagement in news reading: Spectral and entropy analyses of eeg activity patterns.",
        "Trans- fertransfo: A transfer learning approach for neural network based conversational agents.",
        "Domain adaptive dialog generation via meta learning.",
        "A deep sequential model for dis- course parsing on multi-party dialogues.",
        "Learning to improve persona consistency in multi-party dia- logue generation via text knowledge enhancement.",
        "Communication-robust multi-agent learning by adaptable auxiliary multi-agent adversary generation.",
        "Predicting persuasive- ness of participants in multiparty conversations.",
        "Speaker-aware bert for multi-turn response selec- tion in retrieval-based chatbots.",
        "Gift: Graph- induced fine-tuning for multi-party conversation un- derstanding.",
        "Interpretability and analysis in neural nlp.",
        "Towards faithfully interpretable nlp systems: How should we define and evaluate faith- fulness?",
        "Semantics of the black- box: Can knowledge graphs help make deep learning systems more interpretable and explainable?",
        "Qa-gnn: Reasoning with language models and knowl- edge graphs for question answering.",
        "Multimodal human discourse: gesture and speech.",
        "Multimodal interaction: A review.",
        "Multimodal human–computer in- teraction: A survey.",
        "Multimodal ma- chine learning: A survey and taxonomy.",
        "Two causal principles for improving visual dialog.",
        "Audio visual scene-aware dialog.",
        "Vi- sual chatgpt: Talking, drawing and editing with visual foundation models.",
        "Vilbert: Pretrain- ing task-agnostic visiolinguistic representations for vision-and-language tasks.",
        "A domain-specific next-generation large language model (llm) or chatgpt is required for biomedical engineer- ing and research.",
        "Code as policies: Language model programs for embodied control.",
        "Empowering llm to use smart- phone for intelligent task automation.",
        "Soda: Million-scale dialogue distillation with so- cial commonsense contextualization.",
        "Augesc: Large- scale data augmentation for emotional support con- versation with pre-trained language models."
      ],
      "meta_data": {
        "arxiv_id": "2402.04631v1",
        "authors": [
          "Mengqi Chen",
          "Bin Guo",
          "Hao Wang",
          "Haoyu Li",
          "Qian Zhao",
          "Jingqi Liu",
          "Yasan Ding",
          "Yan Pan",
          "Zhiwen Yu"
        ],
        "published_date": "2024-02-07T07:28:34Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Surveys and systematizes research on Cognitive Strategy-enhanced Persuasive Dialogue Agents (CogAgent): dialogue systems that achieve persuasive goals by explicitly incorporating cognitive-psychology-inspired strategies. Key contributions are (1) grounding persuasive dialogue in cognitive psychology theories (e.g., pre-suasion, consistency, Theory of Mind, Aristotle’s ethos/pathos/logos); (2) formalizing three core cognitive strategies—persuasion strategy, topic path planning strategy, and argument structure prediction strategy—with examples/taxonomies of persuasion strategies; (3) proposing a formalized concept model and a generic modular system architecture that separates cognitive-strategy mining/prediction from response generation (often via LLMs); (4) organizing representative methods along the three strategy dimensions and summarizing major datasets and evaluation metrics; and (5) articulating open issues and future trends for the field (generalization, multi-party, multimodal, interpretability, better benchmarks, LLM/data co-optimization).",
        "methodology": "As a review, the paper’s “method” is a formalization-and-taxonomy framework plus an architectural abstraction. It defines a CogAgent generation objective by augmenting standard dialogue modeling with a cognitive strategy predictor: given context/history H and current query Q_S, predict strategies (Per, Top, Arg)=StrPre(Q_S,H) and generate response conditioned on them: P(A|Q_S,H,Per,Top,Arg). It then maps prior work into technical buckets: (i) persuasion-strategy integration via context-based strategy classification, long-horizon strategy planning (A*/lookahead, MCTS with LLM prompting), graph-based strategy modeling (GNN/FST), knowledge/memory-enhanced strategy modeling, and RL reward shaping; (ii) topic-path planning via hierarchical RL, KG/GNN-based topic transition modeling, Transformer-based target-driven planning, and stochastic-process planning (Brownian bridge); (iii) argument mining and argument-structure prediction via retrieval+planning+generation pipelines, structured prediction (factor graphs/structured SVM), link prediction/Transformer encoders, and argument graphs used as knowledge bases for rebuttal/selection.",
        "experimental_setup": "No new empirical experiments are conducted; instead the paper aggregates benchmarks and evaluation practice. It catalogs datasets by scenario: psychological counseling (ESConv, AUGESC, PsyQA), debate/argumentation (IAC/IAC2, ChangeMyView/Winning Arguments, DebateSum), price negotiation (CraigslistBargain, Negotiation-Coach), donation/social good (PersuasionForGood, EPP4G/ETP4G, FaceAct), and recommendation (TG-ReDial, DuRecDial, INSPIRED). It also summarizes evaluation methodologies: automatic metrics—overlap-based (BLEU, ROUGE, METEOR, CIDEr), embedding-based (Greedy Matching, embedding average, vector extrema), learning-based (ADEM)—and human evaluation dimensions (fluency, coherence, contextualization, emotional expression, diversity, persuasiveness).",
        "limitations": "Because it is a survey/positioning work, it does not provide a unified implemented CogAgent system nor experimental validation of the proposed architecture or the completeness/utility of the three-strategy formalization. The persuasion-strategy taxonomy is inherently subjective and may be task- and dataset-dependent; coverage of cognitive psychology theories and strategy sets may be non-exhaustive and lacks standardized operationalization. The review highlights that current evaluation metrics do not directly measure persuasion effectiveness, strategy appropriateness, topic-plan rationality, or argument quality, and that existing datasets are often small, scenario-specific, and sparsely annotated—limitations that also constrain the conclusions drawn from prior work.",
        "future_research_directions": "(1) Deeper, more comprehensive cognitive-psychology modeling in dialogue, including cognitive dissonance management and richer Theory-of-Mind/user modeling; exploring neuroscience/physiological signals (EEG/MEG/fMRI proxies) to study receptivity. (2) Improving adaptivity/generality: cross-domain transfer, meta-learning, and strategy generalization across persuasion tasks. (3) Multi-party CogAgent: coordinated multi-agent persuasive roles and “who-says-what-to-whom” modeling. (4) Interpretability: strategy- and knowledge-grounded explanations of why certain persuasive moves are chosen and how they influence outcomes. (5) Multimodal CogAgent: incorporating visual/audio/environmental context for richer user-state inference and persuasion. (6) Data–model co-optimization with LLMs: using LLMs for scalable data generation/augmentation while enforcing model-driven constraints/planning. (7) Building standardized large-scale datasets, richer cognitive-strategy annotations, and persuasion-specific automatic metrics/benchmarks.",
        "experimental_code": "",
        "experimental_info": ""
      }
    }
  ]
}